{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuxinchao/.conda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # super(TCRDataset, self).__init__()\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        # get the CDR3 region\n",
    "        for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "            # df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "            # df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "            df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "            df.drop(columns=[chain], inplace=True)\n",
    "        df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "        df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "        df_ng_em = df.copy()\n",
    "        df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "        df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "        df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "        df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "        df_ng.index = range(len(df_ng))\n",
    "        df = pd.concat([df_ps, df_ng], axis=0)\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR_3\": df[\"AseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR_3\": df[\"BseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode HLA type through one-hot encoding\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            # X_features = df[seq]\n",
    "            # print(df[seq].values.shape)\n",
    "            # convert the df[seq] into torch tensor\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "        \n",
    "        X = torch.cat((torch.from_numpy(X_HLA_encoded), X_features), dim=1)\n",
    "        # encode the class label\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"~/data/project/data/seqData/230221.csv\"\n",
    "TCRData = TCRDataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the autoencoder to encode the TCR sequence\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    TCR_encode_losses = []\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(data.size(0), 1, 41, 5)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        TCR_encode_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    return TCR_encode_losses / len(model.batch_size)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 2\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 5\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots((1, 1), figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size=16, \n",
    "                 batch_size=32, \n",
    "                 num_layers=2, \n",
    "                 device=\"cpu\", \n",
    "                 use_whole_data=False) -> None:\n",
    "        super(pMHC_TCR_model, self).__init__()\n",
    "        if use_whole_data:\n",
    "            self.batch_size = 0\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        # self.label = nn.Linear(hidden_size, 1)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size/2)\n",
    "        self.linear2 = nn.Linear(hidden_size/2, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.batch_size == 0:\n",
    "            self.batch_size = input.shape[0]\n",
    "            x = input.float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "            pred = self.linear1(output[-1])\n",
    "            pred = self.linear2(pred)\n",
    "            # pred = self.label(output[-1])\n",
    "        else:\n",
    "            x = input.view(-1, self.batch_size, self.input_size).float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "            # pred = self.label(output[-1])\n",
    "            pred = self.linear1(output[-1])\n",
    "            pred = self.linear2(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, target.shape)\n",
    "        output = output.to(torch.float32)\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        # print(output.shape, target.shape)\n",
    "        loss = nn.CrossEntropyLoss()(output.view(1,-1), target.view(1,-1))\n",
    "        train_loss += loss.item() / len(train_loader.dataset)  # sum up batch loss\n",
    "        pred = output.sigmoid().round()\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Fold/Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                fold, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(train_loader.dataset)))\n",
    "    # return the average loss\n",
    "    print(f\"The batch size is {model.batch_size}\")\n",
    "    return train_loss, correct / len(train_loader.dataset)\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).to(torch.float32)\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output.reshape(1,-1), target.reshape(1,-1)).item()  # sum up batch loss\n",
    "            # print(test_loss)\n",
    "            pred = output.sigmoid().round()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= model.batch_size\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "seq_length = 6\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = pMHC_TCR_model(input_size=237, hidden_size=16, batch_size=batch_size, num_layers=2, device=device, use_whole_data=False).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "weights = torch.FloatTensor([5,6])\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "    \n",
    "    # print(f\"The length of train_loader is {len(train_loader)}\") # 34\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\") # 8\n",
    "    # print(f\"The length of train_loader is {len(train_loader.dataset)}\")\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder model and dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCRDataset(Dataset):\n",
    "    def __init__(self, file_path, only_TCR_seq=False):\n",
    "        # super(TCRDataset, self).__init__()\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        # get the CDR3 region\n",
    "        for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "            # df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "            # df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "            df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "            df.drop(columns=[chain], inplace=True)\n",
    "        df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "        df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "        df_ng_em = df.copy()\n",
    "        df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "        df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "        df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "        df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "        df_ng.index = range(len(df_ng))\n",
    "        df = pd.concat([df_ps, df_ng], axis=0)\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR_3\": df[\"AseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR_3\": df[\"BseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode HLA type through one-hot encoding\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            # X_features = df[seq]\n",
    "            # print(df[seq].values.shape)\n",
    "            # convert the df[seq] into torch tensor\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "        \n",
    "        X = torch.cat((torch.from_numpy(X_HLA_encoded), X_features), dim=1)\n",
    "        # encode the class label\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        if only_TCR_seq:\n",
    "            self.X = X_features\n",
    "            self.y = torch.from_numpy(y).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "            self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=5, stride=2, padding=1):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        # The input size is 41*5, the hidden size is 10, we use two convolutional layers\n",
    "        # to encode the sequence. The first layer: input_channel=5, output_channel=3,\n",
    "        # kernel size is 5, stride is 2, padding is 1.\n",
    "        # Based on the formula: (W-F+2P)/S + 1 = (41-5+2*1)/2 + 1 = 20\n",
    "        # The second layer: input_channel=3, output_channel=1, kernel size is 5,\n",
    "        # stride is 2, padding is 1.\n",
    "        # Based on the formula: (W-F+2P)/S + 1 = (20-5+2*1)/2 + 1 = 10\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(5, 3, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(3, 1, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # For the decoder, we use the same structure as the encoder. But the ConvTranspose2d is\n",
    "        # the deconvolutional neural network. The input size is 10, the output size is 41*5.\n",
    "        # The first layer: input_channel=1, output_channel=3, kernel size is 5,\n",
    "        # stride is 2, padding is 1.\n",
    "        # Based on the formula: (W-1)*S - 2P + F + O = (10-1)*2 - 2*1 + 5 + 0 = 20\n",
    "        # The second layer: input_channel=3, output_channel=5, kernel size is 5,\n",
    "        # stride is 2, padding is 1.\n",
    "        # Based on the formula: (W-1)*S - 2P + F + O = (20-1)*2 - 2*1 + 5 + 0 = 41\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 3, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(3, 5, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Only Experimental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    def __init__(self, file_path, only_TCR_seq=False, only_experimental=True):\n",
    "        # super(TCRDataset, self).__init__()\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        # get the CDR3 region\n",
    "        for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "            # df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "            # df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "            df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "            df.drop(columns=[chain], inplace=True)\n",
    "\n",
    "        # generate emulated negative samples \n",
    "        if only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR_3\": df[\"AseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR_3\": df[\"BseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode HLA type through one-hot encoding\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            # X_features = df[seq]\n",
    "            # print(df[seq].values.shape)\n",
    "            # convert the df[seq] into torch tensor\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "        \n",
    "        X = torch.cat((torch.from_numpy(X_HLA_encoded), X_features), dim=1)\n",
    "        # encode the class label\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        if only_TCR_seq:\n",
    "            self.X = X_features\n",
    "            self.y = torch.from_numpy(y).float()\n",
    "        else:\n",
    "            self.X = X\n",
    "            self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9a3d897ef0b1e7415fe4468808571913e41281b79a56511723d411ccb064e7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
