{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, WeightedRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(seq):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seq[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seq[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCREncodeData(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        # just use the positive samples\n",
    "        df = df[df[\"Class\"] == \"positive\"]\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 40, :]\n",
    "        df = df.loc[df[\"BseqCDR3\"].str.len() < 40, :]\n",
    "\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "        \n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x))\n",
    "            )\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seq(x))\n",
    "\n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        X = torch.zeros((len(df), 0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X = torch.cat((X, torch.from_numpy(\n",
    "                np.vstack(df[seq].values)\n",
    "            )), dim=1)\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        self.Aseq_len = len_map[\"AseqCDR3\"]\n",
    "        self.Bseq_len = len_map[\"BseqCDR3\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 21}\n",
      "(787, 5)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCREncodeData(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCR autoencoder discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_a_autoencoder(nn.Module):\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_a_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (W + 2P - K)/S + 1\n",
    "            # (batch_size, 5, 25)\n",
    "            nn.Conv1d(5, 7, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 7, 13)\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Conv1d(7, 8, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 8, 7)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv1d(8, 9, kernel_size=5, stride=stride, padding=padding),\n",
    "            # (batch_size, 9, 3)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv1d(9, 10, kernel_size=5, stride=stride, padding=padding),\n",
    "            nn.LeakyReLU(),\n",
    "            # (batch_size, 10, 1)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (w-1)S-2P+F\n",
    "            # (batch_size, 10, 1)\n",
    "            nn.ConvTranspose1d(10, 9, kernel_size=5, stride=2, padding=1),\n",
    "            # (batch_size, 9, 3)\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.ConvTranspose1d(9, 8, kernel_size=5, stride=2, padding=1),\n",
    "            # (batch_size, 8, 7)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(8, 7, kernel_size=3, stride=2, padding=1),\n",
    "            # (batch_size, 7, 13)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(7, 5, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            # (batch_size, 5, 25)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_b_autoencoder(nn.Module):\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_b_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (W + 2P - K)/S + 1\n",
    "            # (batch_size, 5, 21)\n",
    "            nn.Conv1d(5, 7, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 7, 11)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv1d(7, 8, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 8, 6)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv1d(8, 9, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 9, 3)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv1d(9, 10, kernel_size=5, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 1)\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (W-1)S-2P+F\n",
    "            # (batch_size, 10, 1)\n",
    "            nn.ConvTranspose1d(10, 9, kernel_size=5, stride=stride, padding=padding),\n",
    "            # (batch_size, 9, 3)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(9, 8, kernel_size=3, stride=stride, padding=padding),\n",
    "            # (batch_size, 8, 5)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(8, 7, kernel_size=5, stride=stride, padding=padding),\n",
    "            # (batch_size, 7, 11)\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(7, 5, kernel_size=3, stride=stride, padding=padding),\n",
    "            # (batch_size, 5, 21)\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x= input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(model, train_loader, optimizer, criterion, epoch, seq_len):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_len)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "    return batch_loss / len(data)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "seq_len_a = TCRData.Aseq_len\n",
    "seq_len_b = TCRData.Bseq_len\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_a_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "criterion = nn.MSELoss()\n",
    "train_data, test_data = torch.utils.data.random_split(TCRData, lengths=[0.8, 0.2])\n",
    "train_data = Subset(TCRData, train_data.indices)\n",
    "test_data = Subset(TCRData, test_data.indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss \n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "TCR_accuracy = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    TCR_encode_loss = train_autoencoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
