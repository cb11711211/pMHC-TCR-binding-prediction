{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuxinchao/.conda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)\n",
    "af\n",
    "\n",
    "# torch dataset\n",
    "class TCRDataset(Dataset):\n",
    "    '''\n",
    "    Use each two rows of data as a sample (one for alpha chain, one for beta chain), each sample has the common cellname\n",
    "    Different from the chain, the HLA class is the same for each sample\n",
    "    The aaSeqCDR1, aaSeqCDR2, aaSeqCDR3 are the CDR1, CDR2, CDR3 of the alpha chain and beta chain, respectively.\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"Neo\"] = df[\"NeoAA\"].str.slice(0,3) + \"_\" + df[\"NeoAA\"].str.slice(-4,-1)\n",
    "        df.drop(columns=[\"NeoAA\"], inplace=True)\n",
    "        for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "            df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "            df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "            df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "            df.drop(columns=[chain], inplace=True)\n",
    "        seq_list = [\"AseqCDR_1\", \"AseqCDR_2\", \"AseqCDR_3\", \"BseqCDR_1\", \"BseqCDR_2\", \"BseqCDR_3\"]\n",
    "        # Find the longest sequence in each CDR\n",
    "        len_map = df[seq_list].applymap(len).max()\n",
    "        for column in seq_list:\n",
    "            df[column] = df[column].str.ljust(len_map[column], \"*\")\n",
    "        dataset = df[['AseqCDR_1', 'AseqCDR_2', 'AseqCDR_3', 'BseqCDR_1', 'BseqCDR_2', 'BseqCDR_3', 'Neo', 'HLA']]\n",
    "        for seq in seq_list:\n",
    "            encode_seq_result = list()\n",
    "            for i in dataset[seq]:\n",
    "                encode_seq_result.append(encode_seqCDR(i))\n",
    "            col_name = seq + \"_encode\"\n",
    "            dataset[col_name] = encode_seq_result\n",
    "        X_feature = np.zeros((dataset.shape[0], 0))\n",
    "        for seq in seq_list:\n",
    "            col_name = seq + \"_encode\"\n",
    "            col_feature = np.zeros((0, len_map[seq]*5))\n",
    "            for i in range(dataset.shape[0]):\n",
    "                col_feature = np.vstack((col_feature, dataset.loc[i, col_name].reshape(1,-1)))\n",
    "            X_feature = np.hstack((X_feature, col_feature))\n",
    "\n",
    "        # one-hot encode Neo\n",
    "        X_neo = dataset[\"Neo\"].values.reshape(-1,1)\n",
    "        onehotEncoder = OneHotEncoder()\n",
    "        X_neo = onehotEncoder.fit_transform(X_neo).toarray()\n",
    "\n",
    "        # one-hot encode HLA\n",
    "        labelencoder = LabelEncoder()\n",
    "        y = labelencoder.fit_transform(dataset[\"HLA\"].values.reshape(-1,1))\n",
    "\n",
    "        # get the final feature matrix\n",
    "        self.features = np.hstack((X_neo, X_feature))\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuxinchao/.conda/envs/torch/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/wuxinchao/data/project/data/seqData/230215.csv'\n",
    "TCRData = TCRDataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_model(nn.Module):\n",
    "    def __init__(self, input_size, batch_size=32 ,hidden_size=5, num_layers=2, device=\"cpu\", use_whole_data=False):\n",
    "        super(pMHC_TCR_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        \n",
    "        if use_whole_data:\n",
    "            self.batch_size = 0\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        # classification of the output of the last time step\n",
    "        self.label = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input: L, input_size\n",
    "        if self.batch_size==1:\n",
    "            x = input.float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, self.hidden_size).to(self.device))\n",
    "            out, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "            pred = self.label(out[-1, :])\n",
    "        elif self.batch_size==0:\n",
    "            # The whole data is used as a batch, input: data_size, input_size\n",
    "            x = input.float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, self.hidden_size).to(self.device)) # D * num_layers, output_size\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, self.hidden_size).to(self.device)) # D * num_layers, hidden_size\n",
    "            out, (hn, cn) = self.lstm(x, (h_0, c_0)) # out: L, D * output_size\n",
    "            # print(out.shape)\n",
    "            pred = self.label(out) # pred: len_of_dataset * 1\n",
    "            # print(pred.shape)\n",
    "        else:\n",
    "            x = input.view(-1, self.batch_size, self.input_size).float()\n",
    "            # print(x.shape)\n",
    "            # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "            h_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            out, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "            pred = self.label(out[-1, :, :])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device).float()\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.view(-1)\n",
    "        # print(output.shape, target.shape)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        # loss = F.nll_loss(output, target)\n",
    "        # loss = F.cross_entropy(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Fold/Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                fold, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return train_loss / len(data)\n",
    "            \n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            # data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            output = output.view(-1)\n",
    "            # using loss function to calculate the loss\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()  # sum up batch loss\n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            # print(output, target)\n",
    "            # pred = output.argmax(dim=1, keepdim=True)\n",
    "            pred = output.sigmoid().round()  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set for fold {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\\n",
    "    '.format(fold, test_loss, correct, len(test_loader.dataset),  100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([755, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not used\n",
    "epoch_train_loss = []\n",
    "epoch_test_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_test_acc = []\n",
    "\n",
    "x_epoch = []\n",
    "\n",
    "def draw_curve(current_epoch):\n",
    "    x_epoch.append(current_epoch)\n",
    "    ax0.plot(x_epoch, epoch_train_loss, 'bo-', label=\"train\")\n",
    "    ax0.plot(x_epoch, epoch_test_loss, 'ro-', label=\"val\")\n",
    "    ax1.plot(x_epoch, epoch_train_acc, 'bo-', label=\"train\")\n",
    "    ax1.plot(x_epoch, epoch_test_acc, 'ro-', label=\"val\")\n",
    "    if current_epoch == 0:\n",
    "        ax0.legend()\n",
    "        ax1.legend()\n",
    "    fig.savefig(os.path.join(\"./lossGraphs\", \"train.jpg\"))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121, title=\"loss\")\n",
    "ax1 = fig.add_subplot(122, title=\"top1err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training the network\n",
      "-------------------Fold 0-------------------\n",
      "Train Fold/Epoch: 0/1 [0/886 (0%)]\tLoss: 4226.730469\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/2 [0/886 (0%)]\tLoss: 4226.901367\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/3 [0/886 (0%)]\tLoss: 4226.512695\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/4 [0/886 (0%)]\tLoss: 4226.165039\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/5 [0/886 (0%)]\tLoss: 4226.456055\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/6 [0/886 (0%)]\tLoss: 4226.492676\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/7 [0/886 (0%)]\tLoss: 4226.422363\n",
      "Test set for fold 0: Average loss: 0.9474, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/8 [0/886 (0%)]\tLoss: 4226.149414\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/9 [0/886 (0%)]\tLoss: 4226.121094\n",
      "Test set for fold 0: Average loss: 0.9474, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/10 [0/886 (0%)]\tLoss: 4226.193848\n",
      "Test set for fold 0: Average loss: 0.9474, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/11 [0/886 (0%)]\tLoss: 4225.955566\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/12 [0/886 (0%)]\tLoss: 4226.173828\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/13 [0/886 (0%)]\tLoss: 4225.842773\n",
      "Test set for fold 0: Average loss: 0.9474, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/14 [0/886 (0%)]\tLoss: 4225.829102\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/15 [0/886 (0%)]\tLoss: 4226.064453\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/16 [0/886 (0%)]\tLoss: 4225.830566\n",
      "Test set for fold 0: Average loss: 0.9478, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/17 [0/886 (0%)]\tLoss: 4225.966797\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/18 [0/886 (0%)]\tLoss: 4225.777344\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/19 [0/886 (0%)]\tLoss: 4225.532227\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/20 [0/886 (0%)]\tLoss: 4225.303711\n",
      "Test set for fold 0: Average loss: 0.9474, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/21 [0/886 (0%)]\tLoss: 4225.533203\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/22 [0/886 (0%)]\tLoss: 4225.219238\n",
      "Test set for fold 0: Average loss: 0.9474, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/23 [0/886 (0%)]\tLoss: 4224.936523\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/24 [0/886 (0%)]\tLoss: 4225.274414\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/25 [0/886 (0%)]\tLoss: 4225.196777\n",
      "Test set for fold 0: Average loss: 0.9473, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/26 [0/886 (0%)]\tLoss: 4225.284180\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/27 [0/886 (0%)]\tLoss: 4224.915527\n",
      "Test set for fold 0: Average loss: 0.9474, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/28 [0/886 (0%)]\tLoss: 4224.744629\n",
      "Test set for fold 0: Average loss: 0.9475, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/29 [0/886 (0%)]\tLoss: 4224.361328\n",
      "Test set for fold 0: Average loss: 0.9479, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/30 [0/886 (0%)]\tLoss: 4223.938965\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/31 [0/886 (0%)]\tLoss: 4223.889648\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/32 [0/886 (0%)]\tLoss: 4224.225586\n",
      "Test set for fold 0: Average loss: 0.9477, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/33 [0/886 (0%)]\tLoss: 4223.390625\n",
      "Test set for fold 0: Average loss: 0.9476, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/34 [0/886 (0%)]\tLoss: 4223.682129\n",
      "Test set for fold 0: Average loss: 0.9477, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/35 [0/886 (0%)]\tLoss: 4222.989258\n",
      "Test set for fold 0: Average loss: 0.9481, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/36 [0/886 (0%)]\tLoss: 4222.832520\n",
      "Test set for fold 0: Average loss: 0.9480, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/37 [0/886 (0%)]\tLoss: 4222.259766\n",
      "Test set for fold 0: Average loss: 0.9480, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/38 [0/886 (0%)]\tLoss: 4222.140137\n",
      "Test set for fold 0: Average loss: 0.9481, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/39 [0/886 (0%)]\tLoss: 4221.096191\n",
      "Test set for fold 0: Average loss: 0.9477, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/40 [0/886 (0%)]\tLoss: 4221.708984\n",
      "Test set for fold 0: Average loss: 0.9481, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/41 [0/886 (0%)]\tLoss: 4221.120117\n",
      "Test set for fold 0: Average loss: 0.9477, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/42 [0/886 (0%)]\tLoss: 4219.858398\n",
      "Test set for fold 0: Average loss: 0.9481, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/43 [0/886 (0%)]\tLoss: 4219.894531\n",
      "Test set for fold 0: Average loss: 0.9479, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/44 [0/886 (0%)]\tLoss: 4219.058105\n",
      "Test set for fold 0: Average loss: 0.9482, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 0/45 [0/886 (0%)]\tLoss: 4218.706055\n",
      "Test set for fold 0: Average loss: 0.9482, Accuracy: 17/886 (2%)    \n",
      "Train Fold/Epoch: 0/46 [0/886 (0%)]\tLoss: 4217.467285\n",
      "Test set for fold 0: Average loss: 0.9483, Accuracy: 18/886 (2%)    \n",
      "Train Fold/Epoch: 0/47 [0/886 (0%)]\tLoss: 4216.275391\n",
      "Test set for fold 0: Average loss: 0.9483, Accuracy: 17/886 (2%)    \n",
      "Train Fold/Epoch: 0/48 [0/886 (0%)]\tLoss: 4215.987305\n",
      "Test set for fold 0: Average loss: 0.9487, Accuracy: 17/886 (2%)    \n",
      "Train Fold/Epoch: 0/49 [0/886 (0%)]\tLoss: 4215.393555\n",
      "Test set for fold 0: Average loss: 0.9490, Accuracy: 18/886 (2%)    \n",
      "Train Fold/Epoch: 0/50 [0/886 (0%)]\tLoss: 4214.568848\n",
      "Test set for fold 0: Average loss: 0.9488, Accuracy: 17/886 (2%)    \n",
      "Train Fold/Epoch: 0/51 [0/886 (0%)]\tLoss: 4214.182617\n",
      "Test set for fold 0: Average loss: 0.9485, Accuracy: 19/886 (2%)    \n",
      "Train Fold/Epoch: 0/52 [0/886 (0%)]\tLoss: 4211.312500\n",
      "Test set for fold 0: Average loss: 0.9493, Accuracy: 18/886 (2%)    \n",
      "Train Fold/Epoch: 0/53 [0/886 (0%)]\tLoss: 4211.136230\n",
      "Test set for fold 0: Average loss: 0.9483, Accuracy: 25/886 (3%)    \n",
      "Train Fold/Epoch: 0/54 [0/886 (0%)]\tLoss: 4209.362305\n",
      "Test set for fold 0: Average loss: 0.9502, Accuracy: 23/886 (3%)    \n",
      "Train Fold/Epoch: 0/55 [0/886 (0%)]\tLoss: 4210.760742\n",
      "Test set for fold 0: Average loss: 0.9496, Accuracy: 33/886 (4%)    \n",
      "Train Fold/Epoch: 0/56 [0/886 (0%)]\tLoss: 4209.549805\n",
      "Test set for fold 0: Average loss: 0.9504, Accuracy: 30/886 (3%)    \n",
      "Train Fold/Epoch: 0/57 [0/886 (0%)]\tLoss: 4206.367188\n",
      "Test set for fold 0: Average loss: 0.9511, Accuracy: 50/886 (6%)    \n",
      "Train Fold/Epoch: 0/58 [0/886 (0%)]\tLoss: 4206.418945\n",
      "Test set for fold 0: Average loss: 0.9496, Accuracy: 51/886 (6%)    \n",
      "Train Fold/Epoch: 0/59 [0/886 (0%)]\tLoss: 4202.441406\n",
      "Test set for fold 0: Average loss: 0.9495, Accuracy: 80/886 (9%)    \n",
      "Train Fold/Epoch: 0/60 [0/886 (0%)]\tLoss: 4200.648438\n",
      "Test set for fold 0: Average loss: 0.9515, Accuracy: 82/886 (9%)    \n",
      "Train Fold/Epoch: 0/61 [0/886 (0%)]\tLoss: 4201.021484\n",
      "Test set for fold 0: Average loss: 0.9486, Accuracy: 122/886 (14%)    \n",
      "Train Fold/Epoch: 0/62 [0/886 (0%)]\tLoss: 4201.066406\n",
      "Test set for fold 0: Average loss: 0.9530, Accuracy: 85/886 (10%)    \n",
      "Train Fold/Epoch: 0/63 [0/886 (0%)]\tLoss: 4200.909180\n",
      "Test set for fold 0: Average loss: 0.9494, Accuracy: 133/886 (15%)    \n",
      "Train Fold/Epoch: 0/64 [0/886 (0%)]\tLoss: 4197.455078\n",
      "Test set for fold 0: Average loss: 0.9524, Accuracy: 113/886 (13%)    \n",
      "Train Fold/Epoch: 0/65 [0/886 (0%)]\tLoss: 4196.002441\n",
      "Test set for fold 0: Average loss: 0.9509, Accuracy: 134/886 (15%)    \n",
      "Train Fold/Epoch: 0/66 [0/886 (0%)]\tLoss: 4194.425781\n",
      "Test set for fold 0: Average loss: 0.9521, Accuracy: 131/886 (15%)    \n",
      "Train Fold/Epoch: 0/67 [0/886 (0%)]\tLoss: 4191.315430\n",
      "Test set for fold 0: Average loss: 0.9509, Accuracy: 142/886 (16%)    \n",
      "Train Fold/Epoch: 0/68 [0/886 (0%)]\tLoss: 4190.479492\n",
      "Test set for fold 0: Average loss: 0.9551, Accuracy: 133/886 (15%)    \n",
      "Train Fold/Epoch: 0/69 [0/886 (0%)]\tLoss: 4190.134277\n",
      "Test set for fold 0: Average loss: 0.9512, Accuracy: 151/886 (17%)    \n",
      "Train Fold/Epoch: 0/70 [0/886 (0%)]\tLoss: 4191.858398\n",
      "Test set for fold 0: Average loss: 0.9549, Accuracy: 129/886 (15%)    \n",
      "Train Fold/Epoch: 0/71 [0/886 (0%)]\tLoss: 4191.498047\n",
      "Test set for fold 0: Average loss: 0.9518, Accuracy: 149/886 (17%)    \n",
      "Train Fold/Epoch: 0/72 [0/886 (0%)]\tLoss: 4194.036133\n",
      "Test set for fold 0: Average loss: 0.9550, Accuracy: 142/886 (16%)    \n",
      "Train Fold/Epoch: 0/73 [0/886 (0%)]\tLoss: 4186.521484\n",
      "Test set for fold 0: Average loss: 0.9523, Accuracy: 149/886 (17%)    \n",
      "Train Fold/Epoch: 0/74 [0/886 (0%)]\tLoss: 4185.924805\n",
      "Test set for fold 0: Average loss: 0.9563, Accuracy: 148/886 (17%)    \n",
      "Train Fold/Epoch: 0/75 [0/886 (0%)]\tLoss: 4184.776855\n",
      "Test set for fold 0: Average loss: 0.9529, Accuracy: 150/886 (17%)    \n",
      "Train Fold/Epoch: 0/76 [0/886 (0%)]\tLoss: 4181.750977\n",
      "Test set for fold 0: Average loss: 0.9558, Accuracy: 151/886 (17%)    \n",
      "Train Fold/Epoch: 0/77 [0/886 (0%)]\tLoss: 4182.005859\n",
      "Test set for fold 0: Average loss: 0.9555, Accuracy: 152/886 (17%)    \n",
      "Train Fold/Epoch: 0/78 [0/886 (0%)]\tLoss: 4180.634766\n",
      "Test set for fold 0: Average loss: 0.9564, Accuracy: 152/886 (17%)    \n",
      "Train Fold/Epoch: 0/79 [0/886 (0%)]\tLoss: 4179.580078\n",
      "Test set for fold 0: Average loss: 0.9570, Accuracy: 150/886 (17%)    \n",
      "Train Fold/Epoch: 0/80 [0/886 (0%)]\tLoss: 4178.854492\n",
      "Test set for fold 0: Average loss: 0.9591, Accuracy: 151/886 (17%)    \n",
      "Train Fold/Epoch: 0/81 [0/886 (0%)]\tLoss: 4179.284668\n",
      "Test set for fold 0: Average loss: 0.9560, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/82 [0/886 (0%)]\tLoss: 4177.662598\n",
      "Test set for fold 0: Average loss: 0.9593, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/83 [0/886 (0%)]\tLoss: 4177.399902\n",
      "Test set for fold 0: Average loss: 0.9566, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/84 [0/886 (0%)]\tLoss: 4177.506836\n",
      "Test set for fold 0: Average loss: 0.9594, Accuracy: 150/886 (17%)    \n",
      "Train Fold/Epoch: 0/85 [0/886 (0%)]\tLoss: 4176.942383\n",
      "Test set for fold 0: Average loss: 0.9597, Accuracy: 149/886 (17%)    \n",
      "Train Fold/Epoch: 0/86 [0/886 (0%)]\tLoss: 4175.916992\n",
      "Test set for fold 0: Average loss: 0.9594, Accuracy: 154/886 (17%)    \n",
      "Train Fold/Epoch: 0/87 [0/886 (0%)]\tLoss: 4175.998047\n",
      "Test set for fold 0: Average loss: 0.9590, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/88 [0/886 (0%)]\tLoss: 4174.811523\n",
      "Test set for fold 0: Average loss: 0.9587, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/89 [0/886 (0%)]\tLoss: 4174.938965\n",
      "Test set for fold 0: Average loss: 0.9601, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/90 [0/886 (0%)]\tLoss: 4174.068848\n",
      "Test set for fold 0: Average loss: 0.9584, Accuracy: 155/886 (17%)    \n",
      "Train Fold/Epoch: 0/91 [0/886 (0%)]\tLoss: 4173.757812\n",
      "Test set for fold 0: Average loss: 0.9579, Accuracy: 155/886 (17%)    \n",
      "Train Fold/Epoch: 0/92 [0/886 (0%)]\tLoss: 4174.562500\n",
      "Test set for fold 0: Average loss: 0.9626, Accuracy: 154/886 (17%)    \n",
      "Train Fold/Epoch: 0/93 [0/886 (0%)]\tLoss: 4173.853516\n",
      "Test set for fold 0: Average loss: 0.9583, Accuracy: 155/886 (17%)    \n",
      "Train Fold/Epoch: 0/94 [0/886 (0%)]\tLoss: 4174.469727\n",
      "Test set for fold 0: Average loss: 0.9619, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/95 [0/886 (0%)]\tLoss: 4172.858887\n",
      "Test set for fold 0: Average loss: 0.9600, Accuracy: 157/886 (18%)    \n",
      "Train Fold/Epoch: 0/96 [0/886 (0%)]\tLoss: 4172.083984\n",
      "Test set for fold 0: Average loss: 0.9613, Accuracy: 156/886 (18%)    \n",
      "Train Fold/Epoch: 0/97 [0/886 (0%)]\tLoss: 4173.515137\n",
      "Test set for fold 0: Average loss: 0.9616, Accuracy: 156/886 (18%)    \n",
      "Train Fold/Epoch: 0/98 [0/886 (0%)]\tLoss: 4172.339844\n",
      "Test set for fold 0: Average loss: 0.9615, Accuracy: 156/886 (18%)    \n",
      "Train Fold/Epoch: 0/99 [0/886 (0%)]\tLoss: 4171.915039\n",
      "Test set for fold 0: Average loss: 0.9633, Accuracy: 153/886 (17%)    \n",
      "Train Fold/Epoch: 0/100 [0/886 (0%)]\tLoss: 4171.746094\n",
      "Test set for fold 0: Average loss: 0.9609, Accuracy: 155/886 (17%)    \n",
      "-------------------Fold 1-------------------\n",
      "Train Fold/Epoch: 1/1 [0/886 (0%)]\tLoss: 4219.992188\n",
      "Test set for fold 1: Average loss: 0.9341, Accuracy: 168/886 (19%)    \n",
      "Train Fold/Epoch: 1/2 [0/886 (0%)]\tLoss: 4216.287109\n",
      "Test set for fold 1: Average loss: 0.9330, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/3 [0/886 (0%)]\tLoss: 4213.213867\n",
      "Test set for fold 1: Average loss: 0.9322, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/4 [0/886 (0%)]\tLoss: 4210.655273\n",
      "Test set for fold 1: Average loss: 0.9315, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/5 [0/886 (0%)]\tLoss: 4208.479980\n",
      "Test set for fold 1: Average loss: 0.9311, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/6 [0/886 (0%)]\tLoss: 4205.328125\n",
      "Test set for fold 1: Average loss: 0.9302, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/7 [0/886 (0%)]\tLoss: 4204.197754\n",
      "Test set for fold 1: Average loss: 0.9300, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/8 [0/886 (0%)]\tLoss: 4202.811523\n",
      "Test set for fold 1: Average loss: 0.9294, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/9 [0/886 (0%)]\tLoss: 4200.759766\n",
      "Test set for fold 1: Average loss: 0.9290, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/10 [0/886 (0%)]\tLoss: 4199.720703\n",
      "Test set for fold 1: Average loss: 0.9290, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/11 [0/886 (0%)]\tLoss: 4198.719727\n",
      "Test set for fold 1: Average loss: 0.9283, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/12 [0/886 (0%)]\tLoss: 4197.863281\n",
      "Test set for fold 1: Average loss: 0.9288, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/13 [0/886 (0%)]\tLoss: 4196.855957\n",
      "Test set for fold 1: Average loss: 0.9279, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/14 [0/886 (0%)]\tLoss: 4195.906250\n",
      "Test set for fold 1: Average loss: 0.9278, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/15 [0/886 (0%)]\tLoss: 4195.682617\n",
      "Test set for fold 1: Average loss: 0.9279, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/16 [0/886 (0%)]\tLoss: 4194.029785\n",
      "Test set for fold 1: Average loss: 0.9275, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/17 [0/886 (0%)]\tLoss: 4193.717773\n",
      "Test set for fold 1: Average loss: 0.9277, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/18 [0/886 (0%)]\tLoss: 4193.332520\n",
      "Test set for fold 1: Average loss: 0.9276, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/19 [0/886 (0%)]\tLoss: 4192.286133\n",
      "Test set for fold 1: Average loss: 0.9276, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/20 [0/886 (0%)]\tLoss: 4192.499512\n",
      "Test set for fold 1: Average loss: 0.9278, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/21 [0/886 (0%)]\tLoss: 4191.546875\n",
      "Test set for fold 1: Average loss: 0.9279, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/22 [0/886 (0%)]\tLoss: 4192.008789\n",
      "Test set for fold 1: Average loss: 0.9271, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/23 [0/886 (0%)]\tLoss: 4192.524414\n",
      "Test set for fold 1: Average loss: 0.9275, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/24 [0/886 (0%)]\tLoss: 4190.282227\n",
      "Test set for fold 1: Average loss: 0.9271, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/25 [0/886 (0%)]\tLoss: 4190.653320\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/26 [0/886 (0%)]\tLoss: 4190.252930\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/27 [0/886 (0%)]\tLoss: 4191.296875\n",
      "Test set for fold 1: Average loss: 0.9275, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/28 [0/886 (0%)]\tLoss: 4191.559570\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/29 [0/886 (0%)]\tLoss: 4190.061523\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/30 [0/886 (0%)]\tLoss: 4189.439941\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/31 [0/886 (0%)]\tLoss: 4189.432129\n",
      "Test set for fold 1: Average loss: 0.9263, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/32 [0/886 (0%)]\tLoss: 4190.873047\n",
      "Test set for fold 1: Average loss: 0.9272, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/33 [0/886 (0%)]\tLoss: 4189.028320\n",
      "Test set for fold 1: Average loss: 0.9269, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/34 [0/886 (0%)]\tLoss: 4187.924805\n",
      "Test set for fold 1: Average loss: 0.9274, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/35 [0/886 (0%)]\tLoss: 4188.740723\n",
      "Test set for fold 1: Average loss: 0.9285, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/36 [0/886 (0%)]\tLoss: 4188.618164\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/37 [0/886 (0%)]\tLoss: 4189.542969\n",
      "Test set for fold 1: Average loss: 0.9265, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/38 [0/886 (0%)]\tLoss: 4187.426758\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/39 [0/886 (0%)]\tLoss: 4187.329590\n",
      "Test set for fold 1: Average loss: 0.9263, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/40 [0/886 (0%)]\tLoss: 4187.093262\n",
      "Test set for fold 1: Average loss: 0.9267, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/41 [0/886 (0%)]\tLoss: 4186.360352\n",
      "Test set for fold 1: Average loss: 0.9271, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/42 [0/886 (0%)]\tLoss: 4185.768555\n",
      "Test set for fold 1: Average loss: 0.9261, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 1/43 [0/886 (0%)]\tLoss: 4186.709961\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/44 [0/886 (0%)]\tLoss: 4186.148926\n",
      "Test set for fold 1: Average loss: 0.9264, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/45 [0/886 (0%)]\tLoss: 4186.372559\n",
      "Test set for fold 1: Average loss: 0.9262, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/46 [0/886 (0%)]\tLoss: 4185.980469\n",
      "Test set for fold 1: Average loss: 0.9267, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/47 [0/886 (0%)]\tLoss: 4186.097656\n",
      "Test set for fold 1: Average loss: 0.9261, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/48 [0/886 (0%)]\tLoss: 4186.715332\n",
      "Test set for fold 1: Average loss: 0.9265, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/49 [0/886 (0%)]\tLoss: 4185.827637\n",
      "Test set for fold 1: Average loss: 0.9262, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/50 [0/886 (0%)]\tLoss: 4185.371582\n",
      "Test set for fold 1: Average loss: 0.9264, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/51 [0/886 (0%)]\tLoss: 4186.020508\n",
      "Test set for fold 1: Average loss: 0.9261, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/52 [0/886 (0%)]\tLoss: 4185.432617\n",
      "Test set for fold 1: Average loss: 0.9264, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/53 [0/886 (0%)]\tLoss: 4185.751953\n",
      "Test set for fold 1: Average loss: 0.9264, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/54 [0/886 (0%)]\tLoss: 4185.696777\n",
      "Test set for fold 1: Average loss: 0.9263, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/55 [0/886 (0%)]\tLoss: 4185.347168\n",
      "Test set for fold 1: Average loss: 0.9265, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/56 [0/886 (0%)]\tLoss: 4184.899414\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 1/57 [0/886 (0%)]\tLoss: 4185.643555\n",
      "Test set for fold 1: Average loss: 0.9272, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/58 [0/886 (0%)]\tLoss: 4184.874512\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/59 [0/886 (0%)]\tLoss: 4184.050781\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/60 [0/886 (0%)]\tLoss: 4184.903320\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/61 [0/886 (0%)]\tLoss: 4185.585938\n",
      "Test set for fold 1: Average loss: 0.9265, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/62 [0/886 (0%)]\tLoss: 4183.917969\n",
      "Test set for fold 1: Average loss: 0.9264, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/63 [0/886 (0%)]\tLoss: 4183.568359\n",
      "Test set for fold 1: Average loss: 0.9261, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/64 [0/886 (0%)]\tLoss: 4184.140625\n",
      "Test set for fold 1: Average loss: 0.9263, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/65 [0/886 (0%)]\tLoss: 4183.549805\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/66 [0/886 (0%)]\tLoss: 4183.666016\n",
      "Test set for fold 1: Average loss: 0.9259, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/67 [0/886 (0%)]\tLoss: 4183.938477\n",
      "Test set for fold 1: Average loss: 0.9265, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/68 [0/886 (0%)]\tLoss: 4183.170410\n",
      "Test set for fold 1: Average loss: 0.9264, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/69 [0/886 (0%)]\tLoss: 4183.106445\n",
      "Test set for fold 1: Average loss: 0.9267, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/70 [0/886 (0%)]\tLoss: 4183.142578\n",
      "Test set for fold 1: Average loss: 0.9265, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/71 [0/886 (0%)]\tLoss: 4183.936523\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/72 [0/886 (0%)]\tLoss: 4183.081055\n",
      "Test set for fold 1: Average loss: 0.9269, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/73 [0/886 (0%)]\tLoss: 4182.900879\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/74 [0/886 (0%)]\tLoss: 4183.049805\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/75 [0/886 (0%)]\tLoss: 4183.184570\n",
      "Test set for fold 1: Average loss: 0.9267, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/76 [0/886 (0%)]\tLoss: 4182.395508\n",
      "Test set for fold 1: Average loss: 0.9267, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/77 [0/886 (0%)]\tLoss: 4182.708984\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/78 [0/886 (0%)]\tLoss: 4181.922852\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/79 [0/886 (0%)]\tLoss: 4182.266602\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/80 [0/886 (0%)]\tLoss: 4181.958984\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/81 [0/886 (0%)]\tLoss: 4181.972656\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/82 [0/886 (0%)]\tLoss: 4182.365723\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/83 [0/886 (0%)]\tLoss: 4182.009277\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/84 [0/886 (0%)]\tLoss: 4181.633301\n",
      "Test set for fold 1: Average loss: 0.9269, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/85 [0/886 (0%)]\tLoss: 4181.479980\n",
      "Test set for fold 1: Average loss: 0.9269, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/86 [0/886 (0%)]\tLoss: 4181.630371\n",
      "Test set for fold 1: Average loss: 0.9272, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/87 [0/886 (0%)]\tLoss: 4182.023438\n",
      "Test set for fold 1: Average loss: 0.9266, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/88 [0/886 (0%)]\tLoss: 4181.980469\n",
      "Test set for fold 1: Average loss: 0.9278, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/89 [0/886 (0%)]\tLoss: 4181.801758\n",
      "Test set for fold 1: Average loss: 0.9269, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/90 [0/886 (0%)]\tLoss: 4181.708984\n",
      "Test set for fold 1: Average loss: 0.9270, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/91 [0/886 (0%)]\tLoss: 4181.139160\n",
      "Test set for fold 1: Average loss: 0.9272, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 1/92 [0/886 (0%)]\tLoss: 4181.749512\n",
      "Test set for fold 1: Average loss: 0.9269, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/93 [0/886 (0%)]\tLoss: 4181.173828\n",
      "Test set for fold 1: Average loss: 0.9267, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/94 [0/886 (0%)]\tLoss: 4181.385254\n",
      "Test set for fold 1: Average loss: 0.9271, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/95 [0/886 (0%)]\tLoss: 4181.538086\n",
      "Test set for fold 1: Average loss: 0.9269, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/96 [0/886 (0%)]\tLoss: 4181.414062\n",
      "Test set for fold 1: Average loss: 0.9267, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/97 [0/886 (0%)]\tLoss: 4183.241211\n",
      "Test set for fold 1: Average loss: 0.9268, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 1/98 [0/886 (0%)]\tLoss: 4182.315430\n",
      "Test set for fold 1: Average loss: 0.9265, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 1/99 [0/886 (0%)]\tLoss: 4183.075195\n",
      "Test set for fold 1: Average loss: 0.9277, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 1/100 [0/886 (0%)]\tLoss: 4181.192383\n",
      "Test set for fold 1: Average loss: 0.9274, Accuracy: 174/886 (20%)    \n",
      "-------------------Fold 2-------------------\n",
      "Train Fold/Epoch: 2/1 [0/886 (0%)]\tLoss: 4226.699219\n",
      "Test set for fold 2: Average loss: 0.9375, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/2 [0/886 (0%)]\tLoss: 4224.790039\n",
      "Test set for fold 2: Average loss: 0.9371, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/3 [0/886 (0%)]\tLoss: 4222.788086\n",
      "Test set for fold 2: Average loss: 0.9361, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/4 [0/886 (0%)]\tLoss: 4220.364258\n",
      "Test set for fold 2: Average loss: 0.9361, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/5 [0/886 (0%)]\tLoss: 4218.203125\n",
      "Test set for fold 2: Average loss: 0.9349, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/6 [0/886 (0%)]\tLoss: 4216.406250\n",
      "Test set for fold 2: Average loss: 0.9344, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/7 [0/886 (0%)]\tLoss: 4214.554688\n",
      "Test set for fold 2: Average loss: 0.9337, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/8 [0/886 (0%)]\tLoss: 4213.315430\n",
      "Test set for fold 2: Average loss: 0.9334, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/9 [0/886 (0%)]\tLoss: 4211.456055\n",
      "Test set for fold 2: Average loss: 0.9334, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/10 [0/886 (0%)]\tLoss: 4210.376465\n",
      "Test set for fold 2: Average loss: 0.9329, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/11 [0/886 (0%)]\tLoss: 4208.482422\n",
      "Test set for fold 2: Average loss: 0.9325, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/12 [0/886 (0%)]\tLoss: 4208.017578\n",
      "Test set for fold 2: Average loss: 0.9321, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/13 [0/886 (0%)]\tLoss: 4206.908203\n",
      "Test set for fold 2: Average loss: 0.9317, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/14 [0/886 (0%)]\tLoss: 4205.269043\n",
      "Test set for fold 2: Average loss: 0.9315, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/15 [0/886 (0%)]\tLoss: 4204.950195\n",
      "Test set for fold 2: Average loss: 0.9312, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/16 [0/886 (0%)]\tLoss: 4203.770508\n",
      "Test set for fold 2: Average loss: 0.9311, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/17 [0/886 (0%)]\tLoss: 4203.087402\n",
      "Test set for fold 2: Average loss: 0.9306, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/18 [0/886 (0%)]\tLoss: 4202.071289\n",
      "Test set for fold 2: Average loss: 0.9307, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/19 [0/886 (0%)]\tLoss: 4201.316895\n",
      "Test set for fold 2: Average loss: 0.9305, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/20 [0/886 (0%)]\tLoss: 4200.978027\n",
      "Test set for fold 2: Average loss: 0.9303, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/21 [0/886 (0%)]\tLoss: 4200.253906\n",
      "Test set for fold 2: Average loss: 0.9301, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/22 [0/886 (0%)]\tLoss: 4199.661133\n",
      "Test set for fold 2: Average loss: 0.9300, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/23 [0/886 (0%)]\tLoss: 4199.300293\n",
      "Test set for fold 2: Average loss: 0.9298, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/24 [0/886 (0%)]\tLoss: 4198.529297\n",
      "Test set for fold 2: Average loss: 0.9298, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/25 [0/886 (0%)]\tLoss: 4198.069336\n",
      "Test set for fold 2: Average loss: 0.9293, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/26 [0/886 (0%)]\tLoss: 4197.095703\n",
      "Test set for fold 2: Average loss: 0.9295, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/27 [0/886 (0%)]\tLoss: 4196.567383\n",
      "Test set for fold 2: Average loss: 0.9294, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/28 [0/886 (0%)]\tLoss: 4196.409180\n",
      "Test set for fold 2: Average loss: 0.9294, Accuracy: 19/886 (2%)    \n",
      "Train Fold/Epoch: 2/29 [0/886 (0%)]\tLoss: 4195.822754\n",
      "Test set for fold 2: Average loss: 0.9291, Accuracy: 16/886 (2%)    \n",
      "Train Fold/Epoch: 2/30 [0/886 (0%)]\tLoss: 4195.257812\n",
      "Test set for fold 2: Average loss: 0.9290, Accuracy: 21/886 (2%)    \n",
      "Train Fold/Epoch: 2/31 [0/886 (0%)]\tLoss: 4194.802734\n",
      "Test set for fold 2: Average loss: 0.9289, Accuracy: 21/886 (2%)    \n",
      "Train Fold/Epoch: 2/32 [0/886 (0%)]\tLoss: 4194.612793\n",
      "Test set for fold 2: Average loss: 0.9287, Accuracy: 25/886 (3%)    \n",
      "Train Fold/Epoch: 2/33 [0/886 (0%)]\tLoss: 4194.413086\n",
      "Test set for fold 2: Average loss: 0.9288, Accuracy: 28/886 (3%)    \n",
      "Train Fold/Epoch: 2/34 [0/886 (0%)]\tLoss: 4193.914551\n",
      "Test set for fold 2: Average loss: 0.9289, Accuracy: 34/886 (4%)    \n",
      "Train Fold/Epoch: 2/35 [0/886 (0%)]\tLoss: 4193.549805\n",
      "Test set for fold 2: Average loss: 0.9286, Accuracy: 37/886 (4%)    \n",
      "Train Fold/Epoch: 2/36 [0/886 (0%)]\tLoss: 4193.066406\n",
      "Test set for fold 2: Average loss: 0.9284, Accuracy: 43/886 (5%)    \n",
      "Train Fold/Epoch: 2/37 [0/886 (0%)]\tLoss: 4193.350586\n",
      "Test set for fold 2: Average loss: 0.9289, Accuracy: 51/886 (6%)    \n",
      "Train Fold/Epoch: 2/38 [0/886 (0%)]\tLoss: 4192.307617\n",
      "Test set for fold 2: Average loss: 0.9284, Accuracy: 62/886 (7%)    \n",
      "Train Fold/Epoch: 2/39 [0/886 (0%)]\tLoss: 4192.289062\n",
      "Test set for fold 2: Average loss: 0.9282, Accuracy: 58/886 (7%)    \n",
      "Train Fold/Epoch: 2/40 [0/886 (0%)]\tLoss: 4191.579102\n",
      "Test set for fold 2: Average loss: 0.9279, Accuracy: 62/886 (7%)    \n",
      "Train Fold/Epoch: 2/41 [0/886 (0%)]\tLoss: 4192.022461\n",
      "Test set for fold 2: Average loss: 0.9280, Accuracy: 75/886 (8%)    \n",
      "Train Fold/Epoch: 2/42 [0/886 (0%)]\tLoss: 4191.351074\n",
      "Test set for fold 2: Average loss: 0.9278, Accuracy: 78/886 (9%)    \n",
      "Train Fold/Epoch: 2/43 [0/886 (0%)]\tLoss: 4190.868164\n",
      "Test set for fold 2: Average loss: 0.9279, Accuracy: 105/886 (12%)    \n",
      "Train Fold/Epoch: 2/44 [0/886 (0%)]\tLoss: 4190.373047\n",
      "Test set for fold 2: Average loss: 0.9280, Accuracy: 91/886 (10%)    \n",
      "Train Fold/Epoch: 2/45 [0/886 (0%)]\tLoss: 4190.255859\n",
      "Test set for fold 2: Average loss: 0.9278, Accuracy: 89/886 (10%)    \n",
      "Train Fold/Epoch: 2/46 [0/886 (0%)]\tLoss: 4189.969727\n",
      "Test set for fold 2: Average loss: 0.9278, Accuracy: 94/886 (11%)    \n",
      "Train Fold/Epoch: 2/47 [0/886 (0%)]\tLoss: 4190.024414\n",
      "Test set for fold 2: Average loss: 0.9277, Accuracy: 128/886 (14%)    \n",
      "Train Fold/Epoch: 2/48 [0/886 (0%)]\tLoss: 4189.161621\n",
      "Test set for fold 2: Average loss: 0.9276, Accuracy: 109/886 (12%)    \n",
      "Train Fold/Epoch: 2/49 [0/886 (0%)]\tLoss: 4189.291992\n",
      "Test set for fold 2: Average loss: 0.9274, Accuracy: 134/886 (15%)    \n",
      "Train Fold/Epoch: 2/50 [0/886 (0%)]\tLoss: 4188.775391\n",
      "Test set for fold 2: Average loss: 0.9275, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 2/51 [0/886 (0%)]\tLoss: 4188.740234\n",
      "Test set for fold 2: Average loss: 0.9274, Accuracy: 143/886 (16%)    \n",
      "Train Fold/Epoch: 2/52 [0/886 (0%)]\tLoss: 4188.587891\n",
      "Test set for fold 2: Average loss: 0.9274, Accuracy: 139/886 (16%)    \n",
      "Train Fold/Epoch: 2/53 [0/886 (0%)]\tLoss: 4187.825195\n",
      "Test set for fold 2: Average loss: 0.9273, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 2/54 [0/886 (0%)]\tLoss: 4188.047852\n",
      "Test set for fold 2: Average loss: 0.9275, Accuracy: 156/886 (18%)    \n",
      "Train Fold/Epoch: 2/55 [0/886 (0%)]\tLoss: 4188.499023\n",
      "Test set for fold 2: Average loss: 0.9279, Accuracy: 157/886 (18%)    \n",
      "Train Fold/Epoch: 2/56 [0/886 (0%)]\tLoss: 4187.806641\n",
      "Test set for fold 2: Average loss: 0.9275, Accuracy: 162/886 (18%)    \n",
      "Train Fold/Epoch: 2/57 [0/886 (0%)]\tLoss: 4187.345703\n",
      "Test set for fold 2: Average loss: 0.9273, Accuracy: 162/886 (18%)    \n",
      "Train Fold/Epoch: 2/58 [0/886 (0%)]\tLoss: 4187.328125\n",
      "Test set for fold 2: Average loss: 0.9273, Accuracy: 167/886 (19%)    \n",
      "Train Fold/Epoch: 2/59 [0/886 (0%)]\tLoss: 4187.053711\n",
      "Test set for fold 2: Average loss: 0.9271, Accuracy: 163/886 (18%)    \n",
      "Train Fold/Epoch: 2/60 [0/886 (0%)]\tLoss: 4187.080078\n",
      "Test set for fold 2: Average loss: 0.9273, Accuracy: 163/886 (18%)    \n",
      "Train Fold/Epoch: 2/61 [0/886 (0%)]\tLoss: 4186.528320\n",
      "Test set for fold 2: Average loss: 0.9274, Accuracy: 165/886 (19%)    \n",
      "Train Fold/Epoch: 2/62 [0/886 (0%)]\tLoss: 4186.400879\n",
      "Test set for fold 2: Average loss: 0.9272, Accuracy: 164/886 (19%)    \n",
      "Train Fold/Epoch: 2/63 [0/886 (0%)]\tLoss: 4186.302734\n",
      "Test set for fold 2: Average loss: 0.9270, Accuracy: 169/886 (19%)    \n",
      "Train Fold/Epoch: 2/64 [0/886 (0%)]\tLoss: 4186.327148\n",
      "Test set for fold 2: Average loss: 0.9267, Accuracy: 172/886 (19%)    \n",
      "Train Fold/Epoch: 2/65 [0/886 (0%)]\tLoss: 4185.574219\n",
      "Test set for fold 2: Average loss: 0.9270, Accuracy: 165/886 (19%)    \n",
      "Train Fold/Epoch: 2/66 [0/886 (0%)]\tLoss: 4185.657227\n",
      "Test set for fold 2: Average loss: 0.9269, Accuracy: 170/886 (19%)    \n",
      "Train Fold/Epoch: 2/67 [0/886 (0%)]\tLoss: 4185.493164\n",
      "Test set for fold 2: Average loss: 0.9273, Accuracy: 169/886 (19%)    \n",
      "Train Fold/Epoch: 2/68 [0/886 (0%)]\tLoss: 4185.127441\n",
      "Test set for fold 2: Average loss: 0.9268, Accuracy: 170/886 (19%)    \n",
      "Train Fold/Epoch: 2/69 [0/886 (0%)]\tLoss: 4185.251953\n",
      "Test set for fold 2: Average loss: 0.9268, Accuracy: 171/886 (19%)    \n",
      "Train Fold/Epoch: 2/70 [0/886 (0%)]\tLoss: 4184.810547\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 172/886 (19%)    \n",
      "Train Fold/Epoch: 2/71 [0/886 (0%)]\tLoss: 4184.737305\n",
      "Test set for fold 2: Average loss: 0.9267, Accuracy: 170/886 (19%)    \n",
      "Train Fold/Epoch: 2/72 [0/886 (0%)]\tLoss: 4184.917969\n",
      "Test set for fold 2: Average loss: 0.9267, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/73 [0/886 (0%)]\tLoss: 4184.732422\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/74 [0/886 (0%)]\tLoss: 4184.111328\n",
      "Test set for fold 2: Average loss: 0.9266, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/75 [0/886 (0%)]\tLoss: 4184.359375\n",
      "Test set for fold 2: Average loss: 0.9267, Accuracy: 172/886 (19%)    \n",
      "Train Fold/Epoch: 2/76 [0/886 (0%)]\tLoss: 4184.033691\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/77 [0/886 (0%)]\tLoss: 4183.885742\n",
      "Test set for fold 2: Average loss: 0.9263, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 2/78 [0/886 (0%)]\tLoss: 4183.927734\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 2/79 [0/886 (0%)]\tLoss: 4183.848633\n",
      "Test set for fold 2: Average loss: 0.9263, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 2/80 [0/886 (0%)]\tLoss: 4184.117188\n",
      "Test set for fold 2: Average loss: 0.9264, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 2/81 [0/886 (0%)]\tLoss: 4183.514648\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 2/82 [0/886 (0%)]\tLoss: 4183.215332\n",
      "Test set for fold 2: Average loss: 0.9268, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/83 [0/886 (0%)]\tLoss: 4183.281738\n",
      "Test set for fold 2: Average loss: 0.9267, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 2/84 [0/886 (0%)]\tLoss: 4182.881348\n",
      "Test set for fold 2: Average loss: 0.9264, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 2/85 [0/886 (0%)]\tLoss: 4182.708984\n",
      "Test set for fold 2: Average loss: 0.9268, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/86 [0/886 (0%)]\tLoss: 4182.989258\n",
      "Test set for fold 2: Average loss: 0.9262, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 2/87 [0/886 (0%)]\tLoss: 4182.451172\n",
      "Test set for fold 2: Average loss: 0.9262, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 2/88 [0/886 (0%)]\tLoss: 4182.298828\n",
      "Test set for fold 2: Average loss: 0.9269, Accuracy: 172/886 (19%)    \n",
      "Train Fold/Epoch: 2/89 [0/886 (0%)]\tLoss: 4182.552734\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/90 [0/886 (0%)]\tLoss: 4182.254883\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 2/91 [0/886 (0%)]\tLoss: 4181.904297\n",
      "Test set for fold 2: Average loss: 0.9260, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 2/92 [0/886 (0%)]\tLoss: 4181.944336\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 2/93 [0/886 (0%)]\tLoss: 4182.089844\n",
      "Test set for fold 2: Average loss: 0.9265, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 2/94 [0/886 (0%)]\tLoss: 4181.751465\n",
      "Test set for fold 2: Average loss: 0.9266, Accuracy: 172/886 (19%)    \n",
      "Train Fold/Epoch: 2/95 [0/886 (0%)]\tLoss: 4181.687500\n",
      "Test set for fold 2: Average loss: 0.9264, Accuracy: 174/886 (20%)    \n",
      "Train Fold/Epoch: 2/96 [0/886 (0%)]\tLoss: 4181.914062\n",
      "Test set for fold 2: Average loss: 0.9263, Accuracy: 172/886 (19%)    \n",
      "Train Fold/Epoch: 2/97 [0/886 (0%)]\tLoss: 4181.538574\n",
      "Test set for fold 2: Average loss: 0.9263, Accuracy: 171/886 (19%)    \n",
      "Train Fold/Epoch: 2/98 [0/886 (0%)]\tLoss: 4181.116699\n",
      "Test set for fold 2: Average loss: 0.9264, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/99 [0/886 (0%)]\tLoss: 4181.101562\n",
      "Test set for fold 2: Average loss: 0.9263, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 2/100 [0/886 (0%)]\tLoss: 4181.273926\n",
      "Test set for fold 2: Average loss: 0.9261, Accuracy: 175/886 (20%)    \n",
      "-------------------Fold 3-------------------\n",
      "Train Fold/Epoch: 3/1 [0/886 (0%)]\tLoss: 4243.554199\n",
      "Test set for fold 3: Average loss: 0.9427, Accuracy: 161/886 (18%)    \n",
      "Train Fold/Epoch: 3/2 [0/886 (0%)]\tLoss: 4239.854004\n",
      "Test set for fold 3: Average loss: 0.9417, Accuracy: 161/886 (18%)    \n",
      "Train Fold/Epoch: 3/3 [0/886 (0%)]\tLoss: 4236.241211\n",
      "Test set for fold 3: Average loss: 0.9407, Accuracy: 161/886 (18%)    \n",
      "Train Fold/Epoch: 3/4 [0/886 (0%)]\tLoss: 4232.728516\n",
      "Test set for fold 3: Average loss: 0.9399, Accuracy: 161/886 (18%)    \n",
      "Train Fold/Epoch: 3/5 [0/886 (0%)]\tLoss: 4229.447266\n",
      "Test set for fold 3: Average loss: 0.9391, Accuracy: 161/886 (18%)    \n",
      "Train Fold/Epoch: 3/6 [0/886 (0%)]\tLoss: 4226.472168\n",
      "Test set for fold 3: Average loss: 0.9382, Accuracy: 163/886 (18%)    \n",
      "Train Fold/Epoch: 3/7 [0/886 (0%)]\tLoss: 4223.821289\n",
      "Test set for fold 3: Average loss: 0.9374, Accuracy: 171/886 (19%)    \n",
      "Train Fold/Epoch: 3/8 [0/886 (0%)]\tLoss: 4221.391602\n",
      "Test set for fold 3: Average loss: 0.9367, Accuracy: 173/886 (20%)    \n",
      "Train Fold/Epoch: 3/9 [0/886 (0%)]\tLoss: 4218.988770\n",
      "Test set for fold 3: Average loss: 0.9360, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/10 [0/886 (0%)]\tLoss: 4216.821289\n",
      "Test set for fold 3: Average loss: 0.9355, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/11 [0/886 (0%)]\tLoss: 4214.710938\n",
      "Test set for fold 3: Average loss: 0.9350, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/12 [0/886 (0%)]\tLoss: 4212.951172\n",
      "Test set for fold 3: Average loss: 0.9348, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/13 [0/886 (0%)]\tLoss: 4211.472656\n",
      "Test set for fold 3: Average loss: 0.9341, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/14 [0/886 (0%)]\tLoss: 4210.184570\n",
      "Test set for fold 3: Average loss: 0.9341, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/15 [0/886 (0%)]\tLoss: 4208.630371\n",
      "Test set for fold 3: Average loss: 0.9333, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/16 [0/886 (0%)]\tLoss: 4207.665039\n",
      "Test set for fold 3: Average loss: 0.9333, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/17 [0/886 (0%)]\tLoss: 4206.486328\n",
      "Test set for fold 3: Average loss: 0.9329, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/18 [0/886 (0%)]\tLoss: 4205.576172\n",
      "Test set for fold 3: Average loss: 0.9326, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/19 [0/886 (0%)]\tLoss: 4204.606445\n",
      "Test set for fold 3: Average loss: 0.9324, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/20 [0/886 (0%)]\tLoss: 4203.928711\n",
      "Test set for fold 3: Average loss: 0.9321, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/21 [0/886 (0%)]\tLoss: 4203.014160\n",
      "Test set for fold 3: Average loss: 0.9318, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/22 [0/886 (0%)]\tLoss: 4202.480957\n",
      "Test set for fold 3: Average loss: 0.9316, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/23 [0/886 (0%)]\tLoss: 4201.591797\n",
      "Test set for fold 3: Average loss: 0.9312, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/24 [0/886 (0%)]\tLoss: 4200.974121\n",
      "Test set for fold 3: Average loss: 0.9312, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/25 [0/886 (0%)]\tLoss: 4200.317871\n",
      "Test set for fold 3: Average loss: 0.9310, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/26 [0/886 (0%)]\tLoss: 4199.475586\n",
      "Test set for fold 3: Average loss: 0.9309, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/27 [0/886 (0%)]\tLoss: 4198.958984\n",
      "Test set for fold 3: Average loss: 0.9306, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/28 [0/886 (0%)]\tLoss: 4198.254395\n",
      "Test set for fold 3: Average loss: 0.9304, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/29 [0/886 (0%)]\tLoss: 4198.048828\n",
      "Test set for fold 3: Average loss: 0.9305, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/30 [0/886 (0%)]\tLoss: 4197.476562\n",
      "Test set for fold 3: Average loss: 0.9302, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/31 [0/886 (0%)]\tLoss: 4196.796875\n",
      "Test set for fold 3: Average loss: 0.9301, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/32 [0/886 (0%)]\tLoss: 4196.518066\n",
      "Test set for fold 3: Average loss: 0.9298, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/33 [0/886 (0%)]\tLoss: 4196.075195\n",
      "Test set for fold 3: Average loss: 0.9296, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/34 [0/886 (0%)]\tLoss: 4195.717773\n",
      "Test set for fold 3: Average loss: 0.9295, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/35 [0/886 (0%)]\tLoss: 4195.304688\n",
      "Test set for fold 3: Average loss: 0.9294, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/36 [0/886 (0%)]\tLoss: 4194.826172\n",
      "Test set for fold 3: Average loss: 0.9295, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/37 [0/886 (0%)]\tLoss: 4194.419922\n",
      "Test set for fold 3: Average loss: 0.9295, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/38 [0/886 (0%)]\tLoss: 4194.252441\n",
      "Test set for fold 3: Average loss: 0.9292, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/39 [0/886 (0%)]\tLoss: 4194.062012\n",
      "Test set for fold 3: Average loss: 0.9291, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/40 [0/886 (0%)]\tLoss: 4193.571289\n",
      "Test set for fold 3: Average loss: 0.9290, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/41 [0/886 (0%)]\tLoss: 4193.137695\n",
      "Test set for fold 3: Average loss: 0.9289, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/42 [0/886 (0%)]\tLoss: 4192.771973\n",
      "Test set for fold 3: Average loss: 0.9288, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/43 [0/886 (0%)]\tLoss: 4192.711914\n",
      "Test set for fold 3: Average loss: 0.9289, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/44 [0/886 (0%)]\tLoss: 4192.428223\n",
      "Test set for fold 3: Average loss: 0.9287, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/45 [0/886 (0%)]\tLoss: 4192.120117\n",
      "Test set for fold 3: Average loss: 0.9286, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/46 [0/886 (0%)]\tLoss: 4191.733887\n",
      "Test set for fold 3: Average loss: 0.9285, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/47 [0/886 (0%)]\tLoss: 4191.664551\n",
      "Test set for fold 3: Average loss: 0.9286, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/48 [0/886 (0%)]\tLoss: 4191.329590\n",
      "Test set for fold 3: Average loss: 0.9286, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/49 [0/886 (0%)]\tLoss: 4191.401367\n",
      "Test set for fold 3: Average loss: 0.9284, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/50 [0/886 (0%)]\tLoss: 4190.575195\n",
      "Test set for fold 3: Average loss: 0.9287, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 3/51 [0/886 (0%)]\tLoss: 4190.346680\n",
      "Test set for fold 3: Average loss: 0.9283, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/52 [0/886 (0%)]\tLoss: 4190.124512\n",
      "Test set for fold 3: Average loss: 0.9282, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/53 [0/886 (0%)]\tLoss: 4190.092773\n",
      "Test set for fold 3: Average loss: 0.9282, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/54 [0/886 (0%)]\tLoss: 4189.553711\n",
      "Test set for fold 3: Average loss: 0.9279, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/55 [0/886 (0%)]\tLoss: 4189.504395\n",
      "Test set for fold 3: Average loss: 0.9282, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/56 [0/886 (0%)]\tLoss: 4189.063477\n",
      "Test set for fold 3: Average loss: 0.9279, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/57 [0/886 (0%)]\tLoss: 4189.088867\n",
      "Test set for fold 3: Average loss: 0.9278, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/58 [0/886 (0%)]\tLoss: 4189.074219\n",
      "Test set for fold 3: Average loss: 0.9282, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/59 [0/886 (0%)]\tLoss: 4188.818359\n",
      "Test set for fold 3: Average loss: 0.9280, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/60 [0/886 (0%)]\tLoss: 4188.575195\n",
      "Test set for fold 3: Average loss: 0.9277, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/61 [0/886 (0%)]\tLoss: 4188.432617\n",
      "Test set for fold 3: Average loss: 0.9279, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/62 [0/886 (0%)]\tLoss: 4188.152344\n",
      "Test set for fold 3: Average loss: 0.9277, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/63 [0/886 (0%)]\tLoss: 4187.899414\n",
      "Test set for fold 3: Average loss: 0.9276, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/64 [0/886 (0%)]\tLoss: 4187.843262\n",
      "Test set for fold 3: Average loss: 0.9280, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/65 [0/886 (0%)]\tLoss: 4187.575195\n",
      "Test set for fold 3: Average loss: 0.9276, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/66 [0/886 (0%)]\tLoss: 4187.263672\n",
      "Test set for fold 3: Average loss: 0.9277, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/67 [0/886 (0%)]\tLoss: 4187.386230\n",
      "Test set for fold 3: Average loss: 0.9276, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/68 [0/886 (0%)]\tLoss: 4187.049805\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/69 [0/886 (0%)]\tLoss: 4187.519043\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/70 [0/886 (0%)]\tLoss: 4186.800293\n",
      "Test set for fold 3: Average loss: 0.9277, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/71 [0/886 (0%)]\tLoss: 4186.860840\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/72 [0/886 (0%)]\tLoss: 4186.592773\n",
      "Test set for fold 3: Average loss: 0.9275, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/73 [0/886 (0%)]\tLoss: 4186.166992\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/74 [0/886 (0%)]\tLoss: 4186.242188\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/75 [0/886 (0%)]\tLoss: 4186.263672\n",
      "Test set for fold 3: Average loss: 0.9275, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/76 [0/886 (0%)]\tLoss: 4186.324707\n",
      "Test set for fold 3: Average loss: 0.9271, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/77 [0/886 (0%)]\tLoss: 4185.938477\n",
      "Test set for fold 3: Average loss: 0.9276, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/78 [0/886 (0%)]\tLoss: 4186.208008\n",
      "Test set for fold 3: Average loss: 0.9273, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/79 [0/886 (0%)]\tLoss: 4185.557617\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/80 [0/886 (0%)]\tLoss: 4185.620605\n",
      "Test set for fold 3: Average loss: 0.9275, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/81 [0/886 (0%)]\tLoss: 4185.372070\n",
      "Test set for fold 3: Average loss: 0.9276, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/82 [0/886 (0%)]\tLoss: 4185.592773\n",
      "Test set for fold 3: Average loss: 0.9271, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/83 [0/886 (0%)]\tLoss: 4185.125977\n",
      "Test set for fold 3: Average loss: 0.9273, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/84 [0/886 (0%)]\tLoss: 4185.296387\n",
      "Test set for fold 3: Average loss: 0.9270, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/85 [0/886 (0%)]\tLoss: 4184.655762\n",
      "Test set for fold 3: Average loss: 0.9272, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/86 [0/886 (0%)]\tLoss: 4184.626953\n",
      "Test set for fold 3: Average loss: 0.9270, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/87 [0/886 (0%)]\tLoss: 4184.476562\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/88 [0/886 (0%)]\tLoss: 4184.376465\n",
      "Test set for fold 3: Average loss: 0.9270, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/89 [0/886 (0%)]\tLoss: 4185.165039\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/90 [0/886 (0%)]\tLoss: 4184.341797\n",
      "Test set for fold 3: Average loss: 0.9271, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/91 [0/886 (0%)]\tLoss: 4184.025391\n",
      "Test set for fold 3: Average loss: 0.9271, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/92 [0/886 (0%)]\tLoss: 4183.838867\n",
      "Test set for fold 3: Average loss: 0.9271, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/93 [0/886 (0%)]\tLoss: 4183.803711\n",
      "Test set for fold 3: Average loss: 0.9269, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/94 [0/886 (0%)]\tLoss: 4183.677246\n",
      "Test set for fold 3: Average loss: 0.9270, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/95 [0/886 (0%)]\tLoss: 4183.677734\n",
      "Test set for fold 3: Average loss: 0.9272, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/96 [0/886 (0%)]\tLoss: 4183.402344\n",
      "Test set for fold 3: Average loss: 0.9274, Accuracy: 175/886 (20%)    \n",
      "Train Fold/Epoch: 3/97 [0/886 (0%)]\tLoss: 4183.631836\n",
      "Test set for fold 3: Average loss: 0.9271, Accuracy: 176/886 (20%)    \n",
      "Train Fold/Epoch: 3/98 [0/886 (0%)]\tLoss: 4183.661133\n",
      "Test set for fold 3: Average loss: 0.9270, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/99 [0/886 (0%)]\tLoss: 4183.416992\n",
      "Test set for fold 3: Average loss: 0.9271, Accuracy: 177/886 (20%)    \n",
      "Train Fold/Epoch: 3/100 [0/886 (0%)]\tLoss: 4183.182129\n",
      "Test set for fold 3: Average loss: 0.9269, Accuracy: 177/886 (20%)    \n",
      "-------------------Fold 4-------------------\n",
      "Train Fold/Epoch: 4/1 [0/886 (0%)]\tLoss: 4267.746094\n",
      "Test set for fold 4: Average loss: 0.9496, Accuracy: 27/886 (3%)    \n",
      "Train Fold/Epoch: 4/2 [0/886 (0%)]\tLoss: 4262.521484\n",
      "Test set for fold 4: Average loss: 0.9482, Accuracy: 76/886 (9%)    \n",
      "Train Fold/Epoch: 4/3 [0/886 (0%)]\tLoss: 4256.073730\n",
      "Test set for fold 4: Average loss: 0.9472, Accuracy: 99/886 (11%)    \n",
      "Train Fold/Epoch: 4/4 [0/886 (0%)]\tLoss: 4250.575684\n",
      "Test set for fold 4: Average loss: 0.9435, Accuracy: 109/886 (12%)    \n",
      "Train Fold/Epoch: 4/5 [0/886 (0%)]\tLoss: 4241.333008\n",
      "Test set for fold 4: Average loss: 0.9416, Accuracy: 120/886 (14%)    \n",
      "Train Fold/Epoch: 4/6 [0/886 (0%)]\tLoss: 4236.513672\n",
      "Test set for fold 4: Average loss: 0.9409, Accuracy: 121/886 (14%)    \n",
      "Train Fold/Epoch: 4/7 [0/886 (0%)]\tLoss: 4233.761719\n",
      "Test set for fold 4: Average loss: 0.9404, Accuracy: 125/886 (14%)    \n",
      "Train Fold/Epoch: 4/8 [0/886 (0%)]\tLoss: 4232.699707\n",
      "Test set for fold 4: Average loss: 0.9405, Accuracy: 121/886 (14%)    \n",
      "Train Fold/Epoch: 4/9 [0/886 (0%)]\tLoss: 4232.037109\n",
      "Test set for fold 4: Average loss: 0.9401, Accuracy: 118/886 (13%)    \n",
      "Train Fold/Epoch: 4/10 [0/886 (0%)]\tLoss: 4230.692383\n",
      "Test set for fold 4: Average loss: 0.9400, Accuracy: 119/886 (13%)    \n",
      "Train Fold/Epoch: 4/11 [0/886 (0%)]\tLoss: 4229.854492\n",
      "Test set for fold 4: Average loss: 0.9398, Accuracy: 111/886 (13%)    \n",
      "Train Fold/Epoch: 4/12 [0/886 (0%)]\tLoss: 4228.939453\n",
      "Test set for fold 4: Average loss: 0.9399, Accuracy: 87/886 (10%)    \n",
      "Train Fold/Epoch: 4/13 [0/886 (0%)]\tLoss: 4228.985352\n",
      "Test set for fold 4: Average loss: 0.9397, Accuracy: 79/886 (9%)    \n",
      "Train Fold/Epoch: 4/14 [0/886 (0%)]\tLoss: 4228.205078\n",
      "Test set for fold 4: Average loss: 0.9396, Accuracy: 79/886 (9%)    \n",
      "Train Fold/Epoch: 4/15 [0/886 (0%)]\tLoss: 4227.879883\n",
      "Test set for fold 4: Average loss: 0.9393, Accuracy: 72/886 (8%)    \n",
      "Train Fold/Epoch: 4/16 [0/886 (0%)]\tLoss: 4227.416992\n",
      "Test set for fold 4: Average loss: 0.9396, Accuracy: 71/886 (8%)    \n",
      "Train Fold/Epoch: 4/17 [0/886 (0%)]\tLoss: 4227.371094\n",
      "Test set for fold 4: Average loss: 0.9392, Accuracy: 68/886 (8%)    \n",
      "Train Fold/Epoch: 4/18 [0/886 (0%)]\tLoss: 4226.733398\n",
      "Test set for fold 4: Average loss: 0.9394, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/19 [0/886 (0%)]\tLoss: 4226.506836\n",
      "Test set for fold 4: Average loss: 0.9394, Accuracy: 65/886 (7%)    \n",
      "Train Fold/Epoch: 4/20 [0/886 (0%)]\tLoss: 4225.822754\n",
      "Test set for fold 4: Average loss: 0.9390, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/21 [0/886 (0%)]\tLoss: 4225.699219\n",
      "Test set for fold 4: Average loss: 0.9388, Accuracy: 66/886 (7%)    \n",
      "Train Fold/Epoch: 4/22 [0/886 (0%)]\tLoss: 4224.929199\n",
      "Test set for fold 4: Average loss: 0.9387, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/23 [0/886 (0%)]\tLoss: 4224.896973\n",
      "Test set for fold 4: Average loss: 0.9391, Accuracy: 65/886 (7%)    \n",
      "Train Fold/Epoch: 4/24 [0/886 (0%)]\tLoss: 4224.313965\n",
      "Test set for fold 4: Average loss: 0.9386, Accuracy: 64/886 (7%)    \n",
      "Train Fold/Epoch: 4/25 [0/886 (0%)]\tLoss: 4224.298828\n",
      "Test set for fold 4: Average loss: 0.9387, Accuracy: 68/886 (8%)    \n",
      "Train Fold/Epoch: 4/26 [0/886 (0%)]\tLoss: 4223.281250\n",
      "Test set for fold 4: Average loss: 0.9382, Accuracy: 64/886 (7%)    \n",
      "Train Fold/Epoch: 4/27 [0/886 (0%)]\tLoss: 4222.465820\n",
      "Test set for fold 4: Average loss: 0.9382, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/28 [0/886 (0%)]\tLoss: 4222.116211\n",
      "Test set for fold 4: Average loss: 0.9380, Accuracy: 65/886 (7%)    \n",
      "Train Fold/Epoch: 4/29 [0/886 (0%)]\tLoss: 4221.611328\n",
      "Test set for fold 4: Average loss: 0.9381, Accuracy: 68/886 (8%)    \n",
      "Train Fold/Epoch: 4/30 [0/886 (0%)]\tLoss: 4221.097656\n",
      "Test set for fold 4: Average loss: 0.9378, Accuracy: 65/886 (7%)    \n",
      "Train Fold/Epoch: 4/31 [0/886 (0%)]\tLoss: 4220.686523\n",
      "Test set for fold 4: Average loss: 0.9376, Accuracy: 66/886 (7%)    \n",
      "Train Fold/Epoch: 4/32 [0/886 (0%)]\tLoss: 4219.876465\n",
      "Test set for fold 4: Average loss: 0.9374, Accuracy: 66/886 (7%)    \n",
      "Train Fold/Epoch: 4/33 [0/886 (0%)]\tLoss: 4219.194336\n",
      "Test set for fold 4: Average loss: 0.9377, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/34 [0/886 (0%)]\tLoss: 4218.813477\n",
      "Test set for fold 4: Average loss: 0.9374, Accuracy: 64/886 (7%)    \n",
      "Train Fold/Epoch: 4/35 [0/886 (0%)]\tLoss: 4218.228027\n",
      "Test set for fold 4: Average loss: 0.9375, Accuracy: 64/886 (7%)    \n",
      "Train Fold/Epoch: 4/36 [0/886 (0%)]\tLoss: 4217.587891\n",
      "Test set for fold 4: Average loss: 0.9373, Accuracy: 64/886 (7%)    \n",
      "Train Fold/Epoch: 4/37 [0/886 (0%)]\tLoss: 4216.925293\n",
      "Test set for fold 4: Average loss: 0.9371, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/38 [0/886 (0%)]\tLoss: 4216.033203\n",
      "Test set for fold 4: Average loss: 0.9370, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/39 [0/886 (0%)]\tLoss: 4215.296875\n",
      "Test set for fold 4: Average loss: 0.9367, Accuracy: 65/886 (7%)    \n",
      "Train Fold/Epoch: 4/40 [0/886 (0%)]\tLoss: 4214.787598\n",
      "Test set for fold 4: Average loss: 0.9369, Accuracy: 64/886 (7%)    \n",
      "Train Fold/Epoch: 4/41 [0/886 (0%)]\tLoss: 4213.536133\n",
      "Test set for fold 4: Average loss: 0.9367, Accuracy: 69/886 (8%)    \n",
      "Train Fold/Epoch: 4/42 [0/886 (0%)]\tLoss: 4213.033691\n",
      "Test set for fold 4: Average loss: 0.9364, Accuracy: 71/886 (8%)    \n",
      "Train Fold/Epoch: 4/43 [0/886 (0%)]\tLoss: 4212.110352\n",
      "Test set for fold 4: Average loss: 0.9364, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/44 [0/886 (0%)]\tLoss: 4211.109375\n",
      "Test set for fold 4: Average loss: 0.9363, Accuracy: 67/886 (8%)    \n",
      "Train Fold/Epoch: 4/45 [0/886 (0%)]\tLoss: 4210.657227\n",
      "Test set for fold 4: Average loss: 0.9361, Accuracy: 72/886 (8%)    \n",
      "Train Fold/Epoch: 4/46 [0/886 (0%)]\tLoss: 4209.921875\n",
      "Test set for fold 4: Average loss: 0.9360, Accuracy: 72/886 (8%)    \n",
      "Train Fold/Epoch: 4/47 [0/886 (0%)]\tLoss: 4208.922363\n",
      "Test set for fold 4: Average loss: 0.9355, Accuracy: 79/886 (9%)    \n",
      "Train Fold/Epoch: 4/48 [0/886 (0%)]\tLoss: 4208.262695\n",
      "Test set for fold 4: Average loss: 0.9357, Accuracy: 85/886 (10%)    \n",
      "Train Fold/Epoch: 4/49 [0/886 (0%)]\tLoss: 4207.220215\n",
      "Test set for fold 4: Average loss: 0.9356, Accuracy: 105/886 (12%)    \n",
      "Train Fold/Epoch: 4/50 [0/886 (0%)]\tLoss: 4206.332031\n",
      "Test set for fold 4: Average loss: 0.9354, Accuracy: 112/886 (13%)    \n",
      "Train Fold/Epoch: 4/51 [0/886 (0%)]\tLoss: 4205.864746\n",
      "Test set for fold 4: Average loss: 0.9353, Accuracy: 121/886 (14%)    \n",
      "Train Fold/Epoch: 4/52 [0/886 (0%)]\tLoss: 4204.488281\n",
      "Test set for fold 4: Average loss: 0.9349, Accuracy: 124/886 (14%)    \n",
      "Train Fold/Epoch: 4/53 [0/886 (0%)]\tLoss: 4203.620605\n",
      "Test set for fold 4: Average loss: 0.9348, Accuracy: 128/886 (14%)    \n",
      "Train Fold/Epoch: 4/54 [0/886 (0%)]\tLoss: 4202.715820\n",
      "Test set for fold 4: Average loss: 0.9348, Accuracy: 127/886 (14%)    \n",
      "Train Fold/Epoch: 4/55 [0/886 (0%)]\tLoss: 4202.394531\n",
      "Test set for fold 4: Average loss: 0.9344, Accuracy: 132/886 (15%)    \n",
      "Train Fold/Epoch: 4/56 [0/886 (0%)]\tLoss: 4201.787598\n",
      "Test set for fold 4: Average loss: 0.9348, Accuracy: 134/886 (15%)    \n",
      "Train Fold/Epoch: 4/57 [0/886 (0%)]\tLoss: 4201.012207\n",
      "Test set for fold 4: Average loss: 0.9339, Accuracy: 137/886 (15%)    \n",
      "Train Fold/Epoch: 4/58 [0/886 (0%)]\tLoss: 4201.257812\n",
      "Test set for fold 4: Average loss: 0.9344, Accuracy: 135/886 (15%)    \n",
      "Train Fold/Epoch: 4/59 [0/886 (0%)]\tLoss: 4199.826660\n",
      "Test set for fold 4: Average loss: 0.9342, Accuracy: 140/886 (16%)    \n",
      "Train Fold/Epoch: 4/60 [0/886 (0%)]\tLoss: 4199.229492\n",
      "Test set for fold 4: Average loss: 0.9342, Accuracy: 142/886 (16%)    \n",
      "Train Fold/Epoch: 4/61 [0/886 (0%)]\tLoss: 4198.270508\n",
      "Test set for fold 4: Average loss: 0.9339, Accuracy: 141/886 (16%)    \n",
      "Train Fold/Epoch: 4/62 [0/886 (0%)]\tLoss: 4197.766602\n",
      "Test set for fold 4: Average loss: 0.9340, Accuracy: 140/886 (16%)    \n",
      "Train Fold/Epoch: 4/63 [0/886 (0%)]\tLoss: 4197.497070\n",
      "Test set for fold 4: Average loss: 0.9337, Accuracy: 143/886 (16%)    \n",
      "Train Fold/Epoch: 4/64 [0/886 (0%)]\tLoss: 4196.668945\n",
      "Test set for fold 4: Average loss: 0.9338, Accuracy: 140/886 (16%)    \n",
      "Train Fold/Epoch: 4/65 [0/886 (0%)]\tLoss: 4196.067383\n",
      "Test set for fold 4: Average loss: 0.9335, Accuracy: 144/886 (16%)    \n",
      "Train Fold/Epoch: 4/66 [0/886 (0%)]\tLoss: 4195.738770\n",
      "Test set for fold 4: Average loss: 0.9335, Accuracy: 142/886 (16%)    \n",
      "Train Fold/Epoch: 4/67 [0/886 (0%)]\tLoss: 4195.079590\n",
      "Test set for fold 4: Average loss: 0.9330, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/68 [0/886 (0%)]\tLoss: 4195.276367\n",
      "Test set for fold 4: Average loss: 0.9335, Accuracy: 143/886 (16%)    \n",
      "Train Fold/Epoch: 4/69 [0/886 (0%)]\tLoss: 4194.501953\n",
      "Test set for fold 4: Average loss: 0.9331, Accuracy: 144/886 (16%)    \n",
      "Train Fold/Epoch: 4/70 [0/886 (0%)]\tLoss: 4193.515625\n",
      "Test set for fold 4: Average loss: 0.9329, Accuracy: 143/886 (16%)    \n",
      "Train Fold/Epoch: 4/71 [0/886 (0%)]\tLoss: 4193.198730\n",
      "Test set for fold 4: Average loss: 0.9327, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/72 [0/886 (0%)]\tLoss: 4192.960449\n",
      "Test set for fold 4: Average loss: 0.9328, Accuracy: 145/886 (16%)    \n",
      "Train Fold/Epoch: 4/73 [0/886 (0%)]\tLoss: 4192.249023\n",
      "Test set for fold 4: Average loss: 0.9329, Accuracy: 144/886 (16%)    \n",
      "Train Fold/Epoch: 4/74 [0/886 (0%)]\tLoss: 4191.800293\n",
      "Test set for fold 4: Average loss: 0.9328, Accuracy: 143/886 (16%)    \n",
      "Train Fold/Epoch: 4/75 [0/886 (0%)]\tLoss: 4191.416992\n",
      "Test set for fold 4: Average loss: 0.9327, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/76 [0/886 (0%)]\tLoss: 4190.932617\n",
      "Test set for fold 4: Average loss: 0.9325, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/77 [0/886 (0%)]\tLoss: 4190.456055\n",
      "Test set for fold 4: Average loss: 0.9328, Accuracy: 148/886 (17%)    \n",
      "Train Fold/Epoch: 4/78 [0/886 (0%)]\tLoss: 4190.123047\n",
      "Test set for fold 4: Average loss: 0.9326, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/79 [0/886 (0%)]\tLoss: 4189.647949\n",
      "Test set for fold 4: Average loss: 0.9325, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/80 [0/886 (0%)]\tLoss: 4189.351562\n",
      "Test set for fold 4: Average loss: 0.9325, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/81 [0/886 (0%)]\tLoss: 4188.966797\n",
      "Test set for fold 4: Average loss: 0.9326, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/82 [0/886 (0%)]\tLoss: 4188.614746\n",
      "Test set for fold 4: Average loss: 0.9323, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/83 [0/886 (0%)]\tLoss: 4188.376953\n",
      "Test set for fold 4: Average loss: 0.9321, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/84 [0/886 (0%)]\tLoss: 4188.124023\n",
      "Test set for fold 4: Average loss: 0.9324, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/85 [0/886 (0%)]\tLoss: 4187.718750\n",
      "Test set for fold 4: Average loss: 0.9324, Accuracy: 145/886 (16%)    \n",
      "Train Fold/Epoch: 4/86 [0/886 (0%)]\tLoss: 4187.764160\n",
      "Test set for fold 4: Average loss: 0.9325, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/87 [0/886 (0%)]\tLoss: 4187.645996\n",
      "Test set for fold 4: Average loss: 0.9323, Accuracy: 145/886 (16%)    \n",
      "Train Fold/Epoch: 4/88 [0/886 (0%)]\tLoss: 4187.349121\n",
      "Test set for fold 4: Average loss: 0.9319, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/89 [0/886 (0%)]\tLoss: 4186.594238\n",
      "Test set for fold 4: Average loss: 0.9324, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/90 [0/886 (0%)]\tLoss: 4187.013672\n",
      "Test set for fold 4: Average loss: 0.9327, Accuracy: 145/886 (16%)    \n",
      "Train Fold/Epoch: 4/91 [0/886 (0%)]\tLoss: 4186.107422\n",
      "Test set for fold 4: Average loss: 0.9322, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/92 [0/886 (0%)]\tLoss: 4185.701660\n",
      "Test set for fold 4: Average loss: 0.9324, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/93 [0/886 (0%)]\tLoss: 4185.479980\n",
      "Test set for fold 4: Average loss: 0.9322, Accuracy: 148/886 (17%)    \n",
      "Train Fold/Epoch: 4/94 [0/886 (0%)]\tLoss: 4185.222168\n",
      "Test set for fold 4: Average loss: 0.9325, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/95 [0/886 (0%)]\tLoss: 4185.026367\n",
      "Test set for fold 4: Average loss: 0.9323, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/96 [0/886 (0%)]\tLoss: 4184.890137\n",
      "Test set for fold 4: Average loss: 0.9324, Accuracy: 147/886 (17%)    \n",
      "Train Fold/Epoch: 4/97 [0/886 (0%)]\tLoss: 4184.617676\n",
      "Test set for fold 4: Average loss: 0.9323, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/98 [0/886 (0%)]\tLoss: 4184.478027\n",
      "Test set for fold 4: Average loss: 0.9322, Accuracy: 146/886 (16%)    \n",
      "Train Fold/Epoch: 4/99 [0/886 (0%)]\tLoss: 4184.097168\n",
      "Test set for fold 4: Average loss: 0.9343, Accuracy: 144/886 (16%)    \n",
      "Train Fold/Epoch: 4/100 [0/886 (0%)]\tLoss: 4184.036133\n",
      "Test set for fold 4: Average loss: 0.9322, Accuracy: 146/886 (16%)    \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "use_whole_data = True\n",
    "seq_length = 2\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training the network\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = pMHC_TCR_model(input_size=356, batch_size=batch_size, device=device, use_whole_data=use_whole_data).to(device)\n",
    "\n",
    "# kf = RepeatedKFold(n_splits=10, n_repeats=12, random_state=1234)\n",
    "# kf = KFold(n_splits=10, random_state=1234, shuffle=True)\n",
    "kf = StratifiedKFold(n_splits=folds, random_state=1234, shuffle=True)\n",
    "weights = torch.FloatTensor([1, 9])\n",
    "optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"acc\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.features, TCRData.labels)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        # train_subsampler = WeightedRandomSampler(weights, len(train_idx), replacement=True)\n",
    "        # test_subsampler = WeightedRandomSampler(weights, len(test_idx), replacement=True)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=batch_size, sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=batch_size, sampler=test_subsampler)\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "\n",
    "    accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_losses = train(fold, model, device, train_loader, optimizer, epoch)\n",
    "        test_losses, correctness = test(fold, model, device, test_loader)\n",
    "        accuracy_history.append(correctness)\n",
    "        ax[0].plot(epoch, train_losses, 'b*-')\n",
    "        ax[0].plot(epoch, test_losses, 'r*-')\n",
    "        # plt.plot(epoch, train_losses, 'b*-')\n",
    "        # plt.plot(epoch, test_losses, 'r*-')\n",
    "        if fold==1 and epoch == 1:\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        # plt.savefig(os.path.join(\"./lossGraphs\", \"train.jpg\"))\n",
    "    ax[1].plot(fold, np.mean(accuracy_history), '*-', label=f\"fold {fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F.nll_loss(torch.tensor([-0.1]), torch.tensor([1]))\n",
    "# torch.tensor([-22]).sigmoid()\n",
    "# nn.CrossEntropyLoss()(torch.tensor([-1.]), torch.tensor([1.]))\n",
    "True & False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "\n",
    "# plot the accuracy curve\n",
    "\n",
    "# plot the ROC curve\n",
    "\n",
    "# plot the PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f512df4ec80>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfw0lEQVR4nO3df3RT9f3H8Vd/0FCBpBbXpLUFq6JQRVHg1OCP6cyhKDoZ6MBv9SBj4FirKyiOOinHnzimuJWhFY8HOhF17qgDpigrWHHGilUcAwTcmNSWtGzYpMVRSnO/f/Q0M8Cg4UfzSXk+zsmR3vtJ+7734Mnz3CaXOMuyLAEAABgkPtoDAAAAHIxAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGCcxGgPcCyCwaDq6urUp08fxcXFRXscAADQCZZlqampSRkZGYqPP/I1kpgMlLq6OmVlZUV7DAAAcAxqamqUmZl5xDUxGSh9+vSR1H6Adrs9ytMAAIDOCAQCysrKCr2OH0lMBkrHr3XsdjuBAgBAjOnM2zN4kywAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAODF5ozYA3VRbm7RunbRrl5SeLl15pZSQEO2pAEQBgQLADK+9Jv3sZ9JXX/13W2am9JvfSGPHRm8uAFHBr3gARN9rr0k33xweJ5JUW9u+/bXXojMXgKghUABEV1tb+5UTyzp0X8e2oqL2dQBOGQQKgOhat+7QKyffZllSTU37OgCnDAIFQHTt2nVi1wHoFggUANGVnn5i1wHoFggUANF15ZXtn9aJizv8/rg4KSurfR2AUwaBAiC6EhLaP0osHRopHV//+tfcDwU4xRAoAKJv7FjpD3+QzjwzfHtmZvt27oMCnHIiCpS2tjbNnj1b2dnZSk5O1jnnnKOHH35Y1rc+HmhZlkpKSpSenq7k5GR5PB5t37497Pvs2bNH+fn5stvtSklJ0eTJk9Xc3HxijghAbBo7VvrnP6W1a6Vly9r/u2MHcQKcoiK6k+wvf/lLPfPMMyovL9cFF1ygjz/+WJMmTZLD4dDdd98tSZo3b55KS0tVXl6u7OxszZ49W3l5edq8ebN69uwpScrPz9euXbu0evVqtba2atKkSZo6daqWLVt24o8QQOxISJCuvjraUwAwQJxlHe7uSId3ww03yOl06vnnnw9tGzdunJKTk7V06VJZlqWMjAzdc889uvfeeyVJfr9fTqdTS5Ys0YQJE7Rlyxbl5ORo/fr1GjZsmCRp1apVuv766/XVV18pIyPjqHMEAgE5HA75/X7Z7fZIjxkAAERBJK/fEf2KZ8SIEaqoqNC2bdskSZ999pnef/99XXfddZKkHTt2yOfzyePxhJ7jcDiUm5srr9crSfJ6vUpJSQnFiSR5PB7Fx8erqqrqsD+3paVFgUAg7AEAALqviH7FM2vWLAUCAQ0cOFAJCQlqa2vTo48+qvz8fEmSz+eTJDmdzrDnOZ3O0D6fz6e0tLTwIRITlZqaGlpzsLlz5+rBBx+MZFQAABDDIrqC8vvf/14vvviili1bpk8++UTl5eV64oknVF5efrLmkyQVFxfL7/eHHjU1NSf15wEAgOiK6ArKzJkzNWvWLE2YMEGSNHjwYH355ZeaO3euJk6cKJfLJUmqr69X+rfu+lhfX68hQ4ZIklwulxoaGsK+74EDB7Rnz57Q8w9ms9lks9kiGRUAAMSwiK6gfPPNN4qPD39KQkKCgsGgJCk7O1sul0sVFRWh/YFAQFVVVXK73ZIkt9utxsZGVVdXh9asWbNGwWBQubm5x3wgAACg+4joCsqNN96oRx99VP369dMFF1ygTz/9VPPnz9ePfvQjSVJcXJyKior0yCOPaMCAAaGPGWdkZGjMmDGSpEGDBmnUqFGaMmWKysrK1NraqsLCQk2YMKFTn+ABAADdX0SBsmDBAs2ePVs//elP1dDQoIyMDN15550qKSkJrbnvvvu0d+9eTZ06VY2Njbriiiu0atWq0D1QJOnFF19UYWGhrr32WsXHx2vcuHEqLS09cUcFAABiWkT3QTEF90EBACD2nLT7oAAAAHQFAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGiThQamtrddttt6lv375KTk7W4MGD9fHHH4f2W5alkpISpaenKzk5WR6PR9u3bw/7Hnv27FF+fr7sdrtSUlI0efJkNTc3H//RAACAbiGiQPn66691+eWXq0ePHnrrrbe0efNmPfnkkzr99NNDa+bNm6fS0lKVlZWpqqpKvXr1Ul5envbt2xdak5+fr02bNmn16tVauXKl3nvvPU2dOvXEHRUAAIhpcZZlWZ1dPGvWLP3lL3/RunXrDrvfsixlZGTonnvu0b333itJ8vv9cjqdWrJkiSZMmKAtW7YoJydH69ev17BhwyRJq1at0vXXX6+vvvpKGRkZR50jEAjI4XDI7/fLbrd3dnwAABBFkbx+R3QFZfny5Ro2bJhuueUWpaWl6ZJLLtFzzz0X2r9jxw75fD55PJ7QNofDodzcXHm9XkmS1+tVSkpKKE4kyePxKD4+XlVVVYf9uS0tLQoEAmEPAADQfUUUKP/4xz/0zDPPaMCAAXr77bc1bdo03X333SovL5ck+Xw+SZLT6Qx7ntPpDO3z+XxKS0sL25+YmKjU1NTQmoPNnTtXDocj9MjKyopkbAAAEGMiCpRgMKhLL71Ujz32mC655BJNnTpVU6ZMUVlZ2cmaT5JUXFwsv98fetTU1JzUnwcAAKIrokBJT09XTk5O2LZBgwZp586dkiSXyyVJqq+vD1tTX18f2udyudTQ0BC2/8CBA9qzZ09ozcFsNpvsdnvYAwAAdF8RBcrll1+urVu3hm3btm2b+vfvL0nKzs6Wy+VSRUVFaH8gEFBVVZXcbrckye12q7GxUdXV1aE1a9asUTAYVG5u7jEfCAAA6D4SI1k8ffp0jRgxQo899ph++MMf6qOPPtKiRYu0aNEiSVJcXJyKior0yCOPaMCAAcrOztbs2bOVkZGhMWPGSGq/4jJq1KjQr4ZaW1tVWFioCRMmdOoTPAAAoPuL6GPGkrRy5UoVFxdr+/btys7O1owZMzRlypTQfsuyNGfOHC1atEiNjY264oor9PTTT+u8884LrdmzZ48KCwu1YsUKxcfHa9y4cSotLVXv3r07NQMfMwYAIPZE8vodcaCYgEABACD2nLT7oAAAAHQFAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGOa5AefzxxxUXF6eioqLQtn379qmgoEB9+/ZV7969NW7cONXX14c9b+fOnRo9erROO+00paWlaebMmTpw4MDxjAIAALqRYw6U9evX69lnn9VFF10Utn369OlasWKFXn31VVVWVqqurk5jx44N7W9ra9Po0aO1f/9+ffDBByovL9eSJUtUUlJy7EcBAAC6lWMKlObmZuXn5+u5557T6aefHtru9/v1/PPPa/78+fre976noUOHavHixfrggw/04YcfSpLeeecdbd68WUuXLtWQIUN03XXX6eGHH9bChQu1f//+E3NUAAAgph1ToBQUFGj06NHyeDxh26urq9Xa2hq2feDAgerXr5+8Xq8kyev1avDgwXI6naE1eXl5CgQC2rRp02F/XktLiwKBQNgDAAB0X4mRPuHll1/WJ598ovXr1x+yz+fzKSkpSSkpKWHbnU6nfD5faM2346Rjf8e+w5k7d64efPDBSEcFAAAxKqIrKDU1NfrZz36mF198UT179jxZMx2iuLhYfr8/9Kipqemynw0AALpeRIFSXV2thoYGXXrppUpMTFRiYqIqKytVWlqqxMREOZ1O7d+/X42NjWHPq6+vl8vlkiS5XK5DPtXT8XXHmoPZbDbZ7fawBwAA6L4iCpRrr71WGzdu1IYNG0KPYcOGKT8/P/TnHj16qKKiIvScrVu3aufOnXK73ZIkt9utjRs3qqGhIbRm9erVstvtysnJOUGHBQAAYllE70Hp06ePLrzwwrBtvXr1Ut++fUPbJ0+erBkzZig1NVV2u1133XWX3G63LrvsMknSyJEjlZOTo9tvv13z5s2Tz+fTAw88oIKCAtlsthN0WAAAIJZF/CbZo3nqqacUHx+vcePGqaWlRXl5eXr66adD+xMSErRy5UpNmzZNbrdbvXr10sSJE/XQQw+d6FEAAECMirMsy4r2EJEKBAJyOBzy+/28HwUAgBgRyes3/xYPAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjBNRoMydO1fDhw9Xnz59lJaWpjFjxmjr1q1ha/bt26eCggL17dtXvXv31rhx41RfXx+2ZufOnRo9erROO+00paWlaebMmTpw4MDxHw0AAOgWIgqUyspKFRQU6MMPP9Tq1avV2tqqkSNHau/evaE106dP14oVK/Tqq6+qsrJSdXV1Gjt2bGh/W1ubRo8erf379+uDDz5QeXm5lixZopKSkhN3VAAAIKbFWZZlHeuTd+/erbS0NFVWVuqqq66S3+/Xd77zHS1btkw333yzJOnzzz/XoEGD5PV6ddlll+mtt97SDTfcoLq6OjmdTklSWVmZfv7zn2v37t1KSko66s8NBAJyOBzy+/2y2+3HOj4AAOhCkbx+H9d7UPx+vyQpNTVVklRdXa3W1lZ5PJ7QmoEDB6pfv37yer2SJK/Xq8GDB4fiRJLy8vIUCAS0adOm4xkHAAB0E4nH+sRgMKiioiJdfvnluvDCCyVJPp9PSUlJSklJCVvrdDrl8/lCa74dJx37O/YdTktLi1paWkJfBwKBYx0bAADEgGO+glJQUKC//e1vevnll0/kPIc1d+5cORyO0CMrK+uk/0wAABA9xxQohYWFWrlypdauXavMzMzQdpfLpf3796uxsTFsfX19vVwuV2jNwZ/q6fi6Y83BiouL5ff7Q4+amppjGRsAAMSIiALFsiwVFhbq9ddf15o1a5SdnR22f+jQoerRo4cqKipC27Zu3aqdO3fK7XZLktxutzZu3KiGhobQmtWrV8tutysnJ+ewP9dms8lut4c9AABA9xXRe1AKCgq0bNky/fGPf1SfPn1C7xlxOBxKTk6Ww+HQ5MmTNWPGDKWmpsput+uuu+6S2+3WZZddJkkaOXKkcnJydPvtt2vevHny+Xx64IEHVFBQIJvNduKPEAAAxJyIPmYcFxd32O2LFy/WHXfcIan9Rm333HOPXnrpJbW0tCgvL09PP/102K9vvvzyS02bNk3vvvuuevXqpYkTJ+rxxx9XYmLneomPGQMAEHsief0+rvugRAuBAgBA7Omy+6AAAACcDAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAME5itAcAgA5tbdK6ddKuXVJ6unTllVJCQrSnAhANUb2CsnDhQp111lnq2bOncnNz9dFHH0VzHABR9Npr0llnSddcI/3f/7X/96yz2rcDOPVELVBeeeUVzZgxQ3PmzNEnn3yiiy++WHl5eWpoaIjWSACi5LXXpJtvlr76Knx7bW37diIFOPXEWZZlReMH5+bmavjw4frtb38rSQoGg8rKytJdd92lWbNmHfG5gUBADodDfr9fdru9K8YFcJK0tbVfKTk4TjrExUmZmdKOHfy6B4h1kbx+R+UKyv79+1VdXS2Px/PfQeLj5fF45PV6D1nf0tKiQCAQ9gDQPaxb97/jRJIsS6qpaV8H4NQRlUD517/+pba2NjmdzrDtTqdTPp/vkPVz586Vw+EIPbKysrpqVAAn2a5dJ3YdgO4hJj5mXFxcLL/fH3rU1NREeyQAJ0h6+oldB6B7iMrHjM844wwlJCSovr4+bHt9fb1cLtch6202m2w2W1eNB6ALXXll+3tMamvbf51zsI73oFx5ZdfPBiB6onIFJSkpSUOHDlVFRUVoWzAYVEVFhdxudzRGAhAlCQnSb37T/ue4uPB9HV//+te8QRY41UTtVzwzZszQc889p/Lycm3ZskXTpk3T3r17NWnSpGiNBCBKxo6V/vAH6cwzw7dnZrZvHzs2OnMBiJ6o3Ul2/Pjx2r17t0pKSuTz+TRkyBCtWrXqkDfOAjg1jB0r3XQTd5IF0C5q90E5HtwHBQCA2GP8fVAAAACOhEABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGCdqt7o/Hh03vw0EAlGeBAAAdFbH63ZnbmIfk4HS1NQkScrKyoryJAAAIFJNTU1yOBxHXBOT/xZPMBhUXV2d+vTpo7iD/332U1AgEFBWVpZqamr4t4lOIs5z1+A8dw3Oc9fgPIezLEtNTU3KyMhQfPyR32USk1dQ4uPjlZmZGe0xjGO32/kfoAtwnrsG57lrcJ67Buf5v4525aQDb5IFAADGIVAAAIBxCJRuwGazac6cObLZbNEepVvjPHcNznPX4Dx3Dc7zsYvJN8kCAIDujSsoAADAOAQKAAAwDoECAACMQ6AAAADjECgxpKmpSUVFRerfv7+Sk5M1YsQIrV+/PmzNli1b9P3vf18Oh0O9evXS8OHDtXPnzihNHJuOdp6bm5tVWFiozMxMJScnKycnR2VlZVGc2HzvvfeebrzxRmVkZCguLk5vvPFG2H7LslRSUqL09HQlJyfL4/Fo+/btYWv27Nmj/Px82e12paSkaPLkyWpubu7Co4gNx3uu//nPf2ry5MnKzs5WcnKyzjnnHM2ZM0f79+/v4iMx24n4O92hpaVFQ4YMUVxcnDZs2HDyh48RBEoM+fGPf6zVq1frhRde0MaNGzVy5Eh5PB7V1tZKkv7+97/riiuu0MCBA/Xuu+/qr3/9q2bPnq2ePXtGefLYcrTzPGPGDK1atUpLly7Vli1bVFRUpMLCQi1fvjzKk5tr7969uvjii7Vw4cLD7p83b55KS0tVVlamqqoq9erVS3l5edq3b19oTX5+vjZt2qTVq1dr5cqVeu+99zR16tSuOoSYcbzn+vPPP1cwGNSzzz6rTZs26amnnlJZWZnuv//+rjwM452Iv9Md7rvvPmVkZJzskWOPhZjwzTffWAkJCdbKlSvDtl966aXWL37xC8uyLGv8+PHWbbfdFo3xuo3OnOcLLrjAeuihh/7nfhyZJOv1118PfR0MBi2Xy2X96le/Cm1rbGy0bDab9dJLL1mWZVmbN2+2JFnr168PrXnrrbesuLg4q7a2tstmjzXHcq4PZ968eVZ2dvbJHDWmHc95fvPNN62BAwdamzZtsiRZn376aRdNbT6uoMSIAwcOqK2t7ZCrIcnJyXr//fcVDAb1pz/9Seedd57y8vKUlpam3NzcQy474siOdp4lacSIEVq+fLlqa2tlWZbWrl2rbdu2aeTIkdEYOebt2LFDPp9PHo8ntM3hcCg3N1der1eS5PV6lZKSomHDhoXWeDwexcfHq6qqqstnjlWdOdeH4/f7lZqa2hUjdgudPc/19fWaMmWKXnjhBZ122mnRGNVoBEqM6NOnj9xutx5++GHV1dWpra1NS5culdfr1a5du9TQ0KDm5mY9/vjjGjVqlN555x394Ac/0NixY1VZWRnt8WPG0c6zJC1YsEA5OTnKzMxUUlKSRo0apYULF+qqq66K8vSxyefzSZKcTmfYdqfTGdrn8/mUlpYWtj8xMVGpqamhNTi6zpzrg33xxRdasGCB7rzzzpM+X3fRmfNsWZbuuOMO/eQnPwkLb/wXgRJDXnjhBVmWpTPPPFM2m02lpaW69dZbFR8fr2AwKEm66aabNH36dA0ZMkSzZs3SDTfcwBs4I3Sk8yy1B8qHH36o5cuXq7q6Wk8++aQKCgr05z//OcqTAydWbW2tRo0apVtuuUVTpkyJ9jjdyoIFC9TU1KTi4uJoj2IsAiWGnHPOOaqsrFRzc7Nqamr00UcfqbW1VWeffbbOOOMMJSYmKicnJ+w5gwYN4lM8ETrSef7Pf/6j+++/X/Pnz9eNN96oiy66SIWFhRo/fryeeOKJaI8ek1wul6T2y93fVl9fH9rncrnU0NAQtv/AgQPas2dPaA2OrjPnukNdXZ2uueYajRgxQosWLeqyGbuDzpznNWvWyOv1ymazKTExUeeee64kadiwYZo4cWLXDmwoAiUG9erVS+np6fr666/19ttv66abblJSUpKGDx+urVu3hq3dtm2b+vfvH6VJY9vhznNra6taW1tDV1M6JCQkhK5iITLZ2dlyuVyqqKgIbQsEAqqqqpLb7ZYkud1uNTY2qrq6OrRmzZo1CgaDys3N7fKZY1VnzrXUfuXk6quv1tChQ7V48eJD/r7jyDpznktLS/XZZ59pw4YN2rBhg958801J0iuvvKJHH300KnObJjHaA6Dz3n77bVmWpfPPP19ffPGFZs6cqYEDB2rSpEmSpJkzZ2r8+PG66qqrdM0112jVqlVasWKF3n333egOHmOOdJ579Oih7373u5o5c6aSk5PVv39/VVZW6ne/+53mz58f7dGN1dzcrC+++CL09Y4dO7RhwwalpqaqX79+Kioq0iOPPKIBAwYoOztbs2fPVkZGhsaMGSOp/UrgqFGjNGXKFJWVlam1tVWFhYWaMGECH888yPGe64446d+/v5544gnt3r079L24WvVfx3ue+/XrF/b9evfuLan9Cm5mZmaXHYfRovkRIkTmlVdesc4++2wrKSnJcrlcVkFBgdXY2Bi25vnnn7fOPfdcq2fPntbFF19svfHGG1GaNnYd7Tzv2rXLuuOOO6yMjAyrZ8+e1vnnn289+eSTVjAYjOLUZlu7dq0l6ZDHxIkTLctq/1jm7NmzLafTadlsNuvaa6+1tm7dGvY9/v3vf1u33nqr1bt3b8tut1uTJk2ympqaonA0Zjvec7148eLDPp+Xi3An4u/0t+3YsYOPGR8kzrIsqyuDCAAA4Gj4xSIAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4/w+WVj5WExDHeAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch, train_losses/len(TCRData), 'bo-', label=\"train\")\n",
    "plt.plot(epoch, test_losses, 'ro-', label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/wuxinchao/data/project/data/seqData/230215.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cellname</th>\n",
       "      <th>NeoAA</th>\n",
       "      <th>HLA</th>\n",
       "      <th>AseqCDR</th>\n",
       "      <th>BseqCDR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V350085868_L01_502</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>TSESDYY_QEAYKQQN_CAYRAYMEYGNKLVF</td>\n",
       "      <td>SGHTA_FQGNSA_CASSLGVYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V350085868_L01_505</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>NIATNDY_GYKTK_CLVGTNSNSGYALNF</td>\n",
       "      <td>LGHNT_FRNRAP_CASGSPDRFEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V350085868_L01_506</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>ATGYPS_ATKADDK_CALTVSYGGSQGNLIF</td>\n",
       "      <td>KGHSH_LQKENI_CASSPFSIGQGLTNNEKLFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V350085868_L01_507</td>\n",
       "      <td>SLMEQIPHL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>TSGFNG_NVLDGL_CAVKMNTGFQKLVF</td>\n",
       "      <td>DFQATT_SNEGSKA_CSAKLRGSNQPQHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V350085868_L01_509</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>MNHNS_SASEGT_CASTLRAGWDEQFF</td>\n",
       "      <td>MNHEY_SMNVEV_CASSLSPGGRSEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>V350110758_L02_515</td>\n",
       "      <td>VVVGAGDVGK</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>NSMFDY_ISSIKDK_CAAAYYTGANSKLTF</td>\n",
       "      <td>SGHAT_FQNNGV_CASSSPPGVGNEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>V350110758_L02_522</td>\n",
       "      <td>VVVGAGDVGK</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>NSMFDY_ISSIKDK_CAASGTGTASKLTF</td>\n",
       "      <td>SGHNS_FNNNVP_CASETSNNEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>V350110758_L02_523</td>\n",
       "      <td>VVVGAGDVGK</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>SSNFYA_MTLNGDE_CAFYGSARQLTF</td>\n",
       "      <td>PRHDT_FYEKMQ_CASSPPTTAANYGYTF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>V350110758_L02_531</td>\n",
       "      <td>VVVGAGDVGK</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>DSAIYN_IQSSQRE_CAVRGNNNARLMF</td>\n",
       "      <td>GTSNPN_SVGIG_CAWSPGRSPLHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>V350110758_L02_532</td>\n",
       "      <td>VVVGAGDVGK</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>NSMFDY_ISSIKDK_CAASAGTYKYIF</td>\n",
       "      <td>SGHTS_YDEGEE_CASSLVSSYEQYF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cellname       NeoAA          HLA  \\\n",
       "0    V350085868_L01_502   VLLSHLSYL  HLA-A*02:01   \n",
       "1    V350085868_L01_505   VLLSHLSYL  HLA-A*02:01   \n",
       "2    V350085868_L01_506   VLLSHLSYL  HLA-A*02:01   \n",
       "3    V350085868_L01_507   SLMEQIPHL  HLA-A*02:01   \n",
       "4    V350085868_L01_509   VLLSHLSYL  HLA-A*02:01   \n",
       "..                  ...         ...          ...   \n",
       "881  V350110758_L02_515  VVVGAGDVGK  HLA-A*11:01   \n",
       "882  V350110758_L02_522  VVVGAGDVGK  HLA-A*11:01   \n",
       "883  V350110758_L02_523  VVVGAGDVGK  HLA-A*11:01   \n",
       "884  V350110758_L02_531  VVVGAGDVGK  HLA-A*11:01   \n",
       "885  V350110758_L02_532  VVVGAGDVGK  HLA-A*11:01   \n",
       "\n",
       "                              AseqCDR                            BseqCDR  \n",
       "0    TSESDYY_QEAYKQQN_CAYRAYMEYGNKLVF          SGHTA_FQGNSA_CASSLGVYEQYF  \n",
       "1       NIATNDY_GYKTK_CLVGTNSNSGYALNF         LGHNT_FRNRAP_CASGSPDRFEQYF  \n",
       "2     ATGYPS_ATKADDK_CALTVSYGGSQGNLIF  KGHSH_LQKENI_CASSPFSIGQGLTNNEKLFF  \n",
       "3        TSGFNG_NVLDGL_CAVKMNTGFQKLVF      DFQATT_SNEGSKA_CSAKLRGSNQPQHF  \n",
       "4         MNHNS_SASEGT_CASTLRAGWDEQFF       MNHEY_SMNVEV_CASSLSPGGRSEAFF  \n",
       "..                                ...                                ...  \n",
       "881    NSMFDY_ISSIKDK_CAAAYYTGANSKLTF       SGHAT_FQNNGV_CASSSPPGVGNEQFF  \n",
       "882     NSMFDY_ISSIKDK_CAASGTGTASKLTF          SGHNS_FNNNVP_CASETSNNEQFF  \n",
       "883       SSNFYA_MTLNGDE_CAFYGSARQLTF      PRHDT_FYEKMQ_CASSPPTTAANYGYTF  \n",
       "884      DSAIYN_IQSSQRE_CAVRGNNNARLMF          GTSNPN_SVGIG_CAWSPGRSPLHF  \n",
       "885       NSMFDY_ISSIKDK_CAASAGTYKYIF         SGHTS_YDEGEE_CASSLVSSYEQYF  \n",
       "\n",
       "[886 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data in df are the positive sample pair that could be used to define the affinity value as 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/wuxinchao/data/project/data/seqData/TCR-pMHC_Info_20230220.xlsx'\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>cellname</th>\n",
       "      <th>NeoGeneID</th>\n",
       "      <th>NeoAA</th>\n",
       "      <th>NeoGeneNo</th>\n",
       "      <th>chain</th>\n",
       "      <th>TCR_ID</th>\n",
       "      <th>V_segment</th>\n",
       "      <th>J_segment</th>\n",
       "      <th>TPM</th>\n",
       "      <th>...</th>\n",
       "      <th>nSeqFR3</th>\n",
       "      <th>nSeqCDR3</th>\n",
       "      <th>nSeqFR4</th>\n",
       "      <th>aaSeqFR1</th>\n",
       "      <th>aaSeqCDR1</th>\n",
       "      <th>aaSeqFR2</th>\n",
       "      <th>aaSeqCDR2</th>\n",
       "      <th>aaSeqFR3</th>\n",
       "      <th>aaSeqCDR3</th>\n",
       "      <th>aaSeqFR4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>V350085868_L01_502</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>TRA</td>\n",
       "      <td>TRAV38-2_DV8_ATAGGGCTTACATGGAA_TRAJ47</td>\n",
       "      <td>TRAV38-2_DV8*01</td>\n",
       "      <td>TRAJ47*01</td>\n",
       "      <td>1156.94</td>\n",
       "      <td>...</td>\n",
       "      <td>GCAACAGAGAATCGTTTCTCTGTGAACTTCCAGAAAGCAGCCAAAT...</td>\n",
       "      <td>TGTGCTTATAGGGCTTACATGGAATATGGAAACAAACTGGTCTTT</td>\n",
       "      <td>GGCGCAGGAACCATTCTGAGAGTCAAGTCCT</td>\n",
       "      <td>AQTVTQSQPEMSVQEAETVTLSCTYD</td>\n",
       "      <td>TSESDYY</td>\n",
       "      <td>LFWYKQPPSRQMILVIR</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>ATENRFSVNFQKAAKSFSLKISDSQLGDAAMYF</td>\n",
       "      <td>CAYRAYMEYGNKLVF</td>\n",
       "      <td>GAGTILRVKS_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>V350085868_L01_502</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>TRB</td>\n",
       "      <td>TRBV7-2_CTTAGGCGTTTACGA_TRBJ2-7</td>\n",
       "      <td>TRBV7-2*02</td>\n",
       "      <td>TRBJ2-7*01</td>\n",
       "      <td>456.834</td>\n",
       "      <td>...</td>\n",
       "      <td>CCAGACAAATCAGGGCTGCCCAGTGATCGCTTCTCTGCAGAGAGGA...</td>\n",
       "      <td>TGTGCCAGCAGCTTAGGCGTTTACGAGCAGTACTTC</td>\n",
       "      <td>GGGCCGGGCACCAGGCTCACGGTCACAG</td>\n",
       "      <td>GAGVSQSPSNKVTEKGKDVELRCDPI</td>\n",
       "      <td>SGHTA</td>\n",
       "      <td>LYWYRQRLGQGLEFLIY</td>\n",
       "      <td>FQGNSA</td>\n",
       "      <td>PDKSGLPSDRFSAERTGESVSTLTIQRTQQEDSAVYL</td>\n",
       "      <td>CASSLGVYEQYF</td>\n",
       "      <td>GPGTRLTVT_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>V350085868_L01_503</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>TRA</td>\n",
       "      <td>TRAV13-1_GCAGCCATCATGGAA_TRAJ47</td>\n",
       "      <td>TRAV13-1*01</td>\n",
       "      <td>TRAJ47*01</td>\n",
       "      <td>2585.34</td>\n",
       "      <td>...</td>\n",
       "      <td>AAGAAAGACCAACGAATTGCTGTTACATTGAACAAGACAGCCAAAC...</td>\n",
       "      <td>TGTGCAGCCATCATGGAATATGGAAACAAACTGGTCTTT</td>\n",
       "      <td>GGCGCAGGAACCATTCTGAGAGTCAAGTCCT</td>\n",
       "      <td>GENVEQHPSTLSVQEGDSAVIKCTYS</td>\n",
       "      <td>DSASNY</td>\n",
       "      <td>FPWYKQELGKGPQLIID</td>\n",
       "      <td>IRSNVGE</td>\n",
       "      <td>KKDQRIAVTLNKTAKHFSLHITETQPEDSAVYF</td>\n",
       "      <td>CAAIMEYGNKLVF</td>\n",
       "      <td>GAGTILRVKS_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>V350085868_L01_503</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>TRB</td>\n",
       "      <td>TRBV5-6_CAGCTGGACGGCGAACA_TRBJ1-1</td>\n",
       "      <td>TRBV5-6*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>269.324</td>\n",
       "      <td>...</td>\n",
       "      <td>AGACAGAGAGGCAACTTCCCTGATCGATTCTCAGGTCACCAGTTCC...</td>\n",
       "      <td>TGTGCCAGCAGCTGGACGGCGAACACTGAAGCTTTCTTT</td>\n",
       "      <td>GGACAAGGCACCAGACTCACAGTTGTAG</td>\n",
       "      <td>DAGVTQSPTHLIKTRGQQVTLRCSPK</td>\n",
       "      <td>SGHDT</td>\n",
       "      <td>VSWYQQALGQGPQFIFQ</td>\n",
       "      <td>YYEEEE</td>\n",
       "      <td>RQRGNFPDRFSGHQFPNYSSELNVNALLLGDSALYL</td>\n",
       "      <td>CASSWTANTEAFF</td>\n",
       "      <td>GQGTRLTVV_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>V350085868_L01_504</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>CRC06C1</td>\n",
       "      <td>TRA</td>\n",
       "      <td>TRAV12-3_CAATGGGAACAAC_TRAJ24</td>\n",
       "      <td>TRAV12-3*01</td>\n",
       "      <td>TRAJ24*01,TRAJ24*02</td>\n",
       "      <td>1596.76</td>\n",
       "      <td>...</td>\n",
       "      <td>AAAGAAGATGGAAGGTTTACAGCACAGGTCGATAAATCCAGCAAGT...</td>\n",
       "      <td>TGTGCAATGGGAACAACTGACAGCTGGGGGAAATTCGAGTTT</td>\n",
       "      <td>GGAGCAGGGACCCAGGTTGTGGTCACCCCAG</td>\n",
       "      <td>QKEVEQDPGPLSVPEGAIVSLNCTYS</td>\n",
       "      <td>NSAFQY</td>\n",
       "      <td>FMWYRQYSRKGPELLMY</td>\n",
       "      <td>TYSSGN</td>\n",
       "      <td>KEDGRFTAQVDKSSKYISLFIRDSQPSDSATYL</td>\n",
       "      <td>CAMGTTDSWGKFEF</td>\n",
       "      <td>GAGTQVVVTP_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>negative</td>\n",
       "      <td>V350110758_L02_592</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TRB</td>\n",
       "      <td>TRBV21-1_AGCAAGACCCTAAGATTCCTACCGATCGACCGTACGA...</td>\n",
       "      <td>TRBV21-1*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>553.09</td>\n",
       "      <td>...</td>\n",
       "      <td>ATTCAGAAAGCAGAAATAATCAATGAGCGATTTTTAGCCCAATGCT...</td>\n",
       "      <td>TGTGCCAGCAGCAAGACCCTAAGATTCCTACCGATCGACCGTACGA...</td>\n",
       "      <td>GGGCCAGGGACACGGCTCACCGTGCTAG</td>\n",
       "      <td>DTKVTQRPRLLVKASEQKAKMDCVPI</td>\n",
       "      <td>KAHSY</td>\n",
       "      <td>VYWYRKKLEEELKFLVY</td>\n",
       "      <td>FQNEEL</td>\n",
       "      <td>IQKAEIINERFLAQCSKNSSCTLEIQSTESGDTALYF</td>\n",
       "      <td>CASSKTLRF_TDRPYEQFF</td>\n",
       "      <td>GPGTRLTVL_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>negative</td>\n",
       "      <td>V350110758_L02_595</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TRA</td>\n",
       "      <td>TRAV27_CAAGGGATTT_TRAJ23</td>\n",
       "      <td>TRAV27*01</td>\n",
       "      <td>TRAJ23*01</td>\n",
       "      <td>1672.05</td>\n",
       "      <td>...</td>\n",
       "      <td>AAGAAGCTGAAGAGACTAACCTTTCAGTTTGGTGATGCAAGAAAGG...</td>\n",
       "      <td>TGTCAAGGGATTTATAACCAGGGAGGAAAGCTTATCTTC</td>\n",
       "      <td>GGACAGGGAACGGAGTTATCTGTGAAACCCA</td>\n",
       "      <td>TQLLEQSPQFLSIQEGENLTVYCNSS</td>\n",
       "      <td>SVFSS</td>\n",
       "      <td>LQWYRQEPGEGPVLLVT</td>\n",
       "      <td>VVTGGEV</td>\n",
       "      <td>KKLKRLTFQFGDARKDSSLHITAAQPGDTGLYL</td>\n",
       "      <td>CQGIYNQGGKLIF</td>\n",
       "      <td>GQGTELSVKP_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>negative</td>\n",
       "      <td>V350110758_L02_595</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TRB</td>\n",
       "      <td>TRBV2_AGTGATGGGACAGGGGCGAATGA_TRBJ2-1</td>\n",
       "      <td>TRBV2*01</td>\n",
       "      <td>TRBJ2-1*01</td>\n",
       "      <td>238.21</td>\n",
       "      <td>...</td>\n",
       "      <td>TCAGAGAAGTCTGAAATATTCGATGATCAATTCTCAGTTGAAAGGC...</td>\n",
       "      <td>TGTGCCAGCAGTGATGGGACAGGGGCGAATGAGCAGTTCTTC</td>\n",
       "      <td>GGGCCAGGGACACGGCTCACCGTGCTAG</td>\n",
       "      <td>EPEVTQTPSHQVTQMGQEVILRCVPI</td>\n",
       "      <td>SNHLY</td>\n",
       "      <td>FYWYRQILGQKVEFLVS</td>\n",
       "      <td>FYNNEI</td>\n",
       "      <td>SEKSEIFDDQFSVERPDGSNFTLKIRSTKLEDSAMYF</td>\n",
       "      <td>CASSDGTGANEQFF</td>\n",
       "      <td>GPGTRLTVL_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>negative</td>\n",
       "      <td>V350110758_L02_596</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TRA</td>\n",
       "      <td>TRAV13-2_GAATAGATTAAGAAAGATAG_TRAJ35</td>\n",
       "      <td>TRAV13-2*01</td>\n",
       "      <td>TRAJ35*01</td>\n",
       "      <td>3895.07</td>\n",
       "      <td>...</td>\n",
       "      <td>AGGCAAGGCCAAAGAGTCACCGTTTTATTGAATAAGACAGTGAAAC...</td>\n",
       "      <td>TGTGCAGAGAATAGATTAAGAAAGATAGGCTTTGGGAATGTGCTGC...</td>\n",
       "      <td>GGTCCGGCACTCAAGTGATTGTTTTACCAC</td>\n",
       "      <td>GESVGLHLPTLSVQEGDNSIINCAYS</td>\n",
       "      <td>NSASDY</td>\n",
       "      <td>FIWYKQESGKGPQFIID</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>RQGQRVTVLLNKTVKHLSLQIAATQPGDSAVYF</td>\n",
       "      <td>CAENRLRKI_ALGMCCIA</td>\n",
       "      <td>GPALK*LFYH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2813</th>\n",
       "      <td>negative</td>\n",
       "      <td>V350110758_L02_596</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>TRB</td>\n",
       "      <td>TRBV29-1_AGCGTCGCCGGACAGGGCTTAACCACTG_TRBJ1-1</td>\n",
       "      <td>TRBV29-1*01</td>\n",
       "      <td>TRBJ1-1*01</td>\n",
       "      <td>6926.11</td>\n",
       "      <td>...</td>\n",
       "      <td>ACATATGAGAGTGGATTTGTCATTGACAAGTTTCCCATCAGCCGCC...</td>\n",
       "      <td>TGCAGCGTCGCCGGACAGGGCTTAACCACTGAAGCTTTCTTT</td>\n",
       "      <td>GGACAAGGCACCAGACTCACAGTTGTAG</td>\n",
       "      <td>SAVISQKPSRDICQRGTSLTIQCQVD</td>\n",
       "      <td>SQVTM</td>\n",
       "      <td>MFWYRQQPGQSLTLIAT</td>\n",
       "      <td>ANQGSEA</td>\n",
       "      <td>TYESGFVIDKFPISRPNLTFSTLTVSNMSPEDSSIYL</td>\n",
       "      <td>CSVAGQGLTTEAFF</td>\n",
       "      <td>GQGTRLTVV_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2814 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Class            cellname NeoGeneID      NeoAA NeoGeneNo chain  \\\n",
       "0     positive  V350085868_L01_502   CRC06C1  VLLSHLSYL   CRC06C1   TRA   \n",
       "1     positive  V350085868_L01_502   CRC06C1  VLLSHLSYL   CRC06C1   TRB   \n",
       "2     positive  V350085868_L01_503   CRC06C1  VLLSHLSYL   CRC06C1   TRA   \n",
       "3     positive  V350085868_L01_503   CRC06C1  VLLSHLSYL   CRC06C1   TRB   \n",
       "4     positive  V350085868_L01_504   CRC06C1  VLLSHLSYL   CRC06C1   TRA   \n",
       "...        ...                 ...       ...        ...       ...   ...   \n",
       "2809  negative  V350110758_L02_592         -          -         -   TRB   \n",
       "2810  negative  V350110758_L02_595         -          -         -   TRA   \n",
       "2811  negative  V350110758_L02_595         -          -         -   TRB   \n",
       "2812  negative  V350110758_L02_596         -          -         -   TRA   \n",
       "2813  negative  V350110758_L02_596         -          -         -   TRB   \n",
       "\n",
       "                                                 TCR_ID        V_segment  \\\n",
       "0                 TRAV38-2_DV8_ATAGGGCTTACATGGAA_TRAJ47  TRAV38-2_DV8*01   \n",
       "1                       TRBV7-2_CTTAGGCGTTTACGA_TRBJ2-7       TRBV7-2*02   \n",
       "2                       TRAV13-1_GCAGCCATCATGGAA_TRAJ47      TRAV13-1*01   \n",
       "3                     TRBV5-6_CAGCTGGACGGCGAACA_TRBJ1-1       TRBV5-6*01   \n",
       "4                         TRAV12-3_CAATGGGAACAAC_TRAJ24      TRAV12-3*01   \n",
       "...                                                 ...              ...   \n",
       "2809  TRBV21-1_AGCAAGACCCTAAGATTCCTACCGATCGACCGTACGA...      TRBV21-1*01   \n",
       "2810                           TRAV27_CAAGGGATTT_TRAJ23        TRAV27*01   \n",
       "2811              TRBV2_AGTGATGGGACAGGGGCGAATGA_TRBJ2-1         TRBV2*01   \n",
       "2812               TRAV13-2_GAATAGATTAAGAAAGATAG_TRAJ35      TRAV13-2*01   \n",
       "2813      TRBV29-1_AGCGTCGCCGGACAGGGCTTAACCACTG_TRBJ1-1      TRBV29-1*01   \n",
       "\n",
       "                J_segment      TPM  ...  \\\n",
       "0               TRAJ47*01  1156.94  ...   \n",
       "1              TRBJ2-7*01  456.834  ...   \n",
       "2               TRAJ47*01  2585.34  ...   \n",
       "3              TRBJ1-1*01  269.324  ...   \n",
       "4     TRAJ24*01,TRAJ24*02  1596.76  ...   \n",
       "...                   ...      ...  ...   \n",
       "2809           TRBJ2-1*01   553.09  ...   \n",
       "2810            TRAJ23*01  1672.05  ...   \n",
       "2811           TRBJ2-1*01   238.21  ...   \n",
       "2812            TRAJ35*01  3895.07  ...   \n",
       "2813           TRBJ1-1*01  6926.11  ...   \n",
       "\n",
       "                                                nSeqFR3  \\\n",
       "0     GCAACAGAGAATCGTTTCTCTGTGAACTTCCAGAAAGCAGCCAAAT...   \n",
       "1     CCAGACAAATCAGGGCTGCCCAGTGATCGCTTCTCTGCAGAGAGGA...   \n",
       "2     AAGAAAGACCAACGAATTGCTGTTACATTGAACAAGACAGCCAAAC...   \n",
       "3     AGACAGAGAGGCAACTTCCCTGATCGATTCTCAGGTCACCAGTTCC...   \n",
       "4     AAAGAAGATGGAAGGTTTACAGCACAGGTCGATAAATCCAGCAAGT...   \n",
       "...                                                 ...   \n",
       "2809  ATTCAGAAAGCAGAAATAATCAATGAGCGATTTTTAGCCCAATGCT...   \n",
       "2810  AAGAAGCTGAAGAGACTAACCTTTCAGTTTGGTGATGCAAGAAAGG...   \n",
       "2811  TCAGAGAAGTCTGAAATATTCGATGATCAATTCTCAGTTGAAAGGC...   \n",
       "2812  AGGCAAGGCCAAAGAGTCACCGTTTTATTGAATAAGACAGTGAAAC...   \n",
       "2813  ACATATGAGAGTGGATTTGTCATTGACAAGTTTCCCATCAGCCGCC...   \n",
       "\n",
       "                                               nSeqCDR3  \\\n",
       "0         TGTGCTTATAGGGCTTACATGGAATATGGAAACAAACTGGTCTTT   \n",
       "1                  TGTGCCAGCAGCTTAGGCGTTTACGAGCAGTACTTC   \n",
       "2               TGTGCAGCCATCATGGAATATGGAAACAAACTGGTCTTT   \n",
       "3               TGTGCCAGCAGCTGGACGGCGAACACTGAAGCTTTCTTT   \n",
       "4            TGTGCAATGGGAACAACTGACAGCTGGGGGAAATTCGAGTTT   \n",
       "...                                                 ...   \n",
       "2809  TGTGCCAGCAGCAAGACCCTAAGATTCCTACCGATCGACCGTACGA...   \n",
       "2810            TGTCAAGGGATTTATAACCAGGGAGGAAAGCTTATCTTC   \n",
       "2811         TGTGCCAGCAGTGATGGGACAGGGGCGAATGAGCAGTTCTTC   \n",
       "2812  TGTGCAGAGAATAGATTAAGAAAGATAGGCTTTGGGAATGTGCTGC...   \n",
       "2813         TGCAGCGTCGCCGGACAGGGCTTAACCACTGAAGCTTTCTTT   \n",
       "\n",
       "                              nSeqFR4                    aaSeqFR1 aaSeqCDR1  \\\n",
       "0     GGCGCAGGAACCATTCTGAGAGTCAAGTCCT  AQTVTQSQPEMSVQEAETVTLSCTYD   TSESDYY   \n",
       "1        GGGCCGGGCACCAGGCTCACGGTCACAG  GAGVSQSPSNKVTEKGKDVELRCDPI     SGHTA   \n",
       "2     GGCGCAGGAACCATTCTGAGAGTCAAGTCCT  GENVEQHPSTLSVQEGDSAVIKCTYS    DSASNY   \n",
       "3        GGACAAGGCACCAGACTCACAGTTGTAG  DAGVTQSPTHLIKTRGQQVTLRCSPK     SGHDT   \n",
       "4     GGAGCAGGGACCCAGGTTGTGGTCACCCCAG  QKEVEQDPGPLSVPEGAIVSLNCTYS    NSAFQY   \n",
       "...                               ...                         ...       ...   \n",
       "2809     GGGCCAGGGACACGGCTCACCGTGCTAG  DTKVTQRPRLLVKASEQKAKMDCVPI     KAHSY   \n",
       "2810  GGACAGGGAACGGAGTTATCTGTGAAACCCA  TQLLEQSPQFLSIQEGENLTVYCNSS     SVFSS   \n",
       "2811     GGGCCAGGGACACGGCTCACCGTGCTAG  EPEVTQTPSHQVTQMGQEVILRCVPI     SNHLY   \n",
       "2812   GGTCCGGCACTCAAGTGATTGTTTTACCAC  GESVGLHLPTLSVQEGDNSIINCAYS    NSASDY   \n",
       "2813     GGACAAGGCACCAGACTCACAGTTGTAG  SAVISQKPSRDICQRGTSLTIQCQVD     SQVTM   \n",
       "\n",
       "               aaSeqFR2 aaSeqCDR2                               aaSeqFR3  \\\n",
       "0     LFWYKQPPSRQMILVIR  QEAYKQQN      ATENRFSVNFQKAAKSFSLKISDSQLGDAAMYF   \n",
       "1     LYWYRQRLGQGLEFLIY    FQGNSA  PDKSGLPSDRFSAERTGESVSTLTIQRTQQEDSAVYL   \n",
       "2     FPWYKQELGKGPQLIID   IRSNVGE      KKDQRIAVTLNKTAKHFSLHITETQPEDSAVYF   \n",
       "3     VSWYQQALGQGPQFIFQ    YYEEEE   RQRGNFPDRFSGHQFPNYSSELNVNALLLGDSALYL   \n",
       "4     FMWYRQYSRKGPELLMY    TYSSGN      KEDGRFTAQVDKSSKYISLFIRDSQPSDSATYL   \n",
       "...                 ...       ...                                    ...   \n",
       "2809  VYWYRKKLEEELKFLVY    FQNEEL  IQKAEIINERFLAQCSKNSSCTLEIQSTESGDTALYF   \n",
       "2810  LQWYRQEPGEGPVLLVT   VVTGGEV      KKLKRLTFQFGDARKDSSLHITAAQPGDTGLYL   \n",
       "2811  FYWYRQILGQKVEFLVS    FYNNEI  SEKSEIFDDQFSVERPDGSNFTLKIRSTKLEDSAMYF   \n",
       "2812  FIWYKQESGKGPQFIID   IRSNMDK      RQGQRVTVLLNKTVKHLSLQIAATQPGDSAVYF   \n",
       "2813  MFWYRQQPGQSLTLIAT   ANQGSEA  TYESGFVIDKFPISRPNLTFSTLTVSNMSPEDSSIYL   \n",
       "\n",
       "                aaSeqCDR3     aaSeqFR4  \n",
       "0         CAYRAYMEYGNKLVF  GAGTILRVKS_  \n",
       "1            CASSLGVYEQYF   GPGTRLTVT_  \n",
       "2           CAAIMEYGNKLVF  GAGTILRVKS_  \n",
       "3           CASSWTANTEAFF   GQGTRLTVV_  \n",
       "4          CAMGTTDSWGKFEF  GAGTQVVVTP_  \n",
       "...                   ...          ...  \n",
       "2809  CASSKTLRF_TDRPYEQFF   GPGTRLTVL_  \n",
       "2810        CQGIYNQGGKLIF  GQGTELSVKP_  \n",
       "2811       CASSDGTGANEQFF   GPGTRLTVL_  \n",
       "2812   CAENRLRKI_ALGMCCIA   GPALK*LFYH  \n",
       "2813       CSVAGQGLTTEAFF   GQGTRLTVV_  \n",
       "\n",
       "[2814 rows x 29 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.columns # 'Class', 'cellname', 'NeoGeneID', 'NeoAA', 'NeoGeneNo', 'chain',\n",
    "    #    'TCR_ID', 'V_segment', 'J_segment', 'TPM', 'libraryID', 'TCRnt', 'HLA',\n",
    "    #    'nSeqHLA', 'aaSeqHLA', 'nSeqFR1', 'nSeqCDR1', 'nSeqFR2', 'nSeqCDR2',\n",
    "    #    'nSeqFR3', 'nSeqCDR3', 'nSeqFR4', 'aaSeqFR1', 'aaSeqCDR1', 'aaSeqFR2',\n",
    "    #    'aaSeqCDR2', 'aaSeqFR3', 'aaSeqCDR3', 'aaSeqFR4'\n",
    "# df[\"NeoGeneNo\"].unique()\n",
    "# len(df[\"aaSeqHLA\"].unique()) # 23\n",
    "# len(df[\"aaSeqHLA\"].unique().max()) # 362\n",
    "# df[\"aaSeqHLA\"].describe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = df.loc[df['Class'] == 'positive'] # 2296 samples of positive data\n",
    "neg_data = df.loc[df['Class'] == 'negative'] # 518 samples of negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Class\", \"cellname\", \"NeoAA\", \"chain\", \"HLA\", \"aaSeqCDR1\", \"aaSeqCDR2\", \"aaSeqCDR3\"]]\n",
    "pos_data = df.loc[df['Class'] == 'positive'] # 2296 samples of positive data\n",
    "neg_data = df.loc[df['Class'] == 'negative'] # 518 samples of negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/230220.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['cellname', \"chain\"])\n",
    "# extract the NeoAA, HLA, and aaSeqCDR columns\n",
    "df = df[[\"NeoAA\", \"HLA\", \"aaSeqCDR1\", \"aaSeqCDR2\", \"aaSeqCDR3\", \"Class\"]]\n",
    "df[\"aaSeqCDR\"] = df[df.columns[2:-1]].apply(\n",
    "    # lambda x: x[0] + 'X' * (7 - len(x[0])) + x[1] + x[2],\n",
    "    lambda x: '_'.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1919009/2937025875.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_a[\"AseqCDR\"] = df_a[\"aaSeqCDR\"]\n",
      "/tmp/ipykernel_1919009/2937025875.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_a.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n",
      "/tmp/ipykernel_1919009/2937025875.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_b[\"BseqCDR\"] = df_b[\"aaSeqCDR\"]\n",
      "/tmp/ipykernel_1919009/2937025875.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_b.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NeoAA</th>\n",
       "      <th>HLA</th>\n",
       "      <th>Class</th>\n",
       "      <th>AseqCDR</th>\n",
       "      <th>BseqCDR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cellname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_502</th>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>TSESDYY_QEAYKQQN_CAYRAYMEYGNKLVF</td>\n",
       "      <td>SGHTA_FQGNSA_CASSLGVYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_503</th>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>DSASNY_IRSNVGE_CAAIMEYGNKLVF</td>\n",
       "      <td>SGHDT_YYEEEE_CASSWTANTEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_504</th>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>NSAFQY_TYSSGN_CAMGTTDSWGKFEF</td>\n",
       "      <td>DFQATT_SNEGSKA_CSVRNGYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_505</th>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>NIATNDY_GYKTK_CLVGTNSNSGYALNF</td>\n",
       "      <td>LGHNT_FRNRAP_CASGSPDRFEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_506</th>\n",
       "      <td>VLLSHLSYL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>ATGYPS_ATKADDK_CALTVSYGGSQGNLIF</td>\n",
       "      <td>KGHSH_LQKENI_CASSPFSIGQGLTNNEKLFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_589</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>DRGSQS_IYSNGD_CAVSRYSTLTF</td>\n",
       "      <td>DFQATT_SNEGSKA_CSASTVNTEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_591</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>DSSSTY_IFSNMDM_CAESLTGGFKTIF</td>\n",
       "      <td>KAHSY_FQNEEL_CASSKTLRF_TDRPYEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_592</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>DSVNN_IPSGT_CAVISNFGNEKLTF</td>\n",
       "      <td>KAHSY_FQNEEL_CASSKTLRF_TDRPYEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_595</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>SVFSS_VVTGGEV_CQGIYNQGGKLIF</td>\n",
       "      <td>SNHLY_FYNNEI_CASSDGTGANEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_596</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>NSASDY_IRSNMDK_CAENRLRKI_ALGMCCIA</td>\n",
       "      <td>SQVTM_ANQGSEA_CSVAGQGLTTEAFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1413 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        NeoAA          HLA     Class  \\\n",
       "cellname                                               \n",
       "V350085868_L01_502  VLLSHLSYL  HLA-A*02:01  positive   \n",
       "V350085868_L01_503  VLLSHLSYL  HLA-A*02:01  positive   \n",
       "V350085868_L01_504  VLLSHLSYL  HLA-A*02:01  positive   \n",
       "V350085868_L01_505  VLLSHLSYL  HLA-A*02:01  positive   \n",
       "V350085868_L01_506  VLLSHLSYL  HLA-A*02:01  positive   \n",
       "...                       ...          ...       ...   \n",
       "V350110758_L02_589          -            -  negative   \n",
       "V350110758_L02_591          -            -  negative   \n",
       "V350110758_L02_592          -            -  negative   \n",
       "V350110758_L02_595          -            -  negative   \n",
       "V350110758_L02_596          -            -  negative   \n",
       "\n",
       "                                              AseqCDR  \\\n",
       "cellname                                                \n",
       "V350085868_L01_502   TSESDYY_QEAYKQQN_CAYRAYMEYGNKLVF   \n",
       "V350085868_L01_503       DSASNY_IRSNVGE_CAAIMEYGNKLVF   \n",
       "V350085868_L01_504       NSAFQY_TYSSGN_CAMGTTDSWGKFEF   \n",
       "V350085868_L01_505      NIATNDY_GYKTK_CLVGTNSNSGYALNF   \n",
       "V350085868_L01_506    ATGYPS_ATKADDK_CALTVSYGGSQGNLIF   \n",
       "...                                               ...   \n",
       "V350110758_L02_589          DRGSQS_IYSNGD_CAVSRYSTLTF   \n",
       "V350110758_L02_591       DSSSTY_IFSNMDM_CAESLTGGFKTIF   \n",
       "V350110758_L02_592         DSVNN_IPSGT_CAVISNFGNEKLTF   \n",
       "V350110758_L02_595        SVFSS_VVTGGEV_CQGIYNQGGKLIF   \n",
       "V350110758_L02_596  NSASDY_IRSNMDK_CAENRLRKI_ALGMCCIA   \n",
       "\n",
       "                                              BseqCDR  \n",
       "cellname                                               \n",
       "V350085868_L01_502          SGHTA_FQGNSA_CASSLGVYEQYF  \n",
       "V350085868_L01_503         SGHDT_YYEEEE_CASSWTANTEAFF  \n",
       "V350085868_L01_504         DFQATT_SNEGSKA_CSVRNGYEQYF  \n",
       "V350085868_L01_505         LGHNT_FRNRAP_CASGSPDRFEQYF  \n",
       "V350085868_L01_506  KGHSH_LQKENI_CASSPFSIGQGLTNNEKLFF  \n",
       "...                                               ...  \n",
       "V350110758_L02_589        DFQATT_SNEGSKA_CSASTVNTEAFF  \n",
       "V350110758_L02_591   KAHSY_FQNEEL_CASSKTLRF_TDRPYEQFF  \n",
       "V350110758_L02_592   KAHSY_FQNEEL_CASSKTLRF_TDRPYEQFF  \n",
       "V350110758_L02_595        SNHLY_FYNNEI_CASSDGTGANEQFF  \n",
       "V350110758_L02_596       SQVTM_ANQGSEA_CSVAGQGLTTEAFF  \n",
       "\n",
       "[1413 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "df_a = df.loc[idx[:,\"TRA\"],]\n",
    "df_a[\"AseqCDR\"] = df_a[\"aaSeqCDR\"]\n",
    "df_a.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n",
    "# drop the chain index\n",
    "df_a.index = df_a.index.droplevel(1)\n",
    "# print(df_a)\n",
    "df_b = df.loc[idx[:,\"TRB\"],]\n",
    "df_b[\"BseqCDR\"] = df_b[\"aaSeqCDR\"]\n",
    "df_b.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n",
    "# drop the chain index\n",
    "df_b.index = df_b.index.droplevel(1)\n",
    "# print(df_b)\n",
    "\n",
    "# merge the TRA and TRB dataframes by cellname, HLAs, and NeoAA\n",
    "df_ab = pd.merge(df_a, df_b, on=[\"cellname\", \"HLA\", \"NeoAA\", \"Class\"])\n",
    "df_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Neo\"] = df[\"NeoAA\"].str.slice(0,3) + \"_\" + df[\"NeoAA\"].str.slice(-4,-1)\n",
    "df.drop(columns=[\"NeoAA\"], inplace=True)\n",
    "for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "    df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "    df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "    df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "    df.drop(columns=[chain], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HLA</th>\n",
       "      <th>Class</th>\n",
       "      <th>Neo</th>\n",
       "      <th>AseqCDR_1</th>\n",
       "      <th>AseqCDR_2</th>\n",
       "      <th>AseqCDR_3</th>\n",
       "      <th>BseqCDR_1</th>\n",
       "      <th>BseqCDR_2</th>\n",
       "      <th>BseqCDR_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cellname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_502</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>TSESDYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>CAYRAYMEYGNKLVF</td>\n",
       "      <td>SGHTA</td>\n",
       "      <td>FQGNSA</td>\n",
       "      <td>CASSLGVYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_503</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>DSASNY</td>\n",
       "      <td>IRSNVGE</td>\n",
       "      <td>CAAIMEYGNKLVF</td>\n",
       "      <td>SGHDT</td>\n",
       "      <td>YYEEEE</td>\n",
       "      <td>CASSWTANTEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_504</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>NSAFQY</td>\n",
       "      <td>TYSSGN</td>\n",
       "      <td>CAMGTTDSWGKFEF</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>CSVRNGYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_505</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>NIATNDY</td>\n",
       "      <td>GYKTK</td>\n",
       "      <td>CLVGTNSNSGYALNF</td>\n",
       "      <td>LGHNT</td>\n",
       "      <td>FRNRAP</td>\n",
       "      <td>CASGSPDRFEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_506</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>ATGYPS</td>\n",
       "      <td>ATKADDK</td>\n",
       "      <td>CALTVSYGGSQGNLIF</td>\n",
       "      <td>KGHSH</td>\n",
       "      <td>LQKENI</td>\n",
       "      <td>CASSPFSIGQGLTNNEKLFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_589</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>DRGSQS</td>\n",
       "      <td>IYSNGD</td>\n",
       "      <td>CAVSRYSTLTF</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>CSASTVNTEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_591</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>DSSSTY</td>\n",
       "      <td>IFSNMDM</td>\n",
       "      <td>CAESLTGGFKTIF</td>\n",
       "      <td>KAHSY</td>\n",
       "      <td>FQNEEL</td>\n",
       "      <td>CASSKTLRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_592</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>DSVNN</td>\n",
       "      <td>IPSGT</td>\n",
       "      <td>CAVISNFGNEKLTF</td>\n",
       "      <td>KAHSY</td>\n",
       "      <td>FQNEEL</td>\n",
       "      <td>CASSKTLRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_595</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>SVFSS</td>\n",
       "      <td>VVTGGEV</td>\n",
       "      <td>CQGIYNQGGKLIF</td>\n",
       "      <td>SNHLY</td>\n",
       "      <td>FYNNEI</td>\n",
       "      <td>CASSDGTGANEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_596</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>CAENRLRKI</td>\n",
       "      <td>SQVTM</td>\n",
       "      <td>ANQGSEA</td>\n",
       "      <td>CSVAGQGLTTEAFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1413 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            HLA     Class      Neo AseqCDR_1 AseqCDR_2  \\\n",
       "cellname                                                                 \n",
       "V350085868_L01_502  HLA-A*02:01  positive  VLL_LSY   TSESDYY  QEAYKQQN   \n",
       "V350085868_L01_503  HLA-A*02:01  positive  VLL_LSY    DSASNY   IRSNVGE   \n",
       "V350085868_L01_504  HLA-A*02:01  positive  VLL_LSY    NSAFQY    TYSSGN   \n",
       "V350085868_L01_505  HLA-A*02:01  positive  VLL_LSY   NIATNDY     GYKTK   \n",
       "V350085868_L01_506  HLA-A*02:01  positive  VLL_LSY    ATGYPS   ATKADDK   \n",
       "...                         ...       ...      ...       ...       ...   \n",
       "V350110758_L02_589            -  negative       -_    DRGSQS    IYSNGD   \n",
       "V350110758_L02_591            -  negative       -_    DSSSTY   IFSNMDM   \n",
       "V350110758_L02_592            -  negative       -_     DSVNN     IPSGT   \n",
       "V350110758_L02_595            -  negative       -_     SVFSS   VVTGGEV   \n",
       "V350110758_L02_596            -  negative       -_    NSASDY   IRSNMDK   \n",
       "\n",
       "                           AseqCDR_3 BseqCDR_1 BseqCDR_2             BseqCDR_3  \n",
       "cellname                                                                        \n",
       "V350085868_L01_502   CAYRAYMEYGNKLVF     SGHTA    FQGNSA          CASSLGVYEQYF  \n",
       "V350085868_L01_503     CAAIMEYGNKLVF     SGHDT    YYEEEE         CASSWTANTEAFF  \n",
       "V350085868_L01_504    CAMGTTDSWGKFEF    DFQATT   SNEGSKA           CSVRNGYEQYF  \n",
       "V350085868_L01_505   CLVGTNSNSGYALNF     LGHNT    FRNRAP         CASGSPDRFEQYF  \n",
       "V350085868_L01_506  CALTVSYGGSQGNLIF     KGHSH    LQKENI  CASSPFSIGQGLTNNEKLFF  \n",
       "...                              ...       ...       ...                   ...  \n",
       "V350110758_L02_589       CAVSRYSTLTF    DFQATT   SNEGSKA          CSASTVNTEAFF  \n",
       "V350110758_L02_591     CAESLTGGFKTIF     KAHSY    FQNEEL             CASSKTLRF  \n",
       "V350110758_L02_592    CAVISNFGNEKLTF     KAHSY    FQNEEL             CASSKTLRF  \n",
       "V350110758_L02_595     CQGIYNQGGKLIF     SNHLY    FYNNEI        CASSDGTGANEQFF  \n",
       "V350110758_L02_596         CAENRLRKI     SQVTM   ANQGSEA        CSVAGQGLTTEAFF  \n",
       "\n",
       "[1413 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some negative sample by randomly mutant the aa of CDR3 sequence\n",
    "df_ng = df.copy()\n",
    "df_ng[\"Class\"] = \"negative\"\n",
    "df_ng[\"AseqCDR_3\"] = df_ng[\"AseqCDR_3\"].apply(lambda x: random.choice(list(set(df[\"AseqCDR_3\"]) - set(x))))\n",
    "df_ng[\"BseqCDR_3\"] = df_ng[\"BseqCDR_3\"].apply(lambda x: random.choice(list(set(df[\"BseqCDR_3\"]) - set(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HLA</th>\n",
       "      <th>Class</th>\n",
       "      <th>Neo</th>\n",
       "      <th>AseqCDR_1</th>\n",
       "      <th>AseqCDR_2</th>\n",
       "      <th>AseqCDR_3</th>\n",
       "      <th>BseqCDR_1</th>\n",
       "      <th>BseqCDR_2</th>\n",
       "      <th>BseqCDR_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cellname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_502</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>negative</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>TSESDYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>CAVRRGGADGLTF</td>\n",
       "      <td>SGHTA</td>\n",
       "      <td>FQGNSA</td>\n",
       "      <td>CASSGGPRQPQHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_503</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>negative</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>DSASNY</td>\n",
       "      <td>IRSNVGE</td>\n",
       "      <td>AVGETGANNLF</td>\n",
       "      <td>SGHDT</td>\n",
       "      <td>YYEEEE</td>\n",
       "      <td>CASNPPGGDTEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_504</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>negative</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>NSAFQY</td>\n",
       "      <td>TYSSGN</td>\n",
       "      <td>CIPLRRRF</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>CASSPLGSNQPQHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_505</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>negative</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>NIATNDY</td>\n",
       "      <td>GYKTK</td>\n",
       "      <td>CAVSSNQAGTALIF</td>\n",
       "      <td>LGHNT</td>\n",
       "      <td>FRNRAP</td>\n",
       "      <td>CASEDSSYNEQFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_506</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>negative</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>ATGYPS</td>\n",
       "      <td>ATKADDK</td>\n",
       "      <td>CVVSDRGDMRF</td>\n",
       "      <td>KGHSH</td>\n",
       "      <td>LQKENI</td>\n",
       "      <td>ASSSWDTGELF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_589</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>DRGSQS</td>\n",
       "      <td>IYSNGD</td>\n",
       "      <td>CAMRESGAGSYQLTF</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>CASSYSRMNTEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_591</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>DSSSTY</td>\n",
       "      <td>IFSNMDM</td>\n",
       "      <td>CGTESSQGGSQGNLIF</td>\n",
       "      <td>KAHSY</td>\n",
       "      <td>FQNEEL</td>\n",
       "      <td>CASSLTLTSRYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_592</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>DSVNN</td>\n",
       "      <td>IPSGT</td>\n",
       "      <td>CAVRDGGGFKTIF</td>\n",
       "      <td>KAHSY</td>\n",
       "      <td>FQNEEL</td>\n",
       "      <td>CASHIAGAYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_595</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>SVFSS</td>\n",
       "      <td>VVTGGEV</td>\n",
       "      <td>CAVGGVPGGGSQGNLIF</td>\n",
       "      <td>SNHLY</td>\n",
       "      <td>FYNNEI</td>\n",
       "      <td>CSVHLAGGPYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350110758_L02_596</th>\n",
       "      <td>-</td>\n",
       "      <td>negative</td>\n",
       "      <td>-_</td>\n",
       "      <td>NSASDY</td>\n",
       "      <td>IRSNMDK</td>\n",
       "      <td>AVRMDSSYKLI</td>\n",
       "      <td>SQVTM</td>\n",
       "      <td>ANQGSEA</td>\n",
       "      <td>CASSSNPGLSGNTIYF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1413 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            HLA     Class      Neo AseqCDR_1 AseqCDR_2  \\\n",
       "cellname                                                                 \n",
       "V350085868_L01_502  HLA-A*02:01  negative  VLL_LSY   TSESDYY  QEAYKQQN   \n",
       "V350085868_L01_503  HLA-A*02:01  negative  VLL_LSY    DSASNY   IRSNVGE   \n",
       "V350085868_L01_504  HLA-A*02:01  negative  VLL_LSY    NSAFQY    TYSSGN   \n",
       "V350085868_L01_505  HLA-A*02:01  negative  VLL_LSY   NIATNDY     GYKTK   \n",
       "V350085868_L01_506  HLA-A*02:01  negative  VLL_LSY    ATGYPS   ATKADDK   \n",
       "...                         ...       ...      ...       ...       ...   \n",
       "V350110758_L02_589            -  negative       -_    DRGSQS    IYSNGD   \n",
       "V350110758_L02_591            -  negative       -_    DSSSTY   IFSNMDM   \n",
       "V350110758_L02_592            -  negative       -_     DSVNN     IPSGT   \n",
       "V350110758_L02_595            -  negative       -_     SVFSS   VVTGGEV   \n",
       "V350110758_L02_596            -  negative       -_    NSASDY   IRSNMDK   \n",
       "\n",
       "                            AseqCDR_3 BseqCDR_1 BseqCDR_2         BseqCDR_3  \n",
       "cellname                                                                     \n",
       "V350085868_L01_502      CAVRRGGADGLTF     SGHTA    FQGNSA     CASSGGPRQPQHF  \n",
       "V350085868_L01_503        AVGETGANNLF     SGHDT    YYEEEE    CASNPPGGDTEAFF  \n",
       "V350085868_L01_504           CIPLRRRF    DFQATT   SNEGSKA    CASSPLGSNQPQHF  \n",
       "V350085868_L01_505     CAVSSNQAGTALIF     LGHNT    FRNRAP     CASEDSSYNEQFF  \n",
       "V350085868_L01_506        CVVSDRGDMRF     KGHSH    LQKENI       ASSSWDTGELF  \n",
       "...                               ...       ...       ...               ...  \n",
       "V350110758_L02_589    CAMRESGAGSYQLTF    DFQATT   SNEGSKA    CASSYSRMNTEAFF  \n",
       "V350110758_L02_591   CGTESSQGGSQGNLIF     KAHSY    FQNEEL   CASSLTLTSRYEQYF  \n",
       "V350110758_L02_592      CAVRDGGGFKTIF     KAHSY    FQNEEL     CASHIAGAYEQYF  \n",
       "V350110758_L02_595  CAVGGVPGGGSQGNLIF     SNHLY    FYNNEI    CSVHLAGGPYEQYF  \n",
       "V350110758_L02_596        AVRMDSSYKLI     SQVTM   ANQGSEA  CASSSNPGLSGNTIYF  \n",
       "\n",
       "[1413 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(df[\"AseqCDR_3\"].unique()) \n",
    "df_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df[df[\"Class\"] == \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HLA</th>\n",
       "      <th>Class</th>\n",
       "      <th>Neo</th>\n",
       "      <th>AseqCDR_1</th>\n",
       "      <th>AseqCDR_2</th>\n",
       "      <th>AseqCDR_3</th>\n",
       "      <th>BseqCDR_1</th>\n",
       "      <th>BseqCDR_2</th>\n",
       "      <th>BseqCDR_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cellname</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_502</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>TSESDYY</td>\n",
       "      <td>QEAYKQQN</td>\n",
       "      <td>CAYRAYMEYGNKLVF</td>\n",
       "      <td>SGHTA</td>\n",
       "      <td>FQGNSA</td>\n",
       "      <td>CASSLGVYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_503</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>DSASNY</td>\n",
       "      <td>IRSNVGE</td>\n",
       "      <td>CAAIMEYGNKLVF</td>\n",
       "      <td>SGHDT</td>\n",
       "      <td>YYEEEE</td>\n",
       "      <td>CASSWTANTEAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_504</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>NSAFQY</td>\n",
       "      <td>TYSSGN</td>\n",
       "      <td>CAMGTTDSWGKFEF</td>\n",
       "      <td>DFQATT</td>\n",
       "      <td>SNEGSKA</td>\n",
       "      <td>CSVRNGYEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_505</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>NIATNDY</td>\n",
       "      <td>GYKTK</td>\n",
       "      <td>CLVGTNSNSGYALNF</td>\n",
       "      <td>LGHNT</td>\n",
       "      <td>FRNRAP</td>\n",
       "      <td>CASGSPDRFEQYF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V350085868_L01_506</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>VLL_LSY</td>\n",
       "      <td>ATGYPS</td>\n",
       "      <td>ATKADDK</td>\n",
       "      <td>CALTVSYGGSQGNLIF</td>\n",
       "      <td>KGHSH</td>\n",
       "      <td>LQKENI</td>\n",
       "      <td>CASSPFSIGQGLTNNEKLFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A6</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>LLF_PVA</td>\n",
       "      <td>DRGSQS</td>\n",
       "      <td>IYSNGD</td>\n",
       "      <td>AVTTDSWGKLQ</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SVGAGI</td>\n",
       "      <td>ASRPGLAGGRPEQY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>ELA_ILT</td>\n",
       "      <td>FLGSQS</td>\n",
       "      <td>TYREGD</td>\n",
       "      <td>AVNDGGRLT</td>\n",
       "      <td>GTSNPN</td>\n",
       "      <td>WGPFG</td>\n",
       "      <td>AWSETGLGMGGWQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>ELA_ILT</td>\n",
       "      <td>FLGSQS</td>\n",
       "      <td>TYREGD</td>\n",
       "      <td>AVNDGGRLT</td>\n",
       "      <td>GTSNPN</td>\n",
       "      <td>WGPFG</td>\n",
       "      <td>AWSETGLGMGGWQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>ELA_ALT</td>\n",
       "      <td>FLGSQS</td>\n",
       "      <td>TYREGD</td>\n",
       "      <td>AVNDGGRLT</td>\n",
       "      <td>GTSNPN</td>\n",
       "      <td>WGPFG</td>\n",
       "      <td>AWSETGLGMGGWQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A6</th>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>positive</td>\n",
       "      <td>LLF_AVY</td>\n",
       "      <td>DRGSQS</td>\n",
       "      <td>IYSNGD</td>\n",
       "      <td>AVTTDSWGKLQ</td>\n",
       "      <td>MNHEY</td>\n",
       "      <td>SVGAGI</td>\n",
       "      <td>ASRPGLAGGRPEQY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1154 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            HLA     Class      Neo AseqCDR_1 AseqCDR_2  \\\n",
       "cellname                                                                 \n",
       "V350085868_L01_502  HLA-A*02:01  positive  VLL_LSY   TSESDYY  QEAYKQQN   \n",
       "V350085868_L01_503  HLA-A*02:01  positive  VLL_LSY    DSASNY   IRSNVGE   \n",
       "V350085868_L01_504  HLA-A*02:01  positive  VLL_LSY    NSAFQY    TYSSGN   \n",
       "V350085868_L01_505  HLA-A*02:01  positive  VLL_LSY   NIATNDY     GYKTK   \n",
       "V350085868_L01_506  HLA-A*02:01  positive  VLL_LSY    ATGYPS   ATKADDK   \n",
       "...                         ...       ...      ...       ...       ...   \n",
       "A6                  HLA-A*02:01  positive  LLF_PVA    DRGSQS    IYSNGD   \n",
       "2417              HLA-A*02:01  positive  ELA_ILT    FLGSQS    TYREGD   \n",
       "2417              HLA-A*02:01  positive  ELA_ILT    FLGSQS    TYREGD   \n",
       "2417              HLA-A*02:01  positive  ELA_ALT    FLGSQS    TYREGD   \n",
       "A6                  HLA-A*02:01  positive  LLF_AVY    DRGSQS    IYSNGD   \n",
       "\n",
       "                           AseqCDR_3 BseqCDR_1 BseqCDR_2             BseqCDR_3  \n",
       "cellname                                                                        \n",
       "V350085868_L01_502   CAYRAYMEYGNKLVF     SGHTA    FQGNSA          CASSLGVYEQYF  \n",
       "V350085868_L01_503     CAAIMEYGNKLVF     SGHDT    YYEEEE         CASSWTANTEAFF  \n",
       "V350085868_L01_504    CAMGTTDSWGKFEF    DFQATT   SNEGSKA           CSVRNGYEQYF  \n",
       "V350085868_L01_505   CLVGTNSNSGYALNF     LGHNT    FRNRAP         CASGSPDRFEQYF  \n",
       "V350085868_L01_506  CALTVSYGGSQGNLIF     KGHSH    LQKENI  CASSPFSIGQGLTNNEKLFF  \n",
       "...                              ...       ...       ...                   ...  \n",
       "A6                       AVTTDSWGKLQ     MNHEY    SVGAGI        ASRPGLAGGRPEQY  \n",
       "2417                     AVNDGGRLT    GTSNPN     WGPFG         AWSETGLGMGGWQ  \n",
       "2417                     AVNDGGRLT    GTSNPN     WGPFG         AWSETGLGMGGWQ  \n",
       "2417                     AVNDGGRLT    GTSNPN     WGPFG         AWSETGLGMGGWQ  \n",
       "A6                       AVTTDSWGKLQ     MNHEY    SVGAGI        ASRPGLAGGRPEQY  \n",
       "\n",
       "[1154 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HLA-A*11:01                  988\n",
       "HLA-A*02:01                  121\n",
       "HLA-A*24:02                    6\n",
       "HLA-B*35:01                    6\n",
       "HLA-B*35:08                    5\n",
       "HLA-DQA1*03:01/DQB1*03:02      3\n",
       "HLA-DRB1*04:01                 3\n",
       "HLA-B*44:05                    2\n",
       "HLA-DQA1*05:01/DQB1*02:01      2\n",
       "HLA-B*07:02                    2\n",
       "HLA-DQA1*01:02/DQB1*05:02      2\n",
       "HLA-DRA*01:01/DRB1*04:01       1\n",
       "HLA-DQA1*05:01/DQB1*03:02      1\n",
       "HLA-DQA1*02:01/DQB1*02:02      1\n",
       "HLA-DPB1*02:01                 1\n",
       "HLA-DQ1                        1\n",
       "HLA-B*51:01                    1\n",
       "HLA-DRB1*01:01                 1\n",
       "HLA-DRA*01:01/DRB5*01:01       1\n",
       "HLA-A*01:01                    1\n",
       "HLA-A*02:01 T163A mutant       1\n",
       "HLA-A*02:01 W167A mutant       1\n",
       "HLA-A*02:01 K66A mutant        1\n",
       "HLA-B*08:01                    1\n",
       "HLA-DRA*01:01/DRB1*15:01       1\n",
       "Name: HLA, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[[\"HLA\", \"Neo\"]].value_counts()\n",
    "# df[\"Neo\"].value_counts()\n",
    "df_pos[\"HLA\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build NSP task dataset\n",
    "\n",
    "def read_data(file_path = None):\n",
    "    '''\n",
    "    Read the data from the csv file, and each line is a sample.\n",
    "    Column 1: the name of the sample\n",
    "    Column 2: the sequence of the neo-antigen\n",
    "    Column 3: the classification of HLA\n",
    "    Column 4: the sequence of the TCR alpha chain (only CDR3)\n",
    "    Column 5: the sequence of the TCR beta chain (only CDR3)\n",
    "    After the processing, we would get a one-dimensional list\n",
    "    '''\n",
    "    df = pd.read_csv(file_path)\n",
    "    datasets = []\n",
    "    for sample in tqdm(df, desc='Reading data'):\n",
    "        datasets.append(sample)\n",
    "    random.shuffle(datasets)\n",
    "    return datasets\n",
    "\n",
    "class LoadPretrainingDataset(object):\n",
    "    def __init__(self, \n",
    "                 file_path='./file.csv',\n",
    "                 tokenizer=None,\n",
    "                 batch_size=32,\n",
    "                 max_sen_len=None,\n",
    "                 max_position_embeddings=512,\n",
    "                 pad_index=0,\n",
    "                 is_sample_shuffle=True,\n",
    "                 random_state=2023,\n",
    "                 data_name='data',\n",
    "                 masked_rate=0.15,\n",
    "                 masked_token_rate=0.8,\n",
    "                 masked_token_unchanged_rate=0.5):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = build_data(file_path)\n",
    "        self.PAD_IDX = pad_index\n",
    "        self.SEP_IDX = self.data['[SEP]']\n",
    "        self.CLS_IDX = self.data['[CLS]']\n",
    "        self.MASK_IDX = self.data['[MASK]']\n",
    "        self.batch_size = batch_size\n",
    "        self.max_sen_len = max_sen_len\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.is_sample_shuffle = is_sample_shuffle\n",
    "        self.data_name = data_name\n",
    "        self.masked_rate = masked_rate\n",
    "        self.masked_token_rate = masked_token_rate\n",
    "        self.masked_token_unchanged_rate = masked_token_unchanged_rate\n",
    "        self.random_state = random_state\n",
    "        random.seed(random_state)\n",
    "    \n",
    "    def get_format_data(self, file_path):\n",
    "        '''\n",
    "        There are two ways to get the data:\n",
    "        1. get the entire CDR region data. In this case, each sample is a positive sample, \n",
    "            and the features are the sequence of CDR regions of alpha and beta chains, as well as the\n",
    "        2. Only get the CDR3 region sequence data\n",
    "        '''\n",
    "        if self.data_name == \"partial_data\":\n",
    "            return read_data(file_path)\n",
    "        elif self.data_name == \"full_data\":\n",
    "            return read_full_data(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Please check the data name\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next_sentence_sample(sentence, next_sentence, paragraphs):\n",
    "        if random.random < 0.5:\n",
    "            is_next = True\n",
    "        else:\n",
    "            next_sentence = random.choice(random.choice(paragraphs))\n",
    "        return sentence, next_sentence, is_next\n",
    "    \n",
    "    def replace_masked_token(self, token_ids, candidate_pred_positions, num_mlm_preds):\n",
    "        pred_positions = []\n",
    "        mlm_input_tokens_id = [token_id for token_id in token_ids]\n",
    "        for mlm_pred_position in candidate_pred_positions:\n",
    "            if len(pred_positions) >= num_mlm_preds:\n",
    "                break\n",
    "            masked_token_id = None\n",
    "            if random.random() < self.masked_token_rate: # 80%\n",
    "                masked_token_id = self.MASK_IDX\n",
    "            else:\n",
    "                if random.random() < self.masked_token_unchanged_rate: # 0.5 10% in total\n",
    "                    masked_token_id = token_ids[mlm_pred_position]\n",
    "                else:\n",
    "                    masked_token_id = random.choice(list(self.data.values()))\n",
    "            mlm_input_tokens_id[mlm_pred_position] = masked_token_id\n",
    "            pred_positions.append(mlm_pred_position)\n",
    "        mlm_label = [self.PAD_IDX if idx not in pred_positions else token_ids[idx] for idx in range(len(token_ids))]\n",
    "        return mlm_input_tokens_id, mlm_label\n",
    "\n",
    "    def get_masked_sample(self, token_ids):\n",
    "        candidate_pred_positions = []\n",
    "        for i, ids in enumerate(token_ids):\n",
    "            if ids in [self.CLS_IDX, self.SEP_IDX]:\n",
    "                continue\n",
    "            candidate_pred_positions.append(i)\n",
    "        random.shuffle(candidate_pred_positions)\n",
    "        num_mlm_preds = max(1, round(len(token_ids) * self.masked_rate))\n",
    "        logging.debug(f\"num_mlm_preds: {num_mlm_preds}\")\n",
    "        mlm_input_tokens_id, mlm_label = self.replace_masked_token(token_ids, candidate_pred_positions, num_mlm_preds)\n",
    "        return mlm_input_tokens_id, mlm_label\n",
    "\n",
    "    @cache\n",
    "    def data_process(self, file_path, postfix='cache'):\n",
    "        datasets = self.get_format_data(file_path)\n",
    "        data, max_len = [], 0\n",
    "        desc = f\"## building the NSP and MLM samples ##\"\n",
    "        for sample in tqdm(datasets, ncols=80, desc=desc):\n",
    "            for i in range(len(sample)-1):\n",
    "                sentence, next_sentence, is_next = self.get_next_sentence_sample(sample[i], sample[i+1], datasets)\n",
    "                logging.debug(f\"sentence: {sentence}\")\n",
    "                logging.debug(f\"next_sentence: {next_sentence}\")\n",
    "                logging.debug(f\"is_next: {is_next}\")\n",
    "                token_a_ids = [self.data[token] for token in self.tokenizer(sentence)]\n",
    "                token_b_ids = [self.data[token] for token in self.tokenizer(next_sentence)]\n",
    "                token_ids = [self.CLS_IDX] + token_a_ids + [self.SEP_IDX] + token_b_ids\n",
    "                if len(token_ids) > self.max_position_embeddings-1:\n",
    "                    token_ids = token_ids[:self.max_position_embeddings-1]\n",
    "                token_ids += [self.SEP_IDX]\n",
    "                logging.debug(f\"before masked token: {[self.data.itos[t] for t in token_ids]} \")\n",
    "                seg1 = [0] * (len(token_a_ids) + 2)\n",
    "                seg2 = [1] * (len(token_ids) - len(seg1))\n",
    "                segs = torch.tensor(seg1 + seg2, dtype=torch.long)\n",
    "                logging.debug(f\" before masked token ids : {token_ids}\")\n",
    "                logging.debug(f\" before masked token segs: {segs}\")\n",
    "                nsp_label = torch.tensor(int(is_next), dtype=torch.long)\n",
    "                mlm_input_tokens_id, mlm_label = self.get_masked_sample(token_ids)\n",
    "                token_ids = torch.tensor(mlm_input_tokens_id, dtype=torch.long)\n",
    "                mlm_label = torch.tensor(mlm_label, dtype=torch.long)\n",
    "                max_len = max(max_len, len(token_ids))\n",
    "                logging.debug(f\"after masked token ids: {token_ids.tolist()}\")\n",
    "                logging.debug(f\"after masked token segs: {segs.tolist()}\")\n",
    "                logging.debug(f\"after masked token mlm_label: {mlm_label.tolist()}\")\n",
    "                logging.debug(f\"Finish the sample building\")\n",
    "                data.append([token_ids, segs, nsp_label, mlm_label])\n",
    "            all_data = {\"data\": data, \"max_len\": max_len}\n",
    "            return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForNextSentencePrediction(nn.Module):\n",
    "    def __init__(self, config, bert_pretrained_model_dir=None) -> None:\n",
    "        super(BertForNextSentencePrediction, self).__init__()\n",
    "        if bert_pretrained_model_dir is not None:\n",
    "            self.bert = BertModel.from_pretrained(config, bert_pretrained_model_dir)\n",
    "        else:\n",
    "            self.bert = BertModel(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids, # [src_len, batch_size]\n",
    "                attention_mask=None, # [batch_size, src_len] \n",
    "                token_type_ids=None, # [src_len, batch_size]\n",
    "                position_ids=None,\n",
    "                next_sentence_labels=None): # [batch_size,]\n",
    "        pooled_output = self.bert(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids\n",
    "        ) # pooled_output: [batch_size, hidden_size]\n",
    "        seq_relationship_score = self.classifier(pooled_output)\n",
    "        # seq_relationship_score: [batch_size, 2]\n",
    "        if next_sentence_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForLMTransformHead(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights=None) -> None:\n",
    "        super(BertForLMTransformHead, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = get_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        if bert_model_embedding_weights is not None:\n",
    "            self.decoder.weight = nn.Parameter(bert_model_embedding_weights)\n",
    "        # [hidden_size, vocab_size]\n",
    "        self.decoder.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states # [src_len, batch_size, vocab_size]\n",
    "\n",
    "class BertForMaskedLM(nn.Module):\n",
    "    def __init__(self, config, bert_pretrained_model_dir=None):\n",
    "        super(BertForMaskedLM, self).__init__()\n",
    "        if bert_pretrained_model_dir is not None:\n",
    "            self.bert = BertModel.from_pretrained(config, bert_pretrained_model_dir)\n",
    "        else:\n",
    "            self.bert = BertModel(config)\n",
    "        weights = None\n",
    "        if config.use_embedding_weights:\n",
    "            weights = self.bert.bert_embeddings.word_embeddings.embedding.weight\n",
    "            logging.info(f\" use the embedding weights for the decoder {weights.shape}\")\n",
    "        self.classifier = BertForLMTransformHead(config, weights)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                mlm_labels=None):\n",
    "        _, all_encoder_outputs = self.bert(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "        sequence_output = all_encoder_outputs[-1]\n",
    "        prediction_scores = self.classifier(sequence_output)\n",
    "        if mlm_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            mlm_loss = loss_fct(prediction_scores.reshape(-1, self.config.vocab_size), mlm_labels.reshape(-1))\n",
    "            return mlm_loss\n",
    "        else:\n",
    "            return prediction_scores # [src_len, batch_size, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPretrainingModel(nn.Module):\n",
    "    def __init__(self, config, bert_pretrained_model_dir-=None) -> None:\n",
    "        super(BertForPretrainingModel, self).__init__()\n",
    "        if bert_pretrained_model_dir is not None:\n",
    "            self.bert = BertModel.from_pretrained(config, bert_pretrained_model_dir)\n",
    "        else:\n",
    "            self.bert = BertModel(config)\n",
    "        weights = None\n",
    "        if 'use_embedding_weight' in config.__dict__ and config.use_embedding_weight:\n",
    "            weights = self.bert.bert_embeddings.word_embeddings.embedding.weight\n",
    "            logging.info(f\" use the embedding weights for the decoder {weights.shape}\")\n",
    "        self.mlm_prediction = BertForLMTransformHead(config, weights)\n",
    "        self.nsp_prediction = nn.Linear(config.hidden_size, 2)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                mlm_labels=None,\n",
    "                nsp_labels=None):\n",
    "        pooled_output, all_encoder_outputs = self.bert(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "        sequence_output = all_encoder_outputs[-1] # get the last layer of the output\n",
    "        mlm_prediction_logits = self.mlm_prediction(sequence_output)\n",
    "        nsp_pred_logits = self.nsp_prediction(pooled_output)\n",
    "\n",
    "        if mlm_labels is not None and nsp_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            mlm_loss = loss_fct(mlm_prediction_logits.reshape(-1, self.config.vocab_size), mlm_labels.reshape(-1))\n",
    "            nsp_loss = loss_fct(nsp_pred_logits.view(-1, 2), nsp_labels.view(-1))\n",
    "            return mlm_loss + nsp_loss, mlm_prediction_logits, nsp_pred_logits\n",
    "        else:\n",
    "            return mlm_prediction_logits, nsp_pred_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9a3d897ef0b1e7415fe4468808571913e41281b79a56511723d411ccb064e7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
