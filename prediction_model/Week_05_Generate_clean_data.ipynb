{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, WeightedRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(seq):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seq[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seq[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCREncodeData(Dataset):\n",
    "    '''\n",
    "    The class defines the dataset used to encode TCR sequence data\n",
    "    If the dataset is \n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seq(x))\n",
    "        \n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep='first')\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\"], keep=\"first\")\n",
    "        df = df.drop_duplicates(subset=[\"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x))\n",
    "            )\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seq(x))\n",
    "        \n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        X_features = torch.zeros((len(df), 0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, torch.from_numpy(\n",
    "                np.vstack(df[seq].values)\n",
    "            )), dim=1)\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_features[index], self.y[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCR encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    Param:\n",
    "        input_size: the input feature size\n",
    "        hidden_size: the size of hidden variable\n",
    "        output_size: the size of output features\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 49)\n",
    "            nn.Conv1d(5, 10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 25) based on the formula for conv1d: (W + 2P - K)/S + 1 = (49 + 2*1 - 3)/2 + 1 = 25\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 10, 23), 25 - 2 = 23 \n",
    "\n",
    "            nn.Conv1d(10, 15, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12) based on the formula for conv1d: (W + 2P - K)/S + 1 = (23 + 2*1 - 3)/2 + 1 = 12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 10), 12 - 2 = 10\n",
    "\n",
    "            nn.Conv1d(15, 20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 5) based on the formula for conv1d: (W + 2P - K)/S + 1 = (10 + 2*1 - 3)/2 + 1 = 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 3)\n",
    "\n",
    "            nn.Conv1d(20, 20 , kernel_size=5, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 1) based on the formula for conv1d: (W + 2P - K)/S + 1 = (3 + 2*1 - 5)/2 + 1 = 1\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 1)\n",
    "            nn.ConvTranspose1d(20, 20, kernel_size=5, stride=2, padding=1),\n",
    "            # (batch_size, 20, 3), based on the formula for convtranspose1d: (W−1)S−2P+F = (1-1)*2-2*1+5= 3\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(20, 15, kernel_size=3, stride=3, padding=1),\n",
    "            # (batch_size, 15, 5), based on the formula for convtranspose1d: (W−1)S−2P+F = (3-1)*3-2*1+3= 7\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(15, 10, kernel_size=7, stride=3, padding=1),\n",
    "            # (batch_size, 10, 23) based on the formula for convtranspose1d: (W−1)S−2P+F = (7-1)*3-2*1+7= 23\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(10, 5, kernel_size=7, stride=2, padding=1),\n",
    "            # (batch_size, 5, 49) based on the formula for convtranspose1d: (W−1)S−2P+F = (23-1)*2-2*1+7= 49\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = torch.tensor(x, dtype=np.float32)\n",
    "        # x = torch.tensor(x, dtype=torch.float)\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        # print(f\"encoding shape: {encoded.shape}\")\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2358, 6)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCREncodeData(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2358 (0%)]\tLoss: 1.297911\n",
      "Train Epoch: 1 [1600/2358 (68%)]\tLoss: 1.275421\n",
      "Train Epoch: 2 [0/2358 (0%)]\tLoss: 1.290722\n",
      "Train Epoch: 2 [1600/2358 (68%)]\tLoss: 1.378245\n",
      "Train Epoch: 3 [0/2358 (0%)]\tLoss: 1.242732\n",
      "Train Epoch: 3 [1600/2358 (68%)]\tLoss: 1.217479\n",
      "Train Epoch: 4 [0/2358 (0%)]\tLoss: 1.148395\n",
      "Train Epoch: 4 [1600/2358 (68%)]\tLoss: 1.132191\n",
      "Train Epoch: 5 [0/2358 (0%)]\tLoss: 1.264346\n",
      "Train Epoch: 5 [1600/2358 (68%)]\tLoss: 1.163525\n",
      "Train Epoch: 6 [0/2358 (0%)]\tLoss: 1.191676\n",
      "Train Epoch: 6 [1600/2358 (68%)]\tLoss: 1.279669\n",
      "Train Epoch: 7 [0/2358 (0%)]\tLoss: 1.163980\n",
      "Train Epoch: 7 [1600/2358 (68%)]\tLoss: 1.125402\n",
      "Train Epoch: 8 [0/2358 (0%)]\tLoss: 1.141288\n",
      "Train Epoch: 8 [1600/2358 (68%)]\tLoss: 1.092539\n",
      "Train Epoch: 9 [0/2358 (0%)]\tLoss: 1.077902\n",
      "Train Epoch: 9 [1600/2358 (68%)]\tLoss: 1.209717\n",
      "Train Epoch: 10 [0/2358 (0%)]\tLoss: 1.125266\n",
      "Train Epoch: 10 [1600/2358 (68%)]\tLoss: 1.281549\n",
      "Train Epoch: 11 [0/2358 (0%)]\tLoss: 1.049173\n",
      "Train Epoch: 11 [1600/2358 (68%)]\tLoss: 1.084356\n",
      "Train Epoch: 12 [0/2358 (0%)]\tLoss: 1.100935\n",
      "Train Epoch: 12 [1600/2358 (68%)]\tLoss: 1.101100\n",
      "Train Epoch: 13 [0/2358 (0%)]\tLoss: 1.112522\n",
      "Train Epoch: 13 [1600/2358 (68%)]\tLoss: 1.126669\n",
      "Train Epoch: 14 [0/2358 (0%)]\tLoss: 1.061409\n",
      "Train Epoch: 14 [1600/2358 (68%)]\tLoss: 1.194661\n",
      "Train Epoch: 15 [0/2358 (0%)]\tLoss: 1.172589\n",
      "Train Epoch: 15 [1600/2358 (68%)]\tLoss: 1.109625\n",
      "Train Epoch: 16 [0/2358 (0%)]\tLoss: 1.234767\n",
      "Train Epoch: 16 [1600/2358 (68%)]\tLoss: 1.021572\n",
      "Train Epoch: 17 [0/2358 (0%)]\tLoss: 1.143950\n",
      "Train Epoch: 17 [1600/2358 (68%)]\tLoss: 1.221589\n",
      "Train Epoch: 18 [0/2358 (0%)]\tLoss: 1.129539\n",
      "Train Epoch: 18 [1600/2358 (68%)]\tLoss: 1.163488\n",
      "Train Epoch: 19 [0/2358 (0%)]\tLoss: 1.131168\n",
      "Train Epoch: 19 [1600/2358 (68%)]\tLoss: 1.104256\n",
      "Train Epoch: 20 [0/2358 (0%)]\tLoss: 1.039673\n",
      "Train Epoch: 20 [1600/2358 (68%)]\tLoss: 1.052672\n",
      "Train Epoch: 21 [0/2358 (0%)]\tLoss: 1.097213\n",
      "Train Epoch: 21 [1600/2358 (68%)]\tLoss: 1.163702\n",
      "Train Epoch: 22 [0/2358 (0%)]\tLoss: 1.145915\n",
      "Train Epoch: 22 [1600/2358 (68%)]\tLoss: 1.065968\n",
      "Train Epoch: 23 [0/2358 (0%)]\tLoss: 1.081487\n",
      "Train Epoch: 23 [1600/2358 (68%)]\tLoss: 1.127266\n",
      "Train Epoch: 24 [0/2358 (0%)]\tLoss: 1.148807\n",
      "Train Epoch: 24 [1600/2358 (68%)]\tLoss: 1.100004\n",
      "Train Epoch: 25 [0/2358 (0%)]\tLoss: 1.116004\n",
      "Train Epoch: 25 [1600/2358 (68%)]\tLoss: 1.057255\n",
      "Train Epoch: 26 [0/2358 (0%)]\tLoss: 1.098847\n",
      "Train Epoch: 26 [1600/2358 (68%)]\tLoss: 1.094699\n",
      "Train Epoch: 27 [0/2358 (0%)]\tLoss: 1.059554\n",
      "Train Epoch: 27 [1600/2358 (68%)]\tLoss: 1.017581\n",
      "Train Epoch: 28 [0/2358 (0%)]\tLoss: 1.153056\n",
      "Train Epoch: 28 [1600/2358 (68%)]\tLoss: 1.079800\n",
      "Train Epoch: 29 [0/2358 (0%)]\tLoss: 0.974881\n",
      "Train Epoch: 29 [1600/2358 (68%)]\tLoss: 1.082021\n",
      "Train Epoch: 30 [0/2358 (0%)]\tLoss: 1.056226\n",
      "Train Epoch: 30 [1600/2358 (68%)]\tLoss: 0.997640\n",
      "Train Epoch: 31 [0/2358 (0%)]\tLoss: 1.121511\n",
      "Train Epoch: 31 [1600/2358 (68%)]\tLoss: 1.116818\n",
      "Train Epoch: 32 [0/2358 (0%)]\tLoss: 1.028554\n",
      "Train Epoch: 32 [1600/2358 (68%)]\tLoss: 1.101160\n",
      "Train Epoch: 33 [0/2358 (0%)]\tLoss: 1.158253\n",
      "Train Epoch: 33 [1600/2358 (68%)]\tLoss: 1.081806\n",
      "Train Epoch: 34 [0/2358 (0%)]\tLoss: 1.097056\n",
      "Train Epoch: 34 [1600/2358 (68%)]\tLoss: 1.113997\n",
      "Train Epoch: 35 [0/2358 (0%)]\tLoss: 1.014552\n",
      "Train Epoch: 35 [1600/2358 (68%)]\tLoss: 1.015769\n",
      "Train Epoch: 36 [0/2358 (0%)]\tLoss: 0.961757\n",
      "Train Epoch: 36 [1600/2358 (68%)]\tLoss: 1.126348\n",
      "Train Epoch: 37 [0/2358 (0%)]\tLoss: 1.055023\n",
      "Train Epoch: 37 [1600/2358 (68%)]\tLoss: 1.042256\n",
      "Train Epoch: 38 [0/2358 (0%)]\tLoss: 0.962514\n",
      "Train Epoch: 38 [1600/2358 (68%)]\tLoss: 1.107499\n",
      "Train Epoch: 39 [0/2358 (0%)]\tLoss: 1.118354\n",
      "Train Epoch: 39 [1600/2358 (68%)]\tLoss: 0.947024\n",
      "Train Epoch: 40 [0/2358 (0%)]\tLoss: 1.004734\n",
      "Train Epoch: 40 [1600/2358 (68%)]\tLoss: 1.068561\n",
      "Train Epoch: 41 [0/2358 (0%)]\tLoss: 1.036811\n",
      "Train Epoch: 41 [1600/2358 (68%)]\tLoss: 1.054536\n",
      "Train Epoch: 42 [0/2358 (0%)]\tLoss: 1.121197\n",
      "Train Epoch: 42 [1600/2358 (68%)]\tLoss: 1.101704\n",
      "Train Epoch: 43 [0/2358 (0%)]\tLoss: 1.063147\n",
      "Train Epoch: 43 [1600/2358 (68%)]\tLoss: 1.046162\n",
      "Train Epoch: 44 [0/2358 (0%)]\tLoss: 1.072657\n",
      "Train Epoch: 44 [1600/2358 (68%)]\tLoss: 1.121595\n",
      "Train Epoch: 45 [0/2358 (0%)]\tLoss: 1.218490\n",
      "Train Epoch: 45 [1600/2358 (68%)]\tLoss: 1.065896\n",
      "Train Epoch: 46 [0/2358 (0%)]\tLoss: 1.050693\n",
      "Train Epoch: 46 [1600/2358 (68%)]\tLoss: 1.069183\n",
      "Train Epoch: 47 [0/2358 (0%)]\tLoss: 0.987675\n",
      "Train Epoch: 47 [1600/2358 (68%)]\tLoss: 1.034045\n",
      "Train Epoch: 48 [0/2358 (0%)]\tLoss: 0.916925\n",
      "Train Epoch: 48 [1600/2358 (68%)]\tLoss: 0.958042\n",
      "Train Epoch: 49 [0/2358 (0%)]\tLoss: 1.013866\n",
      "Train Epoch: 49 [1600/2358 (68%)]\tLoss: 1.075450\n",
      "Train Epoch: 50 [0/2358 (0%)]\tLoss: 1.150342\n",
      "Train Epoch: 50 [1600/2358 (68%)]\tLoss: 1.227306\n",
      "Train Epoch: 51 [0/2358 (0%)]\tLoss: 0.997953\n",
      "Train Epoch: 51 [1600/2358 (68%)]\tLoss: 1.063775\n",
      "Train Epoch: 52 [0/2358 (0%)]\tLoss: 0.921924\n",
      "Train Epoch: 52 [1600/2358 (68%)]\tLoss: 1.069623\n",
      "Train Epoch: 53 [0/2358 (0%)]\tLoss: 1.067705\n",
      "Train Epoch: 53 [1600/2358 (68%)]\tLoss: 1.008163\n",
      "Train Epoch: 54 [0/2358 (0%)]\tLoss: 0.990186\n",
      "Train Epoch: 54 [1600/2358 (68%)]\tLoss: 1.070985\n",
      "Train Epoch: 55 [0/2358 (0%)]\tLoss: 0.927738\n",
      "Train Epoch: 55 [1600/2358 (68%)]\tLoss: 0.989222\n",
      "Train Epoch: 56 [0/2358 (0%)]\tLoss: 0.977798\n",
      "Train Epoch: 56 [1600/2358 (68%)]\tLoss: 1.105420\n",
      "Train Epoch: 57 [0/2358 (0%)]\tLoss: 1.029252\n",
      "Train Epoch: 57 [1600/2358 (68%)]\tLoss: 1.030970\n",
      "Train Epoch: 58 [0/2358 (0%)]\tLoss: 0.994985\n",
      "Train Epoch: 58 [1600/2358 (68%)]\tLoss: 0.936713\n",
      "Train Epoch: 59 [0/2358 (0%)]\tLoss: 1.015948\n",
      "Train Epoch: 59 [1600/2358 (68%)]\tLoss: 1.052427\n",
      "Train Epoch: 60 [0/2358 (0%)]\tLoss: 0.940069\n",
      "Train Epoch: 60 [1600/2358 (68%)]\tLoss: 1.106904\n",
      "Train Epoch: 61 [0/2358 (0%)]\tLoss: 0.962465\n",
      "Train Epoch: 61 [1600/2358 (68%)]\tLoss: 1.113600\n",
      "Train Epoch: 62 [0/2358 (0%)]\tLoss: 0.972785\n",
      "Train Epoch: 62 [1600/2358 (68%)]\tLoss: 0.909031\n",
      "Train Epoch: 63 [0/2358 (0%)]\tLoss: 1.054913\n",
      "Train Epoch: 63 [1600/2358 (68%)]\tLoss: 0.928366\n",
      "Train Epoch: 64 [0/2358 (0%)]\tLoss: 0.999242\n",
      "Train Epoch: 64 [1600/2358 (68%)]\tLoss: 1.008765\n",
      "Train Epoch: 65 [0/2358 (0%)]\tLoss: 0.968618\n",
      "Train Epoch: 65 [1600/2358 (68%)]\tLoss: 1.115942\n",
      "Train Epoch: 66 [0/2358 (0%)]\tLoss: 0.980759\n",
      "Train Epoch: 66 [1600/2358 (68%)]\tLoss: 1.015946\n",
      "Train Epoch: 67 [0/2358 (0%)]\tLoss: 1.139930\n",
      "Train Epoch: 67 [1600/2358 (68%)]\tLoss: 0.993319\n",
      "Train Epoch: 68 [0/2358 (0%)]\tLoss: 1.073670\n",
      "Train Epoch: 68 [1600/2358 (68%)]\tLoss: 0.993245\n",
      "Train Epoch: 69 [0/2358 (0%)]\tLoss: 0.995810\n",
      "Train Epoch: 69 [1600/2358 (68%)]\tLoss: 0.988834\n",
      "Train Epoch: 70 [0/2358 (0%)]\tLoss: 0.958115\n",
      "Train Epoch: 70 [1600/2358 (68%)]\tLoss: 1.032474\n",
      "Train Epoch: 71 [0/2358 (0%)]\tLoss: 1.007492\n",
      "Train Epoch: 71 [1600/2358 (68%)]\tLoss: 1.018491\n",
      "Train Epoch: 72 [0/2358 (0%)]\tLoss: 0.995687\n",
      "Train Epoch: 72 [1600/2358 (68%)]\tLoss: 0.999833\n",
      "Train Epoch: 73 [0/2358 (0%)]\tLoss: 0.980958\n",
      "Train Epoch: 73 [1600/2358 (68%)]\tLoss: 0.945920\n",
      "Train Epoch: 74 [0/2358 (0%)]\tLoss: 1.027893\n",
      "Train Epoch: 74 [1600/2358 (68%)]\tLoss: 1.004603\n",
      "Train Epoch: 75 [0/2358 (0%)]\tLoss: 1.010628\n",
      "Train Epoch: 75 [1600/2358 (68%)]\tLoss: 0.975716\n",
      "Train Epoch: 76 [0/2358 (0%)]\tLoss: 0.848723\n",
      "Train Epoch: 76 [1600/2358 (68%)]\tLoss: 0.948217\n",
      "Train Epoch: 77 [0/2358 (0%)]\tLoss: 0.903784\n",
      "Train Epoch: 77 [1600/2358 (68%)]\tLoss: 0.985050\n",
      "Train Epoch: 78 [0/2358 (0%)]\tLoss: 1.019837\n",
      "Train Epoch: 78 [1600/2358 (68%)]\tLoss: 0.992692\n",
      "Train Epoch: 79 [0/2358 (0%)]\tLoss: 1.026744\n",
      "Train Epoch: 79 [1600/2358 (68%)]\tLoss: 0.899809\n",
      "Train Epoch: 80 [0/2358 (0%)]\tLoss: 1.008010\n",
      "Train Epoch: 80 [1600/2358 (68%)]\tLoss: 0.945334\n",
      "Train Epoch: 81 [0/2358 (0%)]\tLoss: 1.035426\n",
      "Train Epoch: 81 [1600/2358 (68%)]\tLoss: 0.925780\n",
      "Train Epoch: 82 [0/2358 (0%)]\tLoss: 1.006277\n",
      "Train Epoch: 82 [1600/2358 (68%)]\tLoss: 1.082803\n",
      "Train Epoch: 83 [0/2358 (0%)]\tLoss: 0.965206\n",
      "Train Epoch: 83 [1600/2358 (68%)]\tLoss: 0.946890\n",
      "Train Epoch: 84 [0/2358 (0%)]\tLoss: 0.907725\n",
      "Train Epoch: 84 [1600/2358 (68%)]\tLoss: 0.998541\n",
      "Train Epoch: 85 [0/2358 (0%)]\tLoss: 0.993666\n",
      "Train Epoch: 85 [1600/2358 (68%)]\tLoss: 0.989705\n",
      "Train Epoch: 86 [0/2358 (0%)]\tLoss: 0.926406\n",
      "Train Epoch: 86 [1600/2358 (68%)]\tLoss: 1.049803\n",
      "Train Epoch: 87 [0/2358 (0%)]\tLoss: 0.936692\n",
      "Train Epoch: 87 [1600/2358 (68%)]\tLoss: 1.038020\n",
      "Train Epoch: 88 [0/2358 (0%)]\tLoss: 0.948421\n",
      "Train Epoch: 88 [1600/2358 (68%)]\tLoss: 1.054600\n",
      "Train Epoch: 89 [0/2358 (0%)]\tLoss: 0.963197\n",
      "Train Epoch: 89 [1600/2358 (68%)]\tLoss: 0.976216\n",
      "Train Epoch: 90 [0/2358 (0%)]\tLoss: 0.955818\n",
      "Train Epoch: 90 [1600/2358 (68%)]\tLoss: 1.091987\n",
      "Train Epoch: 91 [0/2358 (0%)]\tLoss: 1.011740\n",
      "Train Epoch: 91 [1600/2358 (68%)]\tLoss: 0.925750\n",
      "Train Epoch: 92 [0/2358 (0%)]\tLoss: 0.947172\n",
      "Train Epoch: 92 [1600/2358 (68%)]\tLoss: 0.960428\n",
      "Train Epoch: 93 [0/2358 (0%)]\tLoss: 0.993948\n",
      "Train Epoch: 93 [1600/2358 (68%)]\tLoss: 1.059437\n",
      "Train Epoch: 94 [0/2358 (0%)]\tLoss: 0.918030\n",
      "Train Epoch: 94 [1600/2358 (68%)]\tLoss: 0.960949\n",
      "Train Epoch: 95 [0/2358 (0%)]\tLoss: 0.899549\n",
      "Train Epoch: 95 [1600/2358 (68%)]\tLoss: 0.924567\n",
      "Train Epoch: 96 [0/2358 (0%)]\tLoss: 0.949836\n",
      "Train Epoch: 96 [1600/2358 (68%)]\tLoss: 0.931680\n",
      "Train Epoch: 97 [0/2358 (0%)]\tLoss: 1.006989\n",
      "Train Epoch: 97 [1600/2358 (68%)]\tLoss: 0.921585\n",
      "Train Epoch: 98 [0/2358 (0%)]\tLoss: 1.026476\n",
      "Train Epoch: 98 [1600/2358 (68%)]\tLoss: 0.904788\n",
      "Train Epoch: 99 [0/2358 (0%)]\tLoss: 1.006138\n",
      "Train Epoch: 99 [1600/2358 (68%)]\tLoss: 0.916086\n",
      "Train Epoch: 100 [0/2358 (0%)]\tLoss: 0.912434\n",
      "Train Epoch: 100 [1600/2358 (68%)]\tLoss: 0.950247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1e71741f70>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    # model_accuracy = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        # TCR_encode_losses.append(loss.item() / model.batch_size)\n",
    "        # TCR_encode_losses.append(loss.item())\n",
    "        # sum up batch loss\n",
    "        batch_loss += loss.item()\n",
    "        # update the accuracy of the model\n",
    "        # pred = output.data.max(1, keepdim=True)[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            # print(f\"The accuracy of the model is \")\n",
    "            \n",
    "    # return batch_loss / len(train_loader.dataset)\n",
    "    return batch_loss / len(data)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "TCR_accuracy = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.set_title(\"TCR encode loss\")\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2358, 6)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCREncodeData(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 1))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    encode_shape = int(TCR_seq.shape[0] / 5)\n",
    "    TCR_seq = TCR_seq.view(1, 5, encode_shape).float()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    Here the input is the TCR sequence, neoantigen sequence, and HLA type.\n",
    "    The output should be the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 file_path, \n",
    "                 only_CDR3: bool = False, \n",
    "                 only_experimental: bool = False, \n",
    "                 TCR_encode: str = [\"LSTM\", \"CNN\"],\n",
    "                 encoding_model: nn.Module = None,\n",
    "                 encoding_size: int = 20,\n",
    "                 random_state: int = 123) -> None:\n",
    "        df, HLA_encode, y, feature_size  = self.basic_io(file_path, only_experimental=only_experimental)\n",
    "\n",
    "        \n",
    "        # convert from object to tensor\n",
    "        X_TCR_seq = torch.zeros((len(df), 0))\n",
    "        for region in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            TCR_seq = df.loc[:, region].values\n",
    "            TCR_seq_encode = torch.zeros((0, TCR_seq[0].shape[1]))\n",
    "            for i in range(len(TCR_seq)):\n",
    "                encoding = torch.from_numpy(TCR_seq[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                TCR_seq_encode = torch.cat((TCR_seq_encode, encoding), dim=0)\n",
    "\n",
    "            X_TCR_seq = torch.cat((TCR_seq_encode, X_TCR_seq), dim=1)\n",
    "        print(f\"X_TCR_seq shape {X_TCR_seq.shape}\")\n",
    "        if TCR_encode == \"CNN\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, feature_size)\n",
    "        elif TCR_encode == \"LSTM\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, feature_size)\n",
    "        else:\n",
    "            raise ValueError(\"The TCR encoding method is not supported yet.\")\n",
    "        \n",
    "        # encoding model \n",
    "        X_features, _ = encoding_model(X_TCR_seq)\n",
    "        X_features = X_features.view(-1, encoding_size).data\n",
    "\n",
    "        # add the neoantigen sequence encoding features\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            neo = df.loc[:, seq].values\n",
    "            neo_encode = torch.zeros((0, neo[0].shape[1]))\n",
    "            for i in range(len(neo)):\n",
    "                encoding = torch.from_numpy(neo[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                neo_encode = torch.cat((neo_encode, encoding), dim=0)\n",
    "            X_features = torch.cat((X_features, neo_encode), dim=1)\n",
    "\n",
    "        X_features = torch.cat((X_features, torch.from_numpy(HLA_encode)), dim=1)\n",
    "\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "        # self.X_train, self.X_val, self.y_train, self.y_val = \n",
    "    \n",
    "    def basic_io(self, file_path, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "        # drop the random generate samples and duplicated \n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        # df = df[df[\"AseqCDR3\"].str.contains(\"_\")==False].drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seq(x))\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        input_feature_size = sum(len_map.values())\n",
    "\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seq(x))\n",
    "        \n",
    "        # drop the rows with nan\n",
    "        df = df.dropna()\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            # df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            # df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            # df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng_em], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "        y.value_counts().plot.pie(autopct='%.2f')\n",
    "        return df, X_HLA_encoded, y.values, input_feature_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, TCR_encode=\"CNN\", only_experimental=False, encoding_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
