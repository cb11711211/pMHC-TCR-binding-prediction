{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuxinchao/.conda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset 230220.csv\n",
    "class TCRDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_ng = df.copy()\n",
    "        df_ng = df_ng[df_ng[\"HLA\"] != \"-\"]\n",
    "        df_ng[\"Class\"] = \"negative\"\n",
    "        df_ng[\"AseqCDR_3\"] = df_ng[\"AseqCDR_3\"].apply(\n",
    "            lambda x: random.choice(list(set(df[\"AseqCDR_3\"]) - set(x))))\n",
    "        df_ng[\"BseqCDR_3\"] = df_ng[\"BseqCDR_3\"].apply(\n",
    "            lambda x: random.choice(list(set(df[\"BseqCDR_3\"]) - set(x))))\n",
    "        df_pos = df[df[\"Class\"] == \"positive\"]\n",
    "        df = pd.concat([df_pos, df_ng], axis=0)\n",
    "        df = df[\"HLA\", \"Neo\", \"AseqCDR_3\", \"BseqCDR_3\", \"Class\"]\n",
    "        seq_list = [\"AseqCDR_3\", \"BseqCDR_3\"]\n",
    "        len_map = df[seq_list].applymap(len).max()\n",
    "        X_feature = np.zeros((len(df), 0))\n",
    "        for column in seq_list:\n",
    "            df[column] = df[column].str.ljust(len_map[column], \"*\")\n",
    "            encode_seq_result = list()\n",
    "            for i in df[column]:\n",
    "                encode_seq_result.append(encode_seqCDR(i))\n",
    "            col_name = column + \"_encode\"\n",
    "            df[col_name] = encode_seq_result\n",
    "            col_feature = np.zeros((0, len_map[column]*5))\n",
    "            for i in range(len(df)):\n",
    "                col_feature = np.vstack((col_feature, df.loc[i, col_name].reshape(1, -1)))\n",
    "            X_feature = np.hstack((X_feature, col_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for more complicated dataset which the HLA has more than 14 types\n",
    "class HLAAutoEncoder_twoLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(HLAAutoEncoder_twoLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*4, hidden_dim*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*4, input_dim),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HLAAutoEncoder_twoLayer(input_dim=5*len(df[\"aaSeqHLA\"].unique().max()), hidden_dim=10)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "output = []\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for idx, (data) in enumerate(train_loader):\n",
    "        data = Variable(data).float()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 230221 dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_451163/2328245337.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_a[\"AseqCDR\"] = df_a[\"aaSeqCDR\"]\n",
      "/tmp/ipykernel_451163/2328245337.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_a.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n",
      "/tmp/ipykernel_451163/2328245337.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_b[\"BseqCDR\"] = df_b[\"aaSeqCDR\"]\n",
      "/tmp/ipykernel_451163/2328245337.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_b.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SSCMGGMNQR     494\n",
       "VVGAVGVGK      373\n",
       "VVVGAGDVGK     143\n",
       "ATAPSLSGK      118\n",
       "VVVGADGVGK      99\n",
       "SVCAGILSY       79\n",
       "VLLSHLSYL       77\n",
       "ATATAPSLSGK     76\n",
       "TTAPPLSGK       64\n",
       "VVGAGDVGK       42\n",
       "SLMEQIPHL       18\n",
       "LVTDDLLTL       14\n",
       "SLLMWITQC        9\n",
       "ELAGIGILTV       8\n",
       "FLSEQLSIKL       6\n",
       "GVLEVSHSI        6\n",
       "AAGIGILTV        2\n",
       "GILGFVFTL        2\n",
       "NLVPMVATV        2\n",
       "GTSGSPIVNR       1\n",
       "LLFGYAVYV        1\n",
       "ELAGIGALTV       1\n",
       "ELAAIGILTV       1\n",
       "LLFGYPVAV        1\n",
       "YLEPGPVTV        1\n",
       "SLYNTIATL        1\n",
       "EAAGIGILTV       1\n",
       "SLFNTIAVL        1\n",
       "GILEFVFTL        1\n",
       "SLLMWITQV        1\n",
       "ALWGFFPVL        1\n",
       "LLFGYPVYV        1\n",
       "SLYNTVATL        1\n",
       "Name: NeoAA, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"/DATA/User/wuxinchao/project/data/seqData/pMHC-TCR_20230221_Info.xlsx\")\n",
    "# select the HLA:HLA-A*02:01, HLA-A*11:01\n",
    "df = df[(df[\"HLA\"] == \"HLA-A*02:01\") | (df[\"HLA\"] == \"HLA-A*11:01\")]\n",
    "# set the index of cellname and chain\n",
    "df = df.set_index(['cellname', \"chain\"])\n",
    "# extract the NeoAA, HLA, and aaSeqCDR columns\n",
    "df = df[[\"NeoAA\", \"HLA\", \"aaSeqCDR1\", \"aaSeqCDR2\", \"aaSeqCDR3\", \"Class\"]]\n",
    "df[\"aaSeqCDR\"] = df[df.columns[2:-1]].apply(\n",
    "    # lambda x: x[0] + 'X' * (7 - len(x[0])) + x[1] + x[2],\n",
    "    lambda x: '_'.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "df_a = df.loc[idx[:,\"TRA\"],]\n",
    "df_a[\"AseqCDR\"] = df_a[\"aaSeqCDR\"]\n",
    "df_a.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n",
    "# drop the chain index\n",
    "df_a.index = df_a.index.droplevel(1)\n",
    "# print(df_a)\n",
    "df_b = df.loc[idx[:,\"TRB\"],]\n",
    "df_b[\"BseqCDR\"] = df_b[\"aaSeqCDR\"]\n",
    "df_b.drop(columns=[\"aaSeqCDR\",\"aaSeqCDR1\",\"aaSeqCDR2\",\"aaSeqCDR3\"], inplace=True)\n",
    "# drop the chain index\n",
    "df_b.index = df_b.index.droplevel(1)\n",
    "# print(df_b)\n",
    "\n",
    "# merge the TRA and TRB dataframes by cellname, HLAs, and NeoAA\n",
    "df_ab = pd.merge(df_a, df_b, on=[\"cellname\", \"HLA\", \"NeoAA\", \"Class\"])\n",
    "\n",
    "df = df_ab\n",
    "# select the NeoAA first 3 aa and last 3 aa as new column\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "# df[\"NeoAA\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSC_MNQ    494\n",
       "VVG_GVG    373\n",
       "ATA_LSG    194\n",
       "VVV_DVG    143\n",
       "VVV_GVG     99\n",
       "SVC_ILS     79\n",
       "VLL_LSY     77\n",
       "TTA_LSG     64\n",
       "VVG_DVG     42\n",
       "SLM_IPH     18\n",
       "LVT_LLT     14\n",
       "SLL_ITQ     10\n",
       "ELA_ILT      9\n",
       "GVL_SHS      6\n",
       "FLS_SIK      6\n",
       "GIL_VFT      3\n",
       "AAG_ILT      2\n",
       "NLV_VAT      2\n",
       "YLE_PVT      1\n",
       "LLF_PVY      1\n",
       "SLF_IAV      1\n",
       "SLY_VAT      1\n",
       "SLY_IAT      1\n",
       "GTS_IVN      1\n",
       "LLF_PVA      1\n",
       "ELA_ALT      1\n",
       "LLF_AVY      1\n",
       "ALW_FPV      1\n",
       "EAA_ILT      1\n",
       "Name: Neo, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine whether to use Neo column, and the decision is not to use\n",
    "# Instead, we use encoding of NeoAA\n",
    "df[\"Neo\"] = df[\"NeoAA\"].str.slice(0,3) + \"_\" + df[\"NeoAA\"].str.slice(-4,-1)\n",
    "df.drop(columns=[\"NeoAA\"], inplace=True)\n",
    "df[\"Neo\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HLA-A*11:01    1489\n",
       "HLA-A*02:01     157\n",
       "Name: HLA, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"HLA\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    1109\n",
       "negative     537\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final output dataframe that we need to use\n",
    "# df = df.drop(columns=[\"Neo\"])\n",
    "df.to_csv(\"~/data/project/data/seqData/230221.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCRDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # super(TCRDataset, self).__init__()\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        # get the CDR3 region\n",
    "        for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "            # df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "            # df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "            df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "            df.drop(columns=[chain], inplace=True)\n",
    "        df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "        df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "        df_ng_em = df.copy()\n",
    "        df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "        df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "        df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "        df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "        df_ng.index = range(len(df_ng))\n",
    "        df = pd.concat([df_ps, df_ng], axis=0)\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR_3\": df[\"AseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR_3\": df[\"BseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode HLA type through one-hot encoding\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "            # X_features = df[seq]\n",
    "            # print(df[seq].values.shape)\n",
    "            # convert the df[seq] into torch tensor\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "        \n",
    "        X = torch.cat((torch.from_numpy(X_HLA_encoded), X_features), dim=1)\n",
    "        # encode the class label\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        self.X = X\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for test\n",
    "file_path = \"~/data/project/data/seqData/230221.csv\"\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "# get the CDR3 region\n",
    "for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "    # df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "    # df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "    df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "    df.drop(columns=[chain], inplace=True)\n",
    "# delete the index\n",
    "df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "df_ng_em = df.copy()\n",
    "df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "df_ng.index = range(len(df_ng))\n",
    "df = pd.concat([df_ps, df_ng], axis=0)\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR_3\": df[\"AseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR_3\": df[\"BseqCDR_3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "    length = len_map[chain]\n",
    "    # print(length)\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x).reshape(1, -1))\n",
    "\n",
    "# encode HLA type through one-hot encoding\n",
    "X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "HLAencoder = OneHotEncoder()\n",
    "X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "X_features = torch.zeros((len(df),0))\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "    # X_features = df[seq]\n",
    "    # print(df[seq].values.shape)\n",
    "    # convert the df[seq] into torch tensor\n",
    "    X_features = torch.cat((X_features, \n",
    "        torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "\n",
    "# put the features together including encoded HLA type, Neo_first3, Neo_last3, and CDR3 region\n",
    "X = torch.cat((torch.from_numpy(X_HLA_encoded), X_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2755, 237])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # 237 = 20*5 + 21*5 + 6*5 + 2 \n",
    "# len_map # A: 20, B: 21\n",
    "# X_HLA_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the object data into torch tensor\n",
    "# np.hstack([X_HLA_encoded, df[[\"Neo_first3\", \"Neo_last3\", \"AseqCDR_3\", \"BseqCDR_3\"]]]).shape\n",
    "X_features = torch.zeros((len(df),0))\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_3\", \"BseqCDR_3\"]:\n",
    "    # X_features = df[seq]\n",
    "    # print(df[seq].values.shape)\n",
    "    # convert the df[seq] into torch tensor\n",
    "    X_features = torch.cat((X_features, torch.from_numpy(np.vstack(df[seq].values))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2755, 235])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"Neo_first3\"].values\n",
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"~/data/project/data/seqData/230221.csv\"\n",
    "TCRData = TCRDataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size=16, \n",
    "                 batch_size=32, \n",
    "                 num_layers=2, \n",
    "                 device=\"cpu\", \n",
    "                 use_whole_data=False) -> None:\n",
    "        super(pMHC_TCR_model, self).__init__()\n",
    "        if use_whole_data:\n",
    "            self.batch_size = 0\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        # self.label = nn.Linear(hidden_size, 1)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size/2)\n",
    "        self.linear2 = nn.Linear(hidden_size/2, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.batch_size == 0:\n",
    "            self.batch_size = input.shape[0]\n",
    "            x = input.float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "            pred = self.linear1(output[-1])\n",
    "            pred = self.linear2(pred)\n",
    "            # pred = self.label(output[-1])\n",
    "        else:\n",
    "            x = input.view(-1, self.batch_size, self.input_size).float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "            # pred = self.label(output[-1])\n",
    "            pred = self.linear1(output[-1])\n",
    "            pred = self.linear2(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, target.shape)\n",
    "        output = output.to(torch.float32)\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        # print(output.shape, target.shape)\n",
    "        loss = nn.CrossEntropyLoss()(output.view(1,-1), target.view(1,-1))\n",
    "        train_loss += loss.item() / len(train_loader.dataset)  # sum up batch loss\n",
    "        pred = output.sigmoid().round()\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Fold/Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                fold, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(train_loader.dataset)))\n",
    "    # return the average loss\n",
    "    print(f\"The batch size is {model.batch_size}\")\n",
    "    return train_loss, correct / len(train_loader.dataset)\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).to(torch.float32)\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output.reshape(1,-1), target.reshape(1,-1)).item()  # sum up batch loss\n",
    "            # print(test_loss)\n",
    "            pred = output.sigmoid().round()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= model.batch_size\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, nn\u001b[39m.\u001b[39mConv2d) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(m, nn\u001b[39m.\u001b[39mLinear):\n\u001b[1;32m     12\u001b[0m         m\u001b[39m.\u001b[39mreset_parameters()\n\u001b[0;32m---> 14\u001b[0m model \u001b[39m=\u001b[39m pMHC_TCR_model(input_size\u001b[39m=\u001b[39;49m\u001b[39m237\u001b[39;49m, hidden_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, num_layers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice, use_whole_data\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m kf \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39mfolds, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     16\u001b[0m weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m])\n",
      "Cell \u001b[0;32mIn [24], line 20\u001b[0m, in \u001b[0;36mpMHC_TCR_model.__init__\u001b[0;34m(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLSTM(input_size, hidden_size, num_layers)\n\u001b[1;32m     19\u001b[0m \u001b[39m# self.label = nn.Linear(hidden_size, 1)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(hidden_size, hidden_size\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_length = 6\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = pMHC_TCR_model(input_size=237, hidden_size=16, batch_size=batch_size, num_layers=2, device=device, use_whole_data=False).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "weights = torch.FloatTensor([5,6])\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "    \n",
    "    # print(f\"The length of train_loader is {len(train_loader)}\") # 34\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\") # 8\n",
    "    # print(f\"The length of train_loader is {len(train_loader.dataset)}\")\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.6927)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((64, 1))\n",
    "b = torch.randn((64, 1))\n",
    "nn.CrossEntropyLoss()(a.view(1,-1), b.view(1,-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful functions and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the sequence are similar, (the length of the sequence are different less than 5 aa) into a batch, and then use the autoencoder to encode the HLA sequence.\n",
    "\n",
    "class LenMatchBatchSampler(data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[] for i in range(300)]\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            count_zeros = int(torch.sum(self.sampler.data_source[idx] == 0) / 5)\n",
    "            buckets[count_zeros].append(idx)\n",
    "\n",
    "            if len(buckets[count_zeros]) == self.batch_size:\n",
    "                batch = list(buckets[count_zeros])\n",
    "                yield batch\n",
    "                buckets[count_zeros] = []\n",
    "\n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not use\n",
    "# random_seq_len = [random.randint(i) for i in range(5)]\n",
    "test_input = torch.empty(0)\n",
    "for i in range(5):\n",
    "    random_seq_len = random.randint(0, 300)\n",
    "    input = torch.randint(5, 100, (1, random_seq_len))\n",
    "    test_input = torch.cat((test_input, input), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not use\n",
    "# # My may need to find a proper way to encode the HLA aa sequence, because there are 10 to 20 different HLA types and some of them have variants which are just one single aa difference.\n",
    "# hla_list = list(set(df[\"HLA\"]))\n",
    "# hla_list.sort()\n",
    "# hla_dict = dict()\n",
    "# for i in range(len(hla_list)):\n",
    "#     hla_dict[hla_list[i]] = i\n",
    "# # The encoding could apply autoencoder to encode the HLA sequence.\n",
    "# df[\"HLA_encode\"] = df[\"HLA\"].map(hla_dict)\n",
    "# X_feature = np.hstack((X_feature, df[\"HLA_encode\"].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 415])\n"
     ]
    }
   ],
   "source": [
    "# torch.randint(0, 10, (3, 5))\n",
    "# torch.randperm(10)\n",
    "random_seq_len = random.randint(0, 300)\n",
    "input = torch.randint(0, 100, (1, 5*random_seq_len))\n",
    "print(input.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9a3d897ef0b1e7415fe4468808571913e41281b79a56511723d411ccb064e7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
