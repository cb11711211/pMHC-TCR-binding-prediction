{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuxinchao/.conda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCRDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_ng = df.copy()\n",
    "        df_ng = df_ng[df_ng[\"HLA\"] != \"-\"]\n",
    "        df_ng[\"Class\"] = \"negative\"\n",
    "        df_ng[\"AseqCDR_3\"] = df_ng[\"AseqCDR_3\"].apply(\n",
    "            lambda x: random.choice(list(set(df[\"AseqCDR_3\"]) - set(x))))\n",
    "        df_ng[\"BseqCDR_3\"] = df_ng[\"BseqCDR_3\"].apply(\n",
    "            lambda x: random.choice(list(set(df[\"BseqCDR_3\"]) - set(x))))\n",
    "        df_pos = df[df[\"Class\"] == \"positive\"]\n",
    "        df = pd.concat([df_pos, df_ng], axis=0)\n",
    "        df = df[\"HLA\", \"Neo\", \"AseqCDR_3\", \"BseqCDR_3\", \"Class\"]\n",
    "        seq_list = [\"AseqCDR_3\", \"BseqCDR_3\"]\n",
    "        len_map = df[seq_list].applymap(len).max()\n",
    "        X_feature = np.zeros((len(df), 0))\n",
    "        for column in seq_list:\n",
    "            df[column] = df[column].str.ljust(len_map[column], \"*\")\n",
    "            encode_seq_result = list()\n",
    "            for i in df[column]:\n",
    "                encode_seq_result.append(encode_seqCDR(i))\n",
    "            col_name = column + \"_encode\"\n",
    "            df[col_name] = encode_seq_result\n",
    "            col_feature = np.zeros((0, len_map[column]*5))\n",
    "            for i in range(len(df)):\n",
    "                col_feature = np.vstack((col_feature, df.loc[i, col_name].reshape(1, -1)))\n",
    "            X_feature = np.hstack((X_feature, col_feature))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not use\n",
    "# My may need to find a proper way to encode the HLA aa sequence, because there are 10 to 20 different HLA types and some of them have variants which are just one single aa difference.\n",
    "hla_list = list(set(df[\"HLA\"]))\n",
    "hla_list.sort()\n",
    "hla_dict = dict()\n",
    "for i in range(len(hla_list)):\n",
    "    hla_dict[hla_list[i]] = i\n",
    "# The encoding could apply autoencoder to encode the HLA sequence.\n",
    "df[\"HLA_encode\"] = df[\"HLA\"].map(hla_dict)\n",
    "X_feature = np.hstack((X_feature, df[\"HLA_encode\"].values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HLAAutoEncoder_twoLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(HLAAutoEncoder_twoLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*4, hidden_dim*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim*4, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HLAAutoEncoder_twoLayer(input_dim=5*len(df[\"aaSeqHLA\"].unique().max()), hidden_dim=5)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "output = []\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for idx, (data) in enumerate(train_loader):\n",
    "        data = Variable(data).float()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the sequence are similar, (the length of the sequence are different less than 5 aa) into a batch, and then use the autoencoder to encode the HLA sequence.\n",
    "\n",
    "class LenMatchBatchSampler(data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[] for i in range(300)]\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            count_zeros = int(torch.sum(self.sampler.data_source[idx] == 0) / 5)\n",
    "            buckets[count_zeros].append(idx)\n",
    "\n",
    "            if len(buckets[count_zeros]) == self.batch_size:\n",
    "                batch = list(buckets[count_zeros])\n",
    "                yield batch\n",
    "                buckets[count_zeros] = []\n",
    "\n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9a3d897ef0b1e7415fe4468808571913e41281b79a56511723d411ccb064e7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
