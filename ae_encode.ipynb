{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                 Type                   Data/Info\n",
      "---------------------------------------------------------\n",
      "A_TCR_seq                ndarray                37476: 37476 elems, type `object`, 299808 bytes (292.78125 kb)\n",
      "A_TCR_seq_encode         Tensor                 tensor([[-1.3430,  0.4650<...>     dtype=torch.float64)\n",
      "DataLoader               type                   <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Dataset                  type                   <class 'torch.utils.data.dataset.Dataset'>\n",
      "F                        module                 <module 'torch.nn.functio<...>/torch/nn/functional.py'>\n",
      "HLAencoder               OneHotEncoder          OneHotEncoder()\n",
      "KFold                    ABCMeta                <class 'sklearn.model_selection._split.KFold'>\n",
      "LabelEncoder             type                   <class 'sklearn.preproces<...>ing._label.LabelEncoder'>\n",
      "OneHotEncoder            type                   <class 'sklearn.preproces<...>_encoders.OneHotEncoder'>\n",
      "RepeatedKFold            ABCMeta                <class 'sklearn.model_sel<...>on._split.RepeatedKFold'>\n",
      "StratifiedKFold          ABCMeta                <class 'sklearn.model_sel<...>._split.StratifiedKFold'>\n",
      "SubsetRandomSampler      type                   <class 'torch.utils.data.<...>ler.SubsetRandomSampler'>\n",
      "TCRData                  pMHC_TCRDataset        <__main__.pMHC_TCRDataset<...>object at 0x7f1d3d3b0b50>\n",
      "TCR_autoencoder          type                   <class '__main__.TCR_autoencoder'>\n",
      "TCR_encode               Tensor                 tensor([[[ 7.3001e+00,  1<...>, grad_fn=<CatBackward0>)\n",
      "TCR_encode_data          type                   <class '__main__.TCR_encode_data'>\n",
      "TCR_encode_loss          float                  0.05397178242237189\n",
      "TCR_encode_losses        list                   n=100\n",
      "TCR_seq                  Tensor                 tensor([[[-1.3430,  0.465<...>000,  0.0000,  0.0000]]])\n",
      "Variable                 VariableMeta           <class 'torch.autograd.variable.Variable'>\n",
      "WeightedRandomSampler    type                   <class 'torch.utils.data.<...>r.WeightedRandomSampler'>\n",
      "X_HLA                    ndarray                37476x1: 37476 elems, type `object`, 299808 bytes (292.78125 kb)\n",
      "X_HLA_encoded            ndarray                37476x2: 74952 elems, type `float64`, 599616 bytes (585.5625 kb)\n",
      "a                        Tensor                 tensor([[False],\\n       <...>alse],\\n        [ True]])\n",
      "af                       DataFrame                          Factor I  Fac<...>.097     -0.838     1.512\n",
      "ax                       ndarray                2: 2 elems, type `object`, 16 bytes\n",
      "b                        Tensor                 tensor([[1., 0.],\\n      <...> 0.],\\n        [0., 1.]])\n",
      "batch_idx                int                    0\n",
      "batch_size               int                    32\n",
      "c                        Tensor                 tensor([[ 0.6416, -0.6369<...>\\n       device='cuda:0')\n",
      "chain                    str                    BseqCDR3\n",
      "correct                  int                    0\n",
      "criterion                CrossEntropyLoss       CrossEntropyLoss()\n",
      "d                        Tensor                 tensor([[-0.1602,  0.0127<...>grad_fn=<AddmmBackward0>)\n",
      "data                     Tensor                 tensor([[ 5.4752e+00, -9.<...>rad_fn=<ToCopyBackward0>)\n",
      "device                   device                 cuda:0\n",
      "df                       DataFrame                        Class          <...>n[37476 rows x 6 columns]\n",
      "encode_seqCDR            function               <function encode_seqCDR at 0x7f1e2291a670>\n",
      "encoded                  Tensor                 tensor([[[ 9.9886e+00,  1<...>ad_fn=<SqueezeBackward1>)\n",
      "encoding                 Tensor                 tensor([[-1.3430,  0.4650<...>0]], dtype=torch.float64)\n",
      "epoch                    int                    1\n",
      "epochs                   int                    100\n",
      "fig                      Figure                 Figure(720x360)\n",
      "file_path                str                    ~/data/project/data/seqData/20230228.csv\n",
      "fold                     int                    0\n",
      "folds                    int                    5\n",
      "i                        int                    37475\n",
      "kernel_size              int                    3\n",
      "kf                       StratifiedKFold        StratifiedKFold(n_splits=<...>m_state=42, shuffle=True)\n",
      "learning_rate            float                  0.001\n",
      "len_map                  dict                   n=2\n",
      "length                   int                    24\n",
      "logging                  module                 <module 'logging' from '/<...>3.9/logging/__init__.py'>\n",
      "loss                     Tensor                 tensor(1.3984, device='cu<...>, grad_fn=<DivBackward1>)\n",
      "loss_fn                  CrossEntropyLoss       CrossEntropyLoss()\n",
      "model                    prediction_model       prediction_model(\\n  (lin<...>res=2, bias=True)\\n  )\\n)\n",
      "nn                       module                 <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\n",
      "np                       module                 <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "one_hot_target           Tensor                 tensor([[1., 0.],\\n      <...>., 0.]], device='cuda:0')\n",
      "optim                    module                 <module 'torch.optim' fro<...>torch/optim/__init__.py'>\n",
      "optimizer                SGD                    SGD (\\nParameter Group 0\\<...>e\\n    weight_decay: 0\\n)\n",
      "os                       module                 <module 'os' from '/home/<...>da3/lib/python3.9/os.py'>\n",
      "output                   Tensor                 tensor([[-0.0433, -0.1083<...> grad_fn=<ViewBackward0>)\n",
      "pMHC_TCRDataset          type                   <class '__main__.pMHC_TCRDataset'>\n",
      "padding                  int                    1\n",
      "pd                       module                 <module 'pandas' from '/h<...>ages/pandas/__init__.py'>\n",
      "plt                      module                 <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "pred                     Tensor                 tensor([[0.],\\n        [0<...>grad_fn=<RoundBackward0>)\n",
      "prediction_model         type                   <class '__main__.prediction_model'>\n",
      "random                   module                 <module 'random' from '/h<...>lib/python3.9/random.py'>\n",
      "repeats                  int                    12\n",
      "reset_weights            function               <function reset_weights at 0x7f1d1d3bb9d0>\n",
      "seq                      str                    Neo_last3\n",
      "seq_length               int                    6\n",
      "sns                      module                 <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n",
      "stride                   int                    2\n",
      "target                   Tensor                 tensor([[False],\\n       <...>alse],\\n        [False]])\n",
      "test                     function               <function test at 0x7f1d36670940>\n",
      "test_accuracy_history    list                   n=0\n",
      "test_correct             float                  0.12848222862632083\n",
      "test_idx                 ndarray                7496: 7496 elems, type `int64`, 59968 bytes\n",
      "test_loader              DataLoader             <torch.utils.data.dataloa<...>object at 0x7f1d1d39d820>\n",
      "test_losses              float                  0.01787345072721542\n",
      "test_losses_history      list                   n=0\n",
      "test_subsampler          SubsetRandomSampler    <torch.utils.data.sampler<...>object at 0x7f1d1d39d7c0>\n",
      "torch                    module                 <module 'torch' from '/ho<...>kages/torch/__init__.py'>\n",
      "tqdm                     module                 <module 'tqdm' from '/hom<...>ckages/tqdm/__init__.py'>\n",
      "train                    function               <function train at 0x7f1d1d3bb5e0>\n",
      "train_accuracy_history   list                   n=0\n",
      "train_autoencoder        function               <function train_autoencoder at 0x7f1edc181dc0>\n",
      "train_correct            list                   n=0\n",
      "train_idx                ndarray                29980: 29980 elems, type `int64`, 239840 bytes (234.21875 kb)\n",
      "train_loader             DataLoader             <torch.utils.data.dataloa<...>object at 0x7f1d1d39d7f0>\n",
      "train_loss               float                  0.8487564325332642\n",
      "train_losses             list                   n=0\n",
      "train_losses_history     list                   n=0\n",
      "train_subsampler         SubsetRandomSampler    <torch.utils.data.sampler<...>object at 0x7f1d1d3bc7c0>\n",
      "train_test_split         function               <function train_test_split at 0x7f1e22a40af0>\n",
      "weights                  Tensor                 tensor([ 1., 10.])\n",
      "y                        ndarray                37476: 37476 elems, type `int64`, 299808 bytes (292.78125 kb)\n"
     ]
    }
   ],
   "source": [
    "# check the kernel running in the notebook\n",
    "# !uname -a\n",
    "# find the variables in the notebook\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seqCDR[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_encode_data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # drop the rows with duplicate CDR3 sequences\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        \n",
    "        # drop the rows with length == max length, which is much longer than the others\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # If there is any NaN value, drop the row\n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        # concatenate the encoded features\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "        \n",
    "        # discard the duplicate rows, keep the first one\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 49)\n",
    "            nn.Conv1d(5, 10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 25) based on the formula for conv1d: (W + 2P - K)/S + 1 = (49 + 2*1 - 3)/2 + 1 = 25\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 10, 23), 25 - 2 = 23 \n",
    "\n",
    "            nn.Conv1d(10, 15, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12) based on the formula for conv1d: (W + 2P - K)/S + 1 = (23 + 2*1 - 3)/2 + 1 = 12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 10), 12 - 2 = 10\n",
    "\n",
    "            nn.Conv1d(15, 20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 5) based on the formula for conv1d: (W + 2P - K)/S + 1 = (10 + 2*1 - 3)/2 + 1 = 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 3)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 3)\n",
    "            nn.ConvTranspose1d(20, 15, kernel_size=3, stride=3, padding=1),\n",
    "            # (batch_size, 15, 5), based on the formula for convtranspose1d: (W−1)S−2P+F = (3-1)*3-2*1+3= 7\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(15, 10, kernel_size=7, stride=3, padding=1),\n",
    "            # (batch_size, 10, 23) based on the formula for convtranspose1d: (W−1)S−2P+F = (7-1)*3-2*1+7= 23\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(10, 5, kernel_size=7, stride=2, padding=1),\n",
    "            # (batch_size, 5, 49) based on the formula for convtranspose1d: (W−1)S−2P+F = (23-1)*2-2*1+7= 49\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = torch.tensor(x, dtype=np.float32)\n",
    "        # x = torch.tensor(x, dtype=torch.float)\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        # print(f\"encoding shape: {encoded.shape}\")\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2492 (0%)]\tLoss: 1.422211\n",
      "Train Epoch: 1 [1600/2492 (65%)]\tLoss: 1.269276\n",
      "Train Epoch: 2 [0/2492 (0%)]\tLoss: 1.270869\n",
      "Train Epoch: 2 [1600/2492 (65%)]\tLoss: 1.301361\n",
      "Train Epoch: 3 [0/2492 (0%)]\tLoss: 1.353347\n",
      "Train Epoch: 3 [1600/2492 (65%)]\tLoss: 1.198825\n",
      "Train Epoch: 4 [0/2492 (0%)]\tLoss: 1.316884\n",
      "Train Epoch: 4 [1600/2492 (65%)]\tLoss: 1.105351\n",
      "Train Epoch: 5 [0/2492 (0%)]\tLoss: 1.217188\n",
      "Train Epoch: 5 [1600/2492 (65%)]\tLoss: 1.153005\n",
      "Train Epoch: 6 [0/2492 (0%)]\tLoss: 0.985455\n",
      "Train Epoch: 6 [1600/2492 (65%)]\tLoss: 1.065668\n",
      "Train Epoch: 7 [0/2492 (0%)]\tLoss: 1.051921\n",
      "Train Epoch: 7 [1600/2492 (65%)]\tLoss: 1.016856\n",
      "Train Epoch: 8 [0/2492 (0%)]\tLoss: 1.158589\n",
      "Train Epoch: 8 [1600/2492 (65%)]\tLoss: 1.101415\n",
      "Train Epoch: 9 [0/2492 (0%)]\tLoss: 1.090983\n",
      "Train Epoch: 9 [1600/2492 (65%)]\tLoss: 1.065062\n",
      "Train Epoch: 10 [0/2492 (0%)]\tLoss: 1.122251\n",
      "Train Epoch: 10 [1600/2492 (65%)]\tLoss: 1.083244\n",
      "Train Epoch: 11 [0/2492 (0%)]\tLoss: 1.141127\n",
      "Train Epoch: 11 [1600/2492 (65%)]\tLoss: 1.032887\n",
      "Train Epoch: 12 [0/2492 (0%)]\tLoss: 1.072594\n",
      "Train Epoch: 12 [1600/2492 (65%)]\tLoss: 0.974644\n",
      "Train Epoch: 13 [0/2492 (0%)]\tLoss: 1.030072\n",
      "Train Epoch: 13 [1600/2492 (65%)]\tLoss: 0.946859\n",
      "Train Epoch: 14 [0/2492 (0%)]\tLoss: 1.069577\n",
      "Train Epoch: 14 [1600/2492 (65%)]\tLoss: 1.167268\n",
      "Train Epoch: 15 [0/2492 (0%)]\tLoss: 1.114951\n",
      "Train Epoch: 15 [1600/2492 (65%)]\tLoss: 1.155847\n",
      "Train Epoch: 16 [0/2492 (0%)]\tLoss: 0.954150\n",
      "Train Epoch: 16 [1600/2492 (65%)]\tLoss: 0.981289\n",
      "Train Epoch: 17 [0/2492 (0%)]\tLoss: 1.093097\n",
      "Train Epoch: 17 [1600/2492 (65%)]\tLoss: 0.955089\n",
      "Train Epoch: 18 [0/2492 (0%)]\tLoss: 1.086022\n",
      "Train Epoch: 18 [1600/2492 (65%)]\tLoss: 0.995435\n",
      "Train Epoch: 19 [0/2492 (0%)]\tLoss: 0.971045\n",
      "Train Epoch: 19 [1600/2492 (65%)]\tLoss: 1.015016\n",
      "Train Epoch: 20 [0/2492 (0%)]\tLoss: 1.036492\n",
      "Train Epoch: 20 [1600/2492 (65%)]\tLoss: 1.102179\n",
      "Train Epoch: 21 [0/2492 (0%)]\tLoss: 1.119652\n",
      "Train Epoch: 21 [1600/2492 (65%)]\tLoss: 0.969431\n",
      "Train Epoch: 22 [0/2492 (0%)]\tLoss: 1.091771\n",
      "Train Epoch: 22 [1600/2492 (65%)]\tLoss: 0.892847\n",
      "Train Epoch: 23 [0/2492 (0%)]\tLoss: 1.033807\n",
      "Train Epoch: 23 [1600/2492 (65%)]\tLoss: 0.962311\n",
      "Train Epoch: 24 [0/2492 (0%)]\tLoss: 0.925607\n",
      "Train Epoch: 24 [1600/2492 (65%)]\tLoss: 1.076444\n",
      "Train Epoch: 25 [0/2492 (0%)]\tLoss: 0.914540\n",
      "Train Epoch: 25 [1600/2492 (65%)]\tLoss: 0.929081\n",
      "Train Epoch: 26 [0/2492 (0%)]\tLoss: 1.018931\n",
      "Train Epoch: 26 [1600/2492 (65%)]\tLoss: 0.876093\n",
      "Train Epoch: 27 [0/2492 (0%)]\tLoss: 0.947629\n",
      "Train Epoch: 27 [1600/2492 (65%)]\tLoss: 1.014299\n",
      "Train Epoch: 28 [0/2492 (0%)]\tLoss: 0.915245\n",
      "Train Epoch: 28 [1600/2492 (65%)]\tLoss: 1.008010\n",
      "Train Epoch: 29 [0/2492 (0%)]\tLoss: 0.914160\n",
      "Train Epoch: 29 [1600/2492 (65%)]\tLoss: 1.048587\n",
      "Train Epoch: 30 [0/2492 (0%)]\tLoss: 1.006159\n",
      "Train Epoch: 30 [1600/2492 (65%)]\tLoss: 1.031503\n",
      "Train Epoch: 31 [0/2492 (0%)]\tLoss: 0.994206\n",
      "Train Epoch: 31 [1600/2492 (65%)]\tLoss: 0.911761\n",
      "Train Epoch: 32 [0/2492 (0%)]\tLoss: 0.950366\n",
      "Train Epoch: 32 [1600/2492 (65%)]\tLoss: 0.910195\n",
      "Train Epoch: 33 [0/2492 (0%)]\tLoss: 0.905822\n",
      "Train Epoch: 33 [1600/2492 (65%)]\tLoss: 1.010210\n",
      "Train Epoch: 34 [0/2492 (0%)]\tLoss: 0.950278\n",
      "Train Epoch: 34 [1600/2492 (65%)]\tLoss: 0.983163\n",
      "Train Epoch: 35 [0/2492 (0%)]\tLoss: 1.048163\n",
      "Train Epoch: 35 [1600/2492 (65%)]\tLoss: 0.928133\n",
      "Train Epoch: 36 [0/2492 (0%)]\tLoss: 0.911688\n",
      "Train Epoch: 36 [1600/2492 (65%)]\tLoss: 0.957520\n",
      "Train Epoch: 37 [0/2492 (0%)]\tLoss: 0.936063\n",
      "Train Epoch: 37 [1600/2492 (65%)]\tLoss: 0.939569\n",
      "Train Epoch: 38 [0/2492 (0%)]\tLoss: 0.940515\n",
      "Train Epoch: 38 [1600/2492 (65%)]\tLoss: 1.011984\n",
      "Train Epoch: 39 [0/2492 (0%)]\tLoss: 0.997811\n",
      "Train Epoch: 39 [1600/2492 (65%)]\tLoss: 0.863154\n",
      "Train Epoch: 40 [0/2492 (0%)]\tLoss: 0.981072\n",
      "Train Epoch: 40 [1600/2492 (65%)]\tLoss: 0.923593\n",
      "Train Epoch: 41 [0/2492 (0%)]\tLoss: 1.038069\n",
      "Train Epoch: 41 [1600/2492 (65%)]\tLoss: 0.883902\n",
      "Train Epoch: 42 [0/2492 (0%)]\tLoss: 0.912875\n",
      "Train Epoch: 42 [1600/2492 (65%)]\tLoss: 0.912833\n",
      "Train Epoch: 43 [0/2492 (0%)]\tLoss: 1.001985\n",
      "Train Epoch: 43 [1600/2492 (65%)]\tLoss: 0.869817\n",
      "Train Epoch: 44 [0/2492 (0%)]\tLoss: 0.906789\n",
      "Train Epoch: 44 [1600/2492 (65%)]\tLoss: 1.009579\n",
      "Train Epoch: 45 [0/2492 (0%)]\tLoss: 0.936641\n",
      "Train Epoch: 45 [1600/2492 (65%)]\tLoss: 1.073481\n",
      "Train Epoch: 46 [0/2492 (0%)]\tLoss: 1.054508\n",
      "Train Epoch: 46 [1600/2492 (65%)]\tLoss: 0.877092\n",
      "Train Epoch: 47 [0/2492 (0%)]\tLoss: 0.914637\n",
      "Train Epoch: 47 [1600/2492 (65%)]\tLoss: 0.916702\n",
      "Train Epoch: 48 [0/2492 (0%)]\tLoss: 1.088040\n",
      "Train Epoch: 48 [1600/2492 (65%)]\tLoss: 0.891321\n",
      "Train Epoch: 49 [0/2492 (0%)]\tLoss: 0.850180\n",
      "Train Epoch: 49 [1600/2492 (65%)]\tLoss: 0.955480\n",
      "Train Epoch: 50 [0/2492 (0%)]\tLoss: 0.892897\n",
      "Train Epoch: 50 [1600/2492 (65%)]\tLoss: 0.935784\n",
      "Train Epoch: 51 [0/2492 (0%)]\tLoss: 0.947202\n",
      "Train Epoch: 51 [1600/2492 (65%)]\tLoss: 0.836113\n",
      "Train Epoch: 52 [0/2492 (0%)]\tLoss: 1.016443\n",
      "Train Epoch: 52 [1600/2492 (65%)]\tLoss: 0.869260\n",
      "Train Epoch: 53 [0/2492 (0%)]\tLoss: 0.880446\n",
      "Train Epoch: 53 [1600/2492 (65%)]\tLoss: 0.927822\n",
      "Train Epoch: 54 [0/2492 (0%)]\tLoss: 0.883210\n",
      "Train Epoch: 54 [1600/2492 (65%)]\tLoss: 0.923077\n",
      "Train Epoch: 55 [0/2492 (0%)]\tLoss: 0.895967\n",
      "Train Epoch: 55 [1600/2492 (65%)]\tLoss: 0.941418\n",
      "Train Epoch: 56 [0/2492 (0%)]\tLoss: 0.890716\n",
      "Train Epoch: 56 [1600/2492 (65%)]\tLoss: 1.000721\n",
      "Train Epoch: 57 [0/2492 (0%)]\tLoss: 0.855821\n",
      "Train Epoch: 57 [1600/2492 (65%)]\tLoss: 0.851223\n",
      "Train Epoch: 58 [0/2492 (0%)]\tLoss: 0.984846\n",
      "Train Epoch: 58 [1600/2492 (65%)]\tLoss: 0.992998\n",
      "Train Epoch: 59 [0/2492 (0%)]\tLoss: 0.874181\n",
      "Train Epoch: 59 [1600/2492 (65%)]\tLoss: 0.969747\n",
      "Train Epoch: 60 [0/2492 (0%)]\tLoss: 0.973944\n",
      "Train Epoch: 60 [1600/2492 (65%)]\tLoss: 0.912120\n",
      "Train Epoch: 61 [0/2492 (0%)]\tLoss: 0.982802\n",
      "Train Epoch: 61 [1600/2492 (65%)]\tLoss: 0.949310\n",
      "Train Epoch: 62 [0/2492 (0%)]\tLoss: 0.870144\n",
      "Train Epoch: 62 [1600/2492 (65%)]\tLoss: 1.011012\n",
      "Train Epoch: 63 [0/2492 (0%)]\tLoss: 0.914965\n",
      "Train Epoch: 63 [1600/2492 (65%)]\tLoss: 0.960154\n",
      "Train Epoch: 64 [0/2492 (0%)]\tLoss: 0.933336\n",
      "Train Epoch: 64 [1600/2492 (65%)]\tLoss: 0.941942\n",
      "Train Epoch: 65 [0/2492 (0%)]\tLoss: 0.881826\n",
      "Train Epoch: 65 [1600/2492 (65%)]\tLoss: 1.053723\n",
      "Train Epoch: 66 [0/2492 (0%)]\tLoss: 0.891281\n",
      "Train Epoch: 66 [1600/2492 (65%)]\tLoss: 0.912338\n",
      "Train Epoch: 67 [0/2492 (0%)]\tLoss: 0.941636\n",
      "Train Epoch: 67 [1600/2492 (65%)]\tLoss: 0.911273\n",
      "Train Epoch: 68 [0/2492 (0%)]\tLoss: 0.927178\n",
      "Train Epoch: 68 [1600/2492 (65%)]\tLoss: 0.801157\n",
      "Train Epoch: 69 [0/2492 (0%)]\tLoss: 0.937126\n",
      "Train Epoch: 69 [1600/2492 (65%)]\tLoss: 0.969854\n",
      "Train Epoch: 70 [0/2492 (0%)]\tLoss: 0.792439\n",
      "Train Epoch: 70 [1600/2492 (65%)]\tLoss: 0.855129\n",
      "Train Epoch: 71 [0/2492 (0%)]\tLoss: 1.037783\n",
      "Train Epoch: 71 [1600/2492 (65%)]\tLoss: 0.815436\n",
      "Train Epoch: 72 [0/2492 (0%)]\tLoss: 0.873313\n",
      "Train Epoch: 72 [1600/2492 (65%)]\tLoss: 0.772607\n",
      "Train Epoch: 73 [0/2492 (0%)]\tLoss: 0.952622\n",
      "Train Epoch: 73 [1600/2492 (65%)]\tLoss: 0.859415\n",
      "Train Epoch: 74 [0/2492 (0%)]\tLoss: 0.861181\n",
      "Train Epoch: 74 [1600/2492 (65%)]\tLoss: 0.826481\n",
      "Train Epoch: 75 [0/2492 (0%)]\tLoss: 0.754362\n",
      "Train Epoch: 75 [1600/2492 (65%)]\tLoss: 0.786918\n",
      "Train Epoch: 76 [0/2492 (0%)]\tLoss: 0.893129\n",
      "Train Epoch: 76 [1600/2492 (65%)]\tLoss: 0.932158\n",
      "Train Epoch: 77 [0/2492 (0%)]\tLoss: 0.835916\n",
      "Train Epoch: 77 [1600/2492 (65%)]\tLoss: 0.887524\n",
      "Train Epoch: 78 [0/2492 (0%)]\tLoss: 0.889617\n",
      "Train Epoch: 78 [1600/2492 (65%)]\tLoss: 0.898048\n",
      "Train Epoch: 79 [0/2492 (0%)]\tLoss: 0.899574\n",
      "Train Epoch: 79 [1600/2492 (65%)]\tLoss: 0.834428\n",
      "Train Epoch: 80 [0/2492 (0%)]\tLoss: 0.848618\n",
      "Train Epoch: 80 [1600/2492 (65%)]\tLoss: 0.831238\n",
      "Train Epoch: 81 [0/2492 (0%)]\tLoss: 0.856094\n",
      "Train Epoch: 81 [1600/2492 (65%)]\tLoss: 0.880659\n",
      "Train Epoch: 82 [0/2492 (0%)]\tLoss: 0.935261\n",
      "Train Epoch: 82 [1600/2492 (65%)]\tLoss: 0.862267\n",
      "Train Epoch: 83 [0/2492 (0%)]\tLoss: 0.895307\n",
      "Train Epoch: 83 [1600/2492 (65%)]\tLoss: 0.833848\n",
      "Train Epoch: 84 [0/2492 (0%)]\tLoss: 0.890672\n",
      "Train Epoch: 84 [1600/2492 (65%)]\tLoss: 0.916411\n",
      "Train Epoch: 85 [0/2492 (0%)]\tLoss: 0.905846\n",
      "Train Epoch: 85 [1600/2492 (65%)]\tLoss: 0.915573\n",
      "Train Epoch: 86 [0/2492 (0%)]\tLoss: 0.825428\n",
      "Train Epoch: 86 [1600/2492 (65%)]\tLoss: 0.829772\n",
      "Train Epoch: 87 [0/2492 (0%)]\tLoss: 0.912543\n",
      "Train Epoch: 87 [1600/2492 (65%)]\tLoss: 0.855514\n",
      "Train Epoch: 88 [0/2492 (0%)]\tLoss: 0.925518\n",
      "Train Epoch: 88 [1600/2492 (65%)]\tLoss: 0.911232\n",
      "Train Epoch: 89 [0/2492 (0%)]\tLoss: 0.820629\n",
      "Train Epoch: 89 [1600/2492 (65%)]\tLoss: 0.813733\n",
      "Train Epoch: 90 [0/2492 (0%)]\tLoss: 0.839303\n",
      "Train Epoch: 90 [1600/2492 (65%)]\tLoss: 0.888941\n",
      "Train Epoch: 91 [0/2492 (0%)]\tLoss: 0.845602\n",
      "Train Epoch: 91 [1600/2492 (65%)]\tLoss: 0.915797\n",
      "Train Epoch: 92 [0/2492 (0%)]\tLoss: 0.892502\n",
      "Train Epoch: 92 [1600/2492 (65%)]\tLoss: 0.918940\n",
      "Train Epoch: 93 [0/2492 (0%)]\tLoss: 0.967052\n",
      "Train Epoch: 93 [1600/2492 (65%)]\tLoss: 0.829256\n",
      "Train Epoch: 94 [0/2492 (0%)]\tLoss: 0.873799\n",
      "Train Epoch: 94 [1600/2492 (65%)]\tLoss: 0.867253\n",
      "Train Epoch: 95 [0/2492 (0%)]\tLoss: 0.861229\n",
      "Train Epoch: 95 [1600/2492 (65%)]\tLoss: 0.749748\n",
      "Train Epoch: 96 [0/2492 (0%)]\tLoss: 0.879034\n",
      "Train Epoch: 96 [1600/2492 (65%)]\tLoss: 0.849366\n",
      "Train Epoch: 97 [0/2492 (0%)]\tLoss: 0.898900\n",
      "Train Epoch: 97 [1600/2492 (65%)]\tLoss: 0.839055\n",
      "Train Epoch: 98 [0/2492 (0%)]\tLoss: 0.789184\n",
      "Train Epoch: 98 [1600/2492 (65%)]\tLoss: 0.810359\n",
      "Train Epoch: 99 [0/2492 (0%)]\tLoss: 0.813414\n",
      "Train Epoch: 99 [1600/2492 (65%)]\tLoss: 0.880330\n",
      "Train Epoch: 100 [0/2492 (0%)]\tLoss: 0.858247\n",
      "Train Epoch: 100 [1600/2492 (65%)]\tLoss: 0.998067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcd1f7c5ac0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training the autoencoder to encode the TCR sequence\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        # TCR_encode_losses.append(loss.item() / model.batch_size)\n",
    "        # TCR_encode_losses.append(loss.item())\n",
    "        # sum up batch loss\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.set_title(\"TCR encode loss\")\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# load the model\n",
    "# model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "# model.load_state_dict(torch.load(\"/DATA/User/wuxinchao/project/pMHC-TCR/ckpt/TCR_autoencoder.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 3))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    TCR_seq = TCR_seq.view(1, 5, 49).float()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test, not used\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "kernel_size, stride, padding, seq_length\n",
    "# pool of size=3, stride=2\n",
    "# m = nn.MaxPool1d(3, stride=1)\n",
    "# m = nn.Conv1d(16, 33, 3, stride=2, padding=1)\n",
    "m = nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=1)\n",
    "# m = nn.MaxUnpool1d(kernel_size=3, stride=1)\n",
    "input = torch.randn(20, 16, 3)\n",
    "output = m(input)\n",
    "output.shape\n",
    "# TCRData[0][0].shape\n",
    "len(TCRData)\n",
    "# model(TCRData[0:3][0].float().view(3,5,seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TCR_encode(nn.Module):\n",
    "    '''\n",
    "    LSTM for TCR sequence encoding.\n",
    "    The input size of LSTM is (batch_size, seq_length, input_size), the output size is (batch_size, seq_length, hidden_size)\n",
    "    '''\n",
    "    def __init__(self, seq_length, hidden_size, num_layers, device):\n",
    "        super(LSTM_TCR_encode, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(seq_length, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model for TCR sequence encoding, this may not be used in the future\n",
    "# How to use the LSTM model to encode the TCR sequence\n",
    "# The optimization \n",
    "def train_LSTM_TCR_encode(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, seq_length, 5)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training: {batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%) \\\n",
    "                  Loss: {loss.item():.6f}\")\n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the LSTM model\n",
    "model = LSTM_TCR_encode(seq_length, hidden_size, num_layers, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    Here the input is the TCR sequence, neoantigen sequence, and HLA type.\n",
    "    The output should be the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 file_path, \n",
    "                 only_CDR3: bool = False, \n",
    "                 only_experimental: bool = False, \n",
    "                 TCR_encode: str = [\"LSTM\", \"CNN\"],\n",
    "                 encoding_model: nn.Module = None) -> None:\n",
    "        df, HLA_encode, y  = self.basic_io(file_path, only_experimental=only_experimental)\n",
    "\n",
    "        # convert from object to tensor\n",
    "        X_TCR_seq = torch.zeros((len(df), 0))\n",
    "        for region in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            TCR_seq = df.loc[:, region].values\n",
    "            TCR_seq_encode = torch.zeros((0, TCR_seq[0].shape[1]))\n",
    "            for i in range(len(TCR_seq)):\n",
    "                encoding = torch.from_numpy(TCR_seq[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                TCR_seq_encode = torch.cat((TCR_seq_encode, encoding), dim=0)\n",
    "\n",
    "            X_TCR_seq = torch.cat((TCR_seq_encode, X_TCR_seq), dim=1)\n",
    "        \n",
    "        if TCR_encode == \"CNN\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        elif TCR_encode == \"LSTM\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        else:\n",
    "            raise ValueError(\"The TCR encoding method is not supported yet.\")\n",
    "        \n",
    "        # encoding model \n",
    "        X_features, _ = encoding_model(X_TCR_seq)\n",
    "        X_features = X_features.view(-1, 20 * 3).data\n",
    "\n",
    "        # add the neoantigen sequence encoding features\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            neo = df.loc[:, seq].values\n",
    "            neo_encode = torch.zeros((0, neo[0].shape[1]))\n",
    "            for i in range(len(neo)):\n",
    "                encoding = torch.from_numpy(neo[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                neo_encode = torch.cat((neo_encode, encoding), dim=0)\n",
    "            X_features = torch.cat((X_features, neo_encode), dim=1)\n",
    "\n",
    "        X_features = torch.cat((X_features, torch.from_numpy(HLA_encode)), dim=1)\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "            \n",
    "    \n",
    "    def basic_io(self, file_path, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # drop the rows with nan\n",
    "        df = df.dropna()\n",
    "\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        return df, X_HLA_encoded, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, TCR_encode=\"CNN\", only_experimental=True, encoding_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCRData[0][0].shape                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "    length = len_map[chain]\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# drop the rows with nan\n",
    "df = df.dropna()\n",
    "\n",
    "X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "HLAencoder = OneHotEncoder()\n",
    "X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 batch_size=32,) -> None:\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(self.batch_size, self.input_size)\n",
    "        # print(f\"The model input shape is : {input.shape}\")\n",
    "        output = self.linear_layer(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # one-hot encoding the target\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        target = target.to(torch.bool)\n",
    "        one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "        one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "        one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "\n",
    "        data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "        data = data.view(-1, 60+5*6+2).to(torch.float32).data\n",
    "        output = model(data)\n",
    "        # based on the one-hot target to decide the weights of loss for each class\n",
    "        weight = (one_hot_target == torch.tensor([1, 0])).sum() / (one_hot_target == torch.tensor([0, 1])).sum()\n",
    "        weights = torch.tensor([1, weight]).to(device)\n",
    "        loss = criterion(output, one_hot_target.data, weight=weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.sigmoid().round()\n",
    "        # correct += pred.eq(one_hot_target.view_as(pred)).sum().item()\n",
    "        correct += (pred.argmax(dim=1) == one_hot_target.argmax(dim=1)).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training stage for Flod {fold} Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \\\n",
    "                ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "            print(f\"The current output is {output}, and the target is {one_hot_target}\")\n",
    "    return train_loss, correct / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            target = target.to(torch.bool)\n",
    "            one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "            one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "            one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "            data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "            data = data.view(-1, 60+5*6+2).to(torch.float32)\n",
    "            output = model(data)\n",
    "            one_hot_target = one_hot_target.to(torch.float32).view(-1, 1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output.reshape(1,-1), one_hot_target.reshape(1,-1)).item()  # sum up loss\n",
    "            # print(test_loss)\n",
    "            pred = output.sigmoid().round()\n",
    "            # correct += pred.eq(one_hot_target.view_as(pred)).sum().item()\n",
    "            correct += (pred.argmax(dim=1) == one_hot_target.argmax(dim=1)).sum().item()\n",
    "    \n",
    "    # test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n",
      "-------------------Fold 0-------------------\n",
      "Training stage for Flod 0 Epoch: 1 [0/37476                 (0%)]\tLoss: 0.397345\n",
      "The current output is tensor([[ 0.3720, -0.3258],\n",
      "        [ 0.3207, -0.3272],\n",
      "        [ 0.4221, -0.4603],\n",
      "        [ 0.3225, -0.3532],\n",
      "        [ 0.4010, -0.4037],\n",
      "        [ 0.3250, -0.3437],\n",
      "        [ 0.3895, -0.4020],\n",
      "        [ 0.2783, -0.3146],\n",
      "        [ 0.3427, -0.3549],\n",
      "        [ 0.3899, -0.4496],\n",
      "        [ 0.3526, -0.3526],\n",
      "        [ 0.3817, -0.3931],\n",
      "        [ 0.3146, -0.3064],\n",
      "        [ 0.3974, -0.4108],\n",
      "        [ 0.3112, -0.3318],\n",
      "        [ 0.2806, -0.3174],\n",
      "        [ 0.3627, -0.3250],\n",
      "        [ 0.3984, -0.3679],\n",
      "        [ 0.3415, -0.3828],\n",
      "        [ 0.3125, -0.4137],\n",
      "        [ 0.2619, -0.2963],\n",
      "        [ 0.5625, -0.4626],\n",
      "        [ 0.3855, -0.3826],\n",
      "        [ 0.3452, -0.3460],\n",
      "        [ 0.3259, -0.3180],\n",
      "        [ 0.3690, -0.3527],\n",
      "        [ 0.4100, -0.4612],\n",
      "        [ 0.3552, -0.3893],\n",
      "        [ 0.3199, -0.3947],\n",
      "        [ 0.2899, -0.3037],\n",
      "        [ 0.4270, -0.4289],\n",
      "        [ 0.2634, -0.2761]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [3200/37476                 (11%)]\tLoss: 0.459055\n",
      "The current output is tensor([[ 1.6317, -1.9050],\n",
      "        [ 1.7208, -1.9942],\n",
      "        [ 1.6312, -1.8990],\n",
      "        [ 1.6221, -1.8902],\n",
      "        [ 1.6832, -2.0176],\n",
      "        [ 1.5159, -1.7699],\n",
      "        [ 1.9042, -2.2677],\n",
      "        [ 1.3486, -1.5597],\n",
      "        [ 1.8344, -2.1606],\n",
      "        [ 1.5365, -1.7878],\n",
      "        [ 1.5031, -1.7865],\n",
      "        [ 1.6681, -1.9299],\n",
      "        [ 1.7458, -2.0863],\n",
      "        [ 1.4200, -1.6311],\n",
      "        [ 1.4993, -1.7561],\n",
      "        [ 1.6042, -1.8599],\n",
      "        [ 1.5600, -1.8174],\n",
      "        [ 2.1592, -2.6174],\n",
      "        [ 1.4064, -1.6076],\n",
      "        [ 1.8484, -2.2251],\n",
      "        [ 1.4788, -1.7186],\n",
      "        [ 1.7683, -2.1182],\n",
      "        [ 1.5419, -1.7793],\n",
      "        [ 1.5367, -1.7913],\n",
      "        [ 1.8430, -2.1668],\n",
      "        [ 1.9167, -2.3407],\n",
      "        [ 1.8327, -2.1479],\n",
      "        [ 1.6613, -1.9396],\n",
      "        [ 1.6630, -1.9565],\n",
      "        [ 1.7753, -2.1100],\n",
      "        [ 1.6727, -2.0269],\n",
      "        [ 1.7006, -1.9761]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [6400/37476                 (21%)]\tLoss: 0.145086\n",
      "The current output is tensor([[ 1.6932, -1.9690],\n",
      "        [ 1.6158, -1.8649],\n",
      "        [ 2.1195, -2.5415],\n",
      "        [ 1.7383, -2.0176],\n",
      "        [ 1.8935, -2.2390],\n",
      "        [ 1.9307, -2.2812],\n",
      "        [ 1.6905, -1.9687],\n",
      "        [ 1.7812, -2.0916],\n",
      "        [ 2.0916, -2.5000],\n",
      "        [ 1.9600, -2.3129],\n",
      "        [ 2.0024, -2.3747],\n",
      "        [ 1.5502, -1.7869],\n",
      "        [ 1.9977, -2.3821],\n",
      "        [ 2.0131, -2.4001],\n",
      "        [ 1.7070, -1.9761],\n",
      "        [ 1.9582, -2.3239],\n",
      "        [ 1.9612, -2.3245],\n",
      "        [ 1.6472, -1.8978],\n",
      "        [ 2.0500, -2.4282],\n",
      "        [ 2.2046, -2.6417],\n",
      "        [ 2.1061, -2.5204],\n",
      "        [ 2.0804, -2.4889],\n",
      "        [ 1.8752, -2.2210],\n",
      "        [ 2.0568, -2.4586],\n",
      "        [ 1.7759, -2.0861],\n",
      "        [ 1.9031, -2.2482],\n",
      "        [ 1.9720, -2.3483],\n",
      "        [ 1.8097, -2.1286],\n",
      "        [ 1.8707, -2.2117],\n",
      "        [ 1.7031, -1.9771],\n",
      "        [ 1.7024, -1.9746],\n",
      "        [ 1.8635, -2.2046]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [9600/37476                 (32%)]\tLoss: 0.135462\n",
      "The current output is tensor([[ 1.3610, -1.5038],\n",
      "        [ 1.4481, -1.6109],\n",
      "        [ 1.1169, -1.1827],\n",
      "        [ 1.4388, -1.5956],\n",
      "        [ 1.4202, -1.5761],\n",
      "        [ 1.7943, -2.0433],\n",
      "        [ 2.1221, -2.4661],\n",
      "        [ 1.6516, -1.8710],\n",
      "        [ 2.0448, -2.3560],\n",
      "        [ 1.7556, -1.9892],\n",
      "        [ 1.8926, -2.1771],\n",
      "        [ 1.9572, -2.2633],\n",
      "        [ 2.0291, -2.3526],\n",
      "        [ 1.0037, -1.0694],\n",
      "        [ 1.9866, -2.2833],\n",
      "        [ 1.3917, -1.5385],\n",
      "        [ 1.7022, -1.9329],\n",
      "        [ 1.6127, -1.8077],\n",
      "        [ 1.5983, -1.8160],\n",
      "        [ 2.0361, -2.3394],\n",
      "        [ 1.9894, -2.2934],\n",
      "        [ 1.1575, -1.2560],\n",
      "        [ 1.4282, -1.5876],\n",
      "        [ 1.9118, -2.1888],\n",
      "        [ 1.4880, -1.6553],\n",
      "        [ 1.6624, -1.8856],\n",
      "        [ 0.8713, -0.8883],\n",
      "        [ 1.3311, -1.4539],\n",
      "        [ 1.7910, -2.0383],\n",
      "        [ 1.4873, -1.6491],\n",
      "        [ 1.9776, -2.2778],\n",
      "        [ 1.7225, -1.9721]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [12800/37476                 (43%)]\tLoss: 0.234983\n",
      "The current output is tensor([[ 2.5717, -2.9236],\n",
      "        [ 2.8975, -3.2736],\n",
      "        [ 2.0434, -2.3394],\n",
      "        [ 1.9367, -2.1416],\n",
      "        [ 2.8851, -3.2747],\n",
      "        [ 2.1288, -2.4202],\n",
      "        [ 2.2311, -2.5230],\n",
      "        [ 2.4973, -2.8477],\n",
      "        [ 1.8858, -2.1265],\n",
      "        [ 2.4654, -2.7969],\n",
      "        [ 2.5826, -2.9533],\n",
      "        [ 2.2219, -2.5313],\n",
      "        [ 2.2566, -2.5335],\n",
      "        [ 2.9962, -3.3731],\n",
      "        [ 2.4745, -2.7599],\n",
      "        [ 2.7120, -3.0867],\n",
      "        [ 2.5373, -2.8875],\n",
      "        [ 2.6861, -3.0554],\n",
      "        [ 2.1320, -2.4070],\n",
      "        [ 2.9297, -3.3622],\n",
      "        [ 1.4587, -1.6308],\n",
      "        [ 2.7373, -3.1123],\n",
      "        [ 2.6194, -2.9426],\n",
      "        [ 2.0204, -2.2810],\n",
      "        [ 1.7564, -1.9831],\n",
      "        [ 3.4374, -3.9212],\n",
      "        [ 1.9523, -2.2006],\n",
      "        [ 3.1632, -3.6117],\n",
      "        [ 2.2418, -2.5352],\n",
      "        [ 3.3749, -3.8604],\n",
      "        [ 2.6116, -2.9299],\n",
      "        [ 1.1715, -1.2563]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [16000/37476                 (53%)]\tLoss: 0.166108\n",
      "The current output is tensor([[ 3.6517, -4.2023],\n",
      "        [ 2.7658, -3.1769],\n",
      "        [ 3.4550, -4.0123],\n",
      "        [ 2.2601, -2.5434],\n",
      "        [ 2.7993, -3.2246],\n",
      "        [ 3.7867, -4.4051],\n",
      "        [ 2.0946, -2.3500],\n",
      "        [ 1.7551, -1.9454],\n",
      "        [ 1.3028, -1.3909],\n",
      "        [ 3.3374, -3.8435],\n",
      "        [ 3.3210, -3.8473],\n",
      "        [ 3.6175, -4.1866],\n",
      "        [ 2.8838, -3.3277],\n",
      "        [ 2.7466, -3.1639],\n",
      "        [ 2.9870, -3.4256],\n",
      "        [ 3.0983, -3.5843],\n",
      "        [ 2.4472, -2.7825],\n",
      "        [ 1.6179, -1.7688],\n",
      "        [ 1.4797, -1.6062],\n",
      "        [ 1.2456, -1.3186],\n",
      "        [ 1.9578, -2.1883],\n",
      "        [ 1.5895, -1.7475],\n",
      "        [ 3.6958, -4.2954],\n",
      "        [ 2.3438, -2.6585],\n",
      "        [ 1.4425, -1.5662],\n",
      "        [ 2.9428, -3.4016],\n",
      "        [ 3.2591, -3.7864],\n",
      "        [ 2.3440, -2.6679],\n",
      "        [ 3.7810, -4.3990],\n",
      "        [ 2.5608, -2.9310],\n",
      "        [ 2.5635, -2.9272],\n",
      "        [ 2.2943, -2.6072]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [19200/37476                 (64%)]\tLoss: 0.060779\n",
      "The current output is tensor([[ 2.1342, -2.3870],\n",
      "        [ 2.3368, -2.6342],\n",
      "        [ 3.8417, -4.4190],\n",
      "        [ 0.5517, -0.5500],\n",
      "        [ 2.1724, -2.4476],\n",
      "        [ 3.8393, -4.4150],\n",
      "        [ 2.7440, -3.1199],\n",
      "        [ 3.7027, -4.2826],\n",
      "        [ 2.2535, -2.5282],\n",
      "        [ 1.4542, -1.5713],\n",
      "        [ 3.0304, -3.4577],\n",
      "        [ 1.1102, -1.1599],\n",
      "        [ 2.2686, -2.5706],\n",
      "        [ 2.7785, -3.1571],\n",
      "        [ 2.8852, -3.3046],\n",
      "        [ 1.7177, -1.8881],\n",
      "        [ 4.2108, -4.8333],\n",
      "        [ 2.1349, -2.3840],\n",
      "        [ 2.5933, -2.9353],\n",
      "        [ 2.8768, -3.2928],\n",
      "        [ 1.6954, -1.8605],\n",
      "        [ 0.9391, -0.9664],\n",
      "        [ 1.7196, -1.8915],\n",
      "        [ 1.8957, -2.1040],\n",
      "        [ 4.0588, -4.6756],\n",
      "        [ 1.4067, -1.5168],\n",
      "        [ 1.6671, -1.8322],\n",
      "        [ 3.9909, -4.5621],\n",
      "        [ 4.4249, -5.1034],\n",
      "        [ 1.8760, -2.0913],\n",
      "        [ 3.9708, -4.5575],\n",
      "        [ 4.4514, -5.1283]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [22400/37476                 (75%)]\tLoss: 0.140652\n",
      "The current output is tensor([[ 3.7678, -4.3196],\n",
      "        [ 3.6350, -4.1624],\n",
      "        [ 4.2753, -4.9024],\n",
      "        [ 3.7486, -4.2633],\n",
      "        [ 4.6046, -5.2888],\n",
      "        [ 5.2890, -6.0588],\n",
      "        [ 2.0969, -2.3716],\n",
      "        [ 3.8379, -4.4018],\n",
      "        [ 3.8505, -4.4107],\n",
      "        [ 4.1165, -4.7146],\n",
      "        [ 2.6444, -3.0085],\n",
      "        [ 4.4392, -5.1107],\n",
      "        [ 4.3766, -5.0180],\n",
      "        [ 5.7113, -6.5645],\n",
      "        [ 4.6930, -5.3776],\n",
      "        [ 4.5159, -5.1985],\n",
      "        [ 4.8962, -5.6328],\n",
      "        [ 5.3232, -6.1207],\n",
      "        [ 3.7841, -4.3641],\n",
      "        [ 5.0563, -5.8262],\n",
      "        [ 4.2879, -4.9267],\n",
      "        [ 3.5408, -4.0587],\n",
      "        [ 3.6531, -4.1839],\n",
      "        [ 3.9418, -4.5616],\n",
      "        [ 2.3812, -2.7008],\n",
      "        [ 2.4734, -2.8452],\n",
      "        [ 4.3187, -4.9506],\n",
      "        [ 3.8470, -4.4013],\n",
      "        [ 4.8921, -5.6316],\n",
      "        [ 2.8611, -3.2750],\n",
      "        [ 4.2048, -4.8053],\n",
      "        [ 4.3191, -4.9549]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [25600/37476                 (85%)]\tLoss: 0.041360\n",
      "The current output is tensor([[ 1.9451, -2.2187],\n",
      "        [ 2.1428, -2.4532],\n",
      "        [ 2.4562, -2.8442],\n",
      "        [ 3.4265, -3.9502],\n",
      "        [ 2.3923, -2.7582],\n",
      "        [ 3.4679, -4.0198],\n",
      "        [ 1.8663, -2.1026],\n",
      "        [ 2.7922, -3.2299],\n",
      "        [ 3.3519, -3.8649],\n",
      "        [ 3.4498, -3.9938],\n",
      "        [ 3.4583, -4.0266],\n",
      "        [ 3.9758, -4.6395],\n",
      "        [ 2.9515, -3.4367],\n",
      "        [ 2.9116, -3.3567],\n",
      "        [ 2.3988, -2.7685],\n",
      "        [ 2.8386, -3.3021],\n",
      "        [ 3.3494, -3.8679],\n",
      "        [ 0.4431, -0.4057],\n",
      "        [ 4.4027, -5.1147],\n",
      "        [ 2.1016, -2.3903],\n",
      "        [ 2.8997, -3.3555],\n",
      "        [ 3.2631, -3.7989],\n",
      "        [ 3.1638, -3.6552],\n",
      "        [ 2.6729, -3.0966],\n",
      "        [ 5.0173, -5.8421],\n",
      "        [ 3.1605, -3.6707],\n",
      "        [ 2.4885, -2.8694],\n",
      "        [ 2.6322, -3.0531],\n",
      "        [ 2.9473, -3.3948],\n",
      "        [ 2.3942, -2.7526],\n",
      "        [ 2.1170, -2.4227],\n",
      "        [ 3.9595, -4.6106]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [28800/37476                 (96%)]\tLoss: 0.046327\n",
      "The current output is tensor([[ 0.6994, -0.6846],\n",
      "        [ 3.1600, -3.6097],\n",
      "        [ 3.6041, -4.1211],\n",
      "        [ 2.7144, -3.0914],\n",
      "        [ 4.4039, -5.0658],\n",
      "        [ 4.0108, -4.6218],\n",
      "        [ 3.3566, -3.8137],\n",
      "        [ 2.1326, -2.3932],\n",
      "        [ 3.3303, -3.8257],\n",
      "        [ 2.1538, -2.4371],\n",
      "        [ 1.5645, -1.7182],\n",
      "        [ 4.1956, -4.8025],\n",
      "        [ 1.9672, -2.1902],\n",
      "        [ 3.1363, -3.5926],\n",
      "        [ 2.0004, -2.2462],\n",
      "        [ 2.2056, -2.4870],\n",
      "        [ 2.1802, -2.4511],\n",
      "        [ 1.7026, -1.8781],\n",
      "        [ 3.1927, -3.6612],\n",
      "        [ 1.3384, -1.4469],\n",
      "        [ 4.5115, -5.2162],\n",
      "        [ 3.0388, -3.4714],\n",
      "        [ 2.1914, -2.4562],\n",
      "        [ 4.9562, -5.7122],\n",
      "        [ 2.8985, -3.3026],\n",
      "        [ 2.2099, -2.4815],\n",
      "        [ 4.2956, -4.9381],\n",
      "        [ 2.2655, -2.5718],\n",
      "        [ 0.2775, -0.2341],\n",
      "        [ 3.2537, -3.7356],\n",
      "        [ 1.7483, -1.9304],\n",
      "        [ 1.8718, -2.0801]], device='cuda:0', grad_fn=<AddmmBackward0>), and the target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     train_losses, train_correct \u001b[39m=\u001b[39m train(fold, model, device, train_loader, optimizer, epoch, criterion)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     test_losses, test_correct \u001b[39m=\u001b[39m test(fold, model, device, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     train_losses_history\u001b[39m.\u001b[39mappend(train_losses)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     test_losses_history\u001b[39m.\u001b[39mappend(test_losses)\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb Cell 18\u001b[0m in \u001b[0;36mtest\u001b[0;34m(fold, model, device, test_loader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m         pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msigmoid()\u001b[39m.\u001b[39mround()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m         \u001b[39m# correct += pred.eq(one_hot_target.view_as(pred)).sum().item()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39;49margmax(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39;49m one_hot_target\u001b[39m.\u001b[39;49margmax(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# test_loss /= len(test_loader.dataset)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest set for fold\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m: Average Loss: \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m      \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mcorrect\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m      (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100.\u001b[39m \u001b[39m*\u001b[39m correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset)\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training \n",
    "batch_size = 32\n",
    "seq_length = 6\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = prediction_model(input_size=92, batch_size=batch_size).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "weights = torch.FloatTensor([1,10])\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X_features, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch, criterion)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    # ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    # ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    # ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    # ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "    ax[0].plot(train_losses_history, label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, label=f\"test accuracy fold{fold}\")\n",
    "    break\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# put the legend out of the figure, and adjust the position, prevent the figure from being covered\n",
    "# ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# save the figure\n",
    "# fig.savefig(\"/DATA/User/wuxinchao/project/pMHC-TCR/result/pMHC_without_em_with_encoder_loss_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((32, 92)).to(device)\n",
    "# # a.shape\n",
    "d = model(c).to(device)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "# loss = nn.CrossEntropyLoss()(model(a).to(torch.float32).view(32, 1, 1), torch.ones((32, 1, 1)).to(torch.float32).to(device)) / 32\n",
    "# loss.to(device)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "a = torch.randint(0,2,(32,1)) # mask\n",
    "b = torch.zeros((32,2))\n",
    "# get the value of b where a is 1\n",
    "a = a.to(torch.bool)\n",
    "b[(a==1).squeeze(),1] = 1\n",
    "b[(a==0).squeeze(),0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1333)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss = loss_fn(d, torch.ones((32,2)).to(device))\n",
    "# loss.backward()\n",
    "# train_losses_history\n",
    "# test_losses_history\n",
    "# test_accuracy_history\n",
    "# train_accuracy_history\n",
    "# nn.Softmax(dim=1)(b)\n",
    "(b == torch.tensor([1,0])).sum() / (b == torch.tensor([0,1])).sum()\n",
    "# b\n",
    "# 17 / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model input shape is : torch.Size([32, 92])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2870, -0.0434],\n",
       "        [-0.3169, -0.0655],\n",
       "        [-0.2865, -0.0411],\n",
       "        [-0.3154, -0.0643],\n",
       "        [-0.3112, -0.0605],\n",
       "        [-0.2862, -0.0417],\n",
       "        [-0.2947, -0.0502],\n",
       "        [-0.3223, -0.0679],\n",
       "        [-0.3114, -0.0614],\n",
       "        [-0.3071, -0.0575],\n",
       "        [-0.3100, -0.0598],\n",
       "        [-0.3044, -0.0542],\n",
       "        [-0.2960, -0.0493],\n",
       "        [-0.3085, -0.0567],\n",
       "        [-0.3342, -0.0772],\n",
       "        [-0.3270, -0.0721],\n",
       "        [-0.3018, -0.0537],\n",
       "        [-0.3169, -0.0647],\n",
       "        [-0.2889, -0.0440],\n",
       "        [-0.3067, -0.0579],\n",
       "        [-0.2876, -0.0417],\n",
       "        [-0.2872, -0.0405],\n",
       "        [-0.3142, -0.0639],\n",
       "        [-0.3049, -0.0558],\n",
       "        [-0.3075, -0.0570],\n",
       "        [-0.2959, -0.0517],\n",
       "        [-0.2968, -0.0511],\n",
       "        [-0.3162, -0.0621],\n",
       "        [-0.2941, -0.0467],\n",
       "        [-0.2973, -0.0524],\n",
       "        [-0.3051, -0.0581],\n",
       "        [-0.3202, -0.0669]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss.backward()\n",
    "# torch.zeros((32,2))\n",
    "model(torch.randn((32,92)).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, the features are concatanated and used to predict the binding affinity of pMHC-TCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_pred(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        super(pMHC_TCR_pred, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        # use the encoded features to predict the binding affinity through MLP\n",
    "        self.Linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_encode(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        self.input_size = input_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.seq_length, self.input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :] # return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 82, 'BseqCDR3': 24}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "print(len_map)\n",
    "# drop the rows with length == max length, which is much longer than the others\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < len_map[\"AseqCDR3\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.200e+02, 2.288e+03, 2.800e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.100e+01]),\n",
       " array([ 5. , 12.6, 20.2, 27.8, 35.4, 43. , 50.6, 58.2, 65.8, 73.4, 81. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df[\"AseqCDR3\"].value_counts()\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0) # find the longest seq\n",
    "# df.loc[df[\"AseqCDR3\"].str.len() == 83, \"AseqCDR3\"]\n",
    "\n",
    "plt.hist(df[\"AseqCDR3\"].str.len().sort_values(axis=0))\n",
    "# plt.show()\n",
    "# df = df.loc[df[\"AseqCDR3\"].str.len() < 83, :]\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_map\n",
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/20230228.csv\")\n",
    "# df.loc[df[\"AseqCDR3\"].str.contains(\"_\"),]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
