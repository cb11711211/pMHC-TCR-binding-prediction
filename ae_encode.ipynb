{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                 Type                   Data/Info\n",
      "---------------------------------------------------------\n",
      "A_TCR_seq                ndarray                37476: 37476 elems, type `object`, 299808 bytes (292.78125 kb)\n",
      "A_TCR_seq_encode         Tensor                 tensor([[-1.3430,  0.4650<...>     dtype=torch.float64)\n",
      "DataLoader               type                   <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Dataset                  type                   <class 'torch.utils.data.dataset.Dataset'>\n",
      "F                        module                 <module 'torch.nn.functio<...>/torch/nn/functional.py'>\n",
      "HLAencoder               OneHotEncoder          OneHotEncoder()\n",
      "KFold                    ABCMeta                <class 'sklearn.model_selection._split.KFold'>\n",
      "LabelEncoder             type                   <class 'sklearn.preproces<...>ing._label.LabelEncoder'>\n",
      "OneHotEncoder            type                   <class 'sklearn.preproces<...>_encoders.OneHotEncoder'>\n",
      "RepeatedKFold            ABCMeta                <class 'sklearn.model_sel<...>on._split.RepeatedKFold'>\n",
      "StratifiedKFold          ABCMeta                <class 'sklearn.model_sel<...>._split.StratifiedKFold'>\n",
      "SubsetRandomSampler      type                   <class 'torch.utils.data.<...>ler.SubsetRandomSampler'>\n",
      "TCRData                  pMHC_TCRDataset        <__main__.pMHC_TCRDataset<...>object at 0x7f1d1d99b490>\n",
      "TCR_accuracy             int                    0\n",
      "TCR_autoencoder          type                   <class '__main__.TCR_autoencoder'>\n",
      "TCR_encode               Tensor                 tensor([[[26.8889],\\n    <...>, grad_fn=<CatBackward0>)\n",
      "TCR_encode_data          type                   <class '__main__.TCR_encode_data'>\n",
      "TCR_encode_loss          float                  9.151262279599905\n",
      "TCR_encode_losses        list                   n=100\n",
      "TCR_seq                  Tensor                 tensor([[[-1.3430,  0.465<...>000,  0.0000,  0.0000]]])\n",
      "Variable                 VariableMeta           <class 'torch.autograd.variable.Variable'>\n",
      "WeightedRandomSampler    type                   <class 'torch.utils.data.<...>r.WeightedRandomSampler'>\n",
      "X_HLA                    ndarray                37476x1: 37476 elems, type `object`, 299808 bytes (292.78125 kb)\n",
      "X_HLA_encoded            ndarray                37476x2: 74952 elems, type `float64`, 599616 bytes (585.5625 kb)\n",
      "a                        Tensor                 tensor([[1., 2., 3., 4., <...>   [3., 3., 4., 5., 6.]])\n",
      "af                       DataFrame                          Factor I  Fac<...>.097     -0.838     1.512\n",
      "ax                       ndarray                2: 2 elems, type `object`, 16 bytes\n",
      "b                        Tensor                 tensor([[1., 0.],\\n      <...> 1.],\\n        [0., 1.]])\n",
      "batch_idx                int                    0\n",
      "batch_size               int                    32\n",
      "c                        Tensor                 tensor([[-0.6368, -0.8428<...>\\n       device='cuda:0')\n",
      "chain                    str                    BseqCDR3\n",
      "correct                  int                    0\n",
      "criterion                CrossEntropyLoss       CrossEntropyLoss()\n",
      "d                        Tensor                 tensor([[ 0.3123, -0.2510<...>grad_fn=<AddmmBackward0>)\n",
      "data                     Tensor                 tensor([[ 5.4752e+00, -9.<...>rad_fn=<ToCopyBackward0>)\n",
      "device                   device                 cuda:0\n",
      "df                       DataFrame                        Class          <...>n[37476 rows x 6 columns]\n",
      "encode_seqCDR            function               <function encode_seqCDR at 0x7f1d1cdad9d0>\n",
      "encoded                  Tensor                 tensor([[[ 3.5331e+01],\\n<...>_fn=<LeakyReluBackward0>)\n",
      "encoding                 Tensor                 tensor([[-1.3430,  0.4650<...>0]], dtype=torch.float64)\n",
      "epoch                    int                    100\n",
      "epochs                   int                    100\n",
      "fig                      Figure                 Figure(720x360)\n",
      "file_path                str                    ~/data/project/data/seqData/20230228.csv\n",
      "fold                     int                    0\n",
      "folds                    int                    5\n",
      "i                        int                    2491\n",
      "kernel_size              int                    3\n",
      "kf                       StratifiedKFold        StratifiedKFold(n_splits=<...>m_state=42, shuffle=True)\n",
      "learning_rate            float                  0.001\n",
      "len_map                  dict                   n=2\n",
      "length                   int                    24\n",
      "logging                  module                 <module 'logging' from '/<...>3.9/logging/__init__.py'>\n",
      "loss                     Tensor                 tensor(1.3984, device='cu<...>, grad_fn=<DivBackward1>)\n",
      "loss_fn                  CrossEntropyLoss       CrossEntropyLoss()\n",
      "model                    prediction_model       prediction_model(\\n  (lin<...>ftmax): Softmax(dim=1)\\n)\n",
      "nn                       module                 <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\n",
      "np                       module                 <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "num_dict                 dict                   n=1\n",
      "one_hot_target           Tensor                 tensor([[1., 0.],\\n      <...>., 0.]], device='cuda:0')\n",
      "optim                    module                 <module 'torch.optim' fro<...>torch/optim/__init__.py'>\n",
      "optimizer                SGD                    SGD (\\nParameter Group 0\\<...>e\\n    weight_decay: 0\\n)\n",
      "os                       module                 <module 'os' from '/home/<...>da3/lib/python3.9/os.py'>\n",
      "output                   Tensor                 tensor([[0.3100, 0.6900],<...>       [0.3100, 0.6900]])\n",
      "pMHC_TCRDataset          type                   <class '__main__.pMHC_TCRDataset'>\n",
      "padding                  int                    1\n",
      "pd                       module                 <module 'pandas' from '/h<...>ages/pandas/__init__.py'>\n",
      "plt                      module                 <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "pred                     Tensor                 tensor([[0.],\\n        [0<...>grad_fn=<RoundBackward0>)\n",
      "prediction_model         type                   <class '__main__.prediction_model'>\n",
      "random                   module                 <module 'random' from '/h<...>lib/python3.9/random.py'>\n",
      "repeats                  int                    12\n",
      "reset_weights            function               <function reset_weights at 0x7f1d06797040>\n",
      "seq                      str                    Neo_last3\n",
      "seq_length               int                    6\n",
      "sns                      module                 <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n",
      "stride                   int                    2\n",
      "target                   Tensor                 tensor([[False],\\n       <...>alse],\\n        [False]])\n",
      "test                     function               <function test at 0x7f1d0661d280>\n",
      "test_accuracy_history    list                   n=100\n",
      "test_correct             float                  0.1946845981428114\n",
      "test_idx                 ndarray                7496: 7496 elems, type `int64`, 59968 bytes\n",
      "test_loader              DataLoader             <torch.utils.data.dataloa<...>object at 0x7f1d06524d90>\n",
      "test_losses              float                  79.30327361822128\n",
      "test_losses_history      list                   n=100\n",
      "test_subsampler          SubsetRandomSampler    <torch.utils.data.sampler<...>object at 0x7f1d068f70d0>\n",
      "torch                    module                 <module 'torch' from '/ho<...>kages/torch/__init__.py'>\n",
      "tqdm                     module                 <module 'tqdm' from '/hom<...>ckages/tqdm/__init__.py'>\n",
      "train                    function               <function train at 0x7f1d0675f4c0>\n",
      "train_accuracy_history   list                   n=100\n",
      "train_autoencoder        function               <function train_autoencoder at 0x7f1d1cf2ddc0>\n",
      "train_correct            float                  0.7785516063614046\n",
      "train_idx                ndarray                29980: 29980 elems, type `int64`, 239840 bytes (234.21875 kb)\n",
      "train_loader             DataLoader             <torch.utils.data.dataloa<...>object at 0x7f1d068f7100>\n",
      "train_loss               float                  0.8487564325332642\n",
      "train_losses             float                  317.4317874610424\n",
      "train_losses_history     list                   n=100\n",
      "train_subsampler         SubsetRandomSampler    <torch.utils.data.sampler<...>object at 0x7f1d068a3af0>\n",
      "train_test_split         function               <function train_test_split at 0x7f1e22a40af0>\n",
      "weights                  Tensor                 tensor([ 1., 10.])\n",
      "y                        ndarray                37476: 37476 elems, type `int64`, 299808 bytes (292.78125 kb)\n"
     ]
    }
   ],
   "source": [
    "# check the kernel running in the notebook\n",
    "# !uname -a\n",
    "# find the variables in the notebook\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold\n",
    "# from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seqCDR[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_encode_data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # drop the rows with duplicate CDR3 sequences\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        \n",
    "        # drop the rows with length == max length, which is much longer than the others\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # If there is any NaN value, drop the row\n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        # concatenate the encoded features\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "        \n",
    "        # discard the duplicate rows, keep the first one\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 49)\n",
    "            nn.Conv1d(5, 10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 25) based on the formula for conv1d: (W + 2P - K)/S + 1 = (49 + 2*1 - 3)/2 + 1 = 25\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 10, 23), 25 - 2 = 23 \n",
    "\n",
    "            nn.Conv1d(10, 15, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12) based on the formula for conv1d: (W + 2P - K)/S + 1 = (23 + 2*1 - 3)/2 + 1 = 12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 10), 12 - 2 = 10\n",
    "\n",
    "            nn.Conv1d(15, 20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 5) based on the formula for conv1d: (W + 2P - K)/S + 1 = (10 + 2*1 - 3)/2 + 1 = 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 3)\n",
    "\n",
    "            nn.Conv1d(20, 20 , kernel_size=5, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 1) based on the formula for conv1d: (W + 2P - K)/S + 1 = (3 + 2*1 - 5)/2 + 1 = 1\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 1)\n",
    "            nn.ConvTranspose1d(20, 20, kernel_size=5, stride=2, padding=1),\n",
    "            # (batch_size, 20, 3), based on the formula for convtranspose1d: (W−1)S−2P+F = (1-1)*2-2*1+5= 3\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(20, 15, kernel_size=3, stride=3, padding=1),\n",
    "            # (batch_size, 15, 5), based on the formula for convtranspose1d: (W−1)S−2P+F = (3-1)*3-2*1+3= 7\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(15, 10, kernel_size=7, stride=3, padding=1),\n",
    "            # (batch_size, 10, 23) based on the formula for convtranspose1d: (W−1)S−2P+F = (7-1)*3-2*1+7= 23\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(10, 5, kernel_size=7, stride=2, padding=1),\n",
    "            # (batch_size, 5, 49) based on the formula for convtranspose1d: (W−1)S−2P+F = (23-1)*2-2*1+7= 49\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = torch.tensor(x, dtype=np.float32)\n",
    "        # x = torch.tensor(x, dtype=torch.float)\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        # print(f\"encoding shape: {encoded.shape}\")\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2492 (0%)]\tLoss: 1.471986\n",
      "Train Epoch: 1 [1600/2492 (65%)]\tLoss: 1.273468\n",
      "Train Epoch: 2 [0/2492 (0%)]\tLoss: 1.280531\n",
      "Train Epoch: 2 [1600/2492 (65%)]\tLoss: 1.232437\n",
      "Train Epoch: 3 [0/2492 (0%)]\tLoss: 1.246841\n",
      "Train Epoch: 3 [1600/2492 (65%)]\tLoss: 1.325655\n",
      "Train Epoch: 4 [0/2492 (0%)]\tLoss: 1.271614\n",
      "Train Epoch: 4 [1600/2492 (65%)]\tLoss: 1.169753\n",
      "Train Epoch: 5 [0/2492 (0%)]\tLoss: 1.247328\n",
      "Train Epoch: 5 [1600/2492 (65%)]\tLoss: 1.224375\n",
      "Train Epoch: 6 [0/2492 (0%)]\tLoss: 1.144084\n",
      "Train Epoch: 6 [1600/2492 (65%)]\tLoss: 1.104665\n",
      "Train Epoch: 7 [0/2492 (0%)]\tLoss: 0.995667\n",
      "Train Epoch: 7 [1600/2492 (65%)]\tLoss: 1.079008\n",
      "Train Epoch: 8 [0/2492 (0%)]\tLoss: 1.105419\n",
      "Train Epoch: 8 [1600/2492 (65%)]\tLoss: 1.142605\n",
      "Train Epoch: 9 [0/2492 (0%)]\tLoss: 1.096186\n",
      "Train Epoch: 9 [1600/2492 (65%)]\tLoss: 1.226829\n",
      "Train Epoch: 10 [0/2492 (0%)]\tLoss: 1.134242\n",
      "Train Epoch: 10 [1600/2492 (65%)]\tLoss: 1.138379\n",
      "Train Epoch: 11 [0/2492 (0%)]\tLoss: 1.133746\n",
      "Train Epoch: 11 [1600/2492 (65%)]\tLoss: 1.046210\n",
      "Train Epoch: 12 [0/2492 (0%)]\tLoss: 1.091588\n",
      "Train Epoch: 12 [1600/2492 (65%)]\tLoss: 1.036043\n",
      "Train Epoch: 13 [0/2492 (0%)]\tLoss: 1.071608\n",
      "Train Epoch: 13 [1600/2492 (65%)]\tLoss: 1.134869\n",
      "Train Epoch: 14 [0/2492 (0%)]\tLoss: 1.138121\n",
      "Train Epoch: 14 [1600/2492 (65%)]\tLoss: 1.172377\n",
      "Train Epoch: 15 [0/2492 (0%)]\tLoss: 1.147845\n",
      "Train Epoch: 15 [1600/2492 (65%)]\tLoss: 1.115098\n",
      "Train Epoch: 16 [0/2492 (0%)]\tLoss: 1.061358\n",
      "Train Epoch: 16 [1600/2492 (65%)]\tLoss: 1.035478\n",
      "Train Epoch: 17 [0/2492 (0%)]\tLoss: 1.101408\n",
      "Train Epoch: 17 [1600/2492 (65%)]\tLoss: 0.991888\n",
      "Train Epoch: 18 [0/2492 (0%)]\tLoss: 1.172102\n",
      "Train Epoch: 18 [1600/2492 (65%)]\tLoss: 1.077362\n",
      "Train Epoch: 19 [0/2492 (0%)]\tLoss: 1.024072\n",
      "Train Epoch: 19 [1600/2492 (65%)]\tLoss: 1.116645\n",
      "Train Epoch: 20 [0/2492 (0%)]\tLoss: 1.006791\n",
      "Train Epoch: 20 [1600/2492 (65%)]\tLoss: 1.050426\n",
      "Train Epoch: 21 [0/2492 (0%)]\tLoss: 1.091079\n",
      "Train Epoch: 21 [1600/2492 (65%)]\tLoss: 1.163630\n",
      "Train Epoch: 22 [0/2492 (0%)]\tLoss: 0.975893\n",
      "Train Epoch: 22 [1600/2492 (65%)]\tLoss: 1.086634\n",
      "Train Epoch: 23 [0/2492 (0%)]\tLoss: 1.095728\n",
      "Train Epoch: 23 [1600/2492 (65%)]\tLoss: 1.102622\n",
      "Train Epoch: 24 [0/2492 (0%)]\tLoss: 0.977205\n",
      "Train Epoch: 24 [1600/2492 (65%)]\tLoss: 1.057594\n",
      "Train Epoch: 25 [0/2492 (0%)]\tLoss: 0.970577\n",
      "Train Epoch: 25 [1600/2492 (65%)]\tLoss: 1.094286\n",
      "Train Epoch: 26 [0/2492 (0%)]\tLoss: 1.113469\n",
      "Train Epoch: 26 [1600/2492 (65%)]\tLoss: 1.151479\n",
      "Train Epoch: 27 [0/2492 (0%)]\tLoss: 1.014042\n",
      "Train Epoch: 27 [1600/2492 (65%)]\tLoss: 1.053791\n",
      "Train Epoch: 28 [0/2492 (0%)]\tLoss: 1.119040\n",
      "Train Epoch: 28 [1600/2492 (65%)]\tLoss: 1.135263\n",
      "Train Epoch: 29 [0/2492 (0%)]\tLoss: 1.093536\n",
      "Train Epoch: 29 [1600/2492 (65%)]\tLoss: 1.002750\n",
      "Train Epoch: 30 [0/2492 (0%)]\tLoss: 1.124099\n",
      "Train Epoch: 30 [1600/2492 (65%)]\tLoss: 1.130397\n",
      "Train Epoch: 31 [0/2492 (0%)]\tLoss: 1.063712\n",
      "Train Epoch: 31 [1600/2492 (65%)]\tLoss: 1.012776\n",
      "Train Epoch: 32 [0/2492 (0%)]\tLoss: 1.033125\n",
      "Train Epoch: 32 [1600/2492 (65%)]\tLoss: 1.017343\n",
      "Train Epoch: 33 [0/2492 (0%)]\tLoss: 1.085116\n",
      "Train Epoch: 33 [1600/2492 (65%)]\tLoss: 0.978626\n",
      "Train Epoch: 34 [0/2492 (0%)]\tLoss: 1.160874\n",
      "Train Epoch: 34 [1600/2492 (65%)]\tLoss: 1.076613\n",
      "Train Epoch: 35 [0/2492 (0%)]\tLoss: 0.995880\n",
      "Train Epoch: 35 [1600/2492 (65%)]\tLoss: 0.918910\n",
      "Train Epoch: 36 [0/2492 (0%)]\tLoss: 1.049870\n",
      "Train Epoch: 36 [1600/2492 (65%)]\tLoss: 1.075718\n",
      "Train Epoch: 37 [0/2492 (0%)]\tLoss: 1.030998\n",
      "Train Epoch: 37 [1600/2492 (65%)]\tLoss: 1.042619\n",
      "Train Epoch: 38 [0/2492 (0%)]\tLoss: 1.065705\n",
      "Train Epoch: 38 [1600/2492 (65%)]\tLoss: 0.957397\n",
      "Train Epoch: 39 [0/2492 (0%)]\tLoss: 1.046255\n",
      "Train Epoch: 39 [1600/2492 (65%)]\tLoss: 1.076612\n",
      "Train Epoch: 40 [0/2492 (0%)]\tLoss: 1.054307\n",
      "Train Epoch: 40 [1600/2492 (65%)]\tLoss: 0.991000\n",
      "Train Epoch: 41 [0/2492 (0%)]\tLoss: 1.063096\n",
      "Train Epoch: 41 [1600/2492 (65%)]\tLoss: 1.008676\n",
      "Train Epoch: 42 [0/2492 (0%)]\tLoss: 0.903448\n",
      "Train Epoch: 42 [1600/2492 (65%)]\tLoss: 1.012477\n",
      "Train Epoch: 43 [0/2492 (0%)]\tLoss: 0.981968\n",
      "Train Epoch: 43 [1600/2492 (65%)]\tLoss: 1.079018\n",
      "Train Epoch: 44 [0/2492 (0%)]\tLoss: 1.033889\n",
      "Train Epoch: 44 [1600/2492 (65%)]\tLoss: 1.091305\n",
      "Train Epoch: 45 [0/2492 (0%)]\tLoss: 0.997556\n",
      "Train Epoch: 45 [1600/2492 (65%)]\tLoss: 0.984096\n",
      "Train Epoch: 46 [0/2492 (0%)]\tLoss: 1.049188\n",
      "Train Epoch: 46 [1600/2492 (65%)]\tLoss: 1.000803\n",
      "Train Epoch: 47 [0/2492 (0%)]\tLoss: 1.084918\n",
      "Train Epoch: 47 [1600/2492 (65%)]\tLoss: 0.986767\n",
      "Train Epoch: 48 [0/2492 (0%)]\tLoss: 1.006732\n",
      "Train Epoch: 48 [1600/2492 (65%)]\tLoss: 0.934662\n",
      "Train Epoch: 49 [0/2492 (0%)]\tLoss: 1.030488\n",
      "Train Epoch: 49 [1600/2492 (65%)]\tLoss: 0.958021\n",
      "Train Epoch: 50 [0/2492 (0%)]\tLoss: 0.972123\n",
      "Train Epoch: 50 [1600/2492 (65%)]\tLoss: 1.000657\n",
      "Train Epoch: 51 [0/2492 (0%)]\tLoss: 1.087757\n",
      "Train Epoch: 51 [1600/2492 (65%)]\tLoss: 1.033272\n",
      "Train Epoch: 52 [0/2492 (0%)]\tLoss: 0.998147\n",
      "Train Epoch: 52 [1600/2492 (65%)]\tLoss: 1.126596\n",
      "Train Epoch: 53 [0/2492 (0%)]\tLoss: 0.989936\n",
      "Train Epoch: 53 [1600/2492 (65%)]\tLoss: 1.018337\n",
      "Train Epoch: 54 [0/2492 (0%)]\tLoss: 1.017847\n",
      "Train Epoch: 54 [1600/2492 (65%)]\tLoss: 1.022949\n",
      "Train Epoch: 55 [0/2492 (0%)]\tLoss: 0.985970\n",
      "Train Epoch: 55 [1600/2492 (65%)]\tLoss: 1.095683\n",
      "Train Epoch: 56 [0/2492 (0%)]\tLoss: 0.952950\n",
      "Train Epoch: 56 [1600/2492 (65%)]\tLoss: 1.010165\n",
      "Train Epoch: 57 [0/2492 (0%)]\tLoss: 0.875329\n",
      "Train Epoch: 57 [1600/2492 (65%)]\tLoss: 0.965711\n",
      "Train Epoch: 58 [0/2492 (0%)]\tLoss: 0.974614\n",
      "Train Epoch: 58 [1600/2492 (65%)]\tLoss: 1.040950\n",
      "Train Epoch: 59 [0/2492 (0%)]\tLoss: 1.070342\n",
      "Train Epoch: 59 [1600/2492 (65%)]\tLoss: 1.028807\n",
      "Train Epoch: 60 [0/2492 (0%)]\tLoss: 1.057253\n",
      "Train Epoch: 60 [1600/2492 (65%)]\tLoss: 0.979801\n",
      "Train Epoch: 61 [0/2492 (0%)]\tLoss: 0.930513\n",
      "Train Epoch: 61 [1600/2492 (65%)]\tLoss: 1.068487\n",
      "Train Epoch: 62 [0/2492 (0%)]\tLoss: 1.154726\n",
      "Train Epoch: 62 [1600/2492 (65%)]\tLoss: 0.988306\n",
      "Train Epoch: 63 [0/2492 (0%)]\tLoss: 1.026507\n",
      "Train Epoch: 63 [1600/2492 (65%)]\tLoss: 0.993078\n",
      "Train Epoch: 64 [0/2492 (0%)]\tLoss: 1.007801\n",
      "Train Epoch: 64 [1600/2492 (65%)]\tLoss: 0.962186\n",
      "Train Epoch: 65 [0/2492 (0%)]\tLoss: 0.948047\n",
      "Train Epoch: 65 [1600/2492 (65%)]\tLoss: 0.979841\n",
      "Train Epoch: 66 [0/2492 (0%)]\tLoss: 1.039992\n",
      "Train Epoch: 66 [1600/2492 (65%)]\tLoss: 0.958463\n",
      "Train Epoch: 67 [0/2492 (0%)]\tLoss: 0.992162\n",
      "Train Epoch: 67 [1600/2492 (65%)]\tLoss: 0.974028\n",
      "Train Epoch: 68 [0/2492 (0%)]\tLoss: 0.899887\n",
      "Train Epoch: 68 [1600/2492 (65%)]\tLoss: 0.984201\n",
      "Train Epoch: 69 [0/2492 (0%)]\tLoss: 0.989608\n",
      "Train Epoch: 69 [1600/2492 (65%)]\tLoss: 0.986969\n",
      "Train Epoch: 70 [0/2492 (0%)]\tLoss: 0.917651\n",
      "Train Epoch: 70 [1600/2492 (65%)]\tLoss: 1.063309\n",
      "Train Epoch: 71 [0/2492 (0%)]\tLoss: 0.902731\n",
      "Train Epoch: 71 [1600/2492 (65%)]\tLoss: 0.888268\n",
      "Train Epoch: 72 [0/2492 (0%)]\tLoss: 0.995470\n",
      "Train Epoch: 72 [1600/2492 (65%)]\tLoss: 0.976098\n",
      "Train Epoch: 73 [0/2492 (0%)]\tLoss: 0.904117\n",
      "Train Epoch: 73 [1600/2492 (65%)]\tLoss: 0.964518\n",
      "Train Epoch: 74 [0/2492 (0%)]\tLoss: 0.918999\n",
      "Train Epoch: 74 [1600/2492 (65%)]\tLoss: 0.997078\n",
      "Train Epoch: 75 [0/2492 (0%)]\tLoss: 0.976390\n",
      "Train Epoch: 75 [1600/2492 (65%)]\tLoss: 0.909841\n",
      "Train Epoch: 76 [0/2492 (0%)]\tLoss: 0.917670\n",
      "Train Epoch: 76 [1600/2492 (65%)]\tLoss: 1.009161\n",
      "Train Epoch: 77 [0/2492 (0%)]\tLoss: 0.954231\n",
      "Train Epoch: 77 [1600/2492 (65%)]\tLoss: 0.950573\n",
      "Train Epoch: 78 [0/2492 (0%)]\tLoss: 0.972171\n",
      "Train Epoch: 78 [1600/2492 (65%)]\tLoss: 0.875407\n",
      "Train Epoch: 79 [0/2492 (0%)]\tLoss: 0.962930\n",
      "Train Epoch: 79 [1600/2492 (65%)]\tLoss: 0.977995\n",
      "Train Epoch: 80 [0/2492 (0%)]\tLoss: 0.931419\n",
      "Train Epoch: 80 [1600/2492 (65%)]\tLoss: 1.011017\n",
      "Train Epoch: 81 [0/2492 (0%)]\tLoss: 0.959214\n",
      "Train Epoch: 81 [1600/2492 (65%)]\tLoss: 0.843545\n",
      "Train Epoch: 82 [0/2492 (0%)]\tLoss: 0.856576\n",
      "Train Epoch: 82 [1600/2492 (65%)]\tLoss: 0.957439\n",
      "Train Epoch: 83 [0/2492 (0%)]\tLoss: 0.963914\n",
      "Train Epoch: 83 [1600/2492 (65%)]\tLoss: 1.000450\n",
      "Train Epoch: 84 [0/2492 (0%)]\tLoss: 0.946143\n",
      "Train Epoch: 84 [1600/2492 (65%)]\tLoss: 0.853314\n",
      "Train Epoch: 85 [0/2492 (0%)]\tLoss: 0.979291\n",
      "Train Epoch: 85 [1600/2492 (65%)]\tLoss: 0.954335\n",
      "Train Epoch: 86 [0/2492 (0%)]\tLoss: 0.937592\n",
      "Train Epoch: 86 [1600/2492 (65%)]\tLoss: 0.871585\n",
      "Train Epoch: 87 [0/2492 (0%)]\tLoss: 0.941539\n",
      "Train Epoch: 87 [1600/2492 (65%)]\tLoss: 0.899941\n",
      "Train Epoch: 88 [0/2492 (0%)]\tLoss: 0.931616\n",
      "Train Epoch: 88 [1600/2492 (65%)]\tLoss: 0.984204\n",
      "Train Epoch: 89 [0/2492 (0%)]\tLoss: 0.914518\n",
      "Train Epoch: 89 [1600/2492 (65%)]\tLoss: 0.904637\n",
      "Train Epoch: 90 [0/2492 (0%)]\tLoss: 0.964604\n",
      "Train Epoch: 90 [1600/2492 (65%)]\tLoss: 0.983171\n",
      "Train Epoch: 91 [0/2492 (0%)]\tLoss: 0.945258\n",
      "Train Epoch: 91 [1600/2492 (65%)]\tLoss: 0.895110\n",
      "Train Epoch: 92 [0/2492 (0%)]\tLoss: 1.012319\n",
      "Train Epoch: 92 [1600/2492 (65%)]\tLoss: 0.998962\n",
      "Train Epoch: 93 [0/2492 (0%)]\tLoss: 0.935931\n",
      "Train Epoch: 93 [1600/2492 (65%)]\tLoss: 0.948335\n",
      "Train Epoch: 94 [0/2492 (0%)]\tLoss: 0.830508\n",
      "Train Epoch: 94 [1600/2492 (65%)]\tLoss: 0.954921\n",
      "Train Epoch: 95 [0/2492 (0%)]\tLoss: 0.950306\n",
      "Train Epoch: 95 [1600/2492 (65%)]\tLoss: 0.917928\n",
      "Train Epoch: 96 [0/2492 (0%)]\tLoss: 0.950032\n",
      "Train Epoch: 96 [1600/2492 (65%)]\tLoss: 1.018080\n",
      "Train Epoch: 97 [0/2492 (0%)]\tLoss: 0.969258\n",
      "Train Epoch: 97 [1600/2492 (65%)]\tLoss: 0.910215\n",
      "Train Epoch: 98 [0/2492 (0%)]\tLoss: 0.916602\n",
      "Train Epoch: 98 [1600/2492 (65%)]\tLoss: 0.931150\n",
      "Train Epoch: 99 [0/2492 (0%)]\tLoss: 0.962040\n",
      "Train Epoch: 99 [1600/2492 (65%)]\tLoss: 0.887071\n",
      "Train Epoch: 100 [0/2492 (0%)]\tLoss: 0.864197\n",
      "Train Epoch: 100 [1600/2492 (65%)]\tLoss: 1.001414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1d1d3a4c70>]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training the autoencoder to encode the TCR sequence\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    # model_accuracy = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        # TCR_encode_losses.append(loss.item() / model.batch_size)\n",
    "        # TCR_encode_losses.append(loss.item())\n",
    "        # sum up batch loss\n",
    "        batch_loss += loss.item()\n",
    "        # update the accuracy of the model\n",
    "        # pred = output.data.max(1, keepdim=True)[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            # print(f\"The accuracy of the model is \")\n",
    "            \n",
    "    # return batch_loss / len(train_loader.dataset)\n",
    "    return batch_loss / len(data)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "TCR_accuracy = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.set_title(\"TCR encode loss\")\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# load the model\n",
    "# model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "# model.load_state_dict(torch.load(\"/DATA/User/wuxinchao/project/pMHC-TCR/ckpt/TCR_autoencoder.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 1))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    TCR_seq = TCR_seq.view(1, 5, 49).float()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test, not used\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "kernel_size, stride, padding, seq_length\n",
    "# pool of size=3, stride=2\n",
    "# m = nn.MaxPool1d(3, stride=1)\n",
    "# m = nn.Conv1d(16, 33, 3, stride=2, padding=1)\n",
    "m = nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=1)\n",
    "# m = nn.MaxUnpool1d(kernel_size=3, stride=1)\n",
    "input = torch.randn(20, 16, 3)\n",
    "output = m(input)\n",
    "output.shape\n",
    "# TCRData[0][0].shape\n",
    "len(TCRData)\n",
    "# model(TCRData[0:3][0].float().view(3,5,seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TCR_encode(nn.Module):\n",
    "    '''\n",
    "    LSTM for TCR sequence encoding.\n",
    "    The input size of LSTM is (batch_size, seq_length, input_size), the output size is (batch_size, seq_length, hidden_size)\n",
    "    '''\n",
    "    def __init__(self, seq_length, hidden_size, num_layers, device):\n",
    "        super(LSTM_TCR_encode, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(seq_length, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model for TCR sequence encoding, this may not be used in the future\n",
    "# How to use the LSTM model to encode the TCR sequence\n",
    "# The optimization \n",
    "def train_LSTM_TCR_encode(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, seq_length, 5)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training: {batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%) \\\n",
    "                  Loss: {loss.item():.6f}\")\n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the LSTM model\n",
    "model = LSTM_TCR_encode(seq_length, hidden_size, num_layers, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    Here the input is the TCR sequence, neoantigen sequence, and HLA type.\n",
    "    The output should be the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 file_path, \n",
    "                 only_CDR3: bool = False, \n",
    "                 only_experimental: bool = False, \n",
    "                 TCR_encode: str = [\"LSTM\", \"CNN\"],\n",
    "                 encoding_model: nn.Module = None,\n",
    "                 encoding_size = 20) -> None:\n",
    "        df, HLA_encode, y  = self.basic_io(file_path, only_experimental=only_experimental)\n",
    "\n",
    "        # convert from object to tensor\n",
    "        X_TCR_seq = torch.zeros((len(df), 0))\n",
    "        for region in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            TCR_seq = df.loc[:, region].values\n",
    "            TCR_seq_encode = torch.zeros((0, TCR_seq[0].shape[1]))\n",
    "            for i in range(len(TCR_seq)):\n",
    "                encoding = torch.from_numpy(TCR_seq[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                TCR_seq_encode = torch.cat((TCR_seq_encode, encoding), dim=0)\n",
    "\n",
    "            X_TCR_seq = torch.cat((TCR_seq_encode, X_TCR_seq), dim=1)\n",
    "        \n",
    "        if TCR_encode == \"CNN\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        elif TCR_encode == \"LSTM\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        else:\n",
    "            raise ValueError(\"The TCR encoding method is not supported yet.\")\n",
    "        \n",
    "        # encoding model \n",
    "        X_features, _ = encoding_model(X_TCR_seq)\n",
    "        X_features = X_features.view(-1, encoding_size).data\n",
    "\n",
    "        # add the neoantigen sequence encoding features\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            neo = df.loc[:, seq].values\n",
    "            neo_encode = torch.zeros((0, neo[0].shape[1]))\n",
    "            for i in range(len(neo)):\n",
    "                encoding = torch.from_numpy(neo[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                neo_encode = torch.cat((neo_encode, encoding), dim=0)\n",
    "            X_features = torch.cat((X_features, neo_encode), dim=1)\n",
    "\n",
    "        X_features = torch.cat((X_features, torch.from_numpy(HLA_encode)), dim=1)\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "            \n",
    "    \n",
    "    def basic_io(self, file_path, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # drop the rows with nan\n",
    "        df = df.dropna()\n",
    "\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "        y.value_counts().plot.pie(autopct='%.2f')\n",
    "        return df, X_HLA_encoded, y.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, TCR_encode=\"CNN\", only_experimental=True, encoding_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.61003861003861"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TCRData[0][0].shape\n",
    "97.41 / 2.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "    length = len_map[chain]\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# drop the rows with nan\n",
    "df = df.dropna()\n",
    "\n",
    "X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "HLAencoder = OneHotEncoder()\n",
    "X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 batch_size=32,) -> None:\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(self.batch_size, self.input_size)\n",
    "        # print(f\"The model input shape is : {input.shape}\")\n",
    "        output = self.linear_layer(input)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # one-hot encoding the target\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        target = target.to(torch.bool)\n",
    "        one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "        one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "        one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "\n",
    "        data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "        # data.data extract the item of the torch without the hook on the computation graph\n",
    "        data = data.view(-1, 20+5*6+2).to(torch.float32)\n",
    "        output = model(data) # output shape: (batch_size, 2)\n",
    "        # based on the one-hot target to decide the weights of loss for each class\n",
    "        # weight = (one_hot_target == torch.tensor([0, 1]).to(device)).sum() / (one_hot_target == torch.tensor([1, 0]).to(device)).sum()\n",
    "        # weights = torch.tensor([1, 1/weight]).to(device)\n",
    "        # loss = nn.CrossEntropyLoss(weight=weights)(output, one_hot_target.data)\n",
    "        loss = criterion(output, one_hot_target.data)\n",
    "        train_loss += loss.item() * batch_size\n",
    "        # The loss has been weighted, and the loss has already been divided by the batch size\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # get the prediction\n",
    "        pred = output.argmax(dim=1, keepdim=True) # shape: (batch_size, 1)\n",
    "        correct += pred.eq(one_hot_target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training stage for Flod {fold} Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \\\n",
    "                ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "            # print(f\"The current output is {output}, and the target is {one_hot_target}\")\n",
    "            # print(f\"The weight of the batch is {weights}, \\n The output is {output}, \\n The one_hot_target is {one_hot_target.data}\")\n",
    "    return train_loss, correct / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # one-hot encoding the target\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            target = target.to(torch.bool)\n",
    "            one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "            one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "            one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "\n",
    "            data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "            data = data.view(-1, 20+5*6+2).to(torch.float32).data\n",
    "            output = model(data) # output shape: (batch_size, 2)\n",
    "            # get the test loss and prediction\n",
    "            test_loss += nn.CrossEntropyLoss()(output, one_hot_target.data).item() * len(test_loader.dataset)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(one_hot_target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    \n",
    "    # test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 32\n",
    "input_size = 52\n",
    "folds = 5\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = prediction_model(input_size=input_size, batch_size=batch_size).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Fold 0-------------------\n",
      "Training stage for Flod 0 Epoch: 1 [0/29980                 (0%)]\tLoss: 0.706105\n",
      "Training stage for Flod 0 Epoch: 1 [3200/29980                 (11%)]\tLoss: 0.477374\n",
      "Training stage for Flod 0 Epoch: 1 [6400/29980                 (21%)]\tLoss: 0.456921\n",
      "Training stage for Flod 0 Epoch: 1 [9600/29980                 (32%)]\tLoss: 0.376935\n",
      "Training stage for Flod 0 Epoch: 1 [12800/29980                 (43%)]\tLoss: 0.403605\n",
      "Training stage for Flod 0 Epoch: 1 [16000/29980                 (53%)]\tLoss: 0.727357\n",
      "Training stage for Flod 0 Epoch: 1 [19200/29980                 (64%)]\tLoss: 0.530165\n",
      "Training stage for Flod 0 Epoch: 1 [22400/29980                 (75%)]\tLoss: 0.498785\n",
      "Training stage for Flod 0 Epoch: 1 [25600/29980                 (85%)]\tLoss: 0.348809\n",
      "Training stage for Flod 0 Epoch: 1 [28800/29980                 (96%)]\tLoss: 0.396906\n",
      "Test set for fold0: Average Loss:           812818.0365, Accuracy: 6340/7496           (85%)\n",
      "Training stage for Flod 0 Epoch: 2 [0/29980                 (0%)]\tLoss: 0.403071\n",
      "Training stage for Flod 0 Epoch: 2 [3200/29980                 (11%)]\tLoss: 0.326880\n",
      "Training stage for Flod 0 Epoch: 2 [6400/29980                 (21%)]\tLoss: 0.406983\n",
      "Training stage for Flod 0 Epoch: 2 [9600/29980                 (32%)]\tLoss: 0.438310\n",
      "Training stage for Flod 0 Epoch: 2 [12800/29980                 (43%)]\tLoss: 0.452597\n",
      "Training stage for Flod 0 Epoch: 2 [16000/29980                 (53%)]\tLoss: 0.396150\n",
      "Training stage for Flod 0 Epoch: 2 [19200/29980                 (64%)]\tLoss: 0.552451\n",
      "Training stage for Flod 0 Epoch: 2 [22400/29980                 (75%)]\tLoss: 0.423992\n",
      "Training stage for Flod 0 Epoch: 2 [25600/29980                 (85%)]\tLoss: 0.435243\n",
      "Training stage for Flod 0 Epoch: 2 [28800/29980                 (96%)]\tLoss: 0.469883\n",
      "Test set for fold0: Average Loss:           796161.7703, Accuracy: 6368/7496           (85%)\n",
      "Training stage for Flod 0 Epoch: 3 [0/29980                 (0%)]\tLoss: 0.335050\n",
      "Training stage for Flod 0 Epoch: 3 [3200/29980                 (11%)]\tLoss: 0.505388\n",
      "Training stage for Flod 0 Epoch: 3 [6400/29980                 (21%)]\tLoss: 0.374663\n",
      "Training stage for Flod 0 Epoch: 3 [9600/29980                 (32%)]\tLoss: 0.370671\n",
      "Training stage for Flod 0 Epoch: 3 [12800/29980                 (43%)]\tLoss: 0.418130\n",
      "Training stage for Flod 0 Epoch: 3 [16000/29980                 (53%)]\tLoss: 0.425361\n",
      "Training stage for Flod 0 Epoch: 3 [19200/29980                 (64%)]\tLoss: 0.368733\n",
      "Training stage for Flod 0 Epoch: 3 [22400/29980                 (75%)]\tLoss: 0.453053\n",
      "Training stage for Flod 0 Epoch: 3 [25600/29980                 (85%)]\tLoss: 0.512268\n",
      "Training stage for Flod 0 Epoch: 3 [28800/29980                 (96%)]\tLoss: 0.493643\n",
      "Test set for fold0: Average Loss:           773511.6857, Accuracy: 6502/7496           (87%)\n",
      "Training stage for Flod 0 Epoch: 4 [0/29980                 (0%)]\tLoss: 0.409417\n",
      "Training stage for Flod 0 Epoch: 4 [3200/29980                 (11%)]\tLoss: 0.338879\n",
      "Training stage for Flod 0 Epoch: 4 [6400/29980                 (21%)]\tLoss: 0.385425\n",
      "Training stage for Flod 0 Epoch: 4 [9600/29980                 (32%)]\tLoss: 0.322803\n",
      "Training stage for Flod 0 Epoch: 4 [12800/29980                 (43%)]\tLoss: 0.377533\n",
      "Training stage for Flod 0 Epoch: 4 [16000/29980                 (53%)]\tLoss: 0.452653\n",
      "Training stage for Flod 0 Epoch: 4 [19200/29980                 (64%)]\tLoss: 0.422094\n",
      "Training stage for Flod 0 Epoch: 4 [22400/29980                 (75%)]\tLoss: 0.408930\n",
      "Training stage for Flod 0 Epoch: 4 [25600/29980                 (85%)]\tLoss: 0.345145\n",
      "Training stage for Flod 0 Epoch: 4 [28800/29980                 (96%)]\tLoss: 0.448610\n",
      "Test set for fold0: Average Loss:           780352.7309, Accuracy: 6437/7496           (86%)\n",
      "Training stage for Flod 0 Epoch: 5 [0/29980                 (0%)]\tLoss: 0.389092\n",
      "Training stage for Flod 0 Epoch: 5 [3200/29980                 (11%)]\tLoss: 0.404858\n",
      "Training stage for Flod 0 Epoch: 5 [6400/29980                 (21%)]\tLoss: 0.420023\n",
      "Training stage for Flod 0 Epoch: 5 [9600/29980                 (32%)]\tLoss: 0.394953\n",
      "Training stage for Flod 0 Epoch: 5 [12800/29980                 (43%)]\tLoss: 0.376524\n",
      "Training stage for Flod 0 Epoch: 5 [16000/29980                 (53%)]\tLoss: 0.469382\n",
      "Training stage for Flod 0 Epoch: 5 [19200/29980                 (64%)]\tLoss: 0.430173\n",
      "Training stage for Flod 0 Epoch: 5 [22400/29980                 (75%)]\tLoss: 0.375875\n",
      "Training stage for Flod 0 Epoch: 5 [25600/29980                 (85%)]\tLoss: 0.502128\n",
      "Training stage for Flod 0 Epoch: 5 [28800/29980                 (96%)]\tLoss: 0.381360\n",
      "Test set for fold0: Average Loss:           762092.2786, Accuracy: 6575/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 6 [0/29980                 (0%)]\tLoss: 0.416273\n",
      "Training stage for Flod 0 Epoch: 6 [3200/29980                 (11%)]\tLoss: 0.416348\n",
      "Training stage for Flod 0 Epoch: 6 [6400/29980                 (21%)]\tLoss: 0.375726\n",
      "Training stage for Flod 0 Epoch: 6 [9600/29980                 (32%)]\tLoss: 0.366538\n",
      "Training stage for Flod 0 Epoch: 6 [12800/29980                 (43%)]\tLoss: 0.391539\n",
      "Training stage for Flod 0 Epoch: 6 [16000/29980                 (53%)]\tLoss: 0.353632\n",
      "Training stage for Flod 0 Epoch: 6 [19200/29980                 (64%)]\tLoss: 0.483338\n",
      "Training stage for Flod 0 Epoch: 6 [22400/29980                 (75%)]\tLoss: 0.383511\n",
      "Training stage for Flod 0 Epoch: 6 [25600/29980                 (85%)]\tLoss: 0.396406\n",
      "Training stage for Flod 0 Epoch: 6 [28800/29980                 (96%)]\tLoss: 0.386084\n",
      "Test set for fold0: Average Loss:           806962.0744, Accuracy: 6333/7496           (84%)\n",
      "Training stage for Flod 0 Epoch: 7 [0/29980                 (0%)]\tLoss: 0.343373\n",
      "Training stage for Flod 0 Epoch: 7 [3200/29980                 (11%)]\tLoss: 0.320699\n",
      "Training stage for Flod 0 Epoch: 7 [6400/29980                 (21%)]\tLoss: 0.339952\n",
      "Training stage for Flod 0 Epoch: 7 [9600/29980                 (32%)]\tLoss: 0.377148\n",
      "Training stage for Flod 0 Epoch: 7 [12800/29980                 (43%)]\tLoss: 0.438045\n",
      "Training stage for Flod 0 Epoch: 7 [16000/29980                 (53%)]\tLoss: 0.377614\n",
      "Training stage for Flod 0 Epoch: 7 [19200/29980                 (64%)]\tLoss: 0.399370\n",
      "Training stage for Flod 0 Epoch: 7 [22400/29980                 (75%)]\tLoss: 0.381521\n",
      "Training stage for Flod 0 Epoch: 7 [25600/29980                 (85%)]\tLoss: 0.465655\n",
      "Training stage for Flod 0 Epoch: 7 [28800/29980                 (96%)]\tLoss: 0.484423\n",
      "Test set for fold0: Average Loss:           744483.0996, Accuracy: 6631/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 8 [0/29980                 (0%)]\tLoss: 0.375736\n",
      "Training stage for Flod 0 Epoch: 8 [3200/29980                 (11%)]\tLoss: 0.314794\n",
      "Training stage for Flod 0 Epoch: 8 [6400/29980                 (21%)]\tLoss: 0.368377\n",
      "Training stage for Flod 0 Epoch: 8 [9600/29980                 (32%)]\tLoss: 0.419540\n",
      "Training stage for Flod 0 Epoch: 8 [12800/29980                 (43%)]\tLoss: 0.382509\n",
      "Training stage for Flod 0 Epoch: 8 [16000/29980                 (53%)]\tLoss: 0.378321\n",
      "Training stage for Flod 0 Epoch: 8 [19200/29980                 (64%)]\tLoss: 0.370376\n",
      "Training stage for Flod 0 Epoch: 8 [22400/29980                 (75%)]\tLoss: 0.371104\n",
      "Training stage for Flod 0 Epoch: 8 [25600/29980                 (85%)]\tLoss: 0.353059\n",
      "Training stage for Flod 0 Epoch: 8 [28800/29980                 (96%)]\tLoss: 0.345734\n",
      "Test set for fold0: Average Loss:           759298.3212, Accuracy: 6559/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 9 [0/29980                 (0%)]\tLoss: 0.375178\n",
      "Training stage for Flod 0 Epoch: 9 [3200/29980                 (11%)]\tLoss: 0.424543\n",
      "Training stage for Flod 0 Epoch: 9 [6400/29980                 (21%)]\tLoss: 0.417656\n",
      "Training stage for Flod 0 Epoch: 9 [9600/29980                 (32%)]\tLoss: 0.403249\n",
      "Training stage for Flod 0 Epoch: 9 [12800/29980                 (43%)]\tLoss: 0.335529\n",
      "Training stage for Flod 0 Epoch: 9 [16000/29980                 (53%)]\tLoss: 0.333724\n",
      "Training stage for Flod 0 Epoch: 9 [19200/29980                 (64%)]\tLoss: 0.407583\n",
      "Training stage for Flod 0 Epoch: 9 [22400/29980                 (75%)]\tLoss: 0.313789\n",
      "Training stage for Flod 0 Epoch: 9 [25600/29980                 (85%)]\tLoss: 0.414720\n",
      "Training stage for Flod 0 Epoch: 9 [28800/29980                 (96%)]\tLoss: 0.331506\n",
      "Test set for fold0: Average Loss:           759012.8071, Accuracy: 6526/7496           (87%)\n",
      "Training stage for Flod 0 Epoch: 10 [0/29980                 (0%)]\tLoss: 0.345397\n",
      "Training stage for Flod 0 Epoch: 10 [3200/29980                 (11%)]\tLoss: 0.360632\n",
      "Training stage for Flod 0 Epoch: 10 [6400/29980                 (21%)]\tLoss: 0.348012\n",
      "Training stage for Flod 0 Epoch: 10 [9600/29980                 (32%)]\tLoss: 0.404639\n",
      "Training stage for Flod 0 Epoch: 10 [12800/29980                 (43%)]\tLoss: 0.315626\n",
      "Training stage for Flod 0 Epoch: 10 [16000/29980                 (53%)]\tLoss: 0.429845\n",
      "Training stage for Flod 0 Epoch: 10 [19200/29980                 (64%)]\tLoss: 0.458700\n",
      "Training stage for Flod 0 Epoch: 10 [22400/29980                 (75%)]\tLoss: 0.462242\n",
      "Training stage for Flod 0 Epoch: 10 [25600/29980                 (85%)]\tLoss: 0.349892\n",
      "Training stage for Flod 0 Epoch: 10 [28800/29980                 (96%)]\tLoss: 0.400258\n",
      "Test set for fold0: Average Loss:           779185.7228, Accuracy: 6462/7496           (86%)\n",
      "Training stage for Flod 0 Epoch: 11 [0/29980                 (0%)]\tLoss: 0.399018\n",
      "Training stage for Flod 0 Epoch: 11 [3200/29980                 (11%)]\tLoss: 0.379710\n",
      "Training stage for Flod 0 Epoch: 11 [6400/29980                 (21%)]\tLoss: 0.389564\n",
      "Training stage for Flod 0 Epoch: 11 [9600/29980                 (32%)]\tLoss: 0.345964\n",
      "Training stage for Flod 0 Epoch: 11 [12800/29980                 (43%)]\tLoss: 0.393881\n",
      "Training stage for Flod 0 Epoch: 11 [16000/29980                 (53%)]\tLoss: 0.381782\n",
      "Training stage for Flod 0 Epoch: 11 [19200/29980                 (64%)]\tLoss: 0.433869\n",
      "Training stage for Flod 0 Epoch: 11 [22400/29980                 (75%)]\tLoss: 0.407177\n",
      "Training stage for Flod 0 Epoch: 11 [25600/29980                 (85%)]\tLoss: 0.388768\n",
      "Training stage for Flod 0 Epoch: 11 [28800/29980                 (96%)]\tLoss: 0.393174\n",
      "Test set for fold0: Average Loss:           758010.9564, Accuracy: 6586/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 12 [0/29980                 (0%)]\tLoss: 0.348882\n",
      "Training stage for Flod 0 Epoch: 12 [3200/29980                 (11%)]\tLoss: 0.376705\n",
      "Training stage for Flod 0 Epoch: 12 [6400/29980                 (21%)]\tLoss: 0.385488\n",
      "Training stage for Flod 0 Epoch: 12 [9600/29980                 (32%)]\tLoss: 0.408180\n",
      "Training stage for Flod 0 Epoch: 12 [12800/29980                 (43%)]\tLoss: 0.345421\n",
      "Training stage for Flod 0 Epoch: 12 [16000/29980                 (53%)]\tLoss: 0.379320\n",
      "Training stage for Flod 0 Epoch: 12 [19200/29980                 (64%)]\tLoss: 0.342560\n",
      "Training stage for Flod 0 Epoch: 12 [22400/29980                 (75%)]\tLoss: 0.428678\n",
      "Training stage for Flod 0 Epoch: 12 [25600/29980                 (85%)]\tLoss: 0.395874\n",
      "Training stage for Flod 0 Epoch: 12 [28800/29980                 (96%)]\tLoss: 0.353587\n",
      "Test set for fold0: Average Loss:           759768.1933, Accuracy: 6569/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 13 [0/29980                 (0%)]\tLoss: 0.332344\n",
      "Training stage for Flod 0 Epoch: 13 [3200/29980                 (11%)]\tLoss: 0.388256\n",
      "Training stage for Flod 0 Epoch: 13 [6400/29980                 (21%)]\tLoss: 0.359892\n",
      "Training stage for Flod 0 Epoch: 13 [9600/29980                 (32%)]\tLoss: 0.374220\n",
      "Training stage for Flod 0 Epoch: 13 [12800/29980                 (43%)]\tLoss: 0.347770\n",
      "Training stage for Flod 0 Epoch: 13 [16000/29980                 (53%)]\tLoss: 0.411584\n",
      "Training stage for Flod 0 Epoch: 13 [19200/29980                 (64%)]\tLoss: 0.377828\n",
      "Training stage for Flod 0 Epoch: 13 [22400/29980                 (75%)]\tLoss: 0.405096\n",
      "Training stage for Flod 0 Epoch: 13 [25600/29980                 (85%)]\tLoss: 0.349014\n",
      "Training stage for Flod 0 Epoch: 13 [28800/29980                 (96%)]\tLoss: 0.378979\n",
      "Test set for fold0: Average Loss:           751809.8287, Accuracy: 6603/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 14 [0/29980                 (0%)]\tLoss: 0.421736\n",
      "Training stage for Flod 0 Epoch: 14 [3200/29980                 (11%)]\tLoss: 0.485517\n",
      "Training stage for Flod 0 Epoch: 14 [6400/29980                 (21%)]\tLoss: 0.481954\n",
      "Training stage for Flod 0 Epoch: 14 [9600/29980                 (32%)]\tLoss: 0.339943\n",
      "Training stage for Flod 0 Epoch: 14 [12800/29980                 (43%)]\tLoss: 0.409504\n",
      "Training stage for Flod 0 Epoch: 14 [16000/29980                 (53%)]\tLoss: 0.487912\n",
      "Training stage for Flod 0 Epoch: 14 [19200/29980                 (64%)]\tLoss: 0.345513\n",
      "Training stage for Flod 0 Epoch: 14 [22400/29980                 (75%)]\tLoss: 0.375692\n",
      "Training stage for Flod 0 Epoch: 14 [25600/29980                 (85%)]\tLoss: 0.321587\n",
      "Training stage for Flod 0 Epoch: 14 [28800/29980                 (96%)]\tLoss: 0.403300\n",
      "Test set for fold0: Average Loss:           778949.6016, Accuracy: 6462/7496           (86%)\n",
      "Training stage for Flod 0 Epoch: 15 [0/29980                 (0%)]\tLoss: 0.375210\n",
      "Training stage for Flod 0 Epoch: 15 [3200/29980                 (11%)]\tLoss: 0.408106\n",
      "Training stage for Flod 0 Epoch: 15 [6400/29980                 (21%)]\tLoss: 0.513421\n",
      "Training stage for Flod 0 Epoch: 15 [9600/29980                 (32%)]\tLoss: 0.426743\n",
      "Training stage for Flod 0 Epoch: 15 [12800/29980                 (43%)]\tLoss: 0.380169\n",
      "Training stage for Flod 0 Epoch: 15 [16000/29980                 (53%)]\tLoss: 0.436906\n",
      "Training stage for Flod 0 Epoch: 15 [19200/29980                 (64%)]\tLoss: 0.402229\n",
      "Training stage for Flod 0 Epoch: 15 [22400/29980                 (75%)]\tLoss: 0.433418\n",
      "Training stage for Flod 0 Epoch: 15 [25600/29980                 (85%)]\tLoss: 0.378193\n",
      "Training stage for Flod 0 Epoch: 15 [28800/29980                 (96%)]\tLoss: 0.344654\n",
      "Test set for fold0: Average Loss:           768617.9453, Accuracy: 6538/7496           (87%)\n",
      "Training stage for Flod 0 Epoch: 16 [0/29980                 (0%)]\tLoss: 0.344743\n",
      "Training stage for Flod 0 Epoch: 16 [3200/29980                 (11%)]\tLoss: 0.344515\n",
      "Training stage for Flod 0 Epoch: 16 [6400/29980                 (21%)]\tLoss: 0.344630\n",
      "Training stage for Flod 0 Epoch: 16 [9600/29980                 (32%)]\tLoss: 0.412737\n",
      "Training stage for Flod 0 Epoch: 16 [12800/29980                 (43%)]\tLoss: 0.437842\n",
      "Training stage for Flod 0 Epoch: 16 [16000/29980                 (53%)]\tLoss: 0.440526\n",
      "Training stage for Flod 0 Epoch: 16 [19200/29980                 (64%)]\tLoss: 0.442365\n",
      "Training stage for Flod 0 Epoch: 16 [22400/29980                 (75%)]\tLoss: 0.471460\n",
      "Training stage for Flod 0 Epoch: 16 [25600/29980                 (85%)]\tLoss: 0.346983\n",
      "Training stage for Flod 0 Epoch: 16 [28800/29980                 (96%)]\tLoss: 0.365873\n",
      "Test set for fold0: Average Loss:           757479.0089, Accuracy: 6570/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 17 [0/29980                 (0%)]\tLoss: 0.439464\n",
      "Training stage for Flod 0 Epoch: 17 [3200/29980                 (11%)]\tLoss: 0.412973\n",
      "Training stage for Flod 0 Epoch: 17 [6400/29980                 (21%)]\tLoss: 0.378297\n",
      "Training stage for Flod 0 Epoch: 17 [9600/29980                 (32%)]\tLoss: 0.497892\n",
      "Training stage for Flod 0 Epoch: 17 [12800/29980                 (43%)]\tLoss: 0.343924\n",
      "Training stage for Flod 0 Epoch: 17 [16000/29980                 (53%)]\tLoss: 0.344886\n",
      "Training stage for Flod 0 Epoch: 17 [19200/29980                 (64%)]\tLoss: 0.496406\n",
      "Training stage for Flod 0 Epoch: 17 [22400/29980                 (75%)]\tLoss: 0.440346\n",
      "Training stage for Flod 0 Epoch: 17 [25600/29980                 (85%)]\tLoss: 0.387292\n",
      "Training stage for Flod 0 Epoch: 17 [28800/29980                 (96%)]\tLoss: 0.371724\n",
      "Test set for fold0: Average Loss:           763289.9280, Accuracy: 6544/7496           (87%)\n",
      "Training stage for Flod 0 Epoch: 18 [0/29980                 (0%)]\tLoss: 0.424432\n",
      "Training stage for Flod 0 Epoch: 18 [3200/29980                 (11%)]\tLoss: 0.516236\n",
      "Training stage for Flod 0 Epoch: 18 [6400/29980                 (21%)]\tLoss: 0.380555\n",
      "Training stage for Flod 0 Epoch: 18 [9600/29980                 (32%)]\tLoss: 0.347033\n",
      "Training stage for Flod 0 Epoch: 18 [12800/29980                 (43%)]\tLoss: 0.373796\n",
      "Training stage for Flod 0 Epoch: 18 [16000/29980                 (53%)]\tLoss: 0.371998\n",
      "Training stage for Flod 0 Epoch: 18 [19200/29980                 (64%)]\tLoss: 0.472082\n",
      "Training stage for Flod 0 Epoch: 18 [22400/29980                 (75%)]\tLoss: 0.362744\n",
      "Training stage for Flod 0 Epoch: 18 [25600/29980                 (85%)]\tLoss: 0.315859\n",
      "Training stage for Flod 0 Epoch: 18 [28800/29980                 (96%)]\tLoss: 0.377123\n",
      "Test set for fold0: Average Loss:           758949.9067, Accuracy: 6571/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 19 [0/29980                 (0%)]\tLoss: 0.413559\n",
      "Training stage for Flod 0 Epoch: 19 [3200/29980                 (11%)]\tLoss: 0.380470\n",
      "Training stage for Flod 0 Epoch: 19 [6400/29980                 (21%)]\tLoss: 0.403920\n",
      "Training stage for Flod 0 Epoch: 19 [9600/29980                 (32%)]\tLoss: 0.357184\n",
      "Training stage for Flod 0 Epoch: 19 [12800/29980                 (43%)]\tLoss: 0.497118\n",
      "Training stage for Flod 0 Epoch: 19 [16000/29980                 (53%)]\tLoss: 0.389983\n",
      "Training stage for Flod 0 Epoch: 19 [19200/29980                 (64%)]\tLoss: 0.364700\n",
      "Training stage for Flod 0 Epoch: 19 [22400/29980                 (75%)]\tLoss: 0.353075\n",
      "Training stage for Flod 0 Epoch: 19 [25600/29980                 (85%)]\tLoss: 0.335409\n",
      "Training stage for Flod 0 Epoch: 19 [28800/29980                 (96%)]\tLoss: 0.409756\n",
      "Test set for fold0: Average Loss:           748913.6597, Accuracy: 6611/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 20 [0/29980                 (0%)]\tLoss: 0.372503\n",
      "Training stage for Flod 0 Epoch: 20 [3200/29980                 (11%)]\tLoss: 0.437132\n",
      "Training stage for Flod 0 Epoch: 20 [6400/29980                 (21%)]\tLoss: 0.380065\n",
      "Training stage for Flod 0 Epoch: 20 [9600/29980                 (32%)]\tLoss: 0.372509\n",
      "Training stage for Flod 0 Epoch: 20 [12800/29980                 (43%)]\tLoss: 0.434707\n",
      "Training stage for Flod 0 Epoch: 20 [16000/29980                 (53%)]\tLoss: 0.407624\n",
      "Training stage for Flod 0 Epoch: 20 [19200/29980                 (64%)]\tLoss: 0.411153\n",
      "Training stage for Flod 0 Epoch: 20 [22400/29980                 (75%)]\tLoss: 0.388921\n",
      "Training stage for Flod 0 Epoch: 20 [25600/29980                 (85%)]\tLoss: 0.323014\n",
      "Training stage for Flod 0 Epoch: 20 [28800/29980                 (96%)]\tLoss: 0.419388\n",
      "Test set for fold0: Average Loss:           733199.8798, Accuracy: 6689/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 21 [0/29980                 (0%)]\tLoss: 0.412478\n",
      "Training stage for Flod 0 Epoch: 21 [3200/29980                 (11%)]\tLoss: 0.343015\n",
      "Training stage for Flod 0 Epoch: 21 [6400/29980                 (21%)]\tLoss: 0.343301\n",
      "Training stage for Flod 0 Epoch: 21 [9600/29980                 (32%)]\tLoss: 0.481204\n",
      "Training stage for Flod 0 Epoch: 21 [12800/29980                 (43%)]\tLoss: 0.374347\n",
      "Training stage for Flod 0 Epoch: 21 [16000/29980                 (53%)]\tLoss: 0.458510\n",
      "Training stage for Flod 0 Epoch: 21 [19200/29980                 (64%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 21 [22400/29980                 (75%)]\tLoss: 0.445151\n",
      "Training stage for Flod 0 Epoch: 21 [25600/29980                 (85%)]\tLoss: 0.426031\n",
      "Training stage for Flod 0 Epoch: 21 [28800/29980                 (96%)]\tLoss: 0.376042\n",
      "Test set for fold0: Average Loss:           744667.4399, Accuracy: 6650/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 22 [0/29980                 (0%)]\tLoss: 0.346889\n",
      "Training stage for Flod 0 Epoch: 22 [3200/29980                 (11%)]\tLoss: 0.429924\n",
      "Training stage for Flod 0 Epoch: 22 [6400/29980                 (21%)]\tLoss: 0.411364\n",
      "Training stage for Flod 0 Epoch: 22 [9600/29980                 (32%)]\tLoss: 0.313533\n",
      "Training stage for Flod 0 Epoch: 22 [12800/29980                 (43%)]\tLoss: 0.461121\n",
      "Training stage for Flod 0 Epoch: 22 [16000/29980                 (53%)]\tLoss: 0.349078\n",
      "Training stage for Flod 0 Epoch: 22 [19200/29980                 (64%)]\tLoss: 0.446077\n",
      "Training stage for Flod 0 Epoch: 22 [22400/29980                 (75%)]\tLoss: 0.388870\n",
      "Training stage for Flod 0 Epoch: 22 [25600/29980                 (85%)]\tLoss: 0.431537\n",
      "Training stage for Flod 0 Epoch: 22 [28800/29980                 (96%)]\tLoss: 0.363047\n",
      "Test set for fold0: Average Loss:           747743.5756, Accuracy: 6585/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 23 [0/29980                 (0%)]\tLoss: 0.357134\n",
      "Training stage for Flod 0 Epoch: 23 [3200/29980                 (11%)]\tLoss: 0.344594\n",
      "Training stage for Flod 0 Epoch: 23 [6400/29980                 (21%)]\tLoss: 0.438267\n",
      "Training stage for Flod 0 Epoch: 23 [9600/29980                 (32%)]\tLoss: 0.406674\n",
      "Training stage for Flod 0 Epoch: 23 [12800/29980                 (43%)]\tLoss: 0.390312\n",
      "Training stage for Flod 0 Epoch: 23 [16000/29980                 (53%)]\tLoss: 0.377917\n",
      "Training stage for Flod 0 Epoch: 23 [19200/29980                 (64%)]\tLoss: 0.378433\n",
      "Training stage for Flod 0 Epoch: 23 [22400/29980                 (75%)]\tLoss: 0.389668\n",
      "Training stage for Flod 0 Epoch: 23 [25600/29980                 (85%)]\tLoss: 0.365116\n",
      "Training stage for Flod 0 Epoch: 23 [28800/29980                 (96%)]\tLoss: 0.419710\n",
      "Test set for fold0: Average Loss:           737129.5429, Accuracy: 6683/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 24 [0/29980                 (0%)]\tLoss: 0.343409\n",
      "Training stage for Flod 0 Epoch: 24 [3200/29980                 (11%)]\tLoss: 0.344695\n",
      "Training stage for Flod 0 Epoch: 24 [6400/29980                 (21%)]\tLoss: 0.344609\n",
      "Training stage for Flod 0 Epoch: 24 [9600/29980                 (32%)]\tLoss: 0.468856\n",
      "Training stage for Flod 0 Epoch: 24 [12800/29980                 (43%)]\tLoss: 0.318039\n",
      "Training stage for Flod 0 Epoch: 24 [16000/29980                 (53%)]\tLoss: 0.432901\n",
      "Training stage for Flod 0 Epoch: 24 [19200/29980                 (64%)]\tLoss: 0.411818\n",
      "Training stage for Flod 0 Epoch: 24 [22400/29980                 (75%)]\tLoss: 0.375801\n",
      "Training stage for Flod 0 Epoch: 24 [25600/29980                 (85%)]\tLoss: 0.407304\n",
      "Training stage for Flod 0 Epoch: 24 [28800/29980                 (96%)]\tLoss: 0.347402\n",
      "Test set for fold0: Average Loss:           761555.0897, Accuracy: 6565/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 25 [0/29980                 (0%)]\tLoss: 0.413187\n",
      "Training stage for Flod 0 Epoch: 25 [3200/29980                 (11%)]\tLoss: 0.349021\n",
      "Training stage for Flod 0 Epoch: 25 [6400/29980                 (21%)]\tLoss: 0.405286\n",
      "Training stage for Flod 0 Epoch: 25 [9600/29980                 (32%)]\tLoss: 0.374066\n",
      "Training stage for Flod 0 Epoch: 25 [12800/29980                 (43%)]\tLoss: 0.409332\n",
      "Training stage for Flod 0 Epoch: 25 [16000/29980                 (53%)]\tLoss: 0.344393\n",
      "Training stage for Flod 0 Epoch: 25 [19200/29980                 (64%)]\tLoss: 0.431417\n",
      "Training stage for Flod 0 Epoch: 25 [22400/29980                 (75%)]\tLoss: 0.324852\n",
      "Training stage for Flod 0 Epoch: 25 [25600/29980                 (85%)]\tLoss: 0.403535\n",
      "Training stage for Flod 0 Epoch: 25 [28800/29980                 (96%)]\tLoss: 0.407935\n",
      "Test set for fold0: Average Loss:           782181.8430, Accuracy: 6483/7496           (86%)\n",
      "Training stage for Flod 0 Epoch: 26 [0/29980                 (0%)]\tLoss: 0.347828\n",
      "Training stage for Flod 0 Epoch: 26 [3200/29980                 (11%)]\tLoss: 0.438751\n",
      "Training stage for Flod 0 Epoch: 26 [6400/29980                 (21%)]\tLoss: 0.404332\n",
      "Training stage for Flod 0 Epoch: 26 [9600/29980                 (32%)]\tLoss: 0.393750\n",
      "Training stage for Flod 0 Epoch: 26 [12800/29980                 (43%)]\tLoss: 0.345345\n",
      "Training stage for Flod 0 Epoch: 26 [16000/29980                 (53%)]\tLoss: 0.379410\n",
      "Training stage for Flod 0 Epoch: 26 [19200/29980                 (64%)]\tLoss: 0.344747\n",
      "Training stage for Flod 0 Epoch: 26 [22400/29980                 (75%)]\tLoss: 0.422552\n",
      "Training stage for Flod 0 Epoch: 26 [25600/29980                 (85%)]\tLoss: 0.378630\n",
      "Training stage for Flod 0 Epoch: 26 [28800/29980                 (96%)]\tLoss: 0.402244\n",
      "Test set for fold0: Average Loss:           733159.6498, Accuracy: 6699/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 27 [0/29980                 (0%)]\tLoss: 0.378677\n",
      "Training stage for Flod 0 Epoch: 27 [3200/29980                 (11%)]\tLoss: 0.345686\n",
      "Training stage for Flod 0 Epoch: 27 [6400/29980                 (21%)]\tLoss: 0.433205\n",
      "Training stage for Flod 0 Epoch: 27 [9600/29980                 (32%)]\tLoss: 0.410827\n",
      "Training stage for Flod 0 Epoch: 27 [12800/29980                 (43%)]\tLoss: 0.411184\n",
      "Training stage for Flod 0 Epoch: 27 [16000/29980                 (53%)]\tLoss: 0.317043\n",
      "Training stage for Flod 0 Epoch: 27 [19200/29980                 (64%)]\tLoss: 0.459653\n",
      "Training stage for Flod 0 Epoch: 27 [22400/29980                 (75%)]\tLoss: 0.407230\n",
      "Training stage for Flod 0 Epoch: 27 [25600/29980                 (85%)]\tLoss: 0.450913\n",
      "Training stage for Flod 0 Epoch: 27 [28800/29980                 (96%)]\tLoss: 0.396219\n",
      "Test set for fold0: Average Loss:           737947.0879, Accuracy: 6675/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 28 [0/29980                 (0%)]\tLoss: 0.412560\n",
      "Training stage for Flod 0 Epoch: 28 [3200/29980                 (11%)]\tLoss: 0.351535\n",
      "Training stage for Flod 0 Epoch: 28 [6400/29980                 (21%)]\tLoss: 0.348215\n",
      "Training stage for Flod 0 Epoch: 28 [9600/29980                 (32%)]\tLoss: 0.344558\n",
      "Training stage for Flod 0 Epoch: 28 [12800/29980                 (43%)]\tLoss: 0.450994\n",
      "Training stage for Flod 0 Epoch: 28 [16000/29980                 (53%)]\tLoss: 0.378512\n",
      "Training stage for Flod 0 Epoch: 28 [19200/29980                 (64%)]\tLoss: 0.427861\n",
      "Training stage for Flod 0 Epoch: 28 [22400/29980                 (75%)]\tLoss: 0.381606\n",
      "Training stage for Flod 0 Epoch: 28 [25600/29980                 (85%)]\tLoss: 0.407301\n",
      "Training stage for Flod 0 Epoch: 28 [28800/29980                 (96%)]\tLoss: 0.362636\n",
      "Test set for fold0: Average Loss:           750719.2847, Accuracy: 6616/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 29 [0/29980                 (0%)]\tLoss: 0.458743\n",
      "Training stage for Flod 0 Epoch: 29 [3200/29980                 (11%)]\tLoss: 0.389681\n",
      "Training stage for Flod 0 Epoch: 29 [6400/29980                 (21%)]\tLoss: 0.377953\n",
      "Training stage for Flod 0 Epoch: 29 [9600/29980                 (32%)]\tLoss: 0.405586\n",
      "Training stage for Flod 0 Epoch: 29 [12800/29980                 (43%)]\tLoss: 0.424872\n",
      "Training stage for Flod 0 Epoch: 29 [16000/29980                 (53%)]\tLoss: 0.437823\n",
      "Training stage for Flod 0 Epoch: 29 [19200/29980                 (64%)]\tLoss: 0.378354\n",
      "Training stage for Flod 0 Epoch: 29 [22400/29980                 (75%)]\tLoss: 0.396814\n",
      "Training stage for Flod 0 Epoch: 29 [25600/29980                 (85%)]\tLoss: 0.313303\n",
      "Training stage for Flod 0 Epoch: 29 [28800/29980                 (96%)]\tLoss: 0.394236\n",
      "Test set for fold0: Average Loss:           767413.8105, Accuracy: 6545/7496           (87%)\n",
      "Training stage for Flod 0 Epoch: 30 [0/29980                 (0%)]\tLoss: 0.407020\n",
      "Training stage for Flod 0 Epoch: 30 [3200/29980                 (11%)]\tLoss: 0.367788\n",
      "Training stage for Flod 0 Epoch: 30 [6400/29980                 (21%)]\tLoss: 0.358353\n",
      "Training stage for Flod 0 Epoch: 30 [9600/29980                 (32%)]\tLoss: 0.398737\n",
      "Training stage for Flod 0 Epoch: 30 [12800/29980                 (43%)]\tLoss: 0.365411\n",
      "Training stage for Flod 0 Epoch: 30 [16000/29980                 (53%)]\tLoss: 0.377517\n",
      "Training stage for Flod 0 Epoch: 30 [19200/29980                 (64%)]\tLoss: 0.406084\n",
      "Training stage for Flod 0 Epoch: 30 [22400/29980                 (75%)]\tLoss: 0.414060\n",
      "Training stage for Flod 0 Epoch: 30 [25600/29980                 (85%)]\tLoss: 0.376440\n",
      "Training stage for Flod 0 Epoch: 30 [28800/29980                 (96%)]\tLoss: 0.313516\n",
      "Test set for fold0: Average Loss:           730357.7258, Accuracy: 6693/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 31 [0/29980                 (0%)]\tLoss: 0.346579\n",
      "Training stage for Flod 0 Epoch: 31 [3200/29980                 (11%)]\tLoss: 0.532654\n",
      "Training stage for Flod 0 Epoch: 31 [6400/29980                 (21%)]\tLoss: 0.376106\n",
      "Training stage for Flod 0 Epoch: 31 [9600/29980                 (32%)]\tLoss: 0.365668\n",
      "Training stage for Flod 0 Epoch: 31 [12800/29980                 (43%)]\tLoss: 0.428416\n",
      "Training stage for Flod 0 Epoch: 31 [16000/29980                 (53%)]\tLoss: 0.392842\n",
      "Training stage for Flod 0 Epoch: 31 [19200/29980                 (64%)]\tLoss: 0.317748\n",
      "Training stage for Flod 0 Epoch: 31 [22400/29980                 (75%)]\tLoss: 0.354861\n",
      "Training stage for Flod 0 Epoch: 31 [25600/29980                 (85%)]\tLoss: 0.406325\n",
      "Training stage for Flod 0 Epoch: 31 [28800/29980                 (96%)]\tLoss: 0.382490\n",
      "Test set for fold0: Average Loss:           762259.7826, Accuracy: 6566/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 32 [0/29980                 (0%)]\tLoss: 0.440097\n",
      "Training stage for Flod 0 Epoch: 32 [3200/29980                 (11%)]\tLoss: 0.432010\n",
      "Training stage for Flod 0 Epoch: 32 [6400/29980                 (21%)]\tLoss: 0.439224\n",
      "Training stage for Flod 0 Epoch: 32 [9600/29980                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 32 [12800/29980                 (43%)]\tLoss: 0.337550\n",
      "Training stage for Flod 0 Epoch: 32 [16000/29980                 (53%)]\tLoss: 0.313266\n",
      "Training stage for Flod 0 Epoch: 32 [19200/29980                 (64%)]\tLoss: 0.407019\n",
      "Training stage for Flod 0 Epoch: 32 [22400/29980                 (75%)]\tLoss: 0.516764\n",
      "Training stage for Flod 0 Epoch: 32 [25600/29980                 (85%)]\tLoss: 0.439016\n",
      "Training stage for Flod 0 Epoch: 32 [28800/29980                 (96%)]\tLoss: 0.384887\n",
      "Test set for fold0: Average Loss:           760049.3641, Accuracy: 6586/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 33 [0/29980                 (0%)]\tLoss: 0.344568\n",
      "Training stage for Flod 0 Epoch: 33 [3200/29980                 (11%)]\tLoss: 0.389879\n",
      "Training stage for Flod 0 Epoch: 33 [6400/29980                 (21%)]\tLoss: 0.438340\n",
      "Training stage for Flod 0 Epoch: 33 [9600/29980                 (32%)]\tLoss: 0.464086\n",
      "Training stage for Flod 0 Epoch: 33 [12800/29980                 (43%)]\tLoss: 0.348766\n",
      "Training stage for Flod 0 Epoch: 33 [16000/29980                 (53%)]\tLoss: 0.344515\n",
      "Training stage for Flod 0 Epoch: 33 [19200/29980                 (64%)]\tLoss: 0.416898\n",
      "Training stage for Flod 0 Epoch: 33 [22400/29980                 (75%)]\tLoss: 0.313884\n",
      "Training stage for Flod 0 Epoch: 33 [25600/29980                 (85%)]\tLoss: 0.382890\n",
      "Training stage for Flod 0 Epoch: 33 [28800/29980                 (96%)]\tLoss: 0.407212\n",
      "Test set for fold0: Average Loss:           750148.0718, Accuracy: 6602/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 34 [0/29980                 (0%)]\tLoss: 0.323607\n",
      "Training stage for Flod 0 Epoch: 34 [3200/29980                 (11%)]\tLoss: 0.439424\n",
      "Training stage for Flod 0 Epoch: 34 [6400/29980                 (21%)]\tLoss: 0.438241\n",
      "Training stage for Flod 0 Epoch: 34 [9600/29980                 (32%)]\tLoss: 0.338371\n",
      "Training stage for Flod 0 Epoch: 34 [12800/29980                 (43%)]\tLoss: 0.315375\n",
      "Training stage for Flod 0 Epoch: 34 [16000/29980                 (53%)]\tLoss: 0.403595\n",
      "Training stage for Flod 0 Epoch: 34 [19200/29980                 (64%)]\tLoss: 0.407570\n",
      "Training stage for Flod 0 Epoch: 34 [22400/29980                 (75%)]\tLoss: 0.313512\n",
      "Training stage for Flod 0 Epoch: 34 [25600/29980                 (85%)]\tLoss: 0.407667\n",
      "Training stage for Flod 0 Epoch: 34 [28800/29980                 (96%)]\tLoss: 0.406958\n",
      "Test set for fold0: Average Loss:           733718.4134, Accuracy: 6708/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 35 [0/29980                 (0%)]\tLoss: 0.439164\n",
      "Training stage for Flod 0 Epoch: 35 [3200/29980                 (11%)]\tLoss: 0.375766\n",
      "Training stage for Flod 0 Epoch: 35 [6400/29980                 (21%)]\tLoss: 0.377717\n",
      "Training stage for Flod 0 Epoch: 35 [9600/29980                 (32%)]\tLoss: 0.399945\n",
      "Training stage for Flod 0 Epoch: 35 [12800/29980                 (43%)]\tLoss: 0.346004\n",
      "Training stage for Flod 0 Epoch: 35 [16000/29980                 (53%)]\tLoss: 0.376775\n",
      "Training stage for Flod 0 Epoch: 35 [19200/29980                 (64%)]\tLoss: 0.439677\n",
      "Training stage for Flod 0 Epoch: 35 [22400/29980                 (75%)]\tLoss: 0.411734\n",
      "Training stage for Flod 0 Epoch: 35 [25600/29980                 (85%)]\tLoss: 0.344980\n",
      "Training stage for Flod 0 Epoch: 35 [28800/29980                 (96%)]\tLoss: 0.376294\n",
      "Test set for fold0: Average Loss:           728202.5425, Accuracy: 6716/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 36 [0/29980                 (0%)]\tLoss: 0.452958\n",
      "Training stage for Flod 0 Epoch: 36 [3200/29980                 (11%)]\tLoss: 0.527112\n",
      "Training stage for Flod 0 Epoch: 36 [6400/29980                 (21%)]\tLoss: 0.391212\n",
      "Training stage for Flod 0 Epoch: 36 [9600/29980                 (32%)]\tLoss: 0.347936\n",
      "Training stage for Flod 0 Epoch: 36 [12800/29980                 (43%)]\tLoss: 0.385621\n",
      "Training stage for Flod 0 Epoch: 36 [16000/29980                 (53%)]\tLoss: 0.352742\n",
      "Training stage for Flod 0 Epoch: 36 [19200/29980                 (64%)]\tLoss: 0.349013\n",
      "Training stage for Flod 0 Epoch: 36 [22400/29980                 (75%)]\tLoss: 0.313428\n",
      "Training stage for Flod 0 Epoch: 36 [25600/29980                 (85%)]\tLoss: 0.406570\n",
      "Training stage for Flod 0 Epoch: 36 [28800/29980                 (96%)]\tLoss: 0.483560\n",
      "Test set for fold0: Average Loss:           767194.6894, Accuracy: 6553/7496           (87%)\n",
      "Training stage for Flod 0 Epoch: 37 [0/29980                 (0%)]\tLoss: 0.368054\n",
      "Training stage for Flod 0 Epoch: 37 [3200/29980                 (11%)]\tLoss: 0.435412\n",
      "Training stage for Flod 0 Epoch: 37 [6400/29980                 (21%)]\tLoss: 0.387388\n",
      "Training stage for Flod 0 Epoch: 37 [9600/29980                 (32%)]\tLoss: 0.375848\n",
      "Training stage for Flod 0 Epoch: 37 [12800/29980                 (43%)]\tLoss: 0.344720\n",
      "Training stage for Flod 0 Epoch: 37 [16000/29980                 (53%)]\tLoss: 0.344550\n",
      "Training stage for Flod 0 Epoch: 37 [19200/29980                 (64%)]\tLoss: 0.438304\n",
      "Training stage for Flod 0 Epoch: 37 [22400/29980                 (75%)]\tLoss: 0.359678\n",
      "Training stage for Flod 0 Epoch: 37 [25600/29980                 (85%)]\tLoss: 0.407221\n",
      "Training stage for Flod 0 Epoch: 37 [28800/29980                 (96%)]\tLoss: 0.319986\n",
      "Test set for fold0: Average Loss:           728986.1092, Accuracy: 6699/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 38 [0/29980                 (0%)]\tLoss: 0.314367\n",
      "Training stage for Flod 0 Epoch: 38 [3200/29980                 (11%)]\tLoss: 0.339753\n",
      "Training stage for Flod 0 Epoch: 38 [6400/29980                 (21%)]\tLoss: 0.375867\n",
      "Training stage for Flod 0 Epoch: 38 [9600/29980                 (32%)]\tLoss: 0.314882\n",
      "Training stage for Flod 0 Epoch: 38 [12800/29980                 (43%)]\tLoss: 0.344599\n",
      "Training stage for Flod 0 Epoch: 38 [16000/29980                 (53%)]\tLoss: 0.346086\n",
      "Training stage for Flod 0 Epoch: 38 [19200/29980                 (64%)]\tLoss: 0.447759\n",
      "Training stage for Flod 0 Epoch: 38 [22400/29980                 (75%)]\tLoss: 0.344633\n",
      "Training stage for Flod 0 Epoch: 38 [25600/29980                 (85%)]\tLoss: 0.343613\n",
      "Training stage for Flod 0 Epoch: 38 [28800/29980                 (96%)]\tLoss: 0.337779\n",
      "Test set for fold0: Average Loss:           731775.2086, Accuracy: 6679/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 39 [0/29980                 (0%)]\tLoss: 0.374495\n",
      "Training stage for Flod 0 Epoch: 39 [3200/29980                 (11%)]\tLoss: 0.344545\n",
      "Training stage for Flod 0 Epoch: 39 [6400/29980                 (21%)]\tLoss: 0.407081\n",
      "Training stage for Flod 0 Epoch: 39 [9600/29980                 (32%)]\tLoss: 0.354470\n",
      "Training stage for Flod 0 Epoch: 39 [12800/29980                 (43%)]\tLoss: 0.376292\n",
      "Training stage for Flod 0 Epoch: 39 [16000/29980                 (53%)]\tLoss: 0.400801\n",
      "Training stage for Flod 0 Epoch: 39 [19200/29980                 (64%)]\tLoss: 0.319483\n",
      "Training stage for Flod 0 Epoch: 39 [22400/29980                 (75%)]\tLoss: 0.375860\n",
      "Training stage for Flod 0 Epoch: 39 [25600/29980                 (85%)]\tLoss: 0.375796\n",
      "Training stage for Flod 0 Epoch: 39 [28800/29980                 (96%)]\tLoss: 0.392947\n",
      "Test set for fold0: Average Loss:           735528.5956, Accuracy: 6675/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 40 [0/29980                 (0%)]\tLoss: 0.377569\n",
      "Training stage for Flod 0 Epoch: 40 [3200/29980                 (11%)]\tLoss: 0.344563\n",
      "Training stage for Flod 0 Epoch: 40 [6400/29980                 (21%)]\tLoss: 0.445764\n",
      "Training stage for Flod 0 Epoch: 40 [9600/29980                 (32%)]\tLoss: 0.344693\n",
      "Training stage for Flod 0 Epoch: 40 [12800/29980                 (43%)]\tLoss: 0.396455\n",
      "Training stage for Flod 0 Epoch: 40 [16000/29980                 (53%)]\tLoss: 0.406693\n",
      "Training stage for Flod 0 Epoch: 40 [19200/29980                 (64%)]\tLoss: 0.325722\n",
      "Training stage for Flod 0 Epoch: 40 [22400/29980                 (75%)]\tLoss: 0.375785\n",
      "Training stage for Flod 0 Epoch: 40 [25600/29980                 (85%)]\tLoss: 0.464875\n",
      "Training stage for Flod 0 Epoch: 40 [28800/29980                 (96%)]\tLoss: 0.375888\n",
      "Test set for fold0: Average Loss:           747419.6333, Accuracy: 6624/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 41 [0/29980                 (0%)]\tLoss: 0.382145\n",
      "Training stage for Flod 0 Epoch: 41 [3200/29980                 (11%)]\tLoss: 0.344517\n",
      "Training stage for Flod 0 Epoch: 41 [6400/29980                 (21%)]\tLoss: 0.542247\n",
      "Training stage for Flod 0 Epoch: 41 [9600/29980                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 41 [12800/29980                 (43%)]\tLoss: 0.375876\n",
      "Training stage for Flod 0 Epoch: 41 [16000/29980                 (53%)]\tLoss: 0.437996\n",
      "Training stage for Flod 0 Epoch: 41 [19200/29980                 (64%)]\tLoss: 0.379139\n",
      "Training stage for Flod 0 Epoch: 41 [22400/29980                 (75%)]\tLoss: 0.432235\n",
      "Training stage for Flod 0 Epoch: 41 [25600/29980                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 41 [28800/29980                 (96%)]\tLoss: 0.325878\n",
      "Test set for fold0: Average Loss:           741245.5540, Accuracy: 6664/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 42 [0/29980                 (0%)]\tLoss: 0.440581\n",
      "Training stage for Flod 0 Epoch: 42 [3200/29980                 (11%)]\tLoss: 0.375760\n",
      "Training stage for Flod 0 Epoch: 42 [6400/29980                 (21%)]\tLoss: 0.498961\n",
      "Training stage for Flod 0 Epoch: 42 [9600/29980                 (32%)]\tLoss: 0.344722\n",
      "Training stage for Flod 0 Epoch: 42 [12800/29980                 (43%)]\tLoss: 0.372657\n",
      "Training stage for Flod 0 Epoch: 42 [16000/29980                 (53%)]\tLoss: 0.313280\n",
      "Training stage for Flod 0 Epoch: 42 [19200/29980                 (64%)]\tLoss: 0.433745\n",
      "Training stage for Flod 0 Epoch: 42 [22400/29980                 (75%)]\tLoss: 0.326597\n",
      "Training stage for Flod 0 Epoch: 42 [25600/29980                 (85%)]\tLoss: 0.387995\n",
      "Training stage for Flod 0 Epoch: 42 [28800/29980                 (96%)]\tLoss: 0.378572\n",
      "Test set for fold0: Average Loss:           724019.2048, Accuracy: 6745/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 43 [0/29980                 (0%)]\tLoss: 0.313282\n",
      "Training stage for Flod 0 Epoch: 43 [3200/29980                 (11%)]\tLoss: 0.317973\n",
      "Training stage for Flod 0 Epoch: 43 [6400/29980                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 43 [9600/29980                 (32%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 43 [12800/29980                 (43%)]\tLoss: 0.337754\n",
      "Training stage for Flod 0 Epoch: 43 [16000/29980                 (53%)]\tLoss: 0.345005\n",
      "Training stage for Flod 0 Epoch: 43 [19200/29980                 (64%)]\tLoss: 0.438263\n",
      "Training stage for Flod 0 Epoch: 43 [22400/29980                 (75%)]\tLoss: 0.469487\n",
      "Training stage for Flod 0 Epoch: 43 [25600/29980                 (85%)]\tLoss: 0.407130\n",
      "Training stage for Flod 0 Epoch: 43 [28800/29980                 (96%)]\tLoss: 0.374399\n",
      "Test set for fold0: Average Loss:           758279.5771, Accuracy: 6573/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 44 [0/29980                 (0%)]\tLoss: 0.438185\n",
      "Training stage for Flod 0 Epoch: 44 [3200/29980                 (11%)]\tLoss: 0.375814\n",
      "Training stage for Flod 0 Epoch: 44 [6400/29980                 (21%)]\tLoss: 0.377337\n",
      "Training stage for Flod 0 Epoch: 44 [9600/29980                 (32%)]\tLoss: 0.410959\n",
      "Training stage for Flod 0 Epoch: 44 [12800/29980                 (43%)]\tLoss: 0.376015\n",
      "Training stage for Flod 0 Epoch: 44 [16000/29980                 (53%)]\tLoss: 0.375597\n",
      "Training stage for Flod 0 Epoch: 44 [19200/29980                 (64%)]\tLoss: 0.388241\n",
      "Training stage for Flod 0 Epoch: 44 [22400/29980                 (75%)]\tLoss: 0.438260\n",
      "Training stage for Flod 0 Epoch: 44 [25600/29980                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 44 [28800/29980                 (96%)]\tLoss: 0.406957\n",
      "Test set for fold0: Average Loss:           725732.6275, Accuracy: 6726/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 45 [0/29980                 (0%)]\tLoss: 0.498061\n",
      "Training stage for Flod 0 Epoch: 45 [3200/29980                 (11%)]\tLoss: 0.375497\n",
      "Training stage for Flod 0 Epoch: 45 [6400/29980                 (21%)]\tLoss: 0.440778\n",
      "Training stage for Flod 0 Epoch: 45 [9600/29980                 (32%)]\tLoss: 0.317845\n",
      "Training stage for Flod 0 Epoch: 45 [12800/29980                 (43%)]\tLoss: 0.320982\n",
      "Training stage for Flod 0 Epoch: 45 [16000/29980                 (53%)]\tLoss: 0.407373\n",
      "Training stage for Flod 0 Epoch: 45 [19200/29980                 (64%)]\tLoss: 0.374415\n",
      "Training stage for Flod 0 Epoch: 45 [22400/29980                 (75%)]\tLoss: 0.328668\n",
      "Training stage for Flod 0 Epoch: 45 [25600/29980                 (85%)]\tLoss: 0.349295\n",
      "Training stage for Flod 0 Epoch: 45 [28800/29980                 (96%)]\tLoss: 0.404385\n",
      "Test set for fold0: Average Loss:           736022.8733, Accuracy: 6666/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 46 [0/29980                 (0%)]\tLoss: 0.345099\n",
      "Training stage for Flod 0 Epoch: 46 [3200/29980                 (11%)]\tLoss: 0.344700\n",
      "Training stage for Flod 0 Epoch: 46 [6400/29980                 (21%)]\tLoss: 0.385339\n",
      "Training stage for Flod 0 Epoch: 46 [9600/29980                 (32%)]\tLoss: 0.371931\n",
      "Training stage for Flod 0 Epoch: 46 [12800/29980                 (43%)]\tLoss: 0.480965\n",
      "Training stage for Flod 0 Epoch: 46 [16000/29980                 (53%)]\tLoss: 0.313290\n",
      "Training stage for Flod 0 Epoch: 46 [19200/29980                 (64%)]\tLoss: 0.344519\n",
      "Training stage for Flod 0 Epoch: 46 [22400/29980                 (75%)]\tLoss: 0.344780\n",
      "Training stage for Flod 0 Epoch: 46 [25600/29980                 (85%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 46 [28800/29980                 (96%)]\tLoss: 0.375886\n",
      "Test set for fold0: Average Loss:           753627.7482, Accuracy: 6600/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 47 [0/29980                 (0%)]\tLoss: 0.413625\n",
      "Training stage for Flod 0 Epoch: 47 [3200/29980                 (11%)]\tLoss: 0.375954\n",
      "Training stage for Flod 0 Epoch: 47 [6400/29980                 (21%)]\tLoss: 0.313760\n",
      "Training stage for Flod 0 Epoch: 47 [9600/29980                 (32%)]\tLoss: 0.376303\n",
      "Training stage for Flod 0 Epoch: 47 [12800/29980                 (43%)]\tLoss: 0.344753\n",
      "Training stage for Flod 0 Epoch: 47 [16000/29980                 (53%)]\tLoss: 0.313883\n",
      "Training stage for Flod 0 Epoch: 47 [19200/29980                 (64%)]\tLoss: 0.390164\n",
      "Training stage for Flod 0 Epoch: 47 [22400/29980                 (75%)]\tLoss: 0.344783\n",
      "Training stage for Flod 0 Epoch: 47 [25600/29980                 (85%)]\tLoss: 0.361596\n",
      "Training stage for Flod 0 Epoch: 47 [28800/29980                 (96%)]\tLoss: 0.432305\n",
      "Test set for fold0: Average Loss:           730692.9120, Accuracy: 6716/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 48 [0/29980                 (0%)]\tLoss: 0.375994\n",
      "Training stage for Flod 0 Epoch: 48 [3200/29980                 (11%)]\tLoss: 0.443849\n",
      "Training stage for Flod 0 Epoch: 48 [6400/29980                 (21%)]\tLoss: 0.390896\n",
      "Training stage for Flod 0 Epoch: 48 [9600/29980                 (32%)]\tLoss: 0.344751\n",
      "Training stage for Flod 0 Epoch: 48 [12800/29980                 (43%)]\tLoss: 0.366100\n",
      "Training stage for Flod 0 Epoch: 48 [16000/29980                 (53%)]\tLoss: 0.315974\n",
      "Training stage for Flod 0 Epoch: 48 [19200/29980                 (64%)]\tLoss: 0.375560\n",
      "Training stage for Flod 0 Epoch: 48 [22400/29980                 (75%)]\tLoss: 0.375929\n",
      "Training stage for Flod 0 Epoch: 48 [25600/29980                 (85%)]\tLoss: 0.375778\n",
      "Training stage for Flod 0 Epoch: 48 [28800/29980                 (96%)]\tLoss: 0.313278\n",
      "Test set for fold0: Average Loss:           730155.5788, Accuracy: 6704/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 49 [0/29980                 (0%)]\tLoss: 0.313308\n",
      "Training stage for Flod 0 Epoch: 49 [3200/29980                 (11%)]\tLoss: 0.314273\n",
      "Training stage for Flod 0 Epoch: 49 [6400/29980                 (21%)]\tLoss: 0.326585\n",
      "Training stage for Flod 0 Epoch: 49 [9600/29980                 (32%)]\tLoss: 0.406758\n",
      "Training stage for Flod 0 Epoch: 49 [12800/29980                 (43%)]\tLoss: 0.438306\n",
      "Training stage for Flod 0 Epoch: 49 [16000/29980                 (53%)]\tLoss: 0.375726\n",
      "Training stage for Flod 0 Epoch: 49 [19200/29980                 (64%)]\tLoss: 0.383989\n",
      "Training stage for Flod 0 Epoch: 49 [22400/29980                 (75%)]\tLoss: 0.357742\n",
      "Training stage for Flod 0 Epoch: 49 [25600/29980                 (85%)]\tLoss: 0.377565\n",
      "Training stage for Flod 0 Epoch: 49 [28800/29980                 (96%)]\tLoss: 0.380021\n",
      "Test set for fold0: Average Loss:           741153.9862, Accuracy: 6656/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 50 [0/29980                 (0%)]\tLoss: 0.345208\n",
      "Training stage for Flod 0 Epoch: 50 [3200/29980                 (11%)]\tLoss: 0.438395\n",
      "Training stage for Flod 0 Epoch: 50 [6400/29980                 (21%)]\tLoss: 0.319922\n",
      "Training stage for Flod 0 Epoch: 50 [9600/29980                 (32%)]\tLoss: 0.347108\n",
      "Training stage for Flod 0 Epoch: 50 [12800/29980                 (43%)]\tLoss: 0.344954\n",
      "Training stage for Flod 0 Epoch: 50 [16000/29980                 (53%)]\tLoss: 0.390343\n",
      "Training stage for Flod 0 Epoch: 50 [19200/29980                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 50 [22400/29980                 (75%)]\tLoss: 0.441325\n",
      "Training stage for Flod 0 Epoch: 50 [25600/29980                 (85%)]\tLoss: 0.313825\n",
      "Training stage for Flod 0 Epoch: 50 [28800/29980                 (96%)]\tLoss: 0.374415\n",
      "Test set for fold0: Average Loss:           737711.4352, Accuracy: 6680/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 51 [0/29980                 (0%)]\tLoss: 0.407135\n",
      "Training stage for Flod 0 Epoch: 51 [3200/29980                 (11%)]\tLoss: 0.313343\n",
      "Training stage for Flod 0 Epoch: 51 [6400/29980                 (21%)]\tLoss: 0.344888\n",
      "Training stage for Flod 0 Epoch: 51 [9600/29980                 (32%)]\tLoss: 0.313299\n",
      "Training stage for Flod 0 Epoch: 51 [12800/29980                 (43%)]\tLoss: 0.374990\n",
      "Training stage for Flod 0 Epoch: 51 [16000/29980                 (53%)]\tLoss: 0.375764\n",
      "Training stage for Flod 0 Epoch: 51 [19200/29980                 (64%)]\tLoss: 0.344134\n",
      "Training stage for Flod 0 Epoch: 51 [22400/29980                 (75%)]\tLoss: 0.388778\n",
      "Training stage for Flod 0 Epoch: 51 [25600/29980                 (85%)]\tLoss: 0.375968\n",
      "Training stage for Flod 0 Epoch: 51 [28800/29980                 (96%)]\tLoss: 0.344259\n",
      "Test set for fold0: Average Loss:           729995.1592, Accuracy: 6707/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 52 [0/29980                 (0%)]\tLoss: 0.376557\n",
      "Training stage for Flod 0 Epoch: 52 [3200/29980                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 52 [6400/29980                 (21%)]\tLoss: 0.375848\n",
      "Training stage for Flod 0 Epoch: 52 [9600/29980                 (32%)]\tLoss: 0.354226\n",
      "Training stage for Flod 0 Epoch: 52 [12800/29980                 (43%)]\tLoss: 0.313322\n",
      "Training stage for Flod 0 Epoch: 52 [16000/29980                 (53%)]\tLoss: 0.313303\n",
      "Training stage for Flod 0 Epoch: 52 [19200/29980                 (64%)]\tLoss: 0.335360\n",
      "Training stage for Flod 0 Epoch: 52 [22400/29980                 (75%)]\tLoss: 0.363869\n",
      "Training stage for Flod 0 Epoch: 52 [25600/29980                 (85%)]\tLoss: 0.408185\n",
      "Training stage for Flod 0 Epoch: 52 [28800/29980                 (96%)]\tLoss: 0.344817\n",
      "Test set for fold0: Average Loss:           740062.9388, Accuracy: 6637/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 53 [0/29980                 (0%)]\tLoss: 0.313684\n",
      "Training stage for Flod 0 Epoch: 53 [3200/29980                 (11%)]\tLoss: 0.434725\n",
      "Training stage for Flod 0 Epoch: 53 [6400/29980                 (21%)]\tLoss: 0.444380\n",
      "Training stage for Flod 0 Epoch: 53 [9600/29980                 (32%)]\tLoss: 0.375787\n",
      "Training stage for Flod 0 Epoch: 53 [12800/29980                 (43%)]\tLoss: 0.344514\n",
      "Training stage for Flod 0 Epoch: 53 [16000/29980                 (53%)]\tLoss: 0.379512\n",
      "Training stage for Flod 0 Epoch: 53 [19200/29980                 (64%)]\tLoss: 0.345083\n",
      "Training stage for Flod 0 Epoch: 53 [22400/29980                 (75%)]\tLoss: 0.313587\n",
      "Training stage for Flod 0 Epoch: 53 [25600/29980                 (85%)]\tLoss: 0.375481\n",
      "Training stage for Flod 0 Epoch: 53 [28800/29980                 (96%)]\tLoss: 0.344518\n",
      "Test set for fold0: Average Loss:           737519.7944, Accuracy: 6667/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 54 [0/29980                 (0%)]\tLoss: 0.376262\n",
      "Training stage for Flod 0 Epoch: 54 [3200/29980                 (11%)]\tLoss: 0.345283\n",
      "Training stage for Flod 0 Epoch: 54 [6400/29980                 (21%)]\tLoss: 0.348615\n",
      "Training stage for Flod 0 Epoch: 54 [9600/29980                 (32%)]\tLoss: 0.344514\n",
      "Training stage for Flod 0 Epoch: 54 [12800/29980                 (43%)]\tLoss: 0.313905\n",
      "Training stage for Flod 0 Epoch: 54 [16000/29980                 (53%)]\tLoss: 0.313396\n",
      "Training stage for Flod 0 Epoch: 54 [19200/29980                 (64%)]\tLoss: 0.345022\n",
      "Training stage for Flod 0 Epoch: 54 [22400/29980                 (75%)]\tLoss: 0.328540\n",
      "Training stage for Flod 0 Epoch: 54 [25600/29980                 (85%)]\tLoss: 0.381607\n",
      "Training stage for Flod 0 Epoch: 54 [28800/29980                 (96%)]\tLoss: 0.345722\n",
      "Test set for fold0: Average Loss:           736525.4313, Accuracy: 6674/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 55 [0/29980                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 55 [3200/29980                 (11%)]\tLoss: 0.407098\n",
      "Training stage for Flod 0 Epoch: 55 [6400/29980                 (21%)]\tLoss: 0.344915\n",
      "Training stage for Flod 0 Epoch: 55 [9600/29980                 (32%)]\tLoss: 0.439864\n",
      "Training stage for Flod 0 Epoch: 55 [12800/29980                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 55 [16000/29980                 (53%)]\tLoss: 0.473258\n",
      "Training stage for Flod 0 Epoch: 55 [19200/29980                 (64%)]\tLoss: 0.407150\n",
      "Training stage for Flod 0 Epoch: 55 [22400/29980                 (75%)]\tLoss: 0.342273\n",
      "Training stage for Flod 0 Epoch: 55 [25600/29980                 (85%)]\tLoss: 0.367527\n",
      "Training stage for Flod 0 Epoch: 55 [28800/29980                 (96%)]\tLoss: 0.344517\n",
      "Test set for fold0: Average Loss:           740586.5480, Accuracy: 6678/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 56 [0/29980                 (0%)]\tLoss: 0.344642\n",
      "Training stage for Flod 0 Epoch: 56 [3200/29980                 (11%)]\tLoss: 0.408605\n",
      "Training stage for Flod 0 Epoch: 56 [6400/29980                 (21%)]\tLoss: 0.383843\n",
      "Training stage for Flod 0 Epoch: 56 [9600/29980                 (32%)]\tLoss: 0.370210\n",
      "Training stage for Flod 0 Epoch: 56 [12800/29980                 (43%)]\tLoss: 0.375951\n",
      "Training stage for Flod 0 Epoch: 56 [16000/29980                 (53%)]\tLoss: 0.387058\n",
      "Training stage for Flod 0 Epoch: 56 [19200/29980                 (64%)]\tLoss: 0.344481\n",
      "Training stage for Flod 0 Epoch: 56 [22400/29980                 (75%)]\tLoss: 0.313324\n",
      "Training stage for Flod 0 Epoch: 56 [25600/29980                 (85%)]\tLoss: 0.407125\n",
      "Training stage for Flod 0 Epoch: 56 [28800/29980                 (96%)]\tLoss: 0.406712\n",
      "Test set for fold0: Average Loss:           721756.4785, Accuracy: 6751/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 57 [0/29980                 (0%)]\tLoss: 0.344505\n",
      "Training stage for Flod 0 Epoch: 57 [3200/29980                 (11%)]\tLoss: 0.415467\n",
      "Training stage for Flod 0 Epoch: 57 [6400/29980                 (21%)]\tLoss: 0.378726\n",
      "Training stage for Flod 0 Epoch: 57 [9600/29980                 (32%)]\tLoss: 0.313265\n",
      "Training stage for Flod 0 Epoch: 57 [12800/29980                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 57 [16000/29980                 (53%)]\tLoss: 0.375876\n",
      "Training stage for Flod 0 Epoch: 57 [19200/29980                 (64%)]\tLoss: 0.407070\n",
      "Training stage for Flod 0 Epoch: 57 [22400/29980                 (75%)]\tLoss: 0.315226\n",
      "Training stage for Flod 0 Epoch: 57 [25600/29980                 (85%)]\tLoss: 0.360887\n",
      "Training stage for Flod 0 Epoch: 57 [28800/29980                 (96%)]\tLoss: 0.313455\n",
      "Test set for fold0: Average Loss:           738257.9959, Accuracy: 6677/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 58 [0/29980                 (0%)]\tLoss: 0.346182\n",
      "Training stage for Flod 0 Epoch: 58 [3200/29980                 (11%)]\tLoss: 0.419842\n",
      "Training stage for Flod 0 Epoch: 58 [6400/29980                 (21%)]\tLoss: 0.374293\n",
      "Training stage for Flod 0 Epoch: 58 [9600/29980                 (32%)]\tLoss: 0.438398\n",
      "Training stage for Flod 0 Epoch: 58 [12800/29980                 (43%)]\tLoss: 0.343405\n",
      "Training stage for Flod 0 Epoch: 58 [16000/29980                 (53%)]\tLoss: 0.345029\n",
      "Training stage for Flod 0 Epoch: 58 [19200/29980                 (64%)]\tLoss: 0.378062\n",
      "Training stage for Flod 0 Epoch: 58 [22400/29980                 (75%)]\tLoss: 0.313271\n",
      "Training stage for Flod 0 Epoch: 58 [25600/29980                 (85%)]\tLoss: 0.408398\n",
      "Training stage for Flod 0 Epoch: 58 [28800/29980                 (96%)]\tLoss: 0.376687\n",
      "Test set for fold0: Average Loss:           749743.4275, Accuracy: 6610/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 59 [0/29980                 (0%)]\tLoss: 0.524025\n",
      "Training stage for Flod 0 Epoch: 59 [3200/29980                 (11%)]\tLoss: 0.449891\n",
      "Training stage for Flod 0 Epoch: 59 [6400/29980                 (21%)]\tLoss: 0.344565\n",
      "Training stage for Flod 0 Epoch: 59 [9600/29980                 (32%)]\tLoss: 0.438590\n",
      "Training stage for Flod 0 Epoch: 59 [12800/29980                 (43%)]\tLoss: 0.375709\n",
      "Training stage for Flod 0 Epoch: 59 [16000/29980                 (53%)]\tLoss: 0.344658\n",
      "Training stage for Flod 0 Epoch: 59 [19200/29980                 (64%)]\tLoss: 0.421088\n",
      "Training stage for Flod 0 Epoch: 59 [22400/29980                 (75%)]\tLoss: 0.406897\n",
      "Training stage for Flod 0 Epoch: 59 [25600/29980                 (85%)]\tLoss: 0.344583\n",
      "Training stage for Flod 0 Epoch: 59 [28800/29980                 (96%)]\tLoss: 0.394827\n",
      "Test set for fold0: Average Loss:           728006.9696, Accuracy: 6731/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 60 [0/29980                 (0%)]\tLoss: 0.389126\n",
      "Training stage for Flod 0 Epoch: 60 [3200/29980                 (11%)]\tLoss: 0.402748\n",
      "Training stage for Flod 0 Epoch: 60 [6400/29980                 (21%)]\tLoss: 0.333939\n",
      "Training stage for Flod 0 Epoch: 60 [9600/29980                 (32%)]\tLoss: 0.410482\n",
      "Training stage for Flod 0 Epoch: 60 [12800/29980                 (43%)]\tLoss: 0.356226\n",
      "Training stage for Flod 0 Epoch: 60 [16000/29980                 (53%)]\tLoss: 0.379496\n",
      "Training stage for Flod 0 Epoch: 60 [19200/29980                 (64%)]\tLoss: 0.375755\n",
      "Training stage for Flod 0 Epoch: 60 [22400/29980                 (75%)]\tLoss: 0.409857\n",
      "Training stage for Flod 0 Epoch: 60 [25600/29980                 (85%)]\tLoss: 0.369947\n",
      "Training stage for Flod 0 Epoch: 60 [28800/29980                 (96%)]\tLoss: 0.356258\n",
      "Test set for fold0: Average Loss:           755914.4243, Accuracy: 6596/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 61 [0/29980                 (0%)]\tLoss: 0.499876\n",
      "Training stage for Flod 0 Epoch: 61 [3200/29980                 (11%)]\tLoss: 0.409613\n",
      "Training stage for Flod 0 Epoch: 61 [6400/29980                 (21%)]\tLoss: 0.438255\n",
      "Training stage for Flod 0 Epoch: 61 [9600/29980                 (32%)]\tLoss: 0.336070\n",
      "Training stage for Flod 0 Epoch: 61 [12800/29980                 (43%)]\tLoss: 0.437875\n",
      "Training stage for Flod 0 Epoch: 61 [16000/29980                 (53%)]\tLoss: 0.385435\n",
      "Training stage for Flod 0 Epoch: 61 [19200/29980                 (64%)]\tLoss: 0.406200\n",
      "Training stage for Flod 0 Epoch: 61 [22400/29980                 (75%)]\tLoss: 0.435854\n",
      "Training stage for Flod 0 Epoch: 61 [25600/29980                 (85%)]\tLoss: 0.405341\n",
      "Training stage for Flod 0 Epoch: 61 [28800/29980                 (96%)]\tLoss: 0.344529\n",
      "Test set for fold0: Average Loss:           767977.4053, Accuracy: 6560/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 62 [0/29980                 (0%)]\tLoss: 0.410879\n",
      "Training stage for Flod 0 Epoch: 62 [3200/29980                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 62 [6400/29980                 (21%)]\tLoss: 0.313864\n",
      "Training stage for Flod 0 Epoch: 62 [9600/29980                 (32%)]\tLoss: 0.346298\n",
      "Training stage for Flod 0 Epoch: 62 [12800/29980                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 62 [16000/29980                 (53%)]\tLoss: 0.356406\n",
      "Training stage for Flod 0 Epoch: 62 [19200/29980                 (64%)]\tLoss: 0.380546\n",
      "Training stage for Flod 0 Epoch: 62 [22400/29980                 (75%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 62 [25600/29980                 (85%)]\tLoss: 0.375875\n",
      "Training stage for Flod 0 Epoch: 62 [28800/29980                 (96%)]\tLoss: 0.361360\n",
      "Test set for fold0: Average Loss:           708876.3232, Accuracy: 6801/7496           (91%)\n",
      "Training stage for Flod 0 Epoch: 63 [0/29980                 (0%)]\tLoss: 0.391713\n",
      "Training stage for Flod 0 Epoch: 63 [3200/29980                 (11%)]\tLoss: 0.404200\n",
      "Training stage for Flod 0 Epoch: 63 [6400/29980                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 63 [9600/29980                 (32%)]\tLoss: 0.344640\n",
      "Training stage for Flod 0 Epoch: 63 [12800/29980                 (43%)]\tLoss: 0.352496\n",
      "Training stage for Flod 0 Epoch: 63 [16000/29980                 (53%)]\tLoss: 0.374448\n",
      "Training stage for Flod 0 Epoch: 63 [19200/29980                 (64%)]\tLoss: 0.464969\n",
      "Training stage for Flod 0 Epoch: 63 [22400/29980                 (75%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 63 [25600/29980                 (85%)]\tLoss: 0.376863\n",
      "Training stage for Flod 0 Epoch: 63 [28800/29980                 (96%)]\tLoss: 0.344531\n",
      "Test set for fold0: Average Loss:           723044.0049, Accuracy: 6754/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 64 [0/29980                 (0%)]\tLoss: 0.470449\n",
      "Training stage for Flod 0 Epoch: 64 [3200/29980                 (11%)]\tLoss: 0.407123\n",
      "Training stage for Flod 0 Epoch: 64 [6400/29980                 (21%)]\tLoss: 0.344504\n",
      "Training stage for Flod 0 Epoch: 64 [9600/29980                 (32%)]\tLoss: 0.465351\n",
      "Training stage for Flod 0 Epoch: 64 [12800/29980                 (43%)]\tLoss: 0.374883\n",
      "Training stage for Flod 0 Epoch: 64 [16000/29980                 (53%)]\tLoss: 0.407013\n",
      "Training stage for Flod 0 Epoch: 64 [19200/29980                 (64%)]\tLoss: 0.375764\n",
      "Training stage for Flod 0 Epoch: 64 [22400/29980                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 64 [25600/29980                 (85%)]\tLoss: 0.407020\n",
      "Training stage for Flod 0 Epoch: 64 [28800/29980                 (96%)]\tLoss: 0.375762\n",
      "Test set for fold0: Average Loss:           725149.7874, Accuracy: 6724/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 65 [0/29980                 (0%)]\tLoss: 0.344537\n",
      "Training stage for Flod 0 Epoch: 65 [3200/29980                 (11%)]\tLoss: 0.375713\n",
      "Training stage for Flod 0 Epoch: 65 [6400/29980                 (21%)]\tLoss: 0.407034\n",
      "Training stage for Flod 0 Epoch: 65 [9600/29980                 (32%)]\tLoss: 0.357883\n",
      "Training stage for Flod 0 Epoch: 65 [12800/29980                 (43%)]\tLoss: 0.378087\n",
      "Training stage for Flod 0 Epoch: 65 [16000/29980                 (53%)]\tLoss: 0.499010\n",
      "Training stage for Flod 0 Epoch: 65 [19200/29980                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 65 [22400/29980                 (75%)]\tLoss: 0.362773\n",
      "Training stage for Flod 0 Epoch: 65 [25600/29980                 (85%)]\tLoss: 0.313357\n",
      "Training stage for Flod 0 Epoch: 65 [28800/29980                 (96%)]\tLoss: 0.344685\n",
      "Test set for fold0: Average Loss:           731139.5873, Accuracy: 6713/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 66 [0/29980                 (0%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 66 [3200/29980                 (11%)]\tLoss: 0.375846\n",
      "Training stage for Flod 0 Epoch: 66 [6400/29980                 (21%)]\tLoss: 0.321483\n",
      "Training stage for Flod 0 Epoch: 66 [9600/29980                 (32%)]\tLoss: 0.407107\n",
      "Training stage for Flod 0 Epoch: 66 [12800/29980                 (43%)]\tLoss: 0.407023\n",
      "Training stage for Flod 0 Epoch: 66 [16000/29980                 (53%)]\tLoss: 0.345050\n",
      "Training stage for Flod 0 Epoch: 66 [19200/29980                 (64%)]\tLoss: 0.394175\n",
      "Training stage for Flod 0 Epoch: 66 [22400/29980                 (75%)]\tLoss: 0.437706\n",
      "Training stage for Flod 0 Epoch: 66 [25600/29980                 (85%)]\tLoss: 0.407359\n",
      "Training stage for Flod 0 Epoch: 66 [28800/29980                 (96%)]\tLoss: 0.476962\n",
      "Test set for fold0: Average Loss:           730848.6683, Accuracy: 6705/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 67 [0/29980                 (0%)]\tLoss: 0.407235\n",
      "Training stage for Flod 0 Epoch: 67 [3200/29980                 (11%)]\tLoss: 0.353424\n",
      "Training stage for Flod 0 Epoch: 67 [6400/29980                 (21%)]\tLoss: 0.408010\n",
      "Training stage for Flod 0 Epoch: 67 [9600/29980                 (32%)]\tLoss: 0.406858\n",
      "Training stage for Flod 0 Epoch: 67 [12800/29980                 (43%)]\tLoss: 0.375505\n",
      "Training stage for Flod 0 Epoch: 67 [16000/29980                 (53%)]\tLoss: 0.313578\n",
      "Training stage for Flod 0 Epoch: 67 [19200/29980                 (64%)]\tLoss: 0.313350\n",
      "Training stage for Flod 0 Epoch: 67 [22400/29980                 (75%)]\tLoss: 0.376540\n",
      "Training stage for Flod 0 Epoch: 67 [25600/29980                 (85%)]\tLoss: 0.348817\n",
      "Training stage for Flod 0 Epoch: 67 [28800/29980                 (96%)]\tLoss: 0.401759\n",
      "Test set for fold0: Average Loss:           749026.9644, Accuracy: 6627/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 68 [0/29980                 (0%)]\tLoss: 0.415658\n",
      "Training stage for Flod 0 Epoch: 68 [3200/29980                 (11%)]\tLoss: 0.367733\n",
      "Training stage for Flod 0 Epoch: 68 [6400/29980                 (21%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 68 [9600/29980                 (32%)]\tLoss: 0.383136\n",
      "Training stage for Flod 0 Epoch: 68 [12800/29980                 (43%)]\tLoss: 0.344517\n",
      "Training stage for Flod 0 Epoch: 68 [16000/29980                 (53%)]\tLoss: 0.366026\n",
      "Training stage for Flod 0 Epoch: 68 [19200/29980                 (64%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 68 [22400/29980                 (75%)]\tLoss: 0.406950\n",
      "Training stage for Flod 0 Epoch: 68 [25600/29980                 (85%)]\tLoss: 0.375733\n",
      "Training stage for Flod 0 Epoch: 68 [28800/29980                 (96%)]\tLoss: 0.324291\n",
      "Test set for fold0: Average Loss:           751836.9743, Accuracy: 6610/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 69 [0/29980                 (0%)]\tLoss: 0.319571\n",
      "Training stage for Flod 0 Epoch: 69 [3200/29980                 (11%)]\tLoss: 0.376876\n",
      "Training stage for Flod 0 Epoch: 69 [6400/29980                 (21%)]\tLoss: 0.375658\n",
      "Training stage for Flod 0 Epoch: 69 [9600/29980                 (32%)]\tLoss: 0.411910\n",
      "Training stage for Flod 0 Epoch: 69 [12800/29980                 (43%)]\tLoss: 0.375783\n",
      "Training stage for Flod 0 Epoch: 69 [16000/29980                 (53%)]\tLoss: 0.347517\n",
      "Training stage for Flod 0 Epoch: 69 [19200/29980                 (64%)]\tLoss: 0.435855\n",
      "Training stage for Flod 0 Epoch: 69 [22400/29980                 (75%)]\tLoss: 0.375703\n",
      "Training stage for Flod 0 Epoch: 69 [25600/29980                 (85%)]\tLoss: 0.344690\n",
      "Training stage for Flod 0 Epoch: 69 [28800/29980                 (96%)]\tLoss: 0.313374\n",
      "Test set for fold0: Average Loss:           743689.8550, Accuracy: 6645/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 70 [0/29980                 (0%)]\tLoss: 0.313284\n",
      "Training stage for Flod 0 Epoch: 70 [3200/29980                 (11%)]\tLoss: 0.375764\n",
      "Training stage for Flod 0 Epoch: 70 [6400/29980                 (21%)]\tLoss: 0.344475\n",
      "Training stage for Flod 0 Epoch: 70 [9600/29980                 (32%)]\tLoss: 0.344812\n",
      "Training stage for Flod 0 Epoch: 70 [12800/29980                 (43%)]\tLoss: 0.327484\n",
      "Training stage for Flod 0 Epoch: 70 [16000/29980                 (53%)]\tLoss: 0.344536\n",
      "Training stage for Flod 0 Epoch: 70 [19200/29980                 (64%)]\tLoss: 0.318175\n",
      "Training stage for Flod 0 Epoch: 70 [22400/29980                 (75%)]\tLoss: 0.313305\n",
      "Training stage for Flod 0 Epoch: 70 [25600/29980                 (85%)]\tLoss: 0.313367\n",
      "Training stage for Flod 0 Epoch: 70 [28800/29980                 (96%)]\tLoss: 0.438429\n",
      "Test set for fold0: Average Loss:           732324.4318, Accuracy: 6694/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 71 [0/29980                 (0%)]\tLoss: 0.426257\n",
      "Training stage for Flod 0 Epoch: 71 [3200/29980                 (11%)]\tLoss: 0.337482\n",
      "Training stage for Flod 0 Epoch: 71 [6400/29980                 (21%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 71 [9600/29980                 (32%)]\tLoss: 0.344869\n",
      "Training stage for Flod 0 Epoch: 71 [12800/29980                 (43%)]\tLoss: 0.408022\n",
      "Training stage for Flod 0 Epoch: 71 [16000/29980                 (53%)]\tLoss: 0.375281\n",
      "Training stage for Flod 0 Epoch: 71 [19200/29980                 (64%)]\tLoss: 0.344633\n",
      "Training stage for Flod 0 Epoch: 71 [22400/29980                 (75%)]\tLoss: 0.377734\n",
      "Training stage for Flod 0 Epoch: 71 [25600/29980                 (85%)]\tLoss: 0.560092\n",
      "Training stage for Flod 0 Epoch: 71 [28800/29980                 (96%)]\tLoss: 0.344949\n",
      "Test set for fold0: Average Loss:           739654.5747, Accuracy: 6686/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 72 [0/29980                 (0%)]\tLoss: 0.313264\n",
      "Training stage for Flod 0 Epoch: 72 [3200/29980                 (11%)]\tLoss: 0.313387\n",
      "Training stage for Flod 0 Epoch: 72 [6400/29980                 (21%)]\tLoss: 0.344566\n",
      "Training stage for Flod 0 Epoch: 72 [9600/29980                 (32%)]\tLoss: 0.375878\n",
      "Training stage for Flod 0 Epoch: 72 [12800/29980                 (43%)]\tLoss: 0.377993\n",
      "Training stage for Flod 0 Epoch: 72 [16000/29980                 (53%)]\tLoss: 0.421616\n",
      "Training stage for Flod 0 Epoch: 72 [19200/29980                 (64%)]\tLoss: 0.375681\n",
      "Training stage for Flod 0 Epoch: 72 [22400/29980                 (75%)]\tLoss: 0.354697\n",
      "Training stage for Flod 0 Epoch: 72 [25600/29980                 (85%)]\tLoss: 0.347793\n",
      "Training stage for Flod 0 Epoch: 72 [28800/29980                 (96%)]\tLoss: 0.375742\n",
      "Test set for fold0: Average Loss:           734056.7033, Accuracy: 6688/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 73 [0/29980                 (0%)]\tLoss: 0.409325\n",
      "Training stage for Flod 0 Epoch: 73 [3200/29980                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 73 [6400/29980                 (21%)]\tLoss: 0.344924\n",
      "Training stage for Flod 0 Epoch: 73 [9600/29980                 (32%)]\tLoss: 0.385481\n",
      "Training stage for Flod 0 Epoch: 73 [12800/29980                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 73 [16000/29980                 (53%)]\tLoss: 0.375764\n",
      "Training stage for Flod 0 Epoch: 73 [19200/29980                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 73 [22400/29980                 (75%)]\tLoss: 0.407146\n",
      "Training stage for Flod 0 Epoch: 73 [25600/29980                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 73 [28800/29980                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold0: Average Loss:           744379.6651, Accuracy: 6610/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 74 [0/29980                 (0%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 74 [3200/29980                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 74 [6400/29980                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 74 [9600/29980                 (32%)]\tLoss: 0.371851\n",
      "Training stage for Flod 0 Epoch: 74 [12800/29980                 (43%)]\tLoss: 0.407248\n",
      "Training stage for Flod 0 Epoch: 74 [16000/29980                 (53%)]\tLoss: 0.386057\n",
      "Training stage for Flod 0 Epoch: 74 [19200/29980                 (64%)]\tLoss: 0.313267\n",
      "Training stage for Flod 0 Epoch: 74 [22400/29980                 (75%)]\tLoss: 0.345115\n",
      "Training stage for Flod 0 Epoch: 74 [25600/29980                 (85%)]\tLoss: 0.376190\n",
      "Training stage for Flod 0 Epoch: 74 [28800/29980                 (96%)]\tLoss: 0.344524\n",
      "Test set for fold0: Average Loss:           741397.9714, Accuracy: 6663/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 75 [0/29980                 (0%)]\tLoss: 0.313382\n",
      "Training stage for Flod 0 Epoch: 75 [3200/29980                 (11%)]\tLoss: 0.469512\n",
      "Training stage for Flod 0 Epoch: 75 [6400/29980                 (21%)]\tLoss: 0.313863\n",
      "Training stage for Flod 0 Epoch: 75 [9600/29980                 (32%)]\tLoss: 0.316016\n",
      "Training stage for Flod 0 Epoch: 75 [12800/29980                 (43%)]\tLoss: 0.313274\n",
      "Training stage for Flod 0 Epoch: 75 [16000/29980                 (53%)]\tLoss: 0.382172\n",
      "Training stage for Flod 0 Epoch: 75 [19200/29980                 (64%)]\tLoss: 0.344514\n",
      "Training stage for Flod 0 Epoch: 75 [22400/29980                 (75%)]\tLoss: 0.357043\n",
      "Training stage for Flod 0 Epoch: 75 [25600/29980                 (85%)]\tLoss: 0.375769\n",
      "Training stage for Flod 0 Epoch: 75 [28800/29980                 (96%)]\tLoss: 0.344562\n",
      "Test set for fold0: Average Loss:           737998.1621, Accuracy: 6685/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 76 [0/29980                 (0%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 76 [3200/29980                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 76 [6400/29980                 (21%)]\tLoss: 0.313362\n",
      "Training stage for Flod 0 Epoch: 76 [9600/29980                 (32%)]\tLoss: 0.375825\n",
      "Training stage for Flod 0 Epoch: 76 [12800/29980                 (43%)]\tLoss: 0.449605\n",
      "Training stage for Flod 0 Epoch: 76 [16000/29980                 (53%)]\tLoss: 0.344395\n",
      "Training stage for Flod 0 Epoch: 76 [19200/29980                 (64%)]\tLoss: 0.346638\n",
      "Training stage for Flod 0 Epoch: 76 [22400/29980                 (75%)]\tLoss: 0.376031\n",
      "Training stage for Flod 0 Epoch: 76 [25600/29980                 (85%)]\tLoss: 0.344616\n",
      "Training stage for Flod 0 Epoch: 76 [28800/29980                 (96%)]\tLoss: 0.375774\n",
      "Test set for fold0: Average Loss:           716366.9313, Accuracy: 6789/7496           (91%)\n",
      "Training stage for Flod 0 Epoch: 77 [0/29980                 (0%)]\tLoss: 0.346251\n",
      "Training stage for Flod 0 Epoch: 77 [3200/29980                 (11%)]\tLoss: 0.313761\n",
      "Training stage for Flod 0 Epoch: 77 [6400/29980                 (21%)]\tLoss: 0.374702\n",
      "Training stage for Flod 0 Epoch: 77 [9600/29980                 (32%)]\tLoss: 0.375923\n",
      "Training stage for Flod 0 Epoch: 77 [12800/29980                 (43%)]\tLoss: 0.363770\n",
      "Training stage for Flod 0 Epoch: 77 [16000/29980                 (53%)]\tLoss: 0.373418\n",
      "Training stage for Flod 0 Epoch: 77 [19200/29980                 (64%)]\tLoss: 0.344285\n",
      "Training stage for Flod 0 Epoch: 77 [22400/29980                 (75%)]\tLoss: 0.313319\n",
      "Training stage for Flod 0 Epoch: 77 [25600/29980                 (85%)]\tLoss: 0.356696\n",
      "Training stage for Flod 0 Epoch: 77 [28800/29980                 (96%)]\tLoss: 0.407281\n",
      "Test set for fold0: Average Loss:           751568.0256, Accuracy: 6601/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 78 [0/29980                 (0%)]\tLoss: 0.406817\n",
      "Training stage for Flod 0 Epoch: 78 [3200/29980                 (11%)]\tLoss: 0.438355\n",
      "Training stage for Flod 0 Epoch: 78 [6400/29980                 (21%)]\tLoss: 0.439087\n",
      "Training stage for Flod 0 Epoch: 78 [9600/29980                 (32%)]\tLoss: 0.344574\n",
      "Training stage for Flod 0 Epoch: 78 [12800/29980                 (43%)]\tLoss: 0.375822\n",
      "Training stage for Flod 0 Epoch: 78 [16000/29980                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 78 [19200/29980                 (64%)]\tLoss: 0.407224\n",
      "Training stage for Flod 0 Epoch: 78 [22400/29980                 (75%)]\tLoss: 0.344523\n",
      "Training stage for Flod 0 Epoch: 78 [25600/29980                 (85%)]\tLoss: 0.406498\n",
      "Training stage for Flod 0 Epoch: 78 [28800/29980                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold0: Average Loss:           751282.7814, Accuracy: 6624/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 79 [0/29980                 (0%)]\tLoss: 0.375763\n",
      "Training stage for Flod 0 Epoch: 79 [3200/29980                 (11%)]\tLoss: 0.350344\n",
      "Training stage for Flod 0 Epoch: 79 [6400/29980                 (21%)]\tLoss: 0.344790\n",
      "Training stage for Flod 0 Epoch: 79 [9600/29980                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 79 [12800/29980                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 79 [16000/29980                 (53%)]\tLoss: 0.375850\n",
      "Training stage for Flod 0 Epoch: 79 [19200/29980                 (64%)]\tLoss: 0.313357\n",
      "Training stage for Flod 0 Epoch: 79 [22400/29980                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 79 [25600/29980                 (85%)]\tLoss: 0.469514\n",
      "Training stage for Flod 0 Epoch: 79 [28800/29980                 (96%)]\tLoss: 0.407795\n",
      "Test set for fold0: Average Loss:           745921.1011, Accuracy: 6615/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 80 [0/29980                 (0%)]\tLoss: 0.407013\n",
      "Training stage for Flod 0 Epoch: 80 [3200/29980                 (11%)]\tLoss: 0.314900\n",
      "Training stage for Flod 0 Epoch: 80 [6400/29980                 (21%)]\tLoss: 0.438266\n",
      "Training stage for Flod 0 Epoch: 80 [9600/29980                 (32%)]\tLoss: 0.375757\n",
      "Training stage for Flod 0 Epoch: 80 [12800/29980                 (43%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 80 [16000/29980                 (53%)]\tLoss: 0.314334\n",
      "Training stage for Flod 0 Epoch: 80 [19200/29980                 (64%)]\tLoss: 0.407091\n",
      "Training stage for Flod 0 Epoch: 80 [22400/29980                 (75%)]\tLoss: 0.377151\n",
      "Training stage for Flod 0 Epoch: 80 [25600/29980                 (85%)]\tLoss: 0.438482\n",
      "Training stage for Flod 0 Epoch: 80 [28800/29980                 (96%)]\tLoss: 0.373574\n",
      "Test set for fold0: Average Loss:           740621.2654, Accuracy: 6667/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 81 [0/29980                 (0%)]\tLoss: 0.407118\n",
      "Training stage for Flod 0 Epoch: 81 [3200/29980                 (11%)]\tLoss: 0.341306\n",
      "Training stage for Flod 0 Epoch: 81 [6400/29980                 (21%)]\tLoss: 0.336466\n",
      "Training stage for Flod 0 Epoch: 81 [9600/29980                 (32%)]\tLoss: 0.348597\n",
      "Training stage for Flod 0 Epoch: 81 [12800/29980                 (43%)]\tLoss: 0.419637\n",
      "Training stage for Flod 0 Epoch: 81 [16000/29980                 (53%)]\tLoss: 0.344782\n",
      "Training stage for Flod 0 Epoch: 81 [19200/29980                 (64%)]\tLoss: 0.344597\n",
      "Training stage for Flod 0 Epoch: 81 [22400/29980                 (75%)]\tLoss: 0.377667\n",
      "Training stage for Flod 0 Epoch: 81 [25600/29980                 (85%)]\tLoss: 0.313339\n",
      "Training stage for Flod 0 Epoch: 81 [28800/29980                 (96%)]\tLoss: 0.370693\n",
      "Test set for fold0: Average Loss:           742191.1483, Accuracy: 6657/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 82 [0/29980                 (0%)]\tLoss: 0.385025\n",
      "Training stage for Flod 0 Epoch: 82 [3200/29980                 (11%)]\tLoss: 0.375787\n",
      "Training stage for Flod 0 Epoch: 82 [6400/29980                 (21%)]\tLoss: 0.313284\n",
      "Training stage for Flod 0 Epoch: 82 [9600/29980                 (32%)]\tLoss: 0.406881\n",
      "Training stage for Flod 0 Epoch: 82 [12800/29980                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 82 [16000/29980                 (53%)]\tLoss: 0.375918\n",
      "Training stage for Flod 0 Epoch: 82 [19200/29980                 (64%)]\tLoss: 0.375770\n",
      "Training stage for Flod 0 Epoch: 82 [22400/29980                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 82 [25600/29980                 (85%)]\tLoss: 0.375808\n",
      "Training stage for Flod 0 Epoch: 82 [28800/29980                 (96%)]\tLoss: 0.376257\n",
      "Test set for fold0: Average Loss:           731233.0064, Accuracy: 6697/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 83 [0/29980                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 83 [3200/29980                 (11%)]\tLoss: 0.313263\n",
      "Training stage for Flod 0 Epoch: 83 [6400/29980                 (21%)]\tLoss: 0.375104\n",
      "Training stage for Flod 0 Epoch: 83 [9600/29980                 (32%)]\tLoss: 0.373073\n",
      "Training stage for Flod 0 Epoch: 83 [12800/29980                 (43%)]\tLoss: 0.382401\n",
      "Training stage for Flod 0 Epoch: 83 [16000/29980                 (53%)]\tLoss: 0.387277\n",
      "Training stage for Flod 0 Epoch: 83 [19200/29980                 (64%)]\tLoss: 0.313619\n",
      "Training stage for Flod 0 Epoch: 83 [22400/29980                 (75%)]\tLoss: 0.407042\n",
      "Training stage for Flod 0 Epoch: 83 [25600/29980                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 83 [28800/29980                 (96%)]\tLoss: 0.393714\n",
      "Test set for fold0: Average Loss:           766377.1858, Accuracy: 6572/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 84 [0/29980                 (0%)]\tLoss: 0.377646\n",
      "Training stage for Flod 0 Epoch: 84 [3200/29980                 (11%)]\tLoss: 0.379817\n",
      "Training stage for Flod 0 Epoch: 84 [6400/29980                 (21%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 84 [9600/29980                 (32%)]\tLoss: 0.313265\n",
      "Training stage for Flod 0 Epoch: 84 [12800/29980                 (43%)]\tLoss: 0.375954\n",
      "Training stage for Flod 0 Epoch: 84 [16000/29980                 (53%)]\tLoss: 0.314714\n",
      "Training stage for Flod 0 Epoch: 84 [19200/29980                 (64%)]\tLoss: 0.348475\n",
      "Training stage for Flod 0 Epoch: 84 [22400/29980                 (75%)]\tLoss: 0.344519\n",
      "Training stage for Flod 0 Epoch: 84 [25600/29980                 (85%)]\tLoss: 0.344525\n",
      "Training stage for Flod 0 Epoch: 84 [28800/29980                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold0: Average Loss:           725952.7594, Accuracy: 6729/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 85 [0/29980                 (0%)]\tLoss: 0.344587\n",
      "Training stage for Flod 0 Epoch: 85 [3200/29980                 (11%)]\tLoss: 0.313276\n",
      "Training stage for Flod 0 Epoch: 85 [6400/29980                 (21%)]\tLoss: 0.344648\n",
      "Training stage for Flod 0 Epoch: 85 [9600/29980                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 85 [12800/29980                 (43%)]\tLoss: 0.332928\n",
      "Training stage for Flod 0 Epoch: 85 [16000/29980                 (53%)]\tLoss: 0.344983\n",
      "Training stage for Flod 0 Epoch: 85 [19200/29980                 (64%)]\tLoss: 0.339784\n",
      "Training stage for Flod 0 Epoch: 85 [22400/29980                 (75%)]\tLoss: 0.376246\n",
      "Training stage for Flod 0 Epoch: 85 [25600/29980                 (85%)]\tLoss: 0.344868\n",
      "Training stage for Flod 0 Epoch: 85 [28800/29980                 (96%)]\tLoss: 0.407743\n",
      "Test set for fold0: Average Loss:           747445.2398, Accuracy: 6635/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 86 [0/29980                 (0%)]\tLoss: 0.347168\n",
      "Training stage for Flod 0 Epoch: 86 [3200/29980                 (11%)]\tLoss: 0.378191\n",
      "Training stage for Flod 0 Epoch: 86 [6400/29980                 (21%)]\tLoss: 0.407012\n",
      "Training stage for Flod 0 Epoch: 86 [9600/29980                 (32%)]\tLoss: 0.375768\n",
      "Training stage for Flod 0 Epoch: 86 [12800/29980                 (43%)]\tLoss: 0.341211\n",
      "Training stage for Flod 0 Epoch: 86 [16000/29980                 (53%)]\tLoss: 0.344785\n",
      "Training stage for Flod 0 Epoch: 86 [19200/29980                 (64%)]\tLoss: 0.346556\n",
      "Training stage for Flod 0 Epoch: 86 [22400/29980                 (75%)]\tLoss: 0.313266\n",
      "Training stage for Flod 0 Epoch: 86 [25600/29980                 (85%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 86 [28800/29980                 (96%)]\tLoss: 0.375768\n",
      "Test set for fold0: Average Loss:           746430.3900, Accuracy: 6621/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 87 [0/29980                 (0%)]\tLoss: 0.344526\n",
      "Training stage for Flod 0 Epoch: 87 [3200/29980                 (11%)]\tLoss: 0.344518\n",
      "Training stage for Flod 0 Epoch: 87 [6400/29980                 (21%)]\tLoss: 0.375765\n",
      "Training stage for Flod 0 Epoch: 87 [9600/29980                 (32%)]\tLoss: 0.377149\n",
      "Training stage for Flod 0 Epoch: 87 [12800/29980                 (43%)]\tLoss: 0.313265\n",
      "Training stage for Flod 0 Epoch: 87 [16000/29980                 (53%)]\tLoss: 0.375661\n",
      "Training stage for Flod 0 Epoch: 87 [19200/29980                 (64%)]\tLoss: 0.344521\n",
      "Training stage for Flod 0 Epoch: 87 [22400/29980                 (75%)]\tLoss: 0.401969\n",
      "Training stage for Flod 0 Epoch: 87 [25600/29980                 (85%)]\tLoss: 0.438262\n",
      "Training stage for Flod 0 Epoch: 87 [28800/29980                 (96%)]\tLoss: 0.331360\n",
      "Test set for fold0: Average Loss:           712403.6726, Accuracy: 6775/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 88 [0/29980                 (0%)]\tLoss: 0.316934\n",
      "Training stage for Flod 0 Epoch: 88 [3200/29980                 (11%)]\tLoss: 0.350333\n",
      "Training stage for Flod 0 Epoch: 88 [6400/29980                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 88 [9600/29980                 (32%)]\tLoss: 0.379492\n",
      "Training stage for Flod 0 Epoch: 88 [12800/29980                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 0 Epoch: 88 [16000/29980                 (53%)]\tLoss: 0.344581\n",
      "Training stage for Flod 0 Epoch: 88 [19200/29980                 (64%)]\tLoss: 0.343592\n",
      "Training stage for Flod 0 Epoch: 88 [22400/29980                 (75%)]\tLoss: 0.344795\n",
      "Training stage for Flod 0 Epoch: 88 [25600/29980                 (85%)]\tLoss: 0.344508\n",
      "Training stage for Flod 0 Epoch: 88 [28800/29980                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold0: Average Loss:           747974.5003, Accuracy: 6627/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 89 [0/29980                 (0%)]\tLoss: 0.322405\n",
      "Training stage for Flod 0 Epoch: 89 [3200/29980                 (11%)]\tLoss: 0.344531\n",
      "Training stage for Flod 0 Epoch: 89 [6400/29980                 (21%)]\tLoss: 0.438262\n",
      "Training stage for Flod 0 Epoch: 89 [9600/29980                 (32%)]\tLoss: 0.344518\n",
      "Training stage for Flod 0 Epoch: 89 [12800/29980                 (43%)]\tLoss: 0.313335\n",
      "Training stage for Flod 0 Epoch: 89 [16000/29980                 (53%)]\tLoss: 0.313644\n",
      "Training stage for Flod 0 Epoch: 89 [19200/29980                 (64%)]\tLoss: 0.313419\n",
      "Training stage for Flod 0 Epoch: 89 [22400/29980                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 89 [25600/29980                 (85%)]\tLoss: 0.381324\n",
      "Training stage for Flod 0 Epoch: 89 [28800/29980                 (96%)]\tLoss: 0.365276\n",
      "Test set for fold0: Average Loss:           740530.1663, Accuracy: 6671/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 90 [0/29980                 (0%)]\tLoss: 0.319225\n",
      "Training stage for Flod 0 Epoch: 90 [3200/29980                 (11%)]\tLoss: 0.376028\n",
      "Training stage for Flod 0 Epoch: 90 [6400/29980                 (21%)]\tLoss: 0.438063\n",
      "Training stage for Flod 0 Epoch: 90 [9600/29980                 (32%)]\tLoss: 0.399476\n",
      "Training stage for Flod 0 Epoch: 90 [12800/29980                 (43%)]\tLoss: 0.375879\n",
      "Training stage for Flod 0 Epoch: 90 [16000/29980                 (53%)]\tLoss: 0.344730\n",
      "Training stage for Flod 0 Epoch: 90 [19200/29980                 (64%)]\tLoss: 0.344498\n",
      "Training stage for Flod 0 Epoch: 90 [22400/29980                 (75%)]\tLoss: 0.391545\n",
      "Training stage for Flod 0 Epoch: 90 [25600/29980                 (85%)]\tLoss: 0.313265\n",
      "Training stage for Flod 0 Epoch: 90 [28800/29980                 (96%)]\tLoss: 0.438234\n",
      "Test set for fold0: Average Loss:           739674.1922, Accuracy: 6654/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 91 [0/29980                 (0%)]\tLoss: 0.435265\n",
      "Training stage for Flod 0 Epoch: 91 [3200/29980                 (11%)]\tLoss: 0.344540\n",
      "Training stage for Flod 0 Epoch: 91 [6400/29980                 (21%)]\tLoss: 0.350154\n",
      "Training stage for Flod 0 Epoch: 91 [9600/29980                 (32%)]\tLoss: 0.344526\n",
      "Training stage for Flod 0 Epoch: 91 [12800/29980                 (43%)]\tLoss: 0.316694\n",
      "Training stage for Flod 0 Epoch: 91 [16000/29980                 (53%)]\tLoss: 0.313336\n",
      "Training stage for Flod 0 Epoch: 91 [19200/29980                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 91 [22400/29980                 (75%)]\tLoss: 0.344768\n",
      "Training stage for Flod 0 Epoch: 91 [25600/29980                 (85%)]\tLoss: 0.377666\n",
      "Training stage for Flod 0 Epoch: 91 [28800/29980                 (96%)]\tLoss: 0.313604\n",
      "Test set for fold0: Average Loss:           740024.9173, Accuracy: 6654/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 92 [0/29980                 (0%)]\tLoss: 0.375948\n",
      "Training stage for Flod 0 Epoch: 92 [3200/29980                 (11%)]\tLoss: 0.314149\n",
      "Training stage for Flod 0 Epoch: 92 [6400/29980                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 92 [9600/29980                 (32%)]\tLoss: 0.344683\n",
      "Training stage for Flod 0 Epoch: 92 [12800/29980                 (43%)]\tLoss: 0.377577\n",
      "Training stage for Flod 0 Epoch: 92 [16000/29980                 (53%)]\tLoss: 0.375765\n",
      "Training stage for Flod 0 Epoch: 92 [19200/29980                 (64%)]\tLoss: 0.417350\n",
      "Training stage for Flod 0 Epoch: 92 [22400/29980                 (75%)]\tLoss: 0.375820\n",
      "Training stage for Flod 0 Epoch: 92 [25600/29980                 (85%)]\tLoss: 0.313454\n",
      "Training stage for Flod 0 Epoch: 92 [28800/29980                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold0: Average Loss:           728578.3677, Accuracy: 6716/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 93 [0/29980                 (0%)]\tLoss: 0.313263\n",
      "Training stage for Flod 0 Epoch: 93 [3200/29980                 (11%)]\tLoss: 0.391673\n",
      "Training stage for Flod 0 Epoch: 93 [6400/29980                 (21%)]\tLoss: 0.375770\n",
      "Training stage for Flod 0 Epoch: 93 [9600/29980                 (32%)]\tLoss: 0.313481\n",
      "Training stage for Flod 0 Epoch: 93 [12800/29980                 (43%)]\tLoss: 0.379908\n",
      "Training stage for Flod 0 Epoch: 93 [16000/29980                 (53%)]\tLoss: 0.313272\n",
      "Training stage for Flod 0 Epoch: 93 [19200/29980                 (64%)]\tLoss: 0.314507\n",
      "Training stage for Flod 0 Epoch: 93 [22400/29980                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 93 [25600/29980                 (85%)]\tLoss: 0.313267\n",
      "Training stage for Flod 0 Epoch: 93 [28800/29980                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold0: Average Loss:           728624.1762, Accuracy: 6723/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 94 [0/29980                 (0%)]\tLoss: 0.408444\n",
      "Training stage for Flod 0 Epoch: 94 [3200/29980                 (11%)]\tLoss: 0.344513\n",
      "Training stage for Flod 0 Epoch: 94 [6400/29980                 (21%)]\tLoss: 0.344530\n",
      "Training stage for Flod 0 Epoch: 94 [9600/29980                 (32%)]\tLoss: 0.344605\n",
      "Training stage for Flod 0 Epoch: 94 [12800/29980                 (43%)]\tLoss: 0.344519\n",
      "Training stage for Flod 0 Epoch: 94 [16000/29980                 (53%)]\tLoss: 0.375765\n",
      "Training stage for Flod 0 Epoch: 94 [19200/29980                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 94 [22400/29980                 (75%)]\tLoss: 0.375784\n",
      "Training stage for Flod 0 Epoch: 94 [25600/29980                 (85%)]\tLoss: 0.348004\n",
      "Training stage for Flod 0 Epoch: 94 [28800/29980                 (96%)]\tLoss: 0.437767\n",
      "Test set for fold0: Average Loss:           749901.1260, Accuracy: 6629/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 95 [0/29980                 (0%)]\tLoss: 0.407814\n",
      "Training stage for Flod 0 Epoch: 95 [3200/29980                 (11%)]\tLoss: 0.375201\n",
      "Training stage for Flod 0 Epoch: 95 [6400/29980                 (21%)]\tLoss: 0.375825\n",
      "Training stage for Flod 0 Epoch: 95 [9600/29980                 (32%)]\tLoss: 0.376068\n",
      "Training stage for Flod 0 Epoch: 95 [12800/29980                 (43%)]\tLoss: 0.313465\n",
      "Training stage for Flod 0 Epoch: 95 [16000/29980                 (53%)]\tLoss: 0.407373\n",
      "Training stage for Flod 0 Epoch: 95 [19200/29980                 (64%)]\tLoss: 0.346192\n",
      "Training stage for Flod 0 Epoch: 95 [22400/29980                 (75%)]\tLoss: 0.388368\n",
      "Training stage for Flod 0 Epoch: 95 [25600/29980                 (85%)]\tLoss: 0.313342\n",
      "Training stage for Flod 0 Epoch: 95 [28800/29980                 (96%)]\tLoss: 0.313726\n",
      "Test set for fold0: Average Loss:           737542.5111, Accuracy: 6662/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 96 [0/29980                 (0%)]\tLoss: 0.469448\n",
      "Training stage for Flod 0 Epoch: 96 [3200/29980                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 96 [6400/29980                 (21%)]\tLoss: 0.313264\n",
      "Training stage for Flod 0 Epoch: 96 [9600/29980                 (32%)]\tLoss: 0.313276\n",
      "Training stage for Flod 0 Epoch: 96 [12800/29980                 (43%)]\tLoss: 0.318518\n",
      "Training stage for Flod 0 Epoch: 96 [16000/29980                 (53%)]\tLoss: 0.375769\n",
      "Training stage for Flod 0 Epoch: 96 [19200/29980                 (64%)]\tLoss: 0.380214\n",
      "Training stage for Flod 0 Epoch: 96 [22400/29980                 (75%)]\tLoss: 0.313390\n",
      "Training stage for Flod 0 Epoch: 96 [25600/29980                 (85%)]\tLoss: 0.415794\n",
      "Training stage for Flod 0 Epoch: 96 [28800/29980                 (96%)]\tLoss: 0.324349\n",
      "Test set for fold0: Average Loss:           733161.5328, Accuracy: 6685/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 97 [0/29980                 (0%)]\tLoss: 0.398000\n",
      "Training stage for Flod 0 Epoch: 97 [3200/29980                 (11%)]\tLoss: 0.437563\n",
      "Training stage for Flod 0 Epoch: 97 [6400/29980                 (21%)]\tLoss: 0.407012\n",
      "Training stage for Flod 0 Epoch: 97 [9600/29980                 (32%)]\tLoss: 0.344535\n",
      "Training stage for Flod 0 Epoch: 97 [12800/29980                 (43%)]\tLoss: 0.361892\n",
      "Training stage for Flod 0 Epoch: 97 [16000/29980                 (53%)]\tLoss: 0.313593\n",
      "Training stage for Flod 0 Epoch: 97 [19200/29980                 (64%)]\tLoss: 0.344545\n",
      "Training stage for Flod 0 Epoch: 97 [22400/29980                 (75%)]\tLoss: 0.348684\n",
      "Training stage for Flod 0 Epoch: 97 [25600/29980                 (85%)]\tLoss: 0.375808\n",
      "Training stage for Flod 0 Epoch: 97 [28800/29980                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold0: Average Loss:           747598.7591, Accuracy: 6610/7496           (88%)\n",
      "Training stage for Flod 0 Epoch: 98 [0/29980                 (0%)]\tLoss: 0.401959\n",
      "Training stage for Flod 0 Epoch: 98 [3200/29980                 (11%)]\tLoss: 0.313297\n",
      "Training stage for Flod 0 Epoch: 98 [6400/29980                 (21%)]\tLoss: 0.313269\n",
      "Training stage for Flod 0 Epoch: 98 [9600/29980                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 98 [12800/29980                 (43%)]\tLoss: 0.346085\n",
      "Training stage for Flod 0 Epoch: 98 [16000/29980                 (53%)]\tLoss: 0.335956\n",
      "Training stage for Flod 0 Epoch: 98 [19200/29980                 (64%)]\tLoss: 0.441540\n",
      "Training stage for Flod 0 Epoch: 98 [22400/29980                 (75%)]\tLoss: 0.313263\n",
      "Training stage for Flod 0 Epoch: 98 [25600/29980                 (85%)]\tLoss: 0.314083\n",
      "Training stage for Flod 0 Epoch: 98 [28800/29980                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold0: Average Loss:           733134.2373, Accuracy: 6695/7496           (89%)\n",
      "Training stage for Flod 0 Epoch: 99 [0/29980                 (0%)]\tLoss: 0.345213\n",
      "Training stage for Flod 0 Epoch: 99 [3200/29980                 (11%)]\tLoss: 0.349164\n",
      "Training stage for Flod 0 Epoch: 99 [6400/29980                 (21%)]\tLoss: 0.313266\n",
      "Training stage for Flod 0 Epoch: 99 [9600/29980                 (32%)]\tLoss: 0.407012\n",
      "Training stage for Flod 0 Epoch: 99 [12800/29980                 (43%)]\tLoss: 0.375781\n",
      "Training stage for Flod 0 Epoch: 99 [16000/29980                 (53%)]\tLoss: 0.501060\n",
      "Training stage for Flod 0 Epoch: 99 [19200/29980                 (64%)]\tLoss: 0.375769\n",
      "Training stage for Flod 0 Epoch: 99 [22400/29980                 (75%)]\tLoss: 0.469512\n",
      "Training stage for Flod 0 Epoch: 99 [25600/29980                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 99 [28800/29980                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold0: Average Loss:           726516.2747, Accuracy: 6729/7496           (90%)\n",
      "Training stage for Flod 0 Epoch: 100 [0/29980                 (0%)]\tLoss: 0.313262\n",
      "Training stage for Flod 0 Epoch: 100 [3200/29980                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 100 [6400/29980                 (21%)]\tLoss: 0.313264\n",
      "Training stage for Flod 0 Epoch: 100 [9600/29980                 (32%)]\tLoss: 0.345232\n",
      "Training stage for Flod 0 Epoch: 100 [12800/29980                 (43%)]\tLoss: 0.313956\n",
      "Training stage for Flod 0 Epoch: 100 [16000/29980                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 0 Epoch: 100 [19200/29980                 (64%)]\tLoss: 0.349107\n",
      "Training stage for Flod 0 Epoch: 100 [22400/29980                 (75%)]\tLoss: 0.438262\n",
      "Training stage for Flod 0 Epoch: 100 [25600/29980                 (85%)]\tLoss: 0.352225\n",
      "Training stage for Flod 0 Epoch: 100 [28800/29980                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold0: Average Loss:           754823.1064, Accuracy: 6586/7496           (88%)\n",
      "-------------------Fold 1-------------------\n",
      "Training stage for Flod 1 Epoch: 1 [0/29981                 (0%)]\tLoss: 0.673956\n",
      "Training stage for Flod 1 Epoch: 1 [3200/29981                 (11%)]\tLoss: 0.449873\n",
      "Training stage for Flod 1 Epoch: 1 [6400/29981                 (21%)]\tLoss: 0.509925\n",
      "Training stage for Flod 1 Epoch: 1 [9600/29981                 (32%)]\tLoss: 0.398251\n",
      "Training stage for Flod 1 Epoch: 1 [12800/29981                 (43%)]\tLoss: 0.453345\n",
      "Training stage for Flod 1 Epoch: 1 [16000/29981                 (53%)]\tLoss: 0.434010\n",
      "Training stage for Flod 1 Epoch: 1 [19200/29981                 (64%)]\tLoss: 0.319938\n",
      "Training stage for Flod 1 Epoch: 1 [22400/29981                 (75%)]\tLoss: 0.344032\n",
      "Training stage for Flod 1 Epoch: 1 [25600/29981                 (85%)]\tLoss: 0.443131\n",
      "Training stage for Flod 1 Epoch: 1 [28800/29981                 (96%)]\tLoss: 0.386839\n",
      "Test set for fold1: Average Loss:           763834.5906, Accuracy: 6531/7495           (87%)\n",
      "Training stage for Flod 1 Epoch: 2 [0/29981                 (0%)]\tLoss: 0.410273\n",
      "Training stage for Flod 1 Epoch: 2 [3200/29981                 (11%)]\tLoss: 0.430246\n",
      "Training stage for Flod 1 Epoch: 2 [6400/29981                 (21%)]\tLoss: 0.411594\n",
      "Training stage for Flod 1 Epoch: 2 [9600/29981                 (32%)]\tLoss: 0.456014\n",
      "Training stage for Flod 1 Epoch: 2 [12800/29981                 (43%)]\tLoss: 0.439303\n",
      "Training stage for Flod 1 Epoch: 2 [16000/29981                 (53%)]\tLoss: 0.472242\n",
      "Training stage for Flod 1 Epoch: 2 [19200/29981                 (64%)]\tLoss: 0.401510\n",
      "Training stage for Flod 1 Epoch: 2 [22400/29981                 (75%)]\tLoss: 0.440559\n",
      "Training stage for Flod 1 Epoch: 2 [25600/29981                 (85%)]\tLoss: 0.403214\n",
      "Training stage for Flod 1 Epoch: 2 [28800/29981                 (96%)]\tLoss: 0.441309\n",
      "Test set for fold1: Average Loss:           767626.2439, Accuracy: 6500/7495           (87%)\n",
      "Training stage for Flod 1 Epoch: 3 [0/29981                 (0%)]\tLoss: 0.465053\n",
      "Training stage for Flod 1 Epoch: 3 [3200/29981                 (11%)]\tLoss: 0.380039\n",
      "Training stage for Flod 1 Epoch: 3 [6400/29981                 (21%)]\tLoss: 0.449281\n",
      "Training stage for Flod 1 Epoch: 3 [9600/29981                 (32%)]\tLoss: 0.420571\n",
      "Training stage for Flod 1 Epoch: 3 [12800/29981                 (43%)]\tLoss: 0.490387\n",
      "Training stage for Flod 1 Epoch: 3 [16000/29981                 (53%)]\tLoss: 0.369736\n",
      "Training stage for Flod 1 Epoch: 3 [19200/29981                 (64%)]\tLoss: 0.447231\n",
      "Training stage for Flod 1 Epoch: 3 [22400/29981                 (75%)]\tLoss: 0.431515\n",
      "Training stage for Flod 1 Epoch: 3 [25600/29981                 (85%)]\tLoss: 0.394003\n",
      "Training stage for Flod 1 Epoch: 3 [28800/29981                 (96%)]\tLoss: 0.361468\n",
      "Test set for fold1: Average Loss:           766281.8272, Accuracy: 6499/7495           (87%)\n",
      "Training stage for Flod 1 Epoch: 4 [0/29981                 (0%)]\tLoss: 0.412493\n",
      "Training stage for Flod 1 Epoch: 4 [3200/29981                 (11%)]\tLoss: 0.376881\n",
      "Training stage for Flod 1 Epoch: 4 [6400/29981                 (21%)]\tLoss: 0.346488\n",
      "Training stage for Flod 1 Epoch: 4 [9600/29981                 (32%)]\tLoss: 0.345125\n",
      "Training stage for Flod 1 Epoch: 4 [12800/29981                 (43%)]\tLoss: 0.409114\n",
      "Training stage for Flod 1 Epoch: 4 [16000/29981                 (53%)]\tLoss: 0.367665\n",
      "Training stage for Flod 1 Epoch: 4 [19200/29981                 (64%)]\tLoss: 0.410548\n",
      "Training stage for Flod 1 Epoch: 4 [22400/29981                 (75%)]\tLoss: 0.393529\n",
      "Training stage for Flod 1 Epoch: 4 [25600/29981                 (85%)]\tLoss: 0.368414\n",
      "Training stage for Flod 1 Epoch: 4 [28800/29981                 (96%)]\tLoss: 0.397152\n",
      "Test set for fold1: Average Loss:           739583.8484, Accuracy: 6645/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 5 [0/29981                 (0%)]\tLoss: 0.321065\n",
      "Training stage for Flod 1 Epoch: 5 [3200/29981                 (11%)]\tLoss: 0.422345\n",
      "Training stage for Flod 1 Epoch: 5 [6400/29981                 (21%)]\tLoss: 0.345296\n",
      "Training stage for Flod 1 Epoch: 5 [9600/29981                 (32%)]\tLoss: 0.420778\n",
      "Training stage for Flod 1 Epoch: 5 [12800/29981                 (43%)]\tLoss: 0.406606\n",
      "Training stage for Flod 1 Epoch: 5 [16000/29981                 (53%)]\tLoss: 0.404629\n",
      "Training stage for Flod 1 Epoch: 5 [19200/29981                 (64%)]\tLoss: 0.358822\n",
      "Training stage for Flod 1 Epoch: 5 [22400/29981                 (75%)]\tLoss: 0.349511\n",
      "Training stage for Flod 1 Epoch: 5 [25600/29981                 (85%)]\tLoss: 0.481146\n",
      "Training stage for Flod 1 Epoch: 5 [28800/29981                 (96%)]\tLoss: 0.345086\n",
      "Test set for fold1: Average Loss:           756892.1538, Accuracy: 6553/7495           (87%)\n",
      "Training stage for Flod 1 Epoch: 6 [0/29981                 (0%)]\tLoss: 0.357633\n",
      "Training stage for Flod 1 Epoch: 6 [3200/29981                 (11%)]\tLoss: 0.350863\n",
      "Training stage for Flod 1 Epoch: 6 [6400/29981                 (21%)]\tLoss: 0.379001\n",
      "Training stage for Flod 1 Epoch: 6 [9600/29981                 (32%)]\tLoss: 0.476747\n",
      "Training stage for Flod 1 Epoch: 6 [12800/29981                 (43%)]\tLoss: 0.393755\n",
      "Training stage for Flod 1 Epoch: 6 [16000/29981                 (53%)]\tLoss: 0.495601\n",
      "Training stage for Flod 1 Epoch: 6 [19200/29981                 (64%)]\tLoss: 0.405431\n",
      "Training stage for Flod 1 Epoch: 6 [22400/29981                 (75%)]\tLoss: 0.435719\n",
      "Training stage for Flod 1 Epoch: 6 [25600/29981                 (85%)]\tLoss: 0.498935\n",
      "Training stage for Flod 1 Epoch: 6 [28800/29981                 (96%)]\tLoss: 0.423875\n",
      "Test set for fold1: Average Loss:           741833.4144, Accuracy: 6630/7495           (88%)\n",
      "Training stage for Flod 1 Epoch: 7 [0/29981                 (0%)]\tLoss: 0.360823\n",
      "Training stage for Flod 1 Epoch: 7 [3200/29981                 (11%)]\tLoss: 0.397050\n",
      "Training stage for Flod 1 Epoch: 7 [6400/29981                 (21%)]\tLoss: 0.437462\n",
      "Training stage for Flod 1 Epoch: 7 [9600/29981                 (32%)]\tLoss: 0.428066\n",
      "Training stage for Flod 1 Epoch: 7 [12800/29981                 (43%)]\tLoss: 0.428996\n",
      "Training stage for Flod 1 Epoch: 7 [16000/29981                 (53%)]\tLoss: 0.345837\n",
      "Training stage for Flod 1 Epoch: 7 [19200/29981                 (64%)]\tLoss: 0.371005\n",
      "Training stage for Flod 1 Epoch: 7 [22400/29981                 (75%)]\tLoss: 0.433041\n",
      "Training stage for Flod 1 Epoch: 7 [25600/29981                 (85%)]\tLoss: 0.436822\n",
      "Training stage for Flod 1 Epoch: 7 [28800/29981                 (96%)]\tLoss: 0.431597\n",
      "Test set for fold1: Average Loss:           724145.1900, Accuracy: 6743/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 8 [0/29981                 (0%)]\tLoss: 0.376484\n",
      "Training stage for Flod 1 Epoch: 8 [3200/29981                 (11%)]\tLoss: 0.339456\n",
      "Training stage for Flod 1 Epoch: 8 [6400/29981                 (21%)]\tLoss: 0.492516\n",
      "Training stage for Flod 1 Epoch: 8 [9600/29981                 (32%)]\tLoss: 0.423683\n",
      "Training stage for Flod 1 Epoch: 8 [12800/29981                 (43%)]\tLoss: 0.408222\n",
      "Training stage for Flod 1 Epoch: 8 [16000/29981                 (53%)]\tLoss: 0.374950\n",
      "Training stage for Flod 1 Epoch: 8 [19200/29981                 (64%)]\tLoss: 0.417082\n",
      "Training stage for Flod 1 Epoch: 8 [22400/29981                 (75%)]\tLoss: 0.441019\n",
      "Training stage for Flod 1 Epoch: 8 [25600/29981                 (85%)]\tLoss: 0.387442\n",
      "Training stage for Flod 1 Epoch: 8 [28800/29981                 (96%)]\tLoss: 0.475760\n",
      "Test set for fold1: Average Loss:           727827.2620, Accuracy: 6705/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 9 [0/29981                 (0%)]\tLoss: 0.494329\n",
      "Training stage for Flod 1 Epoch: 9 [3200/29981                 (11%)]\tLoss: 0.347719\n",
      "Training stage for Flod 1 Epoch: 9 [6400/29981                 (21%)]\tLoss: 0.327220\n",
      "Training stage for Flod 1 Epoch: 9 [9600/29981                 (32%)]\tLoss: 0.377137\n",
      "Training stage for Flod 1 Epoch: 9 [12800/29981                 (43%)]\tLoss: 0.375960\n",
      "Training stage for Flod 1 Epoch: 9 [16000/29981                 (53%)]\tLoss: 0.324363\n",
      "Training stage for Flod 1 Epoch: 9 [19200/29981                 (64%)]\tLoss: 0.439509\n",
      "Training stage for Flod 1 Epoch: 9 [22400/29981                 (75%)]\tLoss: 0.396858\n",
      "Training stage for Flod 1 Epoch: 9 [25600/29981                 (85%)]\tLoss: 0.351117\n",
      "Training stage for Flod 1 Epoch: 9 [28800/29981                 (96%)]\tLoss: 0.479362\n",
      "Test set for fold1: Average Loss:           727438.1723, Accuracy: 6685/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 10 [0/29981                 (0%)]\tLoss: 0.376765\n",
      "Training stage for Flod 1 Epoch: 10 [3200/29981                 (11%)]\tLoss: 0.535326\n",
      "Training stage for Flod 1 Epoch: 10 [6400/29981                 (21%)]\tLoss: 0.395814\n",
      "Training stage for Flod 1 Epoch: 10 [9600/29981                 (32%)]\tLoss: 0.404528\n",
      "Training stage for Flod 1 Epoch: 10 [12800/29981                 (43%)]\tLoss: 0.405903\n",
      "Training stage for Flod 1 Epoch: 10 [16000/29981                 (53%)]\tLoss: 0.442324\n",
      "Training stage for Flod 1 Epoch: 10 [19200/29981                 (64%)]\tLoss: 0.387833\n",
      "Training stage for Flod 1 Epoch: 10 [22400/29981                 (75%)]\tLoss: 0.441137\n",
      "Training stage for Flod 1 Epoch: 10 [25600/29981                 (85%)]\tLoss: 0.377034\n",
      "Training stage for Flod 1 Epoch: 10 [28800/29981                 (96%)]\tLoss: 0.351487\n",
      "Test set for fold1: Average Loss:           703127.1760, Accuracy: 6812/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 11 [0/29981                 (0%)]\tLoss: 0.396605\n",
      "Training stage for Flod 1 Epoch: 11 [3200/29981                 (11%)]\tLoss: 0.547678\n",
      "Training stage for Flod 1 Epoch: 11 [6400/29981                 (21%)]\tLoss: 0.371415\n",
      "Training stage for Flod 1 Epoch: 11 [9600/29981                 (32%)]\tLoss: 0.473214\n",
      "Training stage for Flod 1 Epoch: 11 [12800/29981                 (43%)]\tLoss: 0.378064\n",
      "Training stage for Flod 1 Epoch: 11 [16000/29981                 (53%)]\tLoss: 0.495776\n",
      "Training stage for Flod 1 Epoch: 11 [19200/29981                 (64%)]\tLoss: 0.397279\n",
      "Training stage for Flod 1 Epoch: 11 [22400/29981                 (75%)]\tLoss: 0.415183\n",
      "Training stage for Flod 1 Epoch: 11 [25600/29981                 (85%)]\tLoss: 0.361267\n",
      "Training stage for Flod 1 Epoch: 11 [28800/29981                 (96%)]\tLoss: 0.406859\n",
      "Test set for fold1: Average Loss:           714129.2843, Accuracy: 6758/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 12 [0/29981                 (0%)]\tLoss: 0.344865\n",
      "Training stage for Flod 1 Epoch: 12 [3200/29981                 (11%)]\tLoss: 0.358696\n",
      "Training stage for Flod 1 Epoch: 12 [6400/29981                 (21%)]\tLoss: 0.376211\n",
      "Training stage for Flod 1 Epoch: 12 [9600/29981                 (32%)]\tLoss: 0.407890\n",
      "Training stage for Flod 1 Epoch: 12 [12800/29981                 (43%)]\tLoss: 0.443135\n",
      "Training stage for Flod 1 Epoch: 12 [16000/29981                 (53%)]\tLoss: 0.462311\n",
      "Training stage for Flod 1 Epoch: 12 [19200/29981                 (64%)]\tLoss: 0.376742\n",
      "Training stage for Flod 1 Epoch: 12 [22400/29981                 (75%)]\tLoss: 0.404784\n",
      "Training stage for Flod 1 Epoch: 12 [25600/29981                 (85%)]\tLoss: 0.375178\n",
      "Training stage for Flod 1 Epoch: 12 [28800/29981                 (96%)]\tLoss: 0.346043\n",
      "Test set for fold1: Average Loss:           720484.6167, Accuracy: 6740/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 13 [0/29981                 (0%)]\tLoss: 0.479345\n",
      "Training stage for Flod 1 Epoch: 13 [3200/29981                 (11%)]\tLoss: 0.442916\n",
      "Training stage for Flod 1 Epoch: 13 [6400/29981                 (21%)]\tLoss: 0.492066\n",
      "Training stage for Flod 1 Epoch: 13 [9600/29981                 (32%)]\tLoss: 0.368509\n",
      "Training stage for Flod 1 Epoch: 13 [12800/29981                 (43%)]\tLoss: 0.466596\n",
      "Training stage for Flod 1 Epoch: 13 [16000/29981                 (53%)]\tLoss: 0.446027\n",
      "Training stage for Flod 1 Epoch: 13 [19200/29981                 (64%)]\tLoss: 0.469406\n",
      "Training stage for Flod 1 Epoch: 13 [22400/29981                 (75%)]\tLoss: 0.409853\n",
      "Training stage for Flod 1 Epoch: 13 [25600/29981                 (85%)]\tLoss: 0.409690\n",
      "Training stage for Flod 1 Epoch: 13 [28800/29981                 (96%)]\tLoss: 0.402018\n",
      "Test set for fold1: Average Loss:           731766.3560, Accuracy: 6678/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 14 [0/29981                 (0%)]\tLoss: 0.404097\n",
      "Training stage for Flod 1 Epoch: 14 [3200/29981                 (11%)]\tLoss: 0.438454\n",
      "Training stage for Flod 1 Epoch: 14 [6400/29981                 (21%)]\tLoss: 0.370132\n",
      "Training stage for Flod 1 Epoch: 14 [9600/29981                 (32%)]\tLoss: 0.375050\n",
      "Training stage for Flod 1 Epoch: 14 [12800/29981                 (43%)]\tLoss: 0.459324\n",
      "Training stage for Flod 1 Epoch: 14 [16000/29981                 (53%)]\tLoss: 0.468925\n",
      "Training stage for Flod 1 Epoch: 14 [19200/29981                 (64%)]\tLoss: 0.384475\n",
      "Training stage for Flod 1 Epoch: 14 [22400/29981                 (75%)]\tLoss: 0.437049\n",
      "Training stage for Flod 1 Epoch: 14 [25600/29981                 (85%)]\tLoss: 0.385688\n",
      "Training stage for Flod 1 Epoch: 14 [28800/29981                 (96%)]\tLoss: 0.393417\n",
      "Test set for fold1: Average Loss:           721339.0120, Accuracy: 6749/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 15 [0/29981                 (0%)]\tLoss: 0.405910\n",
      "Training stage for Flod 1 Epoch: 15 [3200/29981                 (11%)]\tLoss: 0.368638\n",
      "Training stage for Flod 1 Epoch: 15 [6400/29981                 (21%)]\tLoss: 0.378824\n",
      "Training stage for Flod 1 Epoch: 15 [9600/29981                 (32%)]\tLoss: 0.347108\n",
      "Training stage for Flod 1 Epoch: 15 [12800/29981                 (43%)]\tLoss: 0.409059\n",
      "Training stage for Flod 1 Epoch: 15 [16000/29981                 (53%)]\tLoss: 0.422635\n",
      "Training stage for Flod 1 Epoch: 15 [19200/29981                 (64%)]\tLoss: 0.438337\n",
      "Training stage for Flod 1 Epoch: 15 [22400/29981                 (75%)]\tLoss: 0.461365\n",
      "Training stage for Flod 1 Epoch: 15 [25600/29981                 (85%)]\tLoss: 0.465112\n",
      "Training stage for Flod 1 Epoch: 15 [28800/29981                 (96%)]\tLoss: 0.407092\n",
      "Test set for fold1: Average Loss:           726560.6640, Accuracy: 6719/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 16 [0/29981                 (0%)]\tLoss: 0.438422\n",
      "Training stage for Flod 1 Epoch: 16 [3200/29981                 (11%)]\tLoss: 0.406839\n",
      "Training stage for Flod 1 Epoch: 16 [6400/29981                 (21%)]\tLoss: 0.345261\n",
      "Training stage for Flod 1 Epoch: 16 [9600/29981                 (32%)]\tLoss: 0.344697\n",
      "Training stage for Flod 1 Epoch: 16 [12800/29981                 (43%)]\tLoss: 0.468658\n",
      "Training stage for Flod 1 Epoch: 16 [16000/29981                 (53%)]\tLoss: 0.436497\n",
      "Training stage for Flod 1 Epoch: 16 [19200/29981                 (64%)]\tLoss: 0.407506\n",
      "Training stage for Flod 1 Epoch: 16 [22400/29981                 (75%)]\tLoss: 0.372735\n",
      "Training stage for Flod 1 Epoch: 16 [25600/29981                 (85%)]\tLoss: 0.437825\n",
      "Training stage for Flod 1 Epoch: 16 [28800/29981                 (96%)]\tLoss: 0.404756\n",
      "Test set for fold1: Average Loss:           724246.4677, Accuracy: 6731/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 17 [0/29981                 (0%)]\tLoss: 0.483291\n",
      "Training stage for Flod 1 Epoch: 17 [3200/29981                 (11%)]\tLoss: 0.344951\n",
      "Training stage for Flod 1 Epoch: 17 [6400/29981                 (21%)]\tLoss: 0.377536\n",
      "Training stage for Flod 1 Epoch: 17 [9600/29981                 (32%)]\tLoss: 0.384101\n",
      "Training stage for Flod 1 Epoch: 17 [12800/29981                 (43%)]\tLoss: 0.455800\n",
      "Training stage for Flod 1 Epoch: 17 [16000/29981                 (53%)]\tLoss: 0.470162\n",
      "Training stage for Flod 1 Epoch: 17 [19200/29981                 (64%)]\tLoss: 0.340374\n",
      "Training stage for Flod 1 Epoch: 17 [22400/29981                 (75%)]\tLoss: 0.330052\n",
      "Training stage for Flod 1 Epoch: 17 [25600/29981                 (85%)]\tLoss: 0.438864\n",
      "Training stage for Flod 1 Epoch: 17 [28800/29981                 (96%)]\tLoss: 0.375396\n",
      "Test set for fold1: Average Loss:           707503.9428, Accuracy: 6792/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 18 [0/29981                 (0%)]\tLoss: 0.380950\n",
      "Training stage for Flod 1 Epoch: 18 [3200/29981                 (11%)]\tLoss: 0.367154\n",
      "Training stage for Flod 1 Epoch: 18 [6400/29981                 (21%)]\tLoss: 0.475243\n",
      "Training stage for Flod 1 Epoch: 18 [9600/29981                 (32%)]\tLoss: 0.396416\n",
      "Training stage for Flod 1 Epoch: 18 [12800/29981                 (43%)]\tLoss: 0.434235\n",
      "Training stage for Flod 1 Epoch: 18 [16000/29981                 (53%)]\tLoss: 0.313389\n",
      "Training stage for Flod 1 Epoch: 18 [19200/29981                 (64%)]\tLoss: 0.359300\n",
      "Training stage for Flod 1 Epoch: 18 [22400/29981                 (75%)]\tLoss: 0.346776\n",
      "Training stage for Flod 1 Epoch: 18 [25600/29981                 (85%)]\tLoss: 0.321267\n",
      "Training stage for Flod 1 Epoch: 18 [28800/29981                 (96%)]\tLoss: 0.378373\n",
      "Test set for fold1: Average Loss:           719257.0838, Accuracy: 6745/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 19 [0/29981                 (0%)]\tLoss: 0.315011\n",
      "Training stage for Flod 1 Epoch: 19 [3200/29981                 (11%)]\tLoss: 0.438304\n",
      "Training stage for Flod 1 Epoch: 19 [6400/29981                 (21%)]\tLoss: 0.378537\n",
      "Training stage for Flod 1 Epoch: 19 [9600/29981                 (32%)]\tLoss: 0.517779\n",
      "Training stage for Flod 1 Epoch: 19 [12800/29981                 (43%)]\tLoss: 0.397955\n",
      "Training stage for Flod 1 Epoch: 19 [16000/29981                 (53%)]\tLoss: 0.442136\n",
      "Training stage for Flod 1 Epoch: 19 [19200/29981                 (64%)]\tLoss: 0.344646\n",
      "Training stage for Flod 1 Epoch: 19 [22400/29981                 (75%)]\tLoss: 0.385131\n",
      "Training stage for Flod 1 Epoch: 19 [25600/29981                 (85%)]\tLoss: 0.365800\n",
      "Training stage for Flod 1 Epoch: 19 [28800/29981                 (96%)]\tLoss: 0.420774\n",
      "Test set for fold1: Average Loss:           722384.4838, Accuracy: 6726/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 20 [0/29981                 (0%)]\tLoss: 0.499052\n",
      "Training stage for Flod 1 Epoch: 20 [3200/29981                 (11%)]\tLoss: 0.429905\n",
      "Training stage for Flod 1 Epoch: 20 [6400/29981                 (21%)]\tLoss: 0.377471\n",
      "Training stage for Flod 1 Epoch: 20 [9600/29981                 (32%)]\tLoss: 0.372620\n",
      "Training stage for Flod 1 Epoch: 20 [12800/29981                 (43%)]\tLoss: 0.423455\n",
      "Training stage for Flod 1 Epoch: 20 [16000/29981                 (53%)]\tLoss: 0.407624\n",
      "Training stage for Flod 1 Epoch: 20 [19200/29981                 (64%)]\tLoss: 0.459869\n",
      "Training stage for Flod 1 Epoch: 20 [22400/29981                 (75%)]\tLoss: 0.406592\n",
      "Training stage for Flod 1 Epoch: 20 [25600/29981                 (85%)]\tLoss: 0.376352\n",
      "Training stage for Flod 1 Epoch: 20 [28800/29981                 (96%)]\tLoss: 0.411473\n",
      "Test set for fold1: Average Loss:           719785.1566, Accuracy: 6733/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 21 [0/29981                 (0%)]\tLoss: 0.433726\n",
      "Training stage for Flod 1 Epoch: 21 [3200/29981                 (11%)]\tLoss: 0.359419\n",
      "Training stage for Flod 1 Epoch: 21 [6400/29981                 (21%)]\tLoss: 0.473835\n",
      "Training stage for Flod 1 Epoch: 21 [9600/29981                 (32%)]\tLoss: 0.313453\n",
      "Training stage for Flod 1 Epoch: 21 [12800/29981                 (43%)]\tLoss: 0.375560\n",
      "Training stage for Flod 1 Epoch: 21 [16000/29981                 (53%)]\tLoss: 0.376396\n",
      "Training stage for Flod 1 Epoch: 21 [19200/29981                 (64%)]\tLoss: 0.369441\n",
      "Training stage for Flod 1 Epoch: 21 [22400/29981                 (75%)]\tLoss: 0.375886\n",
      "Training stage for Flod 1 Epoch: 21 [25600/29981                 (85%)]\tLoss: 0.403491\n",
      "Training stage for Flod 1 Epoch: 21 [28800/29981                 (96%)]\tLoss: 0.375819\n",
      "Test set for fold1: Average Loss:           741337.9400, Accuracy: 6611/7495           (88%)\n",
      "Training stage for Flod 1 Epoch: 22 [0/29981                 (0%)]\tLoss: 0.365261\n",
      "Training stage for Flod 1 Epoch: 22 [3200/29981                 (11%)]\tLoss: 0.407373\n",
      "Training stage for Flod 1 Epoch: 22 [6400/29981                 (21%)]\tLoss: 0.471627\n",
      "Training stage for Flod 1 Epoch: 22 [9600/29981                 (32%)]\tLoss: 0.346680\n",
      "Training stage for Flod 1 Epoch: 22 [12800/29981                 (43%)]\tLoss: 0.367366\n",
      "Training stage for Flod 1 Epoch: 22 [16000/29981                 (53%)]\tLoss: 0.317737\n",
      "Training stage for Flod 1 Epoch: 22 [19200/29981                 (64%)]\tLoss: 0.352995\n",
      "Training stage for Flod 1 Epoch: 22 [22400/29981                 (75%)]\tLoss: 0.376000\n",
      "Training stage for Flod 1 Epoch: 22 [25600/29981                 (85%)]\tLoss: 0.436608\n",
      "Training stage for Flod 1 Epoch: 22 [28800/29981                 (96%)]\tLoss: 0.344352\n",
      "Test set for fold1: Average Loss:           721544.9586, Accuracy: 6737/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 23 [0/29981                 (0%)]\tLoss: 0.319044\n",
      "Training stage for Flod 1 Epoch: 23 [3200/29981                 (11%)]\tLoss: 0.347617\n",
      "Training stage for Flod 1 Epoch: 23 [6400/29981                 (21%)]\tLoss: 0.364706\n",
      "Training stage for Flod 1 Epoch: 23 [9600/29981                 (32%)]\tLoss: 0.418331\n",
      "Training stage for Flod 1 Epoch: 23 [12800/29981                 (43%)]\tLoss: 0.377675\n",
      "Training stage for Flod 1 Epoch: 23 [16000/29981                 (53%)]\tLoss: 0.313656\n",
      "Training stage for Flod 1 Epoch: 23 [19200/29981                 (64%)]\tLoss: 0.441395\n",
      "Training stage for Flod 1 Epoch: 23 [22400/29981                 (75%)]\tLoss: 0.347001\n",
      "Training stage for Flod 1 Epoch: 23 [25600/29981                 (85%)]\tLoss: 0.344514\n",
      "Training stage for Flod 1 Epoch: 23 [28800/29981                 (96%)]\tLoss: 0.313507\n",
      "Test set for fold1: Average Loss:           717808.5510, Accuracy: 6738/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 24 [0/29981                 (0%)]\tLoss: 0.406550\n",
      "Training stage for Flod 1 Epoch: 24 [3200/29981                 (11%)]\tLoss: 0.368200\n",
      "Training stage for Flod 1 Epoch: 24 [6400/29981                 (21%)]\tLoss: 0.406957\n",
      "Training stage for Flod 1 Epoch: 24 [9600/29981                 (32%)]\tLoss: 0.376880\n",
      "Training stage for Flod 1 Epoch: 24 [12800/29981                 (43%)]\tLoss: 0.375925\n",
      "Training stage for Flod 1 Epoch: 24 [16000/29981                 (53%)]\tLoss: 0.376987\n",
      "Training stage for Flod 1 Epoch: 24 [19200/29981                 (64%)]\tLoss: 0.407031\n",
      "Training stage for Flod 1 Epoch: 24 [22400/29981                 (75%)]\tLoss: 0.375928\n",
      "Training stage for Flod 1 Epoch: 24 [25600/29981                 (85%)]\tLoss: 0.417207\n",
      "Training stage for Flod 1 Epoch: 24 [28800/29981                 (96%)]\tLoss: 0.376312\n",
      "Test set for fold1: Average Loss:           828845.8134, Accuracy: 6248/7495           (83%)\n",
      "Training stage for Flod 1 Epoch: 25 [0/29981                 (0%)]\tLoss: 0.398835\n",
      "Training stage for Flod 1 Epoch: 25 [3200/29981                 (11%)]\tLoss: 0.355460\n",
      "Training stage for Flod 1 Epoch: 25 [6400/29981                 (21%)]\tLoss: 0.367946\n",
      "Training stage for Flod 1 Epoch: 25 [9600/29981                 (32%)]\tLoss: 0.396625\n",
      "Training stage for Flod 1 Epoch: 25 [12800/29981                 (43%)]\tLoss: 0.344541\n",
      "Training stage for Flod 1 Epoch: 25 [16000/29981                 (53%)]\tLoss: 0.344689\n",
      "Training stage for Flod 1 Epoch: 25 [19200/29981                 (64%)]\tLoss: 0.376539\n",
      "Training stage for Flod 1 Epoch: 25 [22400/29981                 (75%)]\tLoss: 0.387973\n",
      "Training stage for Flod 1 Epoch: 25 [25600/29981                 (85%)]\tLoss: 0.473519\n",
      "Training stage for Flod 1 Epoch: 25 [28800/29981                 (96%)]\tLoss: 0.462694\n",
      "Test set for fold1: Average Loss:           727845.9745, Accuracy: 6682/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 26 [0/29981                 (0%)]\tLoss: 0.376395\n",
      "Training stage for Flod 1 Epoch: 26 [3200/29981                 (11%)]\tLoss: 0.344646\n",
      "Training stage for Flod 1 Epoch: 26 [6400/29981                 (21%)]\tLoss: 0.400883\n",
      "Training stage for Flod 1 Epoch: 26 [9600/29981                 (32%)]\tLoss: 0.380130\n",
      "Training stage for Flod 1 Epoch: 26 [12800/29981                 (43%)]\tLoss: 0.344829\n",
      "Training stage for Flod 1 Epoch: 26 [16000/29981                 (53%)]\tLoss: 0.404649\n",
      "Training stage for Flod 1 Epoch: 26 [19200/29981                 (64%)]\tLoss: 0.466238\n",
      "Training stage for Flod 1 Epoch: 26 [22400/29981                 (75%)]\tLoss: 0.344430\n",
      "Training stage for Flod 1 Epoch: 26 [25600/29981                 (85%)]\tLoss: 0.375800\n",
      "Training stage for Flod 1 Epoch: 26 [28800/29981                 (96%)]\tLoss: 0.344651\n",
      "Test set for fold1: Average Loss:           745514.0011, Accuracy: 6672/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 27 [0/29981                 (0%)]\tLoss: 0.359552\n",
      "Training stage for Flod 1 Epoch: 27 [3200/29981                 (11%)]\tLoss: 0.395709\n",
      "Training stage for Flod 1 Epoch: 27 [6400/29981                 (21%)]\tLoss: 0.406508\n",
      "Training stage for Flod 1 Epoch: 27 [9600/29981                 (32%)]\tLoss: 0.346696\n",
      "Training stage for Flod 1 Epoch: 27 [12800/29981                 (43%)]\tLoss: 0.384572\n",
      "Training stage for Flod 1 Epoch: 27 [16000/29981                 (53%)]\tLoss: 0.346543\n",
      "Training stage for Flod 1 Epoch: 27 [19200/29981                 (64%)]\tLoss: 0.430432\n",
      "Training stage for Flod 1 Epoch: 27 [22400/29981                 (75%)]\tLoss: 0.346486\n",
      "Training stage for Flod 1 Epoch: 27 [25600/29981                 (85%)]\tLoss: 0.396143\n",
      "Training stage for Flod 1 Epoch: 27 [28800/29981                 (96%)]\tLoss: 0.375781\n",
      "Test set for fold1: Average Loss:           726897.5567, Accuracy: 6724/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 28 [0/29981                 (0%)]\tLoss: 0.443211\n",
      "Training stage for Flod 1 Epoch: 28 [3200/29981                 (11%)]\tLoss: 0.344398\n",
      "Training stage for Flod 1 Epoch: 28 [6400/29981                 (21%)]\tLoss: 0.402817\n",
      "Training stage for Flod 1 Epoch: 28 [9600/29981                 (32%)]\tLoss: 0.345163\n",
      "Training stage for Flod 1 Epoch: 28 [12800/29981                 (43%)]\tLoss: 0.325522\n",
      "Training stage for Flod 1 Epoch: 28 [16000/29981                 (53%)]\tLoss: 0.410851\n",
      "Training stage for Flod 1 Epoch: 28 [19200/29981                 (64%)]\tLoss: 0.407588\n",
      "Training stage for Flod 1 Epoch: 28 [22400/29981                 (75%)]\tLoss: 0.386398\n",
      "Training stage for Flod 1 Epoch: 28 [25600/29981                 (85%)]\tLoss: 0.405134\n",
      "Training stage for Flod 1 Epoch: 28 [28800/29981                 (96%)]\tLoss: 0.347074\n",
      "Test set for fold1: Average Loss:           734899.7164, Accuracy: 6662/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 29 [0/29981                 (0%)]\tLoss: 0.344675\n",
      "Training stage for Flod 1 Epoch: 29 [3200/29981                 (11%)]\tLoss: 0.349029\n",
      "Training stage for Flod 1 Epoch: 29 [6400/29981                 (21%)]\tLoss: 0.344901\n",
      "Training stage for Flod 1 Epoch: 29 [9600/29981                 (32%)]\tLoss: 0.375787\n",
      "Training stage for Flod 1 Epoch: 29 [12800/29981                 (43%)]\tLoss: 0.410603\n",
      "Training stage for Flod 1 Epoch: 29 [16000/29981                 (53%)]\tLoss: 0.375914\n",
      "Training stage for Flod 1 Epoch: 29 [19200/29981                 (64%)]\tLoss: 0.407612\n",
      "Training stage for Flod 1 Epoch: 29 [22400/29981                 (75%)]\tLoss: 0.401002\n",
      "Training stage for Flod 1 Epoch: 29 [25600/29981                 (85%)]\tLoss: 0.376607\n",
      "Training stage for Flod 1 Epoch: 29 [28800/29981                 (96%)]\tLoss: 0.402590\n",
      "Test set for fold1: Average Loss:           716105.4525, Accuracy: 6766/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 30 [0/29981                 (0%)]\tLoss: 0.439637\n",
      "Training stage for Flod 1 Epoch: 30 [3200/29981                 (11%)]\tLoss: 0.428807\n",
      "Training stage for Flod 1 Epoch: 30 [6400/29981                 (21%)]\tLoss: 0.498927\n",
      "Training stage for Flod 1 Epoch: 30 [9600/29981                 (32%)]\tLoss: 0.379043\n",
      "Training stage for Flod 1 Epoch: 30 [12800/29981                 (43%)]\tLoss: 0.356821\n",
      "Training stage for Flod 1 Epoch: 30 [16000/29981                 (53%)]\tLoss: 0.319498\n",
      "Training stage for Flod 1 Epoch: 30 [19200/29981                 (64%)]\tLoss: 0.389899\n",
      "Training stage for Flod 1 Epoch: 30 [22400/29981                 (75%)]\tLoss: 0.332206\n",
      "Training stage for Flod 1 Epoch: 30 [25600/29981                 (85%)]\tLoss: 0.385214\n",
      "Training stage for Flod 1 Epoch: 30 [28800/29981                 (96%)]\tLoss: 0.431638\n",
      "Test set for fold1: Average Loss:           745541.9127, Accuracy: 6640/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 31 [0/29981                 (0%)]\tLoss: 0.347308\n",
      "Training stage for Flod 1 Epoch: 31 [3200/29981                 (11%)]\tLoss: 0.363047\n",
      "Training stage for Flod 1 Epoch: 31 [6400/29981                 (21%)]\tLoss: 0.328409\n",
      "Training stage for Flod 1 Epoch: 31 [9600/29981                 (32%)]\tLoss: 0.457195\n",
      "Training stage for Flod 1 Epoch: 31 [12800/29981                 (43%)]\tLoss: 0.347036\n",
      "Training stage for Flod 1 Epoch: 31 [16000/29981                 (53%)]\tLoss: 0.450561\n",
      "Training stage for Flod 1 Epoch: 31 [19200/29981                 (64%)]\tLoss: 0.370008\n",
      "Training stage for Flod 1 Epoch: 31 [22400/29981                 (75%)]\tLoss: 0.314753\n",
      "Training stage for Flod 1 Epoch: 31 [25600/29981                 (85%)]\tLoss: 0.345827\n",
      "Training stage for Flod 1 Epoch: 31 [28800/29981                 (96%)]\tLoss: 0.403878\n",
      "Test set for fold1: Average Loss:           720656.6497, Accuracy: 6732/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 32 [0/29981                 (0%)]\tLoss: 0.344518\n",
      "Training stage for Flod 1 Epoch: 32 [3200/29981                 (11%)]\tLoss: 0.345798\n",
      "Training stage for Flod 1 Epoch: 32 [6400/29981                 (21%)]\tLoss: 0.498379\n",
      "Training stage for Flod 1 Epoch: 32 [9600/29981                 (32%)]\tLoss: 0.406657\n",
      "Training stage for Flod 1 Epoch: 32 [12800/29981                 (43%)]\tLoss: 0.315203\n",
      "Training stage for Flod 1 Epoch: 32 [16000/29981                 (53%)]\tLoss: 0.344753\n",
      "Training stage for Flod 1 Epoch: 32 [19200/29981                 (64%)]\tLoss: 0.360256\n",
      "Training stage for Flod 1 Epoch: 32 [22400/29981                 (75%)]\tLoss: 0.344517\n",
      "Training stage for Flod 1 Epoch: 32 [25600/29981                 (85%)]\tLoss: 0.440980\n",
      "Training stage for Flod 1 Epoch: 32 [28800/29981                 (96%)]\tLoss: 0.344538\n",
      "Test set for fold1: Average Loss:           710253.8458, Accuracy: 6762/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 33 [0/29981                 (0%)]\tLoss: 0.334591\n",
      "Training stage for Flod 1 Epoch: 33 [3200/29981                 (11%)]\tLoss: 0.424025\n",
      "Training stage for Flod 1 Epoch: 33 [6400/29981                 (21%)]\tLoss: 0.346290\n",
      "Training stage for Flod 1 Epoch: 33 [9600/29981                 (32%)]\tLoss: 0.451188\n",
      "Training stage for Flod 1 Epoch: 33 [12800/29981                 (43%)]\tLoss: 0.344966\n",
      "Training stage for Flod 1 Epoch: 33 [16000/29981                 (53%)]\tLoss: 0.328888\n",
      "Training stage for Flod 1 Epoch: 33 [19200/29981                 (64%)]\tLoss: 0.344639\n",
      "Training stage for Flod 1 Epoch: 33 [22400/29981                 (75%)]\tLoss: 0.447545\n",
      "Training stage for Flod 1 Epoch: 33 [25600/29981                 (85%)]\tLoss: 0.380817\n",
      "Training stage for Flod 1 Epoch: 33 [28800/29981                 (96%)]\tLoss: 0.363575\n",
      "Test set for fold1: Average Loss:           710787.3322, Accuracy: 6763/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 34 [0/29981                 (0%)]\tLoss: 0.337853\n",
      "Training stage for Flod 1 Epoch: 34 [3200/29981                 (11%)]\tLoss: 0.313378\n",
      "Training stage for Flod 1 Epoch: 34 [6400/29981                 (21%)]\tLoss: 0.346142\n",
      "Training stage for Flod 1 Epoch: 34 [9600/29981                 (32%)]\tLoss: 0.344866\n",
      "Training stage for Flod 1 Epoch: 34 [12800/29981                 (43%)]\tLoss: 0.405667\n",
      "Training stage for Flod 1 Epoch: 34 [16000/29981                 (53%)]\tLoss: 0.371588\n",
      "Training stage for Flod 1 Epoch: 34 [19200/29981                 (64%)]\tLoss: 0.380892\n",
      "Training stage for Flod 1 Epoch: 34 [22400/29981                 (75%)]\tLoss: 0.407454\n",
      "Training stage for Flod 1 Epoch: 34 [25600/29981                 (85%)]\tLoss: 0.348544\n",
      "Training stage for Flod 1 Epoch: 34 [28800/29981                 (96%)]\tLoss: 0.344719\n",
      "Test set for fold1: Average Loss:           717650.1350, Accuracy: 6737/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 35 [0/29981                 (0%)]\tLoss: 0.522362\n",
      "Training stage for Flod 1 Epoch: 35 [3200/29981                 (11%)]\tLoss: 0.313591\n",
      "Training stage for Flod 1 Epoch: 35 [6400/29981                 (21%)]\tLoss: 0.356438\n",
      "Training stage for Flod 1 Epoch: 35 [9600/29981                 (32%)]\tLoss: 0.377725\n",
      "Training stage for Flod 1 Epoch: 35 [12800/29981                 (43%)]\tLoss: 0.314207\n",
      "Training stage for Flod 1 Epoch: 35 [16000/29981                 (53%)]\tLoss: 0.357273\n",
      "Training stage for Flod 1 Epoch: 35 [19200/29981                 (64%)]\tLoss: 0.433208\n",
      "Training stage for Flod 1 Epoch: 35 [22400/29981                 (75%)]\tLoss: 0.439337\n",
      "Training stage for Flod 1 Epoch: 35 [25600/29981                 (85%)]\tLoss: 0.398715\n",
      "Training stage for Flod 1 Epoch: 35 [28800/29981                 (96%)]\tLoss: 0.418677\n",
      "Test set for fold1: Average Loss:           773537.4053, Accuracy: 6507/7495           (87%)\n",
      "Training stage for Flod 1 Epoch: 36 [0/29981                 (0%)]\tLoss: 0.362879\n",
      "Training stage for Flod 1 Epoch: 36 [3200/29981                 (11%)]\tLoss: 0.391898\n",
      "Training stage for Flod 1 Epoch: 36 [6400/29981                 (21%)]\tLoss: 0.426214\n",
      "Training stage for Flod 1 Epoch: 36 [9600/29981                 (32%)]\tLoss: 0.376587\n",
      "Training stage for Flod 1 Epoch: 36 [12800/29981                 (43%)]\tLoss: 0.313530\n",
      "Training stage for Flod 1 Epoch: 36 [16000/29981                 (53%)]\tLoss: 0.317005\n",
      "Training stage for Flod 1 Epoch: 36 [19200/29981                 (64%)]\tLoss: 0.314330\n",
      "Training stage for Flod 1 Epoch: 36 [22400/29981                 (75%)]\tLoss: 0.433163\n",
      "Training stage for Flod 1 Epoch: 36 [25600/29981                 (85%)]\tLoss: 0.411228\n",
      "Training stage for Flod 1 Epoch: 36 [28800/29981                 (96%)]\tLoss: 0.477493\n",
      "Test set for fold1: Average Loss:           705884.7245, Accuracy: 6802/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 37 [0/29981                 (0%)]\tLoss: 0.313763\n",
      "Training stage for Flod 1 Epoch: 37 [3200/29981                 (11%)]\tLoss: 0.365598\n",
      "Training stage for Flod 1 Epoch: 37 [6400/29981                 (21%)]\tLoss: 0.313730\n",
      "Training stage for Flod 1 Epoch: 37 [9600/29981                 (32%)]\tLoss: 0.369568\n",
      "Training stage for Flod 1 Epoch: 37 [12800/29981                 (43%)]\tLoss: 0.408368\n",
      "Training stage for Flod 1 Epoch: 37 [16000/29981                 (53%)]\tLoss: 0.345118\n",
      "Training stage for Flod 1 Epoch: 37 [19200/29981                 (64%)]\tLoss: 0.375845\n",
      "Training stage for Flod 1 Epoch: 37 [22400/29981                 (75%)]\tLoss: 0.512617\n",
      "Training stage for Flod 1 Epoch: 37 [25600/29981                 (85%)]\tLoss: 0.407670\n",
      "Training stage for Flod 1 Epoch: 37 [28800/29981                 (96%)]\tLoss: 0.377661\n",
      "Test set for fold1: Average Loss:           695655.3280, Accuracy: 6856/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 38 [0/29981                 (0%)]\tLoss: 0.420993\n",
      "Training stage for Flod 1 Epoch: 38 [3200/29981                 (11%)]\tLoss: 0.313287\n",
      "Training stage for Flod 1 Epoch: 38 [6400/29981                 (21%)]\tLoss: 0.344521\n",
      "Training stage for Flod 1 Epoch: 38 [9600/29981                 (32%)]\tLoss: 0.313438\n",
      "Training stage for Flod 1 Epoch: 38 [12800/29981                 (43%)]\tLoss: 0.411153\n",
      "Training stage for Flod 1 Epoch: 38 [16000/29981                 (53%)]\tLoss: 0.406956\n",
      "Training stage for Flod 1 Epoch: 38 [19200/29981                 (64%)]\tLoss: 0.400409\n",
      "Training stage for Flod 1 Epoch: 38 [22400/29981                 (75%)]\tLoss: 0.406903\n",
      "Training stage for Flod 1 Epoch: 38 [25600/29981                 (85%)]\tLoss: 0.455727\n",
      "Training stage for Flod 1 Epoch: 38 [28800/29981                 (96%)]\tLoss: 0.431915\n",
      "Test set for fold1: Average Loss:           717297.1136, Accuracy: 6735/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 39 [0/29981                 (0%)]\tLoss: 0.348593\n",
      "Training stage for Flod 1 Epoch: 39 [3200/29981                 (11%)]\tLoss: 0.497049\n",
      "Training stage for Flod 1 Epoch: 39 [6400/29981                 (21%)]\tLoss: 0.344519\n",
      "Training stage for Flod 1 Epoch: 39 [9600/29981                 (32%)]\tLoss: 0.352676\n",
      "Training stage for Flod 1 Epoch: 39 [12800/29981                 (43%)]\tLoss: 0.422113\n",
      "Training stage for Flod 1 Epoch: 39 [16000/29981                 (53%)]\tLoss: 0.375517\n",
      "Training stage for Flod 1 Epoch: 39 [19200/29981                 (64%)]\tLoss: 0.431877\n",
      "Training stage for Flod 1 Epoch: 39 [22400/29981                 (75%)]\tLoss: 0.402577\n",
      "Training stage for Flod 1 Epoch: 39 [25600/29981                 (85%)]\tLoss: 0.346858\n",
      "Training stage for Flod 1 Epoch: 39 [28800/29981                 (96%)]\tLoss: 0.375154\n",
      "Test set for fold1: Average Loss:           713617.3130, Accuracy: 6771/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 40 [0/29981                 (0%)]\tLoss: 0.407951\n",
      "Training stage for Flod 1 Epoch: 40 [3200/29981                 (11%)]\tLoss: 0.316121\n",
      "Training stage for Flod 1 Epoch: 40 [6400/29981                 (21%)]\tLoss: 0.403459\n",
      "Training stage for Flod 1 Epoch: 40 [9600/29981                 (32%)]\tLoss: 0.377071\n",
      "Training stage for Flod 1 Epoch: 40 [12800/29981                 (43%)]\tLoss: 0.345369\n",
      "Training stage for Flod 1 Epoch: 40 [16000/29981                 (53%)]\tLoss: 0.410282\n",
      "Training stage for Flod 1 Epoch: 40 [19200/29981                 (64%)]\tLoss: 0.369656\n",
      "Training stage for Flod 1 Epoch: 40 [22400/29981                 (75%)]\tLoss: 0.377635\n",
      "Training stage for Flod 1 Epoch: 40 [25600/29981                 (85%)]\tLoss: 0.355050\n",
      "Training stage for Flod 1 Epoch: 40 [28800/29981                 (96%)]\tLoss: 0.342259\n",
      "Test set for fold1: Average Loss:           720073.1386, Accuracy: 6746/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 41 [0/29981                 (0%)]\tLoss: 0.450405\n",
      "Training stage for Flod 1 Epoch: 41 [3200/29981                 (11%)]\tLoss: 0.378641\n",
      "Training stage for Flod 1 Epoch: 41 [6400/29981                 (21%)]\tLoss: 0.392942\n",
      "Training stage for Flod 1 Epoch: 41 [9600/29981                 (32%)]\tLoss: 0.402109\n",
      "Training stage for Flod 1 Epoch: 41 [12800/29981                 (43%)]\tLoss: 0.353972\n",
      "Training stage for Flod 1 Epoch: 41 [16000/29981                 (53%)]\tLoss: 0.347581\n",
      "Training stage for Flod 1 Epoch: 41 [19200/29981                 (64%)]\tLoss: 0.344556\n",
      "Training stage for Flod 1 Epoch: 41 [22400/29981                 (75%)]\tLoss: 0.322697\n",
      "Training stage for Flod 1 Epoch: 41 [25600/29981                 (85%)]\tLoss: 0.375730\n",
      "Training stage for Flod 1 Epoch: 41 [28800/29981                 (96%)]\tLoss: 0.329291\n",
      "Test set for fold1: Average Loss:           712479.1636, Accuracy: 6786/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 42 [0/29981                 (0%)]\tLoss: 0.322131\n",
      "Training stage for Flod 1 Epoch: 42 [3200/29981                 (11%)]\tLoss: 0.377059\n",
      "Training stage for Flod 1 Epoch: 42 [6400/29981                 (21%)]\tLoss: 0.437108\n",
      "Training stage for Flod 1 Epoch: 42 [9600/29981                 (32%)]\tLoss: 0.344523\n",
      "Training stage for Flod 1 Epoch: 42 [12800/29981                 (43%)]\tLoss: 0.344887\n",
      "Training stage for Flod 1 Epoch: 42 [16000/29981                 (53%)]\tLoss: 0.398233\n",
      "Training stage for Flod 1 Epoch: 42 [19200/29981                 (64%)]\tLoss: 0.399085\n",
      "Training stage for Flod 1 Epoch: 42 [22400/29981                 (75%)]\tLoss: 0.365465\n",
      "Training stage for Flod 1 Epoch: 42 [25600/29981                 (85%)]\tLoss: 0.314067\n",
      "Training stage for Flod 1 Epoch: 42 [28800/29981                 (96%)]\tLoss: 0.385175\n",
      "Test set for fold1: Average Loss:           695049.1716, Accuracy: 6861/7495           (92%)\n",
      "Training stage for Flod 1 Epoch: 43 [0/29981                 (0%)]\tLoss: 0.379390\n",
      "Training stage for Flod 1 Epoch: 43 [3200/29981                 (11%)]\tLoss: 0.345678\n",
      "Training stage for Flod 1 Epoch: 43 [6400/29981                 (21%)]\tLoss: 0.354463\n",
      "Training stage for Flod 1 Epoch: 43 [9600/29981                 (32%)]\tLoss: 0.438011\n",
      "Training stage for Flod 1 Epoch: 43 [12800/29981                 (43%)]\tLoss: 0.344539\n",
      "Training stage for Flod 1 Epoch: 43 [16000/29981                 (53%)]\tLoss: 0.353470\n",
      "Training stage for Flod 1 Epoch: 43 [19200/29981                 (64%)]\tLoss: 0.451908\n",
      "Training stage for Flod 1 Epoch: 43 [22400/29981                 (75%)]\tLoss: 0.407017\n",
      "Training stage for Flod 1 Epoch: 43 [25600/29981                 (85%)]\tLoss: 0.344496\n",
      "Training stage for Flod 1 Epoch: 43 [28800/29981                 (96%)]\tLoss: 0.403398\n",
      "Test set for fold1: Average Loss:           716805.1391, Accuracy: 6760/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 44 [0/29981                 (0%)]\tLoss: 0.376087\n",
      "Training stage for Flod 1 Epoch: 44 [3200/29981                 (11%)]\tLoss: 0.344786\n",
      "Training stage for Flod 1 Epoch: 44 [6400/29981                 (21%)]\tLoss: 0.406621\n",
      "Training stage for Flod 1 Epoch: 44 [9600/29981                 (32%)]\tLoss: 0.421847\n",
      "Training stage for Flod 1 Epoch: 44 [12800/29981                 (43%)]\tLoss: 0.348833\n",
      "Training stage for Flod 1 Epoch: 44 [16000/29981                 (53%)]\tLoss: 0.377371\n",
      "Training stage for Flod 1 Epoch: 44 [19200/29981                 (64%)]\tLoss: 0.375381\n",
      "Training stage for Flod 1 Epoch: 44 [22400/29981                 (75%)]\tLoss: 0.344516\n",
      "Training stage for Flod 1 Epoch: 44 [25600/29981                 (85%)]\tLoss: 0.405857\n",
      "Training stage for Flod 1 Epoch: 44 [28800/29981                 (96%)]\tLoss: 0.375765\n",
      "Test set for fold1: Average Loss:           736405.1670, Accuracy: 6702/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 45 [0/29981                 (0%)]\tLoss: 0.350596\n",
      "Training stage for Flod 1 Epoch: 45 [3200/29981                 (11%)]\tLoss: 0.412918\n",
      "Training stage for Flod 1 Epoch: 45 [6400/29981                 (21%)]\tLoss: 0.381996\n",
      "Training stage for Flod 1 Epoch: 45 [9600/29981                 (32%)]\tLoss: 0.314129\n",
      "Training stage for Flod 1 Epoch: 45 [12800/29981                 (43%)]\tLoss: 0.365949\n",
      "Training stage for Flod 1 Epoch: 45 [16000/29981                 (53%)]\tLoss: 0.346818\n",
      "Training stage for Flod 1 Epoch: 45 [19200/29981                 (64%)]\tLoss: 0.326739\n",
      "Training stage for Flod 1 Epoch: 45 [22400/29981                 (75%)]\tLoss: 0.320144\n",
      "Training stage for Flod 1 Epoch: 45 [25600/29981                 (85%)]\tLoss: 0.371239\n",
      "Training stage for Flod 1 Epoch: 45 [28800/29981                 (96%)]\tLoss: 0.379046\n",
      "Test set for fold1: Average Loss:           719790.3995, Accuracy: 6755/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 46 [0/29981                 (0%)]\tLoss: 0.367944\n",
      "Training stage for Flod 1 Epoch: 46 [3200/29981                 (11%)]\tLoss: 0.414782\n",
      "Training stage for Flod 1 Epoch: 46 [6400/29981                 (21%)]\tLoss: 0.437471\n",
      "Training stage for Flod 1 Epoch: 46 [9600/29981                 (32%)]\tLoss: 0.313758\n",
      "Training stage for Flod 1 Epoch: 46 [12800/29981                 (43%)]\tLoss: 0.391099\n",
      "Training stage for Flod 1 Epoch: 46 [16000/29981                 (53%)]\tLoss: 0.344514\n",
      "Training stage for Flod 1 Epoch: 46 [19200/29981                 (64%)]\tLoss: 0.471803\n",
      "Training stage for Flod 1 Epoch: 46 [22400/29981                 (75%)]\tLoss: 0.347643\n",
      "Training stage for Flod 1 Epoch: 46 [25600/29981                 (85%)]\tLoss: 0.374760\n",
      "Training stage for Flod 1 Epoch: 46 [28800/29981                 (96%)]\tLoss: 0.438262\n",
      "Test set for fold1: Average Loss:           744996.5610, Accuracy: 6641/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 47 [0/29981                 (0%)]\tLoss: 0.375803\n",
      "Training stage for Flod 1 Epoch: 47 [3200/29981                 (11%)]\tLoss: 0.344527\n",
      "Training stage for Flod 1 Epoch: 47 [6400/29981                 (21%)]\tLoss: 0.328252\n",
      "Training stage for Flod 1 Epoch: 47 [9600/29981                 (32%)]\tLoss: 0.370228\n",
      "Training stage for Flod 1 Epoch: 47 [12800/29981                 (43%)]\tLoss: 0.394594\n",
      "Training stage for Flod 1 Epoch: 47 [16000/29981                 (53%)]\tLoss: 0.344357\n",
      "Training stage for Flod 1 Epoch: 47 [19200/29981                 (64%)]\tLoss: 0.376403\n",
      "Training stage for Flod 1 Epoch: 47 [22400/29981                 (75%)]\tLoss: 0.382546\n",
      "Training stage for Flod 1 Epoch: 47 [25600/29981                 (85%)]\tLoss: 0.348128\n",
      "Training stage for Flod 1 Epoch: 47 [28800/29981                 (96%)]\tLoss: 0.459228\n",
      "Test set for fold1: Average Loss:           741217.8906, Accuracy: 6612/7495           (88%)\n",
      "Training stage for Flod 1 Epoch: 48 [0/29981                 (0%)]\tLoss: 0.414018\n",
      "Training stage for Flod 1 Epoch: 48 [3200/29981                 (11%)]\tLoss: 0.405657\n",
      "Training stage for Flod 1 Epoch: 48 [6400/29981                 (21%)]\tLoss: 0.315769\n",
      "Training stage for Flod 1 Epoch: 48 [9600/29981                 (32%)]\tLoss: 0.406189\n",
      "Training stage for Flod 1 Epoch: 48 [12800/29981                 (43%)]\tLoss: 0.386174\n",
      "Training stage for Flod 1 Epoch: 48 [16000/29981                 (53%)]\tLoss: 0.346999\n",
      "Training stage for Flod 1 Epoch: 48 [19200/29981                 (64%)]\tLoss: 0.367237\n",
      "Training stage for Flod 1 Epoch: 48 [22400/29981                 (75%)]\tLoss: 0.373458\n",
      "Training stage for Flod 1 Epoch: 48 [25600/29981                 (85%)]\tLoss: 0.344942\n",
      "Training stage for Flod 1 Epoch: 48 [28800/29981                 (96%)]\tLoss: 0.313404\n",
      "Test set for fold1: Average Loss:           708786.4366, Accuracy: 6790/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 49 [0/29981                 (0%)]\tLoss: 0.372091\n",
      "Training stage for Flod 1 Epoch: 49 [3200/29981                 (11%)]\tLoss: 0.407228\n",
      "Training stage for Flod 1 Epoch: 49 [6400/29981                 (21%)]\tLoss: 0.344522\n",
      "Training stage for Flod 1 Epoch: 49 [9600/29981                 (32%)]\tLoss: 0.409132\n",
      "Training stage for Flod 1 Epoch: 49 [12800/29981                 (43%)]\tLoss: 0.348128\n",
      "Training stage for Flod 1 Epoch: 49 [16000/29981                 (53%)]\tLoss: 0.345311\n",
      "Training stage for Flod 1 Epoch: 49 [19200/29981                 (64%)]\tLoss: 0.386421\n",
      "Training stage for Flod 1 Epoch: 49 [22400/29981                 (75%)]\tLoss: 0.399507\n",
      "Training stage for Flod 1 Epoch: 49 [25600/29981                 (85%)]\tLoss: 0.313713\n",
      "Training stage for Flod 1 Epoch: 49 [28800/29981                 (96%)]\tLoss: 0.371677\n",
      "Test set for fold1: Average Loss:           739505.1388, Accuracy: 6650/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 50 [0/29981                 (0%)]\tLoss: 0.410243\n",
      "Training stage for Flod 1 Epoch: 50 [3200/29981                 (11%)]\tLoss: 0.344651\n",
      "Training stage for Flod 1 Epoch: 50 [6400/29981                 (21%)]\tLoss: 0.376714\n",
      "Training stage for Flod 1 Epoch: 50 [9600/29981                 (32%)]\tLoss: 0.341702\n",
      "Training stage for Flod 1 Epoch: 50 [12800/29981                 (43%)]\tLoss: 0.406761\n",
      "Training stage for Flod 1 Epoch: 50 [16000/29981                 (53%)]\tLoss: 0.313756\n",
      "Training stage for Flod 1 Epoch: 50 [19200/29981                 (64%)]\tLoss: 0.344669\n",
      "Training stage for Flod 1 Epoch: 50 [22400/29981                 (75%)]\tLoss: 0.419938\n",
      "Training stage for Flod 1 Epoch: 50 [25600/29981                 (85%)]\tLoss: 0.422782\n",
      "Training stage for Flod 1 Epoch: 50 [28800/29981                 (96%)]\tLoss: 0.336810\n",
      "Test set for fold1: Average Loss:           715068.2031, Accuracy: 6774/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 51 [0/29981                 (0%)]\tLoss: 0.375846\n",
      "Training stage for Flod 1 Epoch: 51 [3200/29981                 (11%)]\tLoss: 0.408947\n",
      "Training stage for Flod 1 Epoch: 51 [6400/29981                 (21%)]\tLoss: 0.344723\n",
      "Training stage for Flod 1 Epoch: 51 [9600/29981                 (32%)]\tLoss: 0.417404\n",
      "Training stage for Flod 1 Epoch: 51 [12800/29981                 (43%)]\tLoss: 0.384913\n",
      "Training stage for Flod 1 Epoch: 51 [16000/29981                 (53%)]\tLoss: 0.432820\n",
      "Training stage for Flod 1 Epoch: 51 [19200/29981                 (64%)]\tLoss: 0.375795\n",
      "Training stage for Flod 1 Epoch: 51 [22400/29981                 (75%)]\tLoss: 0.407322\n",
      "Training stage for Flod 1 Epoch: 51 [25600/29981                 (85%)]\tLoss: 0.344539\n",
      "Training stage for Flod 1 Epoch: 51 [28800/29981                 (96%)]\tLoss: 0.375755\n",
      "Test set for fold1: Average Loss:           720910.2894, Accuracy: 6722/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 52 [0/29981                 (0%)]\tLoss: 0.442114\n",
      "Training stage for Flod 1 Epoch: 52 [3200/29981                 (11%)]\tLoss: 0.391414\n",
      "Training stage for Flod 1 Epoch: 52 [6400/29981                 (21%)]\tLoss: 0.344700\n",
      "Training stage for Flod 1 Epoch: 52 [9600/29981                 (32%)]\tLoss: 0.344912\n",
      "Training stage for Flod 1 Epoch: 52 [12800/29981                 (43%)]\tLoss: 0.315255\n",
      "Training stage for Flod 1 Epoch: 52 [16000/29981                 (53%)]\tLoss: 0.407029\n",
      "Training stage for Flod 1 Epoch: 52 [19200/29981                 (64%)]\tLoss: 0.351300\n",
      "Training stage for Flod 1 Epoch: 52 [22400/29981                 (75%)]\tLoss: 0.428379\n",
      "Training stage for Flod 1 Epoch: 52 [25600/29981                 (85%)]\tLoss: 0.405785\n",
      "Training stage for Flod 1 Epoch: 52 [28800/29981                 (96%)]\tLoss: 0.343249\n",
      "Test set for fold1: Average Loss:           716208.0034, Accuracy: 6762/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 53 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 53 [3200/29981                 (11%)]\tLoss: 0.340934\n",
      "Training stage for Flod 1 Epoch: 53 [6400/29981                 (21%)]\tLoss: 0.375764\n",
      "Training stage for Flod 1 Epoch: 53 [9600/29981                 (32%)]\tLoss: 0.438294\n",
      "Training stage for Flod 1 Epoch: 53 [12800/29981                 (43%)]\tLoss: 0.374914\n",
      "Training stage for Flod 1 Epoch: 53 [16000/29981                 (53%)]\tLoss: 0.336102\n",
      "Training stage for Flod 1 Epoch: 53 [19200/29981                 (64%)]\tLoss: 0.323189\n",
      "Training stage for Flod 1 Epoch: 53 [22400/29981                 (75%)]\tLoss: 0.407161\n",
      "Training stage for Flod 1 Epoch: 53 [25600/29981                 (85%)]\tLoss: 0.332962\n",
      "Training stage for Flod 1 Epoch: 53 [28800/29981                 (96%)]\tLoss: 0.344708\n",
      "Test set for fold1: Average Loss:           717074.7322, Accuracy: 6760/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 54 [0/29981                 (0%)]\tLoss: 0.348213\n",
      "Training stage for Flod 1 Epoch: 54 [3200/29981                 (11%)]\tLoss: 0.370870\n",
      "Training stage for Flod 1 Epoch: 54 [6400/29981                 (21%)]\tLoss: 0.367784\n",
      "Training stage for Flod 1 Epoch: 54 [9600/29981                 (32%)]\tLoss: 0.373655\n",
      "Training stage for Flod 1 Epoch: 54 [12800/29981                 (43%)]\tLoss: 0.313264\n",
      "Training stage for Flod 1 Epoch: 54 [16000/29981                 (53%)]\tLoss: 0.376310\n",
      "Training stage for Flod 1 Epoch: 54 [19200/29981                 (64%)]\tLoss: 0.459356\n",
      "Training stage for Flod 1 Epoch: 54 [22400/29981                 (75%)]\tLoss: 0.344571\n",
      "Training stage for Flod 1 Epoch: 54 [25600/29981                 (85%)]\tLoss: 0.407080\n",
      "Training stage for Flod 1 Epoch: 54 [28800/29981                 (96%)]\tLoss: 0.375485\n",
      "Test set for fold1: Average Loss:           741702.4866, Accuracy: 6665/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 55 [0/29981                 (0%)]\tLoss: 0.350076\n",
      "Training stage for Flod 1 Epoch: 55 [3200/29981                 (11%)]\tLoss: 0.377813\n",
      "Training stage for Flod 1 Epoch: 55 [6400/29981                 (21%)]\tLoss: 0.395769\n",
      "Training stage for Flod 1 Epoch: 55 [9600/29981                 (32%)]\tLoss: 0.406597\n",
      "Training stage for Flod 1 Epoch: 55 [12800/29981                 (43%)]\tLoss: 0.344593\n",
      "Training stage for Flod 1 Epoch: 55 [16000/29981                 (53%)]\tLoss: 0.349964\n",
      "Training stage for Flod 1 Epoch: 55 [19200/29981                 (64%)]\tLoss: 0.332813\n",
      "Training stage for Flod 1 Epoch: 55 [22400/29981                 (75%)]\tLoss: 0.390289\n",
      "Training stage for Flod 1 Epoch: 55 [25600/29981                 (85%)]\tLoss: 0.344588\n",
      "Training stage for Flod 1 Epoch: 55 [28800/29981                 (96%)]\tLoss: 0.438362\n",
      "Test set for fold1: Average Loss:           763430.3302, Accuracy: 6567/7495           (88%)\n",
      "Training stage for Flod 1 Epoch: 56 [0/29981                 (0%)]\tLoss: 0.375763\n",
      "Training stage for Flod 1 Epoch: 56 [3200/29981                 (11%)]\tLoss: 0.313273\n",
      "Training stage for Flod 1 Epoch: 56 [6400/29981                 (21%)]\tLoss: 0.370697\n",
      "Training stage for Flod 1 Epoch: 56 [9600/29981                 (32%)]\tLoss: 0.435990\n",
      "Training stage for Flod 1 Epoch: 56 [12800/29981                 (43%)]\tLoss: 0.346231\n",
      "Training stage for Flod 1 Epoch: 56 [16000/29981                 (53%)]\tLoss: 0.353555\n",
      "Training stage for Flod 1 Epoch: 56 [19200/29981                 (64%)]\tLoss: 0.407012\n",
      "Training stage for Flod 1 Epoch: 56 [22400/29981                 (75%)]\tLoss: 0.381475\n",
      "Training stage for Flod 1 Epoch: 56 [25600/29981                 (85%)]\tLoss: 0.375230\n",
      "Training stage for Flod 1 Epoch: 56 [28800/29981                 (96%)]\tLoss: 0.465711\n",
      "Test set for fold1: Average Loss:           724445.3265, Accuracy: 6728/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 57 [0/29981                 (0%)]\tLoss: 0.345573\n",
      "Training stage for Flod 1 Epoch: 57 [3200/29981                 (11%)]\tLoss: 0.374193\n",
      "Training stage for Flod 1 Epoch: 57 [6400/29981                 (21%)]\tLoss: 0.384532\n",
      "Training stage for Flod 1 Epoch: 57 [9600/29981                 (32%)]\tLoss: 0.376168\n",
      "Training stage for Flod 1 Epoch: 57 [12800/29981                 (43%)]\tLoss: 0.410133\n",
      "Training stage for Flod 1 Epoch: 57 [16000/29981                 (53%)]\tLoss: 0.354791\n",
      "Training stage for Flod 1 Epoch: 57 [19200/29981                 (64%)]\tLoss: 0.393395\n",
      "Training stage for Flod 1 Epoch: 57 [22400/29981                 (75%)]\tLoss: 0.375741\n",
      "Training stage for Flod 1 Epoch: 57 [25600/29981                 (85%)]\tLoss: 0.347766\n",
      "Training stage for Flod 1 Epoch: 57 [28800/29981                 (96%)]\tLoss: 0.313977\n",
      "Test set for fold1: Average Loss:           717816.6435, Accuracy: 6752/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 58 [0/29981                 (0%)]\tLoss: 0.344608\n",
      "Training stage for Flod 1 Epoch: 58 [3200/29981                 (11%)]\tLoss: 0.336562\n",
      "Training stage for Flod 1 Epoch: 58 [6400/29981                 (21%)]\tLoss: 0.368731\n",
      "Training stage for Flod 1 Epoch: 58 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 58 [12800/29981                 (43%)]\tLoss: 0.323398\n",
      "Training stage for Flod 1 Epoch: 58 [16000/29981                 (53%)]\tLoss: 0.382451\n",
      "Training stage for Flod 1 Epoch: 58 [19200/29981                 (64%)]\tLoss: 0.470426\n",
      "Training stage for Flod 1 Epoch: 58 [22400/29981                 (75%)]\tLoss: 0.447194\n",
      "Training stage for Flod 1 Epoch: 58 [25600/29981                 (85%)]\tLoss: 0.313374\n",
      "Training stage for Flod 1 Epoch: 58 [28800/29981                 (96%)]\tLoss: 0.344664\n",
      "Test set for fold1: Average Loss:           721189.5678, Accuracy: 6741/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 59 [0/29981                 (0%)]\tLoss: 0.344524\n",
      "Training stage for Flod 1 Epoch: 59 [3200/29981                 (11%)]\tLoss: 0.375882\n",
      "Training stage for Flod 1 Epoch: 59 [6400/29981                 (21%)]\tLoss: 0.389373\n",
      "Training stage for Flod 1 Epoch: 59 [9600/29981                 (32%)]\tLoss: 0.409113\n",
      "Training stage for Flod 1 Epoch: 59 [12800/29981                 (43%)]\tLoss: 0.375807\n",
      "Training stage for Flod 1 Epoch: 59 [16000/29981                 (53%)]\tLoss: 0.313317\n",
      "Training stage for Flod 1 Epoch: 59 [19200/29981                 (64%)]\tLoss: 0.385037\n",
      "Training stage for Flod 1 Epoch: 59 [22400/29981                 (75%)]\tLoss: 0.375704\n",
      "Training stage for Flod 1 Epoch: 59 [25600/29981                 (85%)]\tLoss: 0.348532\n",
      "Training stage for Flod 1 Epoch: 59 [28800/29981                 (96%)]\tLoss: 0.377198\n",
      "Test set for fold1: Average Loss:           746474.6896, Accuracy: 6631/7495           (88%)\n",
      "Training stage for Flod 1 Epoch: 60 [0/29981                 (0%)]\tLoss: 0.375760\n",
      "Training stage for Flod 1 Epoch: 60 [3200/29981                 (11%)]\tLoss: 0.407834\n",
      "Training stage for Flod 1 Epoch: 60 [6400/29981                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 60 [9600/29981                 (32%)]\tLoss: 0.391488\n",
      "Training stage for Flod 1 Epoch: 60 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 60 [16000/29981                 (53%)]\tLoss: 0.375910\n",
      "Training stage for Flod 1 Epoch: 60 [19200/29981                 (64%)]\tLoss: 0.376146\n",
      "Training stage for Flod 1 Epoch: 60 [22400/29981                 (75%)]\tLoss: 0.376311\n",
      "Training stage for Flod 1 Epoch: 60 [25600/29981                 (85%)]\tLoss: 0.375779\n",
      "Training stage for Flod 1 Epoch: 60 [28800/29981                 (96%)]\tLoss: 0.411329\n",
      "Test set for fold1: Average Loss:           699352.1660, Accuracy: 6851/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 61 [0/29981                 (0%)]\tLoss: 0.375697\n",
      "Training stage for Flod 1 Epoch: 61 [3200/29981                 (11%)]\tLoss: 0.313331\n",
      "Training stage for Flod 1 Epoch: 61 [6400/29981                 (21%)]\tLoss: 0.375859\n",
      "Training stage for Flod 1 Epoch: 61 [9600/29981                 (32%)]\tLoss: 0.313898\n",
      "Training stage for Flod 1 Epoch: 61 [12800/29981                 (43%)]\tLoss: 0.344648\n",
      "Training stage for Flod 1 Epoch: 61 [16000/29981                 (53%)]\tLoss: 0.378578\n",
      "Training stage for Flod 1 Epoch: 61 [19200/29981                 (64%)]\tLoss: 0.314265\n",
      "Training stage for Flod 1 Epoch: 61 [22400/29981                 (75%)]\tLoss: 0.406735\n",
      "Training stage for Flod 1 Epoch: 61 [25600/29981                 (85%)]\tLoss: 0.344499\n",
      "Training stage for Flod 1 Epoch: 61 [28800/29981                 (96%)]\tLoss: 0.374671\n",
      "Test set for fold1: Average Loss:           712627.8387, Accuracy: 6786/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 62 [0/29981                 (0%)]\tLoss: 0.376288\n",
      "Training stage for Flod 1 Epoch: 62 [3200/29981                 (11%)]\tLoss: 0.313269\n",
      "Training stage for Flod 1 Epoch: 62 [6400/29981                 (21%)]\tLoss: 0.344516\n",
      "Training stage for Flod 1 Epoch: 62 [9600/29981                 (32%)]\tLoss: 0.407112\n",
      "Training stage for Flod 1 Epoch: 62 [12800/29981                 (43%)]\tLoss: 0.313345\n",
      "Training stage for Flod 1 Epoch: 62 [16000/29981                 (53%)]\tLoss: 0.347037\n",
      "Training stage for Flod 1 Epoch: 62 [19200/29981                 (64%)]\tLoss: 0.313421\n",
      "Training stage for Flod 1 Epoch: 62 [22400/29981                 (75%)]\tLoss: 0.381341\n",
      "Training stage for Flod 1 Epoch: 62 [25600/29981                 (85%)]\tLoss: 0.344545\n",
      "Training stage for Flod 1 Epoch: 62 [28800/29981                 (96%)]\tLoss: 0.406680\n",
      "Test set for fold1: Average Loss:           704849.0402, Accuracy: 6801/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 63 [0/29981                 (0%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 63 [3200/29981                 (11%)]\tLoss: 0.313308\n",
      "Training stage for Flod 1 Epoch: 63 [6400/29981                 (21%)]\tLoss: 0.378796\n",
      "Training stage for Flod 1 Epoch: 63 [9600/29981                 (32%)]\tLoss: 0.407012\n",
      "Training stage for Flod 1 Epoch: 63 [12800/29981                 (43%)]\tLoss: 0.394526\n",
      "Training stage for Flod 1 Epoch: 63 [16000/29981                 (53%)]\tLoss: 0.394737\n",
      "Training stage for Flod 1 Epoch: 63 [19200/29981                 (64%)]\tLoss: 0.315289\n",
      "Training stage for Flod 1 Epoch: 63 [22400/29981                 (75%)]\tLoss: 0.313296\n",
      "Training stage for Flod 1 Epoch: 63 [25600/29981                 (85%)]\tLoss: 0.346515\n",
      "Training stage for Flod 1 Epoch: 63 [28800/29981                 (96%)]\tLoss: 0.359744\n",
      "Test set for fold1: Average Loss:           681416.6870, Accuracy: 6912/7495           (92%)\n",
      "Training stage for Flod 1 Epoch: 64 [0/29981                 (0%)]\tLoss: 0.353239\n",
      "Training stage for Flod 1 Epoch: 64 [3200/29981                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 64 [6400/29981                 (21%)]\tLoss: 0.407186\n",
      "Training stage for Flod 1 Epoch: 64 [9600/29981                 (32%)]\tLoss: 0.346014\n",
      "Training stage for Flod 1 Epoch: 64 [12800/29981                 (43%)]\tLoss: 0.375804\n",
      "Training stage for Flod 1 Epoch: 64 [16000/29981                 (53%)]\tLoss: 0.313281\n",
      "Training stage for Flod 1 Epoch: 64 [19200/29981                 (64%)]\tLoss: 0.360358\n",
      "Training stage for Flod 1 Epoch: 64 [22400/29981                 (75%)]\tLoss: 0.377289\n",
      "Training stage for Flod 1 Epoch: 64 [25600/29981                 (85%)]\tLoss: 0.375665\n",
      "Training stage for Flod 1 Epoch: 64 [28800/29981                 (96%)]\tLoss: 0.353933\n",
      "Test set for fold1: Average Loss:           697120.7983, Accuracy: 6853/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 65 [0/29981                 (0%)]\tLoss: 0.345137\n",
      "Training stage for Flod 1 Epoch: 65 [3200/29981                 (11%)]\tLoss: 0.407090\n",
      "Training stage for Flod 1 Epoch: 65 [6400/29981                 (21%)]\tLoss: 0.375195\n",
      "Training stage for Flod 1 Epoch: 65 [9600/29981                 (32%)]\tLoss: 0.373827\n",
      "Training stage for Flod 1 Epoch: 65 [12800/29981                 (43%)]\tLoss: 0.406666\n",
      "Training stage for Flod 1 Epoch: 65 [16000/29981                 (53%)]\tLoss: 0.401927\n",
      "Training stage for Flod 1 Epoch: 65 [19200/29981                 (64%)]\tLoss: 0.394309\n",
      "Training stage for Flod 1 Epoch: 65 [22400/29981                 (75%)]\tLoss: 0.406753\n",
      "Training stage for Flod 1 Epoch: 65 [25600/29981                 (85%)]\tLoss: 0.407012\n",
      "Training stage for Flod 1 Epoch: 65 [28800/29981                 (96%)]\tLoss: 0.375441\n",
      "Test set for fold1: Average Loss:           705807.5009, Accuracy: 6792/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 66 [0/29981                 (0%)]\tLoss: 0.375473\n",
      "Training stage for Flod 1 Epoch: 66 [3200/29981                 (11%)]\tLoss: 0.313312\n",
      "Training stage for Flod 1 Epoch: 66 [6400/29981                 (21%)]\tLoss: 0.375773\n",
      "Training stage for Flod 1 Epoch: 66 [9600/29981                 (32%)]\tLoss: 0.438076\n",
      "Training stage for Flod 1 Epoch: 66 [12800/29981                 (43%)]\tLoss: 0.344609\n",
      "Training stage for Flod 1 Epoch: 66 [16000/29981                 (53%)]\tLoss: 0.428128\n",
      "Training stage for Flod 1 Epoch: 66 [19200/29981                 (64%)]\tLoss: 0.358784\n",
      "Training stage for Flod 1 Epoch: 66 [22400/29981                 (75%)]\tLoss: 0.468664\n",
      "Training stage for Flod 1 Epoch: 66 [25600/29981                 (85%)]\tLoss: 0.378150\n",
      "Training stage for Flod 1 Epoch: 66 [28800/29981                 (96%)]\tLoss: 0.321541\n",
      "Test set for fold1: Average Loss:           707877.8446, Accuracy: 6804/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 67 [0/29981                 (0%)]\tLoss: 0.321499\n",
      "Training stage for Flod 1 Epoch: 67 [3200/29981                 (11%)]\tLoss: 0.350535\n",
      "Training stage for Flod 1 Epoch: 67 [6400/29981                 (21%)]\tLoss: 0.375701\n",
      "Training stage for Flod 1 Epoch: 67 [9600/29981                 (32%)]\tLoss: 0.474089\n",
      "Training stage for Flod 1 Epoch: 67 [12800/29981                 (43%)]\tLoss: 0.380367\n",
      "Training stage for Flod 1 Epoch: 67 [16000/29981                 (53%)]\tLoss: 0.372962\n",
      "Training stage for Flod 1 Epoch: 67 [19200/29981                 (64%)]\tLoss: 0.375772\n",
      "Training stage for Flod 1 Epoch: 67 [22400/29981                 (75%)]\tLoss: 0.346153\n",
      "Training stage for Flod 1 Epoch: 67 [25600/29981                 (85%)]\tLoss: 0.344538\n",
      "Training stage for Flod 1 Epoch: 67 [28800/29981                 (96%)]\tLoss: 0.346309\n",
      "Test set for fold1: Average Loss:           686204.5158, Accuracy: 6864/7495           (92%)\n",
      "Training stage for Flod 1 Epoch: 68 [0/29981                 (0%)]\tLoss: 0.313263\n",
      "Training stage for Flod 1 Epoch: 68 [3200/29981                 (11%)]\tLoss: 0.402752\n",
      "Training stage for Flod 1 Epoch: 68 [6400/29981                 (21%)]\tLoss: 0.344576\n",
      "Training stage for Flod 1 Epoch: 68 [9600/29981                 (32%)]\tLoss: 0.344522\n",
      "Training stage for Flod 1 Epoch: 68 [12800/29981                 (43%)]\tLoss: 0.407107\n",
      "Training stage for Flod 1 Epoch: 68 [16000/29981                 (53%)]\tLoss: 0.375826\n",
      "Training stage for Flod 1 Epoch: 68 [19200/29981                 (64%)]\tLoss: 0.377481\n",
      "Training stage for Flod 1 Epoch: 68 [22400/29981                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 68 [25600/29981                 (85%)]\tLoss: 0.325567\n",
      "Training stage for Flod 1 Epoch: 68 [28800/29981                 (96%)]\tLoss: 0.313302\n",
      "Test set for fold1: Average Loss:           722475.7586, Accuracy: 6762/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 69 [0/29981                 (0%)]\tLoss: 0.344515\n",
      "Training stage for Flod 1 Epoch: 69 [3200/29981                 (11%)]\tLoss: 0.316393\n",
      "Training stage for Flod 1 Epoch: 69 [6400/29981                 (21%)]\tLoss: 0.359366\n",
      "Training stage for Flod 1 Epoch: 69 [9600/29981                 (32%)]\tLoss: 0.460737\n",
      "Training stage for Flod 1 Epoch: 69 [12800/29981                 (43%)]\tLoss: 0.344361\n",
      "Training stage for Flod 1 Epoch: 69 [16000/29981                 (53%)]\tLoss: 0.406997\n",
      "Training stage for Flod 1 Epoch: 69 [19200/29981                 (64%)]\tLoss: 0.344541\n",
      "Training stage for Flod 1 Epoch: 69 [22400/29981                 (75%)]\tLoss: 0.313367\n",
      "Training stage for Flod 1 Epoch: 69 [25600/29981                 (85%)]\tLoss: 0.376099\n",
      "Training stage for Flod 1 Epoch: 69 [28800/29981                 (96%)]\tLoss: 0.440861\n",
      "Test set for fold1: Average Loss:           733854.9424, Accuracy: 6661/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 70 [0/29981                 (0%)]\tLoss: 0.343483\n",
      "Training stage for Flod 1 Epoch: 70 [3200/29981                 (11%)]\tLoss: 0.351856\n",
      "Training stage for Flod 1 Epoch: 70 [6400/29981                 (21%)]\tLoss: 0.344600\n",
      "Training stage for Flod 1 Epoch: 70 [9600/29981                 (32%)]\tLoss: 0.375798\n",
      "Training stage for Flod 1 Epoch: 70 [12800/29981                 (43%)]\tLoss: 0.313525\n",
      "Training stage for Flod 1 Epoch: 70 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 70 [19200/29981                 (64%)]\tLoss: 0.375938\n",
      "Training stage for Flod 1 Epoch: 70 [22400/29981                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 70 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 70 [28800/29981                 (96%)]\tLoss: 0.313267\n",
      "Test set for fold1: Average Loss:           712616.3627, Accuracy: 6786/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 71 [0/29981                 (0%)]\tLoss: 0.438661\n",
      "Training stage for Flod 1 Epoch: 71 [3200/29981                 (11%)]\tLoss: 0.313341\n",
      "Training stage for Flod 1 Epoch: 71 [6400/29981                 (21%)]\tLoss: 0.375782\n",
      "Training stage for Flod 1 Epoch: 71 [9600/29981                 (32%)]\tLoss: 0.344564\n",
      "Training stage for Flod 1 Epoch: 71 [12800/29981                 (43%)]\tLoss: 0.376402\n",
      "Training stage for Flod 1 Epoch: 71 [16000/29981                 (53%)]\tLoss: 0.376092\n",
      "Training stage for Flod 1 Epoch: 71 [19200/29981                 (64%)]\tLoss: 0.400406\n",
      "Training stage for Flod 1 Epoch: 71 [22400/29981                 (75%)]\tLoss: 0.375765\n",
      "Training stage for Flod 1 Epoch: 71 [25600/29981                 (85%)]\tLoss: 0.360549\n",
      "Training stage for Flod 1 Epoch: 71 [28800/29981                 (96%)]\tLoss: 0.326762\n",
      "Test set for fold1: Average Loss:           728435.4884, Accuracy: 6712/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 72 [0/29981                 (0%)]\tLoss: 0.345901\n",
      "Training stage for Flod 1 Epoch: 72 [3200/29981                 (11%)]\tLoss: 0.369474\n",
      "Training stage for Flod 1 Epoch: 72 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 72 [9600/29981                 (32%)]\tLoss: 0.438190\n",
      "Training stage for Flod 1 Epoch: 72 [12800/29981                 (43%)]\tLoss: 0.375766\n",
      "Training stage for Flod 1 Epoch: 72 [16000/29981                 (53%)]\tLoss: 0.313274\n",
      "Training stage for Flod 1 Epoch: 72 [19200/29981                 (64%)]\tLoss: 0.313263\n",
      "Training stage for Flod 1 Epoch: 72 [22400/29981                 (75%)]\tLoss: 0.377500\n",
      "Training stage for Flod 1 Epoch: 72 [25600/29981                 (85%)]\tLoss: 0.351234\n",
      "Training stage for Flod 1 Epoch: 72 [28800/29981                 (96%)]\tLoss: 0.438696\n",
      "Test set for fold1: Average Loss:           696887.1201, Accuracy: 6850/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 73 [0/29981                 (0%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 73 [3200/29981                 (11%)]\tLoss: 0.368241\n",
      "Training stage for Flod 1 Epoch: 73 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 73 [9600/29981                 (32%)]\tLoss: 0.344532\n",
      "Training stage for Flod 1 Epoch: 73 [12800/29981                 (43%)]\tLoss: 0.344929\n",
      "Training stage for Flod 1 Epoch: 73 [16000/29981                 (53%)]\tLoss: 0.344521\n",
      "Training stage for Flod 1 Epoch: 73 [19200/29981                 (64%)]\tLoss: 0.375745\n",
      "Training stage for Flod 1 Epoch: 73 [22400/29981                 (75%)]\tLoss: 0.376783\n",
      "Training stage for Flod 1 Epoch: 73 [25600/29981                 (85%)]\tLoss: 0.344499\n",
      "Training stage for Flod 1 Epoch: 73 [28800/29981                 (96%)]\tLoss: 0.313528\n",
      "Test set for fold1: Average Loss:           708115.7726, Accuracy: 6798/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 74 [0/29981                 (0%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 74 [3200/29981                 (11%)]\tLoss: 0.378060\n",
      "Training stage for Flod 1 Epoch: 74 [6400/29981                 (21%)]\tLoss: 0.406496\n",
      "Training stage for Flod 1 Epoch: 74 [9600/29981                 (32%)]\tLoss: 0.344560\n",
      "Training stage for Flod 1 Epoch: 74 [12800/29981                 (43%)]\tLoss: 0.313264\n",
      "Training stage for Flod 1 Epoch: 74 [16000/29981                 (53%)]\tLoss: 0.329327\n",
      "Training stage for Flod 1 Epoch: 74 [19200/29981                 (64%)]\tLoss: 0.382752\n",
      "Training stage for Flod 1 Epoch: 74 [22400/29981                 (75%)]\tLoss: 0.313263\n",
      "Training stage for Flod 1 Epoch: 74 [25600/29981                 (85%)]\tLoss: 0.376962\n",
      "Training stage for Flod 1 Epoch: 74 [28800/29981                 (96%)]\tLoss: 0.375761\n",
      "Test set for fold1: Average Loss:           698861.9936, Accuracy: 6825/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 75 [0/29981                 (0%)]\tLoss: 0.344513\n",
      "Training stage for Flod 1 Epoch: 75 [3200/29981                 (11%)]\tLoss: 0.406708\n",
      "Training stage for Flod 1 Epoch: 75 [6400/29981                 (21%)]\tLoss: 0.408891\n",
      "Training stage for Flod 1 Epoch: 75 [9600/29981                 (32%)]\tLoss: 0.396249\n",
      "Training stage for Flod 1 Epoch: 75 [12800/29981                 (43%)]\tLoss: 0.345314\n",
      "Training stage for Flod 1 Epoch: 75 [16000/29981                 (53%)]\tLoss: 0.372606\n",
      "Training stage for Flod 1 Epoch: 75 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 75 [22400/29981                 (75%)]\tLoss: 0.422791\n",
      "Training stage for Flod 1 Epoch: 75 [25600/29981                 (85%)]\tLoss: 0.342363\n",
      "Training stage for Flod 1 Epoch: 75 [28800/29981                 (96%)]\tLoss: 0.400939\n",
      "Test set for fold1: Average Loss:           697081.2283, Accuracy: 6841/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 76 [0/29981                 (0%)]\tLoss: 0.406125\n",
      "Training stage for Flod 1 Epoch: 76 [3200/29981                 (11%)]\tLoss: 0.374498\n",
      "Training stage for Flod 1 Epoch: 76 [6400/29981                 (21%)]\tLoss: 0.384194\n",
      "Training stage for Flod 1 Epoch: 76 [9600/29981                 (32%)]\tLoss: 0.388092\n",
      "Training stage for Flod 1 Epoch: 76 [12800/29981                 (43%)]\tLoss: 0.436476\n",
      "Training stage for Flod 1 Epoch: 76 [16000/29981                 (53%)]\tLoss: 0.345119\n",
      "Training stage for Flod 1 Epoch: 76 [19200/29981                 (64%)]\tLoss: 0.350912\n",
      "Training stage for Flod 1 Epoch: 76 [22400/29981                 (75%)]\tLoss: 0.344514\n",
      "Training stage for Flod 1 Epoch: 76 [25600/29981                 (85%)]\tLoss: 0.344514\n",
      "Training stage for Flod 1 Epoch: 76 [28800/29981                 (96%)]\tLoss: 0.424066\n",
      "Test set for fold1: Average Loss:           720610.5085, Accuracy: 6763/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 77 [0/29981                 (0%)]\tLoss: 0.313277\n",
      "Training stage for Flod 1 Epoch: 77 [3200/29981                 (11%)]\tLoss: 0.407012\n",
      "Training stage for Flod 1 Epoch: 77 [6400/29981                 (21%)]\tLoss: 0.321420\n",
      "Training stage for Flod 1 Epoch: 77 [9600/29981                 (32%)]\tLoss: 0.344358\n",
      "Training stage for Flod 1 Epoch: 77 [12800/29981                 (43%)]\tLoss: 0.344794\n",
      "Training stage for Flod 1 Epoch: 77 [16000/29981                 (53%)]\tLoss: 0.344521\n",
      "Training stage for Flod 1 Epoch: 77 [19200/29981                 (64%)]\tLoss: 0.396901\n",
      "Training stage for Flod 1 Epoch: 77 [22400/29981                 (75%)]\tLoss: 0.344641\n",
      "Training stage for Flod 1 Epoch: 77 [25600/29981                 (85%)]\tLoss: 0.313424\n",
      "Training stage for Flod 1 Epoch: 77 [28800/29981                 (96%)]\tLoss: 0.344523\n",
      "Test set for fold1: Average Loss:           717627.1634, Accuracy: 6779/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 78 [0/29981                 (0%)]\tLoss: 0.313270\n",
      "Training stage for Flod 1 Epoch: 78 [3200/29981                 (11%)]\tLoss: 0.403898\n",
      "Training stage for Flod 1 Epoch: 78 [6400/29981                 (21%)]\tLoss: 0.399164\n",
      "Training stage for Flod 1 Epoch: 78 [9600/29981                 (32%)]\tLoss: 0.344519\n",
      "Training stage for Flod 1 Epoch: 78 [12800/29981                 (43%)]\tLoss: 0.356857\n",
      "Training stage for Flod 1 Epoch: 78 [16000/29981                 (53%)]\tLoss: 0.372577\n",
      "Training stage for Flod 1 Epoch: 78 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 78 [22400/29981                 (75%)]\tLoss: 0.353945\n",
      "Training stage for Flod 1 Epoch: 78 [25600/29981                 (85%)]\tLoss: 0.372613\n",
      "Training stage for Flod 1 Epoch: 78 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold1: Average Loss:           749299.3287, Accuracy: 6636/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 79 [0/29981                 (0%)]\tLoss: 0.384878\n",
      "Training stage for Flod 1 Epoch: 79 [3200/29981                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 79 [6400/29981                 (21%)]\tLoss: 0.438264\n",
      "Training stage for Flod 1 Epoch: 79 [9600/29981                 (32%)]\tLoss: 0.313361\n",
      "Training stage for Flod 1 Epoch: 79 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 79 [16000/29981                 (53%)]\tLoss: 0.344518\n",
      "Training stage for Flod 1 Epoch: 79 [19200/29981                 (64%)]\tLoss: 0.316734\n",
      "Training stage for Flod 1 Epoch: 79 [22400/29981                 (75%)]\tLoss: 0.446141\n",
      "Training stage for Flod 1 Epoch: 79 [25600/29981                 (85%)]\tLoss: 0.313464\n",
      "Training stage for Flod 1 Epoch: 79 [28800/29981                 (96%)]\tLoss: 0.316874\n",
      "Test set for fold1: Average Loss:           703850.9904, Accuracy: 6818/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 80 [0/29981                 (0%)]\tLoss: 0.349265\n",
      "Training stage for Flod 1 Epoch: 80 [3200/29981                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 80 [6400/29981                 (21%)]\tLoss: 0.372437\n",
      "Training stage for Flod 1 Epoch: 80 [9600/29981                 (32%)]\tLoss: 0.313410\n",
      "Training stage for Flod 1 Epoch: 80 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 80 [16000/29981                 (53%)]\tLoss: 0.344524\n",
      "Training stage for Flod 1 Epoch: 80 [19200/29981                 (64%)]\tLoss: 0.313326\n",
      "Training stage for Flod 1 Epoch: 80 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 80 [25600/29981                 (85%)]\tLoss: 0.406416\n",
      "Training stage for Flod 1 Epoch: 80 [28800/29981                 (96%)]\tLoss: 0.409490\n",
      "Test set for fold1: Average Loss:           738082.4417, Accuracy: 6672/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 81 [0/29981                 (0%)]\tLoss: 0.375769\n",
      "Training stage for Flod 1 Epoch: 81 [3200/29981                 (11%)]\tLoss: 0.402153\n",
      "Training stage for Flod 1 Epoch: 81 [6400/29981                 (21%)]\tLoss: 0.407014\n",
      "Training stage for Flod 1 Epoch: 81 [9600/29981                 (32%)]\tLoss: 0.375812\n",
      "Training stage for Flod 1 Epoch: 81 [12800/29981                 (43%)]\tLoss: 0.345913\n",
      "Training stage for Flod 1 Epoch: 81 [16000/29981                 (53%)]\tLoss: 0.440016\n",
      "Training stage for Flod 1 Epoch: 81 [19200/29981                 (64%)]\tLoss: 0.345021\n",
      "Training stage for Flod 1 Epoch: 81 [22400/29981                 (75%)]\tLoss: 0.375774\n",
      "Training stage for Flod 1 Epoch: 81 [25600/29981                 (85%)]\tLoss: 0.331384\n",
      "Training stage for Flod 1 Epoch: 81 [28800/29981                 (96%)]\tLoss: 0.314074\n",
      "Test set for fold1: Average Loss:           727093.6203, Accuracy: 6711/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 82 [0/29981                 (0%)]\tLoss: 0.323442\n",
      "Training stage for Flod 1 Epoch: 82 [3200/29981                 (11%)]\tLoss: 0.347144\n",
      "Training stage for Flod 1 Epoch: 82 [6400/29981                 (21%)]\tLoss: 0.316042\n",
      "Training stage for Flod 1 Epoch: 82 [9600/29981                 (32%)]\tLoss: 0.343959\n",
      "Training stage for Flod 1 Epoch: 82 [12800/29981                 (43%)]\tLoss: 0.367834\n",
      "Training stage for Flod 1 Epoch: 82 [16000/29981                 (53%)]\tLoss: 0.375771\n",
      "Training stage for Flod 1 Epoch: 82 [19200/29981                 (64%)]\tLoss: 0.344770\n",
      "Training stage for Flod 1 Epoch: 82 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 82 [25600/29981                 (85%)]\tLoss: 0.313265\n",
      "Training stage for Flod 1 Epoch: 82 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold1: Average Loss:           708820.5973, Accuracy: 6819/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 83 [0/29981                 (0%)]\tLoss: 0.313263\n",
      "Training stage for Flod 1 Epoch: 83 [3200/29981                 (11%)]\tLoss: 0.313263\n",
      "Training stage for Flod 1 Epoch: 83 [6400/29981                 (21%)]\tLoss: 0.313268\n",
      "Training stage for Flod 1 Epoch: 83 [9600/29981                 (32%)]\tLoss: 0.314009\n",
      "Training stage for Flod 1 Epoch: 83 [12800/29981                 (43%)]\tLoss: 0.344505\n",
      "Training stage for Flod 1 Epoch: 83 [16000/29981                 (53%)]\tLoss: 0.345264\n",
      "Training stage for Flod 1 Epoch: 83 [19200/29981                 (64%)]\tLoss: 0.438262\n",
      "Training stage for Flod 1 Epoch: 83 [22400/29981                 (75%)]\tLoss: 0.344586\n",
      "Training stage for Flod 1 Epoch: 83 [25600/29981                 (85%)]\tLoss: 0.375769\n",
      "Training stage for Flod 1 Epoch: 83 [28800/29981                 (96%)]\tLoss: 0.346012\n",
      "Test set for fold1: Average Loss:           742717.8695, Accuracy: 6671/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 84 [0/29981                 (0%)]\tLoss: 0.375178\n",
      "Training stage for Flod 1 Epoch: 84 [3200/29981                 (11%)]\tLoss: 0.344930\n",
      "Training stage for Flod 1 Epoch: 84 [6400/29981                 (21%)]\tLoss: 0.348411\n",
      "Training stage for Flod 1 Epoch: 84 [9600/29981                 (32%)]\tLoss: 0.344560\n",
      "Training stage for Flod 1 Epoch: 84 [12800/29981                 (43%)]\tLoss: 0.407012\n",
      "Training stage for Flod 1 Epoch: 84 [16000/29981                 (53%)]\tLoss: 0.323184\n",
      "Training stage for Flod 1 Epoch: 84 [19200/29981                 (64%)]\tLoss: 0.469512\n",
      "Training stage for Flod 1 Epoch: 84 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 84 [25600/29981                 (85%)]\tLoss: 0.407132\n",
      "Training stage for Flod 1 Epoch: 84 [28800/29981                 (96%)]\tLoss: 0.313534\n",
      "Test set for fold1: Average Loss:           706805.6876, Accuracy: 6768/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 85 [0/29981                 (0%)]\tLoss: 0.313403\n",
      "Training stage for Flod 1 Epoch: 85 [3200/29981                 (11%)]\tLoss: 0.317015\n",
      "Training stage for Flod 1 Epoch: 85 [6400/29981                 (21%)]\tLoss: 0.345512\n",
      "Training stage for Flod 1 Epoch: 85 [9600/29981                 (32%)]\tLoss: 0.355395\n",
      "Training stage for Flod 1 Epoch: 85 [12800/29981                 (43%)]\tLoss: 0.355883\n",
      "Training stage for Flod 1 Epoch: 85 [16000/29981                 (53%)]\tLoss: 0.344521\n",
      "Training stage for Flod 1 Epoch: 85 [19200/29981                 (64%)]\tLoss: 0.344542\n",
      "Training stage for Flod 1 Epoch: 85 [22400/29981                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 85 [25600/29981                 (85%)]\tLoss: 0.375714\n",
      "Training stage for Flod 1 Epoch: 85 [28800/29981                 (96%)]\tLoss: 0.344535\n",
      "Test set for fold1: Average Loss:           722431.6885, Accuracy: 6737/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 86 [0/29981                 (0%)]\tLoss: 0.376502\n",
      "Training stage for Flod 1 Epoch: 86 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 86 [6400/29981                 (21%)]\tLoss: 0.313263\n",
      "Training stage for Flod 1 Epoch: 86 [9600/29981                 (32%)]\tLoss: 0.407114\n",
      "Training stage for Flod 1 Epoch: 86 [12800/29981                 (43%)]\tLoss: 0.407016\n",
      "Training stage for Flod 1 Epoch: 86 [16000/29981                 (53%)]\tLoss: 0.314464\n",
      "Training stage for Flod 1 Epoch: 86 [19200/29981                 (64%)]\tLoss: 0.314162\n",
      "Training stage for Flod 1 Epoch: 86 [22400/29981                 (75%)]\tLoss: 0.342906\n",
      "Training stage for Flod 1 Epoch: 86 [25600/29981                 (85%)]\tLoss: 0.375685\n",
      "Training stage for Flod 1 Epoch: 86 [28800/29981                 (96%)]\tLoss: 0.369675\n",
      "Test set for fold1: Average Loss:           701686.1110, Accuracy: 6816/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 87 [0/29981                 (0%)]\tLoss: 0.407012\n",
      "Training stage for Flod 1 Epoch: 87 [3200/29981                 (11%)]\tLoss: 0.343774\n",
      "Training stage for Flod 1 Epoch: 87 [6400/29981                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 87 [9600/29981                 (32%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 87 [12800/29981                 (43%)]\tLoss: 0.313910\n",
      "Training stage for Flod 1 Epoch: 87 [16000/29981                 (53%)]\tLoss: 0.313267\n",
      "Training stage for Flod 1 Epoch: 87 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 87 [22400/29981                 (75%)]\tLoss: 0.344523\n",
      "Training stage for Flod 1 Epoch: 87 [25600/29981                 (85%)]\tLoss: 0.344516\n",
      "Training stage for Flod 1 Epoch: 87 [28800/29981                 (96%)]\tLoss: 0.380012\n",
      "Test set for fold1: Average Loss:           732785.0969, Accuracy: 6681/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 88 [0/29981                 (0%)]\tLoss: 0.438263\n",
      "Training stage for Flod 1 Epoch: 88 [3200/29981                 (11%)]\tLoss: 0.352062\n",
      "Training stage for Flod 1 Epoch: 88 [6400/29981                 (21%)]\tLoss: 0.344563\n",
      "Training stage for Flod 1 Epoch: 88 [9600/29981                 (32%)]\tLoss: 0.344767\n",
      "Training stage for Flod 1 Epoch: 88 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 88 [16000/29981                 (53%)]\tLoss: 0.344564\n",
      "Training stage for Flod 1 Epoch: 88 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 88 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 88 [25600/29981                 (85%)]\tLoss: 0.407014\n",
      "Training stage for Flod 1 Epoch: 88 [28800/29981                 (96%)]\tLoss: 0.345339\n",
      "Test set for fold1: Average Loss:           690012.7085, Accuracy: 6887/7495           (92%)\n",
      "Training stage for Flod 1 Epoch: 89 [0/29981                 (0%)]\tLoss: 0.365281\n",
      "Training stage for Flod 1 Epoch: 89 [3200/29981                 (11%)]\tLoss: 0.375617\n",
      "Training stage for Flod 1 Epoch: 89 [6400/29981                 (21%)]\tLoss: 0.313270\n",
      "Training stage for Flod 1 Epoch: 89 [9600/29981                 (32%)]\tLoss: 0.359298\n",
      "Training stage for Flod 1 Epoch: 89 [12800/29981                 (43%)]\tLoss: 0.344519\n",
      "Training stage for Flod 1 Epoch: 89 [16000/29981                 (53%)]\tLoss: 0.375853\n",
      "Training stage for Flod 1 Epoch: 89 [19200/29981                 (64%)]\tLoss: 0.438265\n",
      "Training stage for Flod 1 Epoch: 89 [22400/29981                 (75%)]\tLoss: 0.344666\n",
      "Training stage for Flod 1 Epoch: 89 [25600/29981                 (85%)]\tLoss: 0.435012\n",
      "Training stage for Flod 1 Epoch: 89 [28800/29981                 (96%)]\tLoss: 0.347217\n",
      "Test set for fold1: Average Loss:           704768.9673, Accuracy: 6811/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 90 [0/29981                 (0%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 90 [3200/29981                 (11%)]\tLoss: 0.407013\n",
      "Training stage for Flod 1 Epoch: 90 [6400/29981                 (21%)]\tLoss: 0.401649\n",
      "Training stage for Flod 1 Epoch: 90 [9600/29981                 (32%)]\tLoss: 0.438262\n",
      "Training stage for Flod 1 Epoch: 90 [12800/29981                 (43%)]\tLoss: 0.400863\n",
      "Training stage for Flod 1 Epoch: 90 [16000/29981                 (53%)]\tLoss: 0.375765\n",
      "Training stage for Flod 1 Epoch: 90 [19200/29981                 (64%)]\tLoss: 0.375763\n",
      "Training stage for Flod 1 Epoch: 90 [22400/29981                 (75%)]\tLoss: 0.438752\n",
      "Training stage for Flod 1 Epoch: 90 [25600/29981                 (85%)]\tLoss: 0.313266\n",
      "Training stage for Flod 1 Epoch: 90 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold1: Average Loss:           678081.4011, Accuracy: 6936/7495           (93%)\n",
      "Training stage for Flod 1 Epoch: 91 [0/29981                 (0%)]\tLoss: 0.313268\n",
      "Training stage for Flod 1 Epoch: 91 [3200/29981                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 91 [6400/29981                 (21%)]\tLoss: 0.439314\n",
      "Training stage for Flod 1 Epoch: 91 [9600/29981                 (32%)]\tLoss: 0.345475\n",
      "Training stage for Flod 1 Epoch: 91 [12800/29981                 (43%)]\tLoss: 0.344524\n",
      "Training stage for Flod 1 Epoch: 91 [16000/29981                 (53%)]\tLoss: 0.412199\n",
      "Training stage for Flod 1 Epoch: 91 [19200/29981                 (64%)]\tLoss: 0.363234\n",
      "Training stage for Flod 1 Epoch: 91 [22400/29981                 (75%)]\tLoss: 0.402404\n",
      "Training stage for Flod 1 Epoch: 91 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 91 [28800/29981                 (96%)]\tLoss: 0.314464\n",
      "Test set for fold1: Average Loss:           731967.5397, Accuracy: 6683/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 92 [0/29981                 (0%)]\tLoss: 0.431770\n",
      "Training stage for Flod 1 Epoch: 92 [3200/29981                 (11%)]\tLoss: 0.353713\n",
      "Training stage for Flod 1 Epoch: 92 [6400/29981                 (21%)]\tLoss: 0.313919\n",
      "Training stage for Flod 1 Epoch: 92 [9600/29981                 (32%)]\tLoss: 0.437881\n",
      "Training stage for Flod 1 Epoch: 92 [12800/29981                 (43%)]\tLoss: 0.313361\n",
      "Training stage for Flod 1 Epoch: 92 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 92 [19200/29981                 (64%)]\tLoss: 0.438335\n",
      "Training stage for Flod 1 Epoch: 92 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 92 [25600/29981                 (85%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 92 [28800/29981                 (96%)]\tLoss: 0.313955\n",
      "Test set for fold1: Average Loss:           726550.1147, Accuracy: 6711/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 93 [0/29981                 (0%)]\tLoss: 0.395065\n",
      "Training stage for Flod 1 Epoch: 93 [3200/29981                 (11%)]\tLoss: 0.344542\n",
      "Training stage for Flod 1 Epoch: 93 [6400/29981                 (21%)]\tLoss: 0.344513\n",
      "Training stage for Flod 1 Epoch: 93 [9600/29981                 (32%)]\tLoss: 0.405330\n",
      "Training stage for Flod 1 Epoch: 93 [12800/29981                 (43%)]\tLoss: 0.344577\n",
      "Training stage for Flod 1 Epoch: 93 [16000/29981                 (53%)]\tLoss: 0.349557\n",
      "Training stage for Flod 1 Epoch: 93 [19200/29981                 (64%)]\tLoss: 0.407027\n",
      "Training stage for Flod 1 Epoch: 93 [22400/29981                 (75%)]\tLoss: 0.406841\n",
      "Training stage for Flod 1 Epoch: 93 [25600/29981                 (85%)]\tLoss: 0.407029\n",
      "Training stage for Flod 1 Epoch: 93 [28800/29981                 (96%)]\tLoss: 0.375762\n",
      "Test set for fold1: Average Loss:           685867.4745, Accuracy: 6898/7495           (92%)\n",
      "Training stage for Flod 1 Epoch: 94 [0/29981                 (0%)]\tLoss: 0.375814\n",
      "Training stage for Flod 1 Epoch: 94 [3200/29981                 (11%)]\tLoss: 0.313267\n",
      "Training stage for Flod 1 Epoch: 94 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 94 [9600/29981                 (32%)]\tLoss: 0.393435\n",
      "Training stage for Flod 1 Epoch: 94 [12800/29981                 (43%)]\tLoss: 0.343209\n",
      "Training stage for Flod 1 Epoch: 94 [16000/29981                 (53%)]\tLoss: 0.344523\n",
      "Training stage for Flod 1 Epoch: 94 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 94 [22400/29981                 (75%)]\tLoss: 0.313683\n",
      "Training stage for Flod 1 Epoch: 94 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 94 [28800/29981                 (96%)]\tLoss: 0.375758\n",
      "Test set for fold1: Average Loss:           739301.1976, Accuracy: 6671/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 95 [0/29981                 (0%)]\tLoss: 0.344590\n",
      "Training stage for Flod 1 Epoch: 95 [3200/29981                 (11%)]\tLoss: 0.318760\n",
      "Training stage for Flod 1 Epoch: 95 [6400/29981                 (21%)]\tLoss: 0.408152\n",
      "Training stage for Flod 1 Epoch: 95 [9600/29981                 (32%)]\tLoss: 0.375894\n",
      "Training stage for Flod 1 Epoch: 95 [12800/29981                 (43%)]\tLoss: 0.375883\n",
      "Training stage for Flod 1 Epoch: 95 [16000/29981                 (53%)]\tLoss: 0.375776\n",
      "Training stage for Flod 1 Epoch: 95 [19200/29981                 (64%)]\tLoss: 0.375752\n",
      "Training stage for Flod 1 Epoch: 95 [22400/29981                 (75%)]\tLoss: 0.344513\n",
      "Training stage for Flod 1 Epoch: 95 [25600/29981                 (85%)]\tLoss: 0.469529\n",
      "Training stage for Flod 1 Epoch: 95 [28800/29981                 (96%)]\tLoss: 0.384521\n",
      "Test set for fold1: Average Loss:           711000.7683, Accuracy: 6776/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 96 [0/29981                 (0%)]\tLoss: 0.401545\n",
      "Training stage for Flod 1 Epoch: 96 [3200/29981                 (11%)]\tLoss: 0.360679\n",
      "Training stage for Flod 1 Epoch: 96 [6400/29981                 (21%)]\tLoss: 0.313334\n",
      "Training stage for Flod 1 Epoch: 96 [9600/29981                 (32%)]\tLoss: 0.385709\n",
      "Training stage for Flod 1 Epoch: 96 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 96 [16000/29981                 (53%)]\tLoss: 0.375791\n",
      "Training stage for Flod 1 Epoch: 96 [19200/29981                 (64%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 96 [22400/29981                 (75%)]\tLoss: 0.375746\n",
      "Training stage for Flod 1 Epoch: 96 [25600/29981                 (85%)]\tLoss: 0.344545\n",
      "Training stage for Flod 1 Epoch: 96 [28800/29981                 (96%)]\tLoss: 0.313312\n",
      "Test set for fold1: Average Loss:           706060.6255, Accuracy: 6822/7495           (91%)\n",
      "Training stage for Flod 1 Epoch: 97 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 97 [3200/29981                 (11%)]\tLoss: 0.405386\n",
      "Training stage for Flod 1 Epoch: 97 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 1 Epoch: 97 [9600/29981                 (32%)]\tLoss: 0.432922\n",
      "Training stage for Flod 1 Epoch: 97 [12800/29981                 (43%)]\tLoss: 0.344515\n",
      "Training stage for Flod 1 Epoch: 97 [16000/29981                 (53%)]\tLoss: 0.347555\n",
      "Training stage for Flod 1 Epoch: 97 [19200/29981                 (64%)]\tLoss: 0.345297\n",
      "Training stage for Flod 1 Epoch: 97 [22400/29981                 (75%)]\tLoss: 0.406995\n",
      "Training stage for Flod 1 Epoch: 97 [25600/29981                 (85%)]\tLoss: 0.344545\n",
      "Training stage for Flod 1 Epoch: 97 [28800/29981                 (96%)]\tLoss: 0.313386\n",
      "Test set for fold1: Average Loss:           726004.3460, Accuracy: 6732/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 98 [0/29981                 (0%)]\tLoss: 0.438531\n",
      "Training stage for Flod 1 Epoch: 98 [3200/29981                 (11%)]\tLoss: 0.344513\n",
      "Training stage for Flod 1 Epoch: 98 [6400/29981                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 98 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 98 [12800/29981                 (43%)]\tLoss: 0.313270\n",
      "Training stage for Flod 1 Epoch: 98 [16000/29981                 (53%)]\tLoss: 0.344515\n",
      "Training stage for Flod 1 Epoch: 98 [19200/29981                 (64%)]\tLoss: 0.345002\n",
      "Training stage for Flod 1 Epoch: 98 [22400/29981                 (75%)]\tLoss: 0.344519\n",
      "Training stage for Flod 1 Epoch: 98 [25600/29981                 (85%)]\tLoss: 0.375889\n",
      "Training stage for Flod 1 Epoch: 98 [28800/29981                 (96%)]\tLoss: 0.344554\n",
      "Test set for fold1: Average Loss:           719100.7929, Accuracy: 6772/7495           (90%)\n",
      "Training stage for Flod 1 Epoch: 99 [0/29981                 (0%)]\tLoss: 0.344628\n",
      "Training stage for Flod 1 Epoch: 99 [3200/29981                 (11%)]\tLoss: 0.353161\n",
      "Training stage for Flod 1 Epoch: 99 [6400/29981                 (21%)]\tLoss: 0.344696\n",
      "Training stage for Flod 1 Epoch: 99 [9600/29981                 (32%)]\tLoss: 0.344516\n",
      "Training stage for Flod 1 Epoch: 99 [12800/29981                 (43%)]\tLoss: 0.395468\n",
      "Training stage for Flod 1 Epoch: 99 [16000/29981                 (53%)]\tLoss: 0.360587\n",
      "Training stage for Flod 1 Epoch: 99 [19200/29981                 (64%)]\tLoss: 0.375889\n",
      "Training stage for Flod 1 Epoch: 99 [22400/29981                 (75%)]\tLoss: 0.355885\n",
      "Training stage for Flod 1 Epoch: 99 [25600/29981                 (85%)]\tLoss: 0.394023\n",
      "Training stage for Flod 1 Epoch: 99 [28800/29981                 (96%)]\tLoss: 0.322137\n",
      "Test set for fold1: Average Loss:           727997.5858, Accuracy: 6704/7495           (89%)\n",
      "Training stage for Flod 1 Epoch: 100 [0/29981                 (0%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 100 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 1 Epoch: 100 [6400/29981                 (21%)]\tLoss: 0.375806\n",
      "Training stage for Flod 1 Epoch: 100 [9600/29981                 (32%)]\tLoss: 0.367444\n",
      "Training stage for Flod 1 Epoch: 100 [12800/29981                 (43%)]\tLoss: 0.357447\n",
      "Training stage for Flod 1 Epoch: 100 [16000/29981                 (53%)]\tLoss: 0.344966\n",
      "Training stage for Flod 1 Epoch: 100 [19200/29981                 (64%)]\tLoss: 0.375882\n",
      "Training stage for Flod 1 Epoch: 100 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 1 Epoch: 100 [25600/29981                 (85%)]\tLoss: 0.344523\n",
      "Training stage for Flod 1 Epoch: 100 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold1: Average Loss:           705824.4501, Accuracy: 6811/7495           (91%)\n",
      "-------------------Fold 2-------------------\n",
      "Training stage for Flod 2 Epoch: 1 [0/29981                 (0%)]\tLoss: 0.690205\n",
      "Training stage for Flod 2 Epoch: 1 [3200/29981                 (11%)]\tLoss: 0.517350\n",
      "Training stage for Flod 2 Epoch: 1 [6400/29981                 (21%)]\tLoss: 0.447429\n",
      "Training stage for Flod 2 Epoch: 1 [9600/29981                 (32%)]\tLoss: 0.530736\n",
      "Training stage for Flod 2 Epoch: 1 [12800/29981                 (43%)]\tLoss: 0.565054\n",
      "Training stage for Flod 2 Epoch: 1 [16000/29981                 (53%)]\tLoss: 0.478276\n",
      "Training stage for Flod 2 Epoch: 1 [19200/29981                 (64%)]\tLoss: 0.407241\n",
      "Training stage for Flod 2 Epoch: 1 [22400/29981                 (75%)]\tLoss: 0.439154\n",
      "Training stage for Flod 2 Epoch: 1 [25600/29981                 (85%)]\tLoss: 0.467238\n",
      "Training stage for Flod 2 Epoch: 1 [28800/29981                 (96%)]\tLoss: 0.429619\n",
      "Test set for fold2: Average Loss:           758854.3943, Accuracy: 6598/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 2 [0/29981                 (0%)]\tLoss: 0.428276\n",
      "Training stage for Flod 2 Epoch: 2 [3200/29981                 (11%)]\tLoss: 0.399996\n",
      "Training stage for Flod 2 Epoch: 2 [6400/29981                 (21%)]\tLoss: 0.435476\n",
      "Training stage for Flod 2 Epoch: 2 [9600/29981                 (32%)]\tLoss: 0.393440\n",
      "Training stage for Flod 2 Epoch: 2 [12800/29981                 (43%)]\tLoss: 0.338313\n",
      "Training stage for Flod 2 Epoch: 2 [16000/29981                 (53%)]\tLoss: 0.406765\n",
      "Training stage for Flod 2 Epoch: 2 [19200/29981                 (64%)]\tLoss: 0.360409\n",
      "Training stage for Flod 2 Epoch: 2 [22400/29981                 (75%)]\tLoss: 0.359073\n",
      "Training stage for Flod 2 Epoch: 2 [25600/29981                 (85%)]\tLoss: 0.445382\n",
      "Training stage for Flod 2 Epoch: 2 [28800/29981                 (96%)]\tLoss: 0.330838\n",
      "Test set for fold2: Average Loss:           744165.7974, Accuracy: 6644/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 3 [0/29981                 (0%)]\tLoss: 0.422358\n",
      "Training stage for Flod 2 Epoch: 3 [3200/29981                 (11%)]\tLoss: 0.368527\n",
      "Training stage for Flod 2 Epoch: 3 [6400/29981                 (21%)]\tLoss: 0.498004\n",
      "Training stage for Flod 2 Epoch: 3 [9600/29981                 (32%)]\tLoss: 0.391128\n",
      "Training stage for Flod 2 Epoch: 3 [12800/29981                 (43%)]\tLoss: 0.415209\n",
      "Training stage for Flod 2 Epoch: 3 [16000/29981                 (53%)]\tLoss: 0.400294\n",
      "Training stage for Flod 2 Epoch: 3 [19200/29981                 (64%)]\tLoss: 0.504097\n",
      "Training stage for Flod 2 Epoch: 3 [22400/29981                 (75%)]\tLoss: 0.433642\n",
      "Training stage for Flod 2 Epoch: 3 [25600/29981                 (85%)]\tLoss: 0.425559\n",
      "Training stage for Flod 2 Epoch: 3 [28800/29981                 (96%)]\tLoss: 0.469503\n",
      "Test set for fold2: Average Loss:           781050.8325, Accuracy: 6453/7495           (86%)\n",
      "Training stage for Flod 2 Epoch: 4 [0/29981                 (0%)]\tLoss: 0.492690\n",
      "Training stage for Flod 2 Epoch: 4 [3200/29981                 (11%)]\tLoss: 0.407995\n",
      "Training stage for Flod 2 Epoch: 4 [6400/29981                 (21%)]\tLoss: 0.417058\n",
      "Training stage for Flod 2 Epoch: 4 [9600/29981                 (32%)]\tLoss: 0.430937\n",
      "Training stage for Flod 2 Epoch: 4 [12800/29981                 (43%)]\tLoss: 0.399880\n",
      "Training stage for Flod 2 Epoch: 4 [16000/29981                 (53%)]\tLoss: 0.389013\n",
      "Training stage for Flod 2 Epoch: 4 [19200/29981                 (64%)]\tLoss: 0.412892\n",
      "Training stage for Flod 2 Epoch: 4 [22400/29981                 (75%)]\tLoss: 0.424823\n",
      "Training stage for Flod 2 Epoch: 4 [25600/29981                 (85%)]\tLoss: 0.472521\n",
      "Training stage for Flod 2 Epoch: 4 [28800/29981                 (96%)]\tLoss: 0.400924\n",
      "Test set for fold2: Average Loss:           758297.6336, Accuracy: 6552/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 5 [0/29981                 (0%)]\tLoss: 0.338786\n",
      "Training stage for Flod 2 Epoch: 5 [3200/29981                 (11%)]\tLoss: 0.345427\n",
      "Training stage for Flod 2 Epoch: 5 [6400/29981                 (21%)]\tLoss: 0.349569\n",
      "Training stage for Flod 2 Epoch: 5 [9600/29981                 (32%)]\tLoss: 0.375448\n",
      "Training stage for Flod 2 Epoch: 5 [12800/29981                 (43%)]\tLoss: 0.532350\n",
      "Training stage for Flod 2 Epoch: 5 [16000/29981                 (53%)]\tLoss: 0.384770\n",
      "Training stage for Flod 2 Epoch: 5 [19200/29981                 (64%)]\tLoss: 0.439292\n",
      "Training stage for Flod 2 Epoch: 5 [22400/29981                 (75%)]\tLoss: 0.416832\n",
      "Training stage for Flod 2 Epoch: 5 [25600/29981                 (85%)]\tLoss: 0.418882\n",
      "Training stage for Flod 2 Epoch: 5 [28800/29981                 (96%)]\tLoss: 0.419861\n",
      "Test set for fold2: Average Loss:           749246.8351, Accuracy: 6571/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 6 [0/29981                 (0%)]\tLoss: 0.477710\n",
      "Training stage for Flod 2 Epoch: 6 [3200/29981                 (11%)]\tLoss: 0.410534\n",
      "Training stage for Flod 2 Epoch: 6 [6400/29981                 (21%)]\tLoss: 0.342654\n",
      "Training stage for Flod 2 Epoch: 6 [9600/29981                 (32%)]\tLoss: 0.417395\n",
      "Training stage for Flod 2 Epoch: 6 [12800/29981                 (43%)]\tLoss: 0.327549\n",
      "Training stage for Flod 2 Epoch: 6 [16000/29981                 (53%)]\tLoss: 0.393585\n",
      "Training stage for Flod 2 Epoch: 6 [19200/29981                 (64%)]\tLoss: 0.394019\n",
      "Training stage for Flod 2 Epoch: 6 [22400/29981                 (75%)]\tLoss: 0.407978\n",
      "Training stage for Flod 2 Epoch: 6 [25600/29981                 (85%)]\tLoss: 0.446949\n",
      "Training stage for Flod 2 Epoch: 6 [28800/29981                 (96%)]\tLoss: 0.363074\n",
      "Test set for fold2: Average Loss:           737601.7272, Accuracy: 6671/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 7 [0/29981                 (0%)]\tLoss: 0.478923\n",
      "Training stage for Flod 2 Epoch: 7 [3200/29981                 (11%)]\tLoss: 0.406938\n",
      "Training stage for Flod 2 Epoch: 7 [6400/29981                 (21%)]\tLoss: 0.386582\n",
      "Training stage for Flod 2 Epoch: 7 [9600/29981                 (32%)]\tLoss: 0.408261\n",
      "Training stage for Flod 2 Epoch: 7 [12800/29981                 (43%)]\tLoss: 0.314829\n",
      "Training stage for Flod 2 Epoch: 7 [16000/29981                 (53%)]\tLoss: 0.456056\n",
      "Training stage for Flod 2 Epoch: 7 [19200/29981                 (64%)]\tLoss: 0.511169\n",
      "Training stage for Flod 2 Epoch: 7 [22400/29981                 (75%)]\tLoss: 0.469978\n",
      "Training stage for Flod 2 Epoch: 7 [25600/29981                 (85%)]\tLoss: 0.436363\n",
      "Training stage for Flod 2 Epoch: 7 [28800/29981                 (96%)]\tLoss: 0.402108\n",
      "Test set for fold2: Average Loss:           736567.3689, Accuracy: 6659/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 8 [0/29981                 (0%)]\tLoss: 0.383077\n",
      "Training stage for Flod 2 Epoch: 8 [3200/29981                 (11%)]\tLoss: 0.427890\n",
      "Training stage for Flod 2 Epoch: 8 [6400/29981                 (21%)]\tLoss: 0.386468\n",
      "Training stage for Flod 2 Epoch: 8 [9600/29981                 (32%)]\tLoss: 0.438486\n",
      "Training stage for Flod 2 Epoch: 8 [12800/29981                 (43%)]\tLoss: 0.355992\n",
      "Training stage for Flod 2 Epoch: 8 [16000/29981                 (53%)]\tLoss: 0.379135\n",
      "Training stage for Flod 2 Epoch: 8 [19200/29981                 (64%)]\tLoss: 0.380976\n",
      "Training stage for Flod 2 Epoch: 8 [22400/29981                 (75%)]\tLoss: 0.494681\n",
      "Training stage for Flod 2 Epoch: 8 [25600/29981                 (85%)]\tLoss: 0.386110\n",
      "Training stage for Flod 2 Epoch: 8 [28800/29981                 (96%)]\tLoss: 0.323029\n",
      "Test set for fold2: Average Loss:           727286.6276, Accuracy: 6717/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 9 [0/29981                 (0%)]\tLoss: 0.380097\n",
      "Training stage for Flod 2 Epoch: 9 [3200/29981                 (11%)]\tLoss: 0.382235\n",
      "Training stage for Flod 2 Epoch: 9 [6400/29981                 (21%)]\tLoss: 0.473033\n",
      "Training stage for Flod 2 Epoch: 9 [9600/29981                 (32%)]\tLoss: 0.354389\n",
      "Training stage for Flod 2 Epoch: 9 [12800/29981                 (43%)]\tLoss: 0.407340\n",
      "Training stage for Flod 2 Epoch: 9 [16000/29981                 (53%)]\tLoss: 0.359676\n",
      "Training stage for Flod 2 Epoch: 9 [19200/29981                 (64%)]\tLoss: 0.387173\n",
      "Training stage for Flod 2 Epoch: 9 [22400/29981                 (75%)]\tLoss: 0.326095\n",
      "Training stage for Flod 2 Epoch: 9 [25600/29981                 (85%)]\tLoss: 0.440612\n",
      "Training stage for Flod 2 Epoch: 9 [28800/29981                 (96%)]\tLoss: 0.356284\n",
      "Test set for fold2: Average Loss:           725841.4152, Accuracy: 6719/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 10 [0/29981                 (0%)]\tLoss: 0.416830\n",
      "Training stage for Flod 2 Epoch: 10 [3200/29981                 (11%)]\tLoss: 0.435785\n",
      "Training stage for Flod 2 Epoch: 10 [6400/29981                 (21%)]\tLoss: 0.411558\n",
      "Training stage for Flod 2 Epoch: 10 [9600/29981                 (32%)]\tLoss: 0.392781\n",
      "Training stage for Flod 2 Epoch: 10 [12800/29981                 (43%)]\tLoss: 0.368663\n",
      "Training stage for Flod 2 Epoch: 10 [16000/29981                 (53%)]\tLoss: 0.392158\n",
      "Training stage for Flod 2 Epoch: 10 [19200/29981                 (64%)]\tLoss: 0.376668\n",
      "Training stage for Flod 2 Epoch: 10 [22400/29981                 (75%)]\tLoss: 0.383096\n",
      "Training stage for Flod 2 Epoch: 10 [25600/29981                 (85%)]\tLoss: 0.358234\n",
      "Training stage for Flod 2 Epoch: 10 [28800/29981                 (96%)]\tLoss: 0.406339\n",
      "Test set for fold2: Average Loss:           727903.0717, Accuracy: 6717/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 11 [0/29981                 (0%)]\tLoss: 0.484900\n",
      "Training stage for Flod 2 Epoch: 11 [3200/29981                 (11%)]\tLoss: 0.387090\n",
      "Training stage for Flod 2 Epoch: 11 [6400/29981                 (21%)]\tLoss: 0.313625\n",
      "Training stage for Flod 2 Epoch: 11 [9600/29981                 (32%)]\tLoss: 0.383030\n",
      "Training stage for Flod 2 Epoch: 11 [12800/29981                 (43%)]\tLoss: 0.352300\n",
      "Training stage for Flod 2 Epoch: 11 [16000/29981                 (53%)]\tLoss: 0.405743\n",
      "Training stage for Flod 2 Epoch: 11 [19200/29981                 (64%)]\tLoss: 0.377023\n",
      "Training stage for Flod 2 Epoch: 11 [22400/29981                 (75%)]\tLoss: 0.392605\n",
      "Training stage for Flod 2 Epoch: 11 [25600/29981                 (85%)]\tLoss: 0.380526\n",
      "Training stage for Flod 2 Epoch: 11 [28800/29981                 (96%)]\tLoss: 0.454280\n",
      "Test set for fold2: Average Loss:           741629.4152, Accuracy: 6648/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 12 [0/29981                 (0%)]\tLoss: 0.499638\n",
      "Training stage for Flod 2 Epoch: 12 [3200/29981                 (11%)]\tLoss: 0.435685\n",
      "Training stage for Flod 2 Epoch: 12 [6400/29981                 (21%)]\tLoss: 0.346179\n",
      "Training stage for Flod 2 Epoch: 12 [9600/29981                 (32%)]\tLoss: 0.462450\n",
      "Training stage for Flod 2 Epoch: 12 [12800/29981                 (43%)]\tLoss: 0.389244\n",
      "Training stage for Flod 2 Epoch: 12 [16000/29981                 (53%)]\tLoss: 0.320628\n",
      "Training stage for Flod 2 Epoch: 12 [19200/29981                 (64%)]\tLoss: 0.346876\n",
      "Training stage for Flod 2 Epoch: 12 [22400/29981                 (75%)]\tLoss: 0.343829\n",
      "Training stage for Flod 2 Epoch: 12 [25600/29981                 (85%)]\tLoss: 0.385387\n",
      "Training stage for Flod 2 Epoch: 12 [28800/29981                 (96%)]\tLoss: 0.408435\n",
      "Test set for fold2: Average Loss:           745049.5208, Accuracy: 6639/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 13 [0/29981                 (0%)]\tLoss: 0.407063\n",
      "Training stage for Flod 2 Epoch: 13 [3200/29981                 (11%)]\tLoss: 0.381163\n",
      "Training stage for Flod 2 Epoch: 13 [6400/29981                 (21%)]\tLoss: 0.364749\n",
      "Training stage for Flod 2 Epoch: 13 [9600/29981                 (32%)]\tLoss: 0.342835\n",
      "Training stage for Flod 2 Epoch: 13 [12800/29981                 (43%)]\tLoss: 0.408182\n",
      "Training stage for Flod 2 Epoch: 13 [16000/29981                 (53%)]\tLoss: 0.433228\n",
      "Training stage for Flod 2 Epoch: 13 [19200/29981                 (64%)]\tLoss: 0.383371\n",
      "Training stage for Flod 2 Epoch: 13 [22400/29981                 (75%)]\tLoss: 0.386833\n",
      "Training stage for Flod 2 Epoch: 13 [25600/29981                 (85%)]\tLoss: 0.412657\n",
      "Training stage for Flod 2 Epoch: 13 [28800/29981                 (96%)]\tLoss: 0.496401\n",
      "Test set for fold2: Average Loss:           723878.5824, Accuracy: 6720/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 14 [0/29981                 (0%)]\tLoss: 0.408925\n",
      "Training stage for Flod 2 Epoch: 14 [3200/29981                 (11%)]\tLoss: 0.407016\n",
      "Training stage for Flod 2 Epoch: 14 [6400/29981                 (21%)]\tLoss: 0.335425\n",
      "Training stage for Flod 2 Epoch: 14 [9600/29981                 (32%)]\tLoss: 0.534692\n",
      "Training stage for Flod 2 Epoch: 14 [12800/29981                 (43%)]\tLoss: 0.384724\n",
      "Training stage for Flod 2 Epoch: 14 [16000/29981                 (53%)]\tLoss: 0.366167\n",
      "Training stage for Flod 2 Epoch: 14 [19200/29981                 (64%)]\tLoss: 0.355639\n",
      "Training stage for Flod 2 Epoch: 14 [22400/29981                 (75%)]\tLoss: 0.433307\n",
      "Training stage for Flod 2 Epoch: 14 [25600/29981                 (85%)]\tLoss: 0.381672\n",
      "Training stage for Flod 2 Epoch: 14 [28800/29981                 (96%)]\tLoss: 0.317861\n",
      "Test set for fold2: Average Loss:           728944.1916, Accuracy: 6694/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 15 [0/29981                 (0%)]\tLoss: 0.405843\n",
      "Training stage for Flod 2 Epoch: 15 [3200/29981                 (11%)]\tLoss: 0.444601\n",
      "Training stage for Flod 2 Epoch: 15 [6400/29981                 (21%)]\tLoss: 0.377164\n",
      "Training stage for Flod 2 Epoch: 15 [9600/29981                 (32%)]\tLoss: 0.471754\n",
      "Training stage for Flod 2 Epoch: 15 [12800/29981                 (43%)]\tLoss: 0.347887\n",
      "Training stage for Flod 2 Epoch: 15 [16000/29981                 (53%)]\tLoss: 0.396671\n",
      "Training stage for Flod 2 Epoch: 15 [19200/29981                 (64%)]\tLoss: 0.375946\n",
      "Training stage for Flod 2 Epoch: 15 [22400/29981                 (75%)]\tLoss: 0.412762\n",
      "Training stage for Flod 2 Epoch: 15 [25600/29981                 (85%)]\tLoss: 0.367416\n",
      "Training stage for Flod 2 Epoch: 15 [28800/29981                 (96%)]\tLoss: 0.472291\n",
      "Test set for fold2: Average Loss:           710265.3193, Accuracy: 6823/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 16 [0/29981                 (0%)]\tLoss: 0.358333\n",
      "Training stage for Flod 2 Epoch: 16 [3200/29981                 (11%)]\tLoss: 0.376265\n",
      "Training stage for Flod 2 Epoch: 16 [6400/29981                 (21%)]\tLoss: 0.407272\n",
      "Training stage for Flod 2 Epoch: 16 [9600/29981                 (32%)]\tLoss: 0.380501\n",
      "Training stage for Flod 2 Epoch: 16 [12800/29981                 (43%)]\tLoss: 0.439028\n",
      "Training stage for Flod 2 Epoch: 16 [16000/29981                 (53%)]\tLoss: 0.375595\n",
      "Training stage for Flod 2 Epoch: 16 [19200/29981                 (64%)]\tLoss: 0.316648\n",
      "Training stage for Flod 2 Epoch: 16 [22400/29981                 (75%)]\tLoss: 0.438046\n",
      "Training stage for Flod 2 Epoch: 16 [25600/29981                 (85%)]\tLoss: 0.406220\n",
      "Training stage for Flod 2 Epoch: 16 [28800/29981                 (96%)]\tLoss: 0.345014\n",
      "Test set for fold2: Average Loss:           738925.9632, Accuracy: 6643/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 17 [0/29981                 (0%)]\tLoss: 0.390709\n",
      "Training stage for Flod 2 Epoch: 17 [3200/29981                 (11%)]\tLoss: 0.375208\n",
      "Training stage for Flod 2 Epoch: 17 [6400/29981                 (21%)]\tLoss: 0.408253\n",
      "Training stage for Flod 2 Epoch: 17 [9600/29981                 (32%)]\tLoss: 0.381454\n",
      "Training stage for Flod 2 Epoch: 17 [12800/29981                 (43%)]\tLoss: 0.454112\n",
      "Training stage for Flod 2 Epoch: 17 [16000/29981                 (53%)]\tLoss: 0.408903\n",
      "Training stage for Flod 2 Epoch: 17 [19200/29981                 (64%)]\tLoss: 0.409295\n",
      "Training stage for Flod 2 Epoch: 17 [22400/29981                 (75%)]\tLoss: 0.464857\n",
      "Training stage for Flod 2 Epoch: 17 [25600/29981                 (85%)]\tLoss: 0.346815\n",
      "Training stage for Flod 2 Epoch: 17 [28800/29981                 (96%)]\tLoss: 0.344845\n",
      "Test set for fold2: Average Loss:           779838.3334, Accuracy: 6438/7495           (86%)\n",
      "Training stage for Flod 2 Epoch: 18 [0/29981                 (0%)]\tLoss: 0.465809\n",
      "Training stage for Flod 2 Epoch: 18 [3200/29981                 (11%)]\tLoss: 0.447753\n",
      "Training stage for Flod 2 Epoch: 18 [6400/29981                 (21%)]\tLoss: 0.398575\n",
      "Training stage for Flod 2 Epoch: 18 [9600/29981                 (32%)]\tLoss: 0.434203\n",
      "Training stage for Flod 2 Epoch: 18 [12800/29981                 (43%)]\tLoss: 0.440244\n",
      "Training stage for Flod 2 Epoch: 18 [16000/29981                 (53%)]\tLoss: 0.379938\n",
      "Training stage for Flod 2 Epoch: 18 [19200/29981                 (64%)]\tLoss: 0.377107\n",
      "Training stage for Flod 2 Epoch: 18 [22400/29981                 (75%)]\tLoss: 0.386055\n",
      "Training stage for Flod 2 Epoch: 18 [25600/29981                 (85%)]\tLoss: 0.386102\n",
      "Training stage for Flod 2 Epoch: 18 [28800/29981                 (96%)]\tLoss: 0.412825\n",
      "Test set for fold2: Average Loss:           746307.7722, Accuracy: 6637/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 19 [0/29981                 (0%)]\tLoss: 0.379399\n",
      "Training stage for Flod 2 Epoch: 19 [3200/29981                 (11%)]\tLoss: 0.381367\n",
      "Training stage for Flod 2 Epoch: 19 [6400/29981                 (21%)]\tLoss: 0.316390\n",
      "Training stage for Flod 2 Epoch: 19 [9600/29981                 (32%)]\tLoss: 0.355551\n",
      "Training stage for Flod 2 Epoch: 19 [12800/29981                 (43%)]\tLoss: 0.375314\n",
      "Training stage for Flod 2 Epoch: 19 [16000/29981                 (53%)]\tLoss: 0.404729\n",
      "Training stage for Flod 2 Epoch: 19 [19200/29981                 (64%)]\tLoss: 0.444060\n",
      "Training stage for Flod 2 Epoch: 19 [22400/29981                 (75%)]\tLoss: 0.439778\n",
      "Training stage for Flod 2 Epoch: 19 [25600/29981                 (85%)]\tLoss: 0.345384\n",
      "Training stage for Flod 2 Epoch: 19 [28800/29981                 (96%)]\tLoss: 0.344603\n",
      "Test set for fold2: Average Loss:           717481.7392, Accuracy: 6759/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 20 [0/29981                 (0%)]\tLoss: 0.397340\n",
      "Training stage for Flod 2 Epoch: 20 [3200/29981                 (11%)]\tLoss: 0.409189\n",
      "Training stage for Flod 2 Epoch: 20 [6400/29981                 (21%)]\tLoss: 0.352508\n",
      "Training stage for Flod 2 Epoch: 20 [9600/29981                 (32%)]\tLoss: 0.503262\n",
      "Training stage for Flod 2 Epoch: 20 [12800/29981                 (43%)]\tLoss: 0.406468\n",
      "Training stage for Flod 2 Epoch: 20 [16000/29981                 (53%)]\tLoss: 0.333920\n",
      "Training stage for Flod 2 Epoch: 20 [19200/29981                 (64%)]\tLoss: 0.353852\n",
      "Training stage for Flod 2 Epoch: 20 [22400/29981                 (75%)]\tLoss: 0.313848\n",
      "Training stage for Flod 2 Epoch: 20 [25600/29981                 (85%)]\tLoss: 0.493387\n",
      "Training stage for Flod 2 Epoch: 20 [28800/29981                 (96%)]\tLoss: 0.426338\n",
      "Test set for fold2: Average Loss:           745573.8102, Accuracy: 6639/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 21 [0/29981                 (0%)]\tLoss: 0.371647\n",
      "Training stage for Flod 2 Epoch: 21 [3200/29981                 (11%)]\tLoss: 0.356424\n",
      "Training stage for Flod 2 Epoch: 21 [6400/29981                 (21%)]\tLoss: 0.376084\n",
      "Training stage for Flod 2 Epoch: 21 [9600/29981                 (32%)]\tLoss: 0.379307\n",
      "Training stage for Flod 2 Epoch: 21 [12800/29981                 (43%)]\tLoss: 0.432780\n",
      "Training stage for Flod 2 Epoch: 21 [16000/29981                 (53%)]\tLoss: 0.348834\n",
      "Training stage for Flod 2 Epoch: 21 [19200/29981                 (64%)]\tLoss: 0.372647\n",
      "Training stage for Flod 2 Epoch: 21 [22400/29981                 (75%)]\tLoss: 0.345052\n",
      "Training stage for Flod 2 Epoch: 21 [25600/29981                 (85%)]\tLoss: 0.410013\n",
      "Training stage for Flod 2 Epoch: 21 [28800/29981                 (96%)]\tLoss: 0.377227\n",
      "Test set for fold2: Average Loss:           729122.7007, Accuracy: 6689/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 22 [0/29981                 (0%)]\tLoss: 0.360351\n",
      "Training stage for Flod 2 Epoch: 22 [3200/29981                 (11%)]\tLoss: 0.368293\n",
      "Training stage for Flod 2 Epoch: 22 [6400/29981                 (21%)]\tLoss: 0.373404\n",
      "Training stage for Flod 2 Epoch: 22 [9600/29981                 (32%)]\tLoss: 0.344594\n",
      "Training stage for Flod 2 Epoch: 22 [12800/29981                 (43%)]\tLoss: 0.345832\n",
      "Training stage for Flod 2 Epoch: 22 [16000/29981                 (53%)]\tLoss: 0.388462\n",
      "Training stage for Flod 2 Epoch: 22 [19200/29981                 (64%)]\tLoss: 0.401980\n",
      "Training stage for Flod 2 Epoch: 22 [22400/29981                 (75%)]\tLoss: 0.345373\n",
      "Training stage for Flod 2 Epoch: 22 [25600/29981                 (85%)]\tLoss: 0.375964\n",
      "Training stage for Flod 2 Epoch: 22 [28800/29981                 (96%)]\tLoss: 0.452157\n",
      "Test set for fold2: Average Loss:           736554.7872, Accuracy: 6705/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 23 [0/29981                 (0%)]\tLoss: 0.382225\n",
      "Training stage for Flod 2 Epoch: 23 [3200/29981                 (11%)]\tLoss: 0.344687\n",
      "Training stage for Flod 2 Epoch: 23 [6400/29981                 (21%)]\tLoss: 0.345745\n",
      "Training stage for Flod 2 Epoch: 23 [9600/29981                 (32%)]\tLoss: 0.461127\n",
      "Training stage for Flod 2 Epoch: 23 [12800/29981                 (43%)]\tLoss: 0.375779\n",
      "Training stage for Flod 2 Epoch: 23 [16000/29981                 (53%)]\tLoss: 0.443280\n",
      "Training stage for Flod 2 Epoch: 23 [19200/29981                 (64%)]\tLoss: 0.373861\n",
      "Training stage for Flod 2 Epoch: 23 [22400/29981                 (75%)]\tLoss: 0.437014\n",
      "Training stage for Flod 2 Epoch: 23 [25600/29981                 (85%)]\tLoss: 0.442877\n",
      "Training stage for Flod 2 Epoch: 23 [28800/29981                 (96%)]\tLoss: 0.406411\n",
      "Test set for fold2: Average Loss:           731143.4146, Accuracy: 6665/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 24 [0/29981                 (0%)]\tLoss: 0.344302\n",
      "Training stage for Flod 2 Epoch: 24 [3200/29981                 (11%)]\tLoss: 0.438260\n",
      "Training stage for Flod 2 Epoch: 24 [6400/29981                 (21%)]\tLoss: 0.345048\n",
      "Training stage for Flod 2 Epoch: 24 [9600/29981                 (32%)]\tLoss: 0.428478\n",
      "Training stage for Flod 2 Epoch: 24 [12800/29981                 (43%)]\tLoss: 0.345224\n",
      "Training stage for Flod 2 Epoch: 24 [16000/29981                 (53%)]\tLoss: 0.346735\n",
      "Training stage for Flod 2 Epoch: 24 [19200/29981                 (64%)]\tLoss: 0.497424\n",
      "Training stage for Flod 2 Epoch: 24 [22400/29981                 (75%)]\tLoss: 0.370832\n",
      "Training stage for Flod 2 Epoch: 24 [25600/29981                 (85%)]\tLoss: 0.418736\n",
      "Training stage for Flod 2 Epoch: 24 [28800/29981                 (96%)]\tLoss: 0.438180\n",
      "Test set for fold2: Average Loss:           759238.6981, Accuracy: 6537/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 25 [0/29981                 (0%)]\tLoss: 0.430895\n",
      "Training stage for Flod 2 Epoch: 25 [3200/29981                 (11%)]\tLoss: 0.500167\n",
      "Training stage for Flod 2 Epoch: 25 [6400/29981                 (21%)]\tLoss: 0.358117\n",
      "Training stage for Flod 2 Epoch: 25 [9600/29981                 (32%)]\tLoss: 0.342765\n",
      "Training stage for Flod 2 Epoch: 25 [12800/29981                 (43%)]\tLoss: 0.351682\n",
      "Training stage for Flod 2 Epoch: 25 [16000/29981                 (53%)]\tLoss: 0.345817\n",
      "Training stage for Flod 2 Epoch: 25 [19200/29981                 (64%)]\tLoss: 0.347597\n",
      "Training stage for Flod 2 Epoch: 25 [22400/29981                 (75%)]\tLoss: 0.464790\n",
      "Training stage for Flod 2 Epoch: 25 [25600/29981                 (85%)]\tLoss: 0.433016\n",
      "Training stage for Flod 2 Epoch: 25 [28800/29981                 (96%)]\tLoss: 0.344682\n",
      "Test set for fold2: Average Loss:           715646.8240, Accuracy: 6769/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 26 [0/29981                 (0%)]\tLoss: 0.401481\n",
      "Training stage for Flod 2 Epoch: 26 [3200/29981                 (11%)]\tLoss: 0.375852\n",
      "Training stage for Flod 2 Epoch: 26 [6400/29981                 (21%)]\tLoss: 0.380008\n",
      "Training stage for Flod 2 Epoch: 26 [9600/29981                 (32%)]\tLoss: 0.378565\n",
      "Training stage for Flod 2 Epoch: 26 [12800/29981                 (43%)]\tLoss: 0.402028\n",
      "Training stage for Flod 2 Epoch: 26 [16000/29981                 (53%)]\tLoss: 0.343246\n",
      "Training stage for Flod 2 Epoch: 26 [19200/29981                 (64%)]\tLoss: 0.381657\n",
      "Training stage for Flod 2 Epoch: 26 [22400/29981                 (75%)]\tLoss: 0.344513\n",
      "Training stage for Flod 2 Epoch: 26 [25600/29981                 (85%)]\tLoss: 0.433609\n",
      "Training stage for Flod 2 Epoch: 26 [28800/29981                 (96%)]\tLoss: 0.381891\n",
      "Test set for fold2: Average Loss:           749189.9119, Accuracy: 6595/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 27 [0/29981                 (0%)]\tLoss: 0.393125\n",
      "Training stage for Flod 2 Epoch: 27 [3200/29981                 (11%)]\tLoss: 0.375767\n",
      "Training stage for Flod 2 Epoch: 27 [6400/29981                 (21%)]\tLoss: 0.346956\n",
      "Training stage for Flod 2 Epoch: 27 [9600/29981                 (32%)]\tLoss: 0.489774\n",
      "Training stage for Flod 2 Epoch: 27 [12800/29981                 (43%)]\tLoss: 0.407493\n",
      "Training stage for Flod 2 Epoch: 27 [16000/29981                 (53%)]\tLoss: 0.314989\n",
      "Training stage for Flod 2 Epoch: 27 [19200/29981                 (64%)]\tLoss: 0.400594\n",
      "Training stage for Flod 2 Epoch: 27 [22400/29981                 (75%)]\tLoss: 0.401366\n",
      "Training stage for Flod 2 Epoch: 27 [25600/29981                 (85%)]\tLoss: 0.345219\n",
      "Training stage for Flod 2 Epoch: 27 [28800/29981                 (96%)]\tLoss: 0.448993\n",
      "Test set for fold2: Average Loss:           842581.9107, Accuracy: 6210/7495           (83%)\n",
      "Training stage for Flod 2 Epoch: 28 [0/29981                 (0%)]\tLoss: 0.482513\n",
      "Training stage for Flod 2 Epoch: 28 [3200/29981                 (11%)]\tLoss: 0.376217\n",
      "Training stage for Flod 2 Epoch: 28 [6400/29981                 (21%)]\tLoss: 0.419367\n",
      "Training stage for Flod 2 Epoch: 28 [9600/29981                 (32%)]\tLoss: 0.407409\n",
      "Training stage for Flod 2 Epoch: 28 [12800/29981                 (43%)]\tLoss: 0.366377\n",
      "Training stage for Flod 2 Epoch: 28 [16000/29981                 (53%)]\tLoss: 0.381712\n",
      "Training stage for Flod 2 Epoch: 28 [19200/29981                 (64%)]\tLoss: 0.344673\n",
      "Training stage for Flod 2 Epoch: 28 [22400/29981                 (75%)]\tLoss: 0.349100\n",
      "Training stage for Flod 2 Epoch: 28 [25600/29981                 (85%)]\tLoss: 0.356482\n",
      "Training stage for Flod 2 Epoch: 28 [28800/29981                 (96%)]\tLoss: 0.360791\n",
      "Test set for fold2: Average Loss:           734439.0984, Accuracy: 6709/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 29 [0/29981                 (0%)]\tLoss: 0.475464\n",
      "Training stage for Flod 2 Epoch: 29 [3200/29981                 (11%)]\tLoss: 0.497674\n",
      "Training stage for Flod 2 Epoch: 29 [6400/29981                 (21%)]\tLoss: 0.344604\n",
      "Training stage for Flod 2 Epoch: 29 [9600/29981                 (32%)]\tLoss: 0.406430\n",
      "Training stage for Flod 2 Epoch: 29 [12800/29981                 (43%)]\tLoss: 0.314727\n",
      "Training stage for Flod 2 Epoch: 29 [16000/29981                 (53%)]\tLoss: 0.376301\n",
      "Training stage for Flod 2 Epoch: 29 [19200/29981                 (64%)]\tLoss: 0.380307\n",
      "Training stage for Flod 2 Epoch: 29 [22400/29981                 (75%)]\tLoss: 0.380924\n",
      "Training stage for Flod 2 Epoch: 29 [25600/29981                 (85%)]\tLoss: 0.447519\n",
      "Training stage for Flod 2 Epoch: 29 [28800/29981                 (96%)]\tLoss: 0.344664\n",
      "Test set for fold2: Average Loss:           739054.1090, Accuracy: 6633/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 30 [0/29981                 (0%)]\tLoss: 0.438542\n",
      "Training stage for Flod 2 Epoch: 30 [3200/29981                 (11%)]\tLoss: 0.447595\n",
      "Training stage for Flod 2 Epoch: 30 [6400/29981                 (21%)]\tLoss: 0.409021\n",
      "Training stage for Flod 2 Epoch: 30 [9600/29981                 (32%)]\tLoss: 0.313266\n",
      "Training stage for Flod 2 Epoch: 30 [12800/29981                 (43%)]\tLoss: 0.425233\n",
      "Training stage for Flod 2 Epoch: 30 [16000/29981                 (53%)]\tLoss: 0.336480\n",
      "Training stage for Flod 2 Epoch: 30 [19200/29981                 (64%)]\tLoss: 0.367560\n",
      "Training stage for Flod 2 Epoch: 30 [22400/29981                 (75%)]\tLoss: 0.347020\n",
      "Training stage for Flod 2 Epoch: 30 [25600/29981                 (85%)]\tLoss: 0.381245\n",
      "Training stage for Flod 2 Epoch: 30 [28800/29981                 (96%)]\tLoss: 0.414782\n",
      "Test set for fold2: Average Loss:           766034.9749, Accuracy: 6551/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 31 [0/29981                 (0%)]\tLoss: 0.442182\n",
      "Training stage for Flod 2 Epoch: 31 [3200/29981                 (11%)]\tLoss: 0.344760\n",
      "Training stage for Flod 2 Epoch: 31 [6400/29981                 (21%)]\tLoss: 0.343429\n",
      "Training stage for Flod 2 Epoch: 31 [9600/29981                 (32%)]\tLoss: 0.377089\n",
      "Training stage for Flod 2 Epoch: 31 [12800/29981                 (43%)]\tLoss: 0.397265\n",
      "Training stage for Flod 2 Epoch: 31 [16000/29981                 (53%)]\tLoss: 0.372802\n",
      "Training stage for Flod 2 Epoch: 31 [19200/29981                 (64%)]\tLoss: 0.338109\n",
      "Training stage for Flod 2 Epoch: 31 [22400/29981                 (75%)]\tLoss: 0.327425\n",
      "Training stage for Flod 2 Epoch: 31 [25600/29981                 (85%)]\tLoss: 0.345102\n",
      "Training stage for Flod 2 Epoch: 31 [28800/29981                 (96%)]\tLoss: 0.373018\n",
      "Test set for fold2: Average Loss:           719336.3469, Accuracy: 6761/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 32 [0/29981                 (0%)]\tLoss: 0.339673\n",
      "Training stage for Flod 2 Epoch: 32 [3200/29981                 (11%)]\tLoss: 0.526168\n",
      "Training stage for Flod 2 Epoch: 32 [6400/29981                 (21%)]\tLoss: 0.368269\n",
      "Training stage for Flod 2 Epoch: 32 [9600/29981                 (32%)]\tLoss: 0.409832\n",
      "Training stage for Flod 2 Epoch: 32 [12800/29981                 (43%)]\tLoss: 0.377410\n",
      "Training stage for Flod 2 Epoch: 32 [16000/29981                 (53%)]\tLoss: 0.403935\n",
      "Training stage for Flod 2 Epoch: 32 [19200/29981                 (64%)]\tLoss: 0.344177\n",
      "Training stage for Flod 2 Epoch: 32 [22400/29981                 (75%)]\tLoss: 0.411036\n",
      "Training stage for Flod 2 Epoch: 32 [25600/29981                 (85%)]\tLoss: 0.357943\n",
      "Training stage for Flod 2 Epoch: 32 [28800/29981                 (96%)]\tLoss: 0.394022\n",
      "Test set for fold2: Average Loss:           776925.3628, Accuracy: 6458/7495           (86%)\n",
      "Training stage for Flod 2 Epoch: 33 [0/29981                 (0%)]\tLoss: 0.354527\n",
      "Training stage for Flod 2 Epoch: 33 [3200/29981                 (11%)]\tLoss: 0.468721\n",
      "Training stage for Flod 2 Epoch: 33 [6400/29981                 (21%)]\tLoss: 0.375763\n",
      "Training stage for Flod 2 Epoch: 33 [9600/29981                 (32%)]\tLoss: 0.342486\n",
      "Training stage for Flod 2 Epoch: 33 [12800/29981                 (43%)]\tLoss: 0.381540\n",
      "Training stage for Flod 2 Epoch: 33 [16000/29981                 (53%)]\tLoss: 0.353966\n",
      "Training stage for Flod 2 Epoch: 33 [19200/29981                 (64%)]\tLoss: 0.341000\n",
      "Training stage for Flod 2 Epoch: 33 [22400/29981                 (75%)]\tLoss: 0.313421\n",
      "Training stage for Flod 2 Epoch: 33 [25600/29981                 (85%)]\tLoss: 0.344520\n",
      "Training stage for Flod 2 Epoch: 33 [28800/29981                 (96%)]\tLoss: 0.398946\n",
      "Test set for fold2: Average Loss:           732832.2792, Accuracy: 6689/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 34 [0/29981                 (0%)]\tLoss: 0.376844\n",
      "Training stage for Flod 2 Epoch: 34 [3200/29981                 (11%)]\tLoss: 0.384829\n",
      "Training stage for Flod 2 Epoch: 34 [6400/29981                 (21%)]\tLoss: 0.324765\n",
      "Training stage for Flod 2 Epoch: 34 [9600/29981                 (32%)]\tLoss: 0.403445\n",
      "Training stage for Flod 2 Epoch: 34 [12800/29981                 (43%)]\tLoss: 0.332925\n",
      "Training stage for Flod 2 Epoch: 34 [16000/29981                 (53%)]\tLoss: 0.476838\n",
      "Training stage for Flod 2 Epoch: 34 [19200/29981                 (64%)]\tLoss: 0.372286\n",
      "Training stage for Flod 2 Epoch: 34 [22400/29981                 (75%)]\tLoss: 0.386484\n",
      "Training stage for Flod 2 Epoch: 34 [25600/29981                 (85%)]\tLoss: 0.345002\n",
      "Training stage for Flod 2 Epoch: 34 [28800/29981                 (96%)]\tLoss: 0.414261\n",
      "Test set for fold2: Average Loss:           720874.0912, Accuracy: 6744/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 35 [0/29981                 (0%)]\tLoss: 0.339337\n",
      "Training stage for Flod 2 Epoch: 35 [3200/29981                 (11%)]\tLoss: 0.446442\n",
      "Training stage for Flod 2 Epoch: 35 [6400/29981                 (21%)]\tLoss: 0.456983\n",
      "Training stage for Flod 2 Epoch: 35 [9600/29981                 (32%)]\tLoss: 0.366206\n",
      "Training stage for Flod 2 Epoch: 35 [12800/29981                 (43%)]\tLoss: 0.345098\n",
      "Training stage for Flod 2 Epoch: 35 [16000/29981                 (53%)]\tLoss: 0.438372\n",
      "Training stage for Flod 2 Epoch: 35 [19200/29981                 (64%)]\tLoss: 0.425844\n",
      "Training stage for Flod 2 Epoch: 35 [22400/29981                 (75%)]\tLoss: 0.388371\n",
      "Training stage for Flod 2 Epoch: 35 [25600/29981                 (85%)]\tLoss: 0.344653\n",
      "Training stage for Flod 2 Epoch: 35 [28800/29981                 (96%)]\tLoss: 0.344520\n",
      "Test set for fold2: Average Loss:           727522.1917, Accuracy: 6728/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 36 [0/29981                 (0%)]\tLoss: 0.461893\n",
      "Training stage for Flod 2 Epoch: 36 [3200/29981                 (11%)]\tLoss: 0.314915\n",
      "Training stage for Flod 2 Epoch: 36 [6400/29981                 (21%)]\tLoss: 0.346654\n",
      "Training stage for Flod 2 Epoch: 36 [9600/29981                 (32%)]\tLoss: 0.313303\n",
      "Training stage for Flod 2 Epoch: 36 [12800/29981                 (43%)]\tLoss: 0.345061\n",
      "Training stage for Flod 2 Epoch: 36 [16000/29981                 (53%)]\tLoss: 0.416126\n",
      "Training stage for Flod 2 Epoch: 36 [19200/29981                 (64%)]\tLoss: 0.339983\n",
      "Training stage for Flod 2 Epoch: 36 [22400/29981                 (75%)]\tLoss: 0.499340\n",
      "Training stage for Flod 2 Epoch: 36 [25600/29981                 (85%)]\tLoss: 0.409489\n",
      "Training stage for Flod 2 Epoch: 36 [28800/29981                 (96%)]\tLoss: 0.321003\n",
      "Test set for fold2: Average Loss:           776438.3725, Accuracy: 6504/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 37 [0/29981                 (0%)]\tLoss: 0.386741\n",
      "Training stage for Flod 2 Epoch: 37 [3200/29981                 (11%)]\tLoss: 0.402151\n",
      "Training stage for Flod 2 Epoch: 37 [6400/29981                 (21%)]\tLoss: 0.378617\n",
      "Training stage for Flod 2 Epoch: 37 [9600/29981                 (32%)]\tLoss: 0.313278\n",
      "Training stage for Flod 2 Epoch: 37 [12800/29981                 (43%)]\tLoss: 0.344961\n",
      "Training stage for Flod 2 Epoch: 37 [16000/29981                 (53%)]\tLoss: 0.435092\n",
      "Training stage for Flod 2 Epoch: 37 [19200/29981                 (64%)]\tLoss: 0.438308\n",
      "Training stage for Flod 2 Epoch: 37 [22400/29981                 (75%)]\tLoss: 0.375557\n",
      "Training stage for Flod 2 Epoch: 37 [25600/29981                 (85%)]\tLoss: 0.404863\n",
      "Training stage for Flod 2 Epoch: 37 [28800/29981                 (96%)]\tLoss: 0.405461\n",
      "Test set for fold2: Average Loss:           716950.6973, Accuracy: 6763/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 38 [0/29981                 (0%)]\tLoss: 0.375972\n",
      "Training stage for Flod 2 Epoch: 38 [3200/29981                 (11%)]\tLoss: 0.346981\n",
      "Training stage for Flod 2 Epoch: 38 [6400/29981                 (21%)]\tLoss: 0.344548\n",
      "Training stage for Flod 2 Epoch: 38 [9600/29981                 (32%)]\tLoss: 0.380751\n",
      "Training stage for Flod 2 Epoch: 38 [12800/29981                 (43%)]\tLoss: 0.314211\n",
      "Training stage for Flod 2 Epoch: 38 [16000/29981                 (53%)]\tLoss: 0.408757\n",
      "Training stage for Flod 2 Epoch: 38 [19200/29981                 (64%)]\tLoss: 0.347383\n",
      "Training stage for Flod 2 Epoch: 38 [22400/29981                 (75%)]\tLoss: 0.401671\n",
      "Training stage for Flod 2 Epoch: 38 [25600/29981                 (85%)]\tLoss: 0.317192\n",
      "Training stage for Flod 2 Epoch: 38 [28800/29981                 (96%)]\tLoss: 0.437701\n",
      "Test set for fold2: Average Loss:           709131.0036, Accuracy: 6798/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 39 [0/29981                 (0%)]\tLoss: 0.344534\n",
      "Training stage for Flod 2 Epoch: 39 [3200/29981                 (11%)]\tLoss: 0.313684\n",
      "Training stage for Flod 2 Epoch: 39 [6400/29981                 (21%)]\tLoss: 0.321798\n",
      "Training stage for Flod 2 Epoch: 39 [9600/29981                 (32%)]\tLoss: 0.344133\n",
      "Training stage for Flod 2 Epoch: 39 [12800/29981                 (43%)]\tLoss: 0.348229\n",
      "Training stage for Flod 2 Epoch: 39 [16000/29981                 (53%)]\tLoss: 0.344529\n",
      "Training stage for Flod 2 Epoch: 39 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 39 [22400/29981                 (75%)]\tLoss: 0.438320\n",
      "Training stage for Flod 2 Epoch: 39 [25600/29981                 (85%)]\tLoss: 0.389295\n",
      "Training stage for Flod 2 Epoch: 39 [28800/29981                 (96%)]\tLoss: 0.407059\n",
      "Test set for fold2: Average Loss:           735649.0216, Accuracy: 6680/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 40 [0/29981                 (0%)]\tLoss: 0.409461\n",
      "Training stage for Flod 2 Epoch: 40 [3200/29981                 (11%)]\tLoss: 0.421116\n",
      "Training stage for Flod 2 Epoch: 40 [6400/29981                 (21%)]\tLoss: 0.399495\n",
      "Training stage for Flod 2 Epoch: 40 [9600/29981                 (32%)]\tLoss: 0.345454\n",
      "Training stage for Flod 2 Epoch: 40 [12800/29981                 (43%)]\tLoss: 0.408265\n",
      "Training stage for Flod 2 Epoch: 40 [16000/29981                 (53%)]\tLoss: 0.387468\n",
      "Training stage for Flod 2 Epoch: 40 [19200/29981                 (64%)]\tLoss: 0.430181\n",
      "Training stage for Flod 2 Epoch: 40 [22400/29981                 (75%)]\tLoss: 0.400045\n",
      "Training stage for Flod 2 Epoch: 40 [25600/29981                 (85%)]\tLoss: 0.448744\n",
      "Training stage for Flod 2 Epoch: 40 [28800/29981                 (96%)]\tLoss: 0.434620\n",
      "Test set for fold2: Average Loss:           731558.8935, Accuracy: 6679/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 41 [0/29981                 (0%)]\tLoss: 0.313627\n",
      "Training stage for Flod 2 Epoch: 41 [3200/29981                 (11%)]\tLoss: 0.378700\n",
      "Training stage for Flod 2 Epoch: 41 [6400/29981                 (21%)]\tLoss: 0.345596\n",
      "Training stage for Flod 2 Epoch: 41 [9600/29981                 (32%)]\tLoss: 0.375765\n",
      "Training stage for Flod 2 Epoch: 41 [12800/29981                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 41 [16000/29981                 (53%)]\tLoss: 0.344514\n",
      "Training stage for Flod 2 Epoch: 41 [19200/29981                 (64%)]\tLoss: 0.405805\n",
      "Training stage for Flod 2 Epoch: 41 [22400/29981                 (75%)]\tLoss: 0.438301\n",
      "Training stage for Flod 2 Epoch: 41 [25600/29981                 (85%)]\tLoss: 0.408708\n",
      "Training stage for Flod 2 Epoch: 41 [28800/29981                 (96%)]\tLoss: 0.344744\n",
      "Test set for fold2: Average Loss:           720438.1683, Accuracy: 6735/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 42 [0/29981                 (0%)]\tLoss: 0.355333\n",
      "Training stage for Flod 2 Epoch: 42 [3200/29981                 (11%)]\tLoss: 0.345376\n",
      "Training stage for Flod 2 Epoch: 42 [6400/29981                 (21%)]\tLoss: 0.313299\n",
      "Training stage for Flod 2 Epoch: 42 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 42 [12800/29981                 (43%)]\tLoss: 0.395883\n",
      "Training stage for Flod 2 Epoch: 42 [16000/29981                 (53%)]\tLoss: 0.445309\n",
      "Training stage for Flod 2 Epoch: 42 [19200/29981                 (64%)]\tLoss: 0.438256\n",
      "Training stage for Flod 2 Epoch: 42 [22400/29981                 (75%)]\tLoss: 0.375805\n",
      "Training stage for Flod 2 Epoch: 42 [25600/29981                 (85%)]\tLoss: 0.346950\n",
      "Training stage for Flod 2 Epoch: 42 [28800/29981                 (96%)]\tLoss: 0.370477\n",
      "Test set for fold2: Average Loss:           770232.9054, Accuracy: 6541/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 43 [0/29981                 (0%)]\tLoss: 0.375782\n",
      "Training stage for Flod 2 Epoch: 43 [3200/29981                 (11%)]\tLoss: 0.377693\n",
      "Training stage for Flod 2 Epoch: 43 [6400/29981                 (21%)]\tLoss: 0.376098\n",
      "Training stage for Flod 2 Epoch: 43 [9600/29981                 (32%)]\tLoss: 0.375771\n",
      "Training stage for Flod 2 Epoch: 43 [12800/29981                 (43%)]\tLoss: 0.375792\n",
      "Training stage for Flod 2 Epoch: 43 [16000/29981                 (53%)]\tLoss: 0.411509\n",
      "Training stage for Flod 2 Epoch: 43 [19200/29981                 (64%)]\tLoss: 0.344518\n",
      "Training stage for Flod 2 Epoch: 43 [22400/29981                 (75%)]\tLoss: 0.410157\n",
      "Training stage for Flod 2 Epoch: 43 [25600/29981                 (85%)]\tLoss: 0.345033\n",
      "Training stage for Flod 2 Epoch: 43 [28800/29981                 (96%)]\tLoss: 0.352352\n",
      "Test set for fold2: Average Loss:           707954.9424, Accuracy: 6804/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 44 [0/29981                 (0%)]\tLoss: 0.313267\n",
      "Training stage for Flod 2 Epoch: 44 [3200/29981                 (11%)]\tLoss: 0.345959\n",
      "Training stage for Flod 2 Epoch: 44 [6400/29981                 (21%)]\tLoss: 0.407014\n",
      "Training stage for Flod 2 Epoch: 44 [9600/29981                 (32%)]\tLoss: 0.375649\n",
      "Training stage for Flod 2 Epoch: 44 [12800/29981                 (43%)]\tLoss: 0.345740\n",
      "Training stage for Flod 2 Epoch: 44 [16000/29981                 (53%)]\tLoss: 0.377020\n",
      "Training stage for Flod 2 Epoch: 44 [19200/29981                 (64%)]\tLoss: 0.438288\n",
      "Training stage for Flod 2 Epoch: 44 [22400/29981                 (75%)]\tLoss: 0.388138\n",
      "Training stage for Flod 2 Epoch: 44 [25600/29981                 (85%)]\tLoss: 0.344774\n",
      "Training stage for Flod 2 Epoch: 44 [28800/29981                 (96%)]\tLoss: 0.313277\n",
      "Test set for fold2: Average Loss:           743324.0351, Accuracy: 6615/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 45 [0/29981                 (0%)]\tLoss: 0.344794\n",
      "Training stage for Flod 2 Epoch: 45 [3200/29981                 (11%)]\tLoss: 0.381162\n",
      "Training stage for Flod 2 Epoch: 45 [6400/29981                 (21%)]\tLoss: 0.407402\n",
      "Training stage for Flod 2 Epoch: 45 [9600/29981                 (32%)]\tLoss: 0.364395\n",
      "Training stage for Flod 2 Epoch: 45 [12800/29981                 (43%)]\tLoss: 0.436686\n",
      "Training stage for Flod 2 Epoch: 45 [16000/29981                 (53%)]\tLoss: 0.344672\n",
      "Training stage for Flod 2 Epoch: 45 [19200/29981                 (64%)]\tLoss: 0.344547\n",
      "Training stage for Flod 2 Epoch: 45 [22400/29981                 (75%)]\tLoss: 0.400130\n",
      "Training stage for Flod 2 Epoch: 45 [25600/29981                 (85%)]\tLoss: 0.344544\n",
      "Training stage for Flod 2 Epoch: 45 [28800/29981                 (96%)]\tLoss: 0.313374\n",
      "Test set for fold2: Average Loss:           705841.6742, Accuracy: 6812/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 46 [0/29981                 (0%)]\tLoss: 0.345343\n",
      "Training stage for Flod 2 Epoch: 46 [3200/29981                 (11%)]\tLoss: 0.345068\n",
      "Training stage for Flod 2 Epoch: 46 [6400/29981                 (21%)]\tLoss: 0.404553\n",
      "Training stage for Flod 2 Epoch: 46 [9600/29981                 (32%)]\tLoss: 0.376355\n",
      "Training stage for Flod 2 Epoch: 46 [12800/29981                 (43%)]\tLoss: 0.407867\n",
      "Training stage for Flod 2 Epoch: 46 [16000/29981                 (53%)]\tLoss: 0.393129\n",
      "Training stage for Flod 2 Epoch: 46 [19200/29981                 (64%)]\tLoss: 0.437507\n",
      "Training stage for Flod 2 Epoch: 46 [22400/29981                 (75%)]\tLoss: 0.407037\n",
      "Training stage for Flod 2 Epoch: 46 [25600/29981                 (85%)]\tLoss: 0.436577\n",
      "Training stage for Flod 2 Epoch: 46 [28800/29981                 (96%)]\tLoss: 0.501725\n",
      "Test set for fold2: Average Loss:           758424.6411, Accuracy: 6573/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 47 [0/29981                 (0%)]\tLoss: 0.483194\n",
      "Training stage for Flod 2 Epoch: 47 [3200/29981                 (11%)]\tLoss: 0.335517\n",
      "Training stage for Flod 2 Epoch: 47 [6400/29981                 (21%)]\tLoss: 0.406084\n",
      "Training stage for Flod 2 Epoch: 47 [9600/29981                 (32%)]\tLoss: 0.336517\n",
      "Training stage for Flod 2 Epoch: 47 [12800/29981                 (43%)]\tLoss: 0.438909\n",
      "Training stage for Flod 2 Epoch: 47 [16000/29981                 (53%)]\tLoss: 0.375764\n",
      "Training stage for Flod 2 Epoch: 47 [19200/29981                 (64%)]\tLoss: 0.406062\n",
      "Training stage for Flod 2 Epoch: 47 [22400/29981                 (75%)]\tLoss: 0.421432\n",
      "Training stage for Flod 2 Epoch: 47 [25600/29981                 (85%)]\tLoss: 0.344552\n",
      "Training stage for Flod 2 Epoch: 47 [28800/29981                 (96%)]\tLoss: 0.411730\n",
      "Test set for fold2: Average Loss:           731798.1505, Accuracy: 6716/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 48 [0/29981                 (0%)]\tLoss: 0.407508\n",
      "Training stage for Flod 2 Epoch: 48 [3200/29981                 (11%)]\tLoss: 0.408799\n",
      "Training stage for Flod 2 Epoch: 48 [6400/29981                 (21%)]\tLoss: 0.314026\n",
      "Training stage for Flod 2 Epoch: 48 [9600/29981                 (32%)]\tLoss: 0.344566\n",
      "Training stage for Flod 2 Epoch: 48 [12800/29981                 (43%)]\tLoss: 0.428978\n",
      "Training stage for Flod 2 Epoch: 48 [16000/29981                 (53%)]\tLoss: 0.375973\n",
      "Training stage for Flod 2 Epoch: 48 [19200/29981                 (64%)]\tLoss: 0.313265\n",
      "Training stage for Flod 2 Epoch: 48 [22400/29981                 (75%)]\tLoss: 0.375763\n",
      "Training stage for Flod 2 Epoch: 48 [25600/29981                 (85%)]\tLoss: 0.316082\n",
      "Training stage for Flod 2 Epoch: 48 [28800/29981                 (96%)]\tLoss: 0.375763\n",
      "Test set for fold2: Average Loss:           730607.3126, Accuracy: 6700/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 49 [0/29981                 (0%)]\tLoss: 0.388642\n",
      "Training stage for Flod 2 Epoch: 49 [3200/29981                 (11%)]\tLoss: 0.375290\n",
      "Training stage for Flod 2 Epoch: 49 [6400/29981                 (21%)]\tLoss: 0.375766\n",
      "Training stage for Flod 2 Epoch: 49 [9600/29981                 (32%)]\tLoss: 0.314642\n",
      "Training stage for Flod 2 Epoch: 49 [12800/29981                 (43%)]\tLoss: 0.313322\n",
      "Training stage for Flod 2 Epoch: 49 [16000/29981                 (53%)]\tLoss: 0.380193\n",
      "Training stage for Flod 2 Epoch: 49 [19200/29981                 (64%)]\tLoss: 0.349804\n",
      "Training stage for Flod 2 Epoch: 49 [22400/29981                 (75%)]\tLoss: 0.345425\n",
      "Training stage for Flod 2 Epoch: 49 [25600/29981                 (85%)]\tLoss: 0.346907\n",
      "Training stage for Flod 2 Epoch: 49 [28800/29981                 (96%)]\tLoss: 0.407782\n",
      "Test set for fold2: Average Loss:           788325.1908, Accuracy: 6469/7495           (86%)\n",
      "Training stage for Flod 2 Epoch: 50 [0/29981                 (0%)]\tLoss: 0.347405\n",
      "Training stage for Flod 2 Epoch: 50 [3200/29981                 (11%)]\tLoss: 0.438272\n",
      "Training stage for Flod 2 Epoch: 50 [6400/29981                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 50 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 50 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 50 [16000/29981                 (53%)]\tLoss: 0.467829\n",
      "Training stage for Flod 2 Epoch: 50 [19200/29981                 (64%)]\tLoss: 0.326893\n",
      "Training stage for Flod 2 Epoch: 50 [22400/29981                 (75%)]\tLoss: 0.373296\n",
      "Training stage for Flod 2 Epoch: 50 [25600/29981                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 50 [28800/29981                 (96%)]\tLoss: 0.394307\n",
      "Test set for fold2: Average Loss:           716471.2721, Accuracy: 6793/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 51 [0/29981                 (0%)]\tLoss: 0.344482\n",
      "Training stage for Flod 2 Epoch: 51 [3200/29981                 (11%)]\tLoss: 0.374419\n",
      "Training stage for Flod 2 Epoch: 51 [6400/29981                 (21%)]\tLoss: 0.391123\n",
      "Training stage for Flod 2 Epoch: 51 [9600/29981                 (32%)]\tLoss: 0.407040\n",
      "Training stage for Flod 2 Epoch: 51 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 51 [16000/29981                 (53%)]\tLoss: 0.350887\n",
      "Training stage for Flod 2 Epoch: 51 [19200/29981                 (64%)]\tLoss: 0.401992\n",
      "Training stage for Flod 2 Epoch: 51 [22400/29981                 (75%)]\tLoss: 0.439261\n",
      "Training stage for Flod 2 Epoch: 51 [25600/29981                 (85%)]\tLoss: 0.449040\n",
      "Training stage for Flod 2 Epoch: 51 [28800/29981                 (96%)]\tLoss: 0.344529\n",
      "Test set for fold2: Average Loss:           722702.3328, Accuracy: 6749/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 52 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 52 [3200/29981                 (11%)]\tLoss: 0.324389\n",
      "Training stage for Flod 2 Epoch: 52 [6400/29981                 (21%)]\tLoss: 0.344544\n",
      "Training stage for Flod 2 Epoch: 52 [9600/29981                 (32%)]\tLoss: 0.377500\n",
      "Training stage for Flod 2 Epoch: 52 [12800/29981                 (43%)]\tLoss: 0.344616\n",
      "Training stage for Flod 2 Epoch: 52 [16000/29981                 (53%)]\tLoss: 0.407041\n",
      "Training stage for Flod 2 Epoch: 52 [19200/29981                 (64%)]\tLoss: 0.380034\n",
      "Training stage for Flod 2 Epoch: 52 [22400/29981                 (75%)]\tLoss: 0.344422\n",
      "Training stage for Flod 2 Epoch: 52 [25600/29981                 (85%)]\tLoss: 0.381809\n",
      "Training stage for Flod 2 Epoch: 52 [28800/29981                 (96%)]\tLoss: 0.426788\n",
      "Test set for fold2: Average Loss:           711339.2002, Accuracy: 6794/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 53 [0/29981                 (0%)]\tLoss: 0.439076\n",
      "Training stage for Flod 2 Epoch: 53 [3200/29981                 (11%)]\tLoss: 0.316519\n",
      "Training stage for Flod 2 Epoch: 53 [6400/29981                 (21%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 53 [9600/29981                 (32%)]\tLoss: 0.407667\n",
      "Training stage for Flod 2 Epoch: 53 [12800/29981                 (43%)]\tLoss: 0.316665\n",
      "Training stage for Flod 2 Epoch: 53 [16000/29981                 (53%)]\tLoss: 0.350765\n",
      "Training stage for Flod 2 Epoch: 53 [19200/29981                 (64%)]\tLoss: 0.407014\n",
      "Training stage for Flod 2 Epoch: 53 [22400/29981                 (75%)]\tLoss: 0.344585\n",
      "Training stage for Flod 2 Epoch: 53 [25600/29981                 (85%)]\tLoss: 0.404089\n",
      "Training stage for Flod 2 Epoch: 53 [28800/29981                 (96%)]\tLoss: 0.397854\n",
      "Test set for fold2: Average Loss:           728322.7251, Accuracy: 6702/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 54 [0/29981                 (0%)]\tLoss: 0.377617\n",
      "Training stage for Flod 2 Epoch: 54 [3200/29981                 (11%)]\tLoss: 0.413781\n",
      "Training stage for Flod 2 Epoch: 54 [6400/29981                 (21%)]\tLoss: 0.315244\n",
      "Training stage for Flod 2 Epoch: 54 [9600/29981                 (32%)]\tLoss: 0.318424\n",
      "Training stage for Flod 2 Epoch: 54 [12800/29981                 (43%)]\tLoss: 0.313286\n",
      "Training stage for Flod 2 Epoch: 54 [16000/29981                 (53%)]\tLoss: 0.407018\n",
      "Training stage for Flod 2 Epoch: 54 [19200/29981                 (64%)]\tLoss: 0.313263\n",
      "Training stage for Flod 2 Epoch: 54 [22400/29981                 (75%)]\tLoss: 0.406863\n",
      "Training stage for Flod 2 Epoch: 54 [25600/29981                 (85%)]\tLoss: 0.406972\n",
      "Training stage for Flod 2 Epoch: 54 [28800/29981                 (96%)]\tLoss: 0.410543\n",
      "Test set for fold2: Average Loss:           743920.5429, Accuracy: 6614/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 55 [0/29981                 (0%)]\tLoss: 0.345654\n",
      "Training stage for Flod 2 Epoch: 55 [3200/29981                 (11%)]\tLoss: 0.344518\n",
      "Training stage for Flod 2 Epoch: 55 [6400/29981                 (21%)]\tLoss: 0.345510\n",
      "Training stage for Flod 2 Epoch: 55 [9600/29981                 (32%)]\tLoss: 0.346747\n",
      "Training stage for Flod 2 Epoch: 55 [12800/29981                 (43%)]\tLoss: 0.366945\n",
      "Training stage for Flod 2 Epoch: 55 [16000/29981                 (53%)]\tLoss: 0.313676\n",
      "Training stage for Flod 2 Epoch: 55 [19200/29981                 (64%)]\tLoss: 0.376711\n",
      "Training stage for Flod 2 Epoch: 55 [22400/29981                 (75%)]\tLoss: 0.385125\n",
      "Training stage for Flod 2 Epoch: 55 [25600/29981                 (85%)]\tLoss: 0.345484\n",
      "Training stage for Flod 2 Epoch: 55 [28800/29981                 (96%)]\tLoss: 0.452245\n",
      "Test set for fold2: Average Loss:           773274.3381, Accuracy: 6509/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 56 [0/29981                 (0%)]\tLoss: 0.437310\n",
      "Training stage for Flod 2 Epoch: 56 [3200/29981                 (11%)]\tLoss: 0.339181\n",
      "Training stage for Flod 2 Epoch: 56 [6400/29981                 (21%)]\tLoss: 0.377886\n",
      "Training stage for Flod 2 Epoch: 56 [9600/29981                 (32%)]\tLoss: 0.313268\n",
      "Training stage for Flod 2 Epoch: 56 [12800/29981                 (43%)]\tLoss: 0.313356\n",
      "Training stage for Flod 2 Epoch: 56 [16000/29981                 (53%)]\tLoss: 0.438574\n",
      "Training stage for Flod 2 Epoch: 56 [19200/29981                 (64%)]\tLoss: 0.438272\n",
      "Training stage for Flod 2 Epoch: 56 [22400/29981                 (75%)]\tLoss: 0.397571\n",
      "Training stage for Flod 2 Epoch: 56 [25600/29981                 (85%)]\tLoss: 0.386018\n",
      "Training stage for Flod 2 Epoch: 56 [28800/29981                 (96%)]\tLoss: 0.412652\n",
      "Test set for fold2: Average Loss:           767573.6051, Accuracy: 6512/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 57 [0/29981                 (0%)]\tLoss: 0.430478\n",
      "Training stage for Flod 2 Epoch: 57 [3200/29981                 (11%)]\tLoss: 0.413414\n",
      "Training stage for Flod 2 Epoch: 57 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 57 [9600/29981                 (32%)]\tLoss: 0.355203\n",
      "Training stage for Flod 2 Epoch: 57 [12800/29981                 (43%)]\tLoss: 0.434843\n",
      "Training stage for Flod 2 Epoch: 57 [16000/29981                 (53%)]\tLoss: 0.344611\n",
      "Training stage for Flod 2 Epoch: 57 [19200/29981                 (64%)]\tLoss: 0.418505\n",
      "Training stage for Flod 2 Epoch: 57 [22400/29981                 (75%)]\tLoss: 0.344525\n",
      "Training stage for Flod 2 Epoch: 57 [25600/29981                 (85%)]\tLoss: 0.344770\n",
      "Training stage for Flod 2 Epoch: 57 [28800/29981                 (96%)]\tLoss: 0.318584\n",
      "Test set for fold2: Average Loss:           744069.7452, Accuracy: 6621/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 58 [0/29981                 (0%)]\tLoss: 0.344681\n",
      "Training stage for Flod 2 Epoch: 58 [3200/29981                 (11%)]\tLoss: 0.407024\n",
      "Training stage for Flod 2 Epoch: 58 [6400/29981                 (21%)]\tLoss: 0.318907\n",
      "Training stage for Flod 2 Epoch: 58 [9600/29981                 (32%)]\tLoss: 0.375933\n",
      "Training stage for Flod 2 Epoch: 58 [12800/29981                 (43%)]\tLoss: 0.360432\n",
      "Training stage for Flod 2 Epoch: 58 [16000/29981                 (53%)]\tLoss: 0.379080\n",
      "Training stage for Flod 2 Epoch: 58 [19200/29981                 (64%)]\tLoss: 0.418337\n",
      "Training stage for Flod 2 Epoch: 58 [22400/29981                 (75%)]\tLoss: 0.378688\n",
      "Training stage for Flod 2 Epoch: 58 [25600/29981                 (85%)]\tLoss: 0.344562\n",
      "Training stage for Flod 2 Epoch: 58 [28800/29981                 (96%)]\tLoss: 0.315854\n",
      "Test set for fold2: Average Loss:           733342.9843, Accuracy: 6684/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 59 [0/29981                 (0%)]\tLoss: 0.382229\n",
      "Training stage for Flod 2 Epoch: 59 [3200/29981                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 59 [6400/29981                 (21%)]\tLoss: 0.344428\n",
      "Training stage for Flod 2 Epoch: 59 [9600/29981                 (32%)]\tLoss: 0.345821\n",
      "Training stage for Flod 2 Epoch: 59 [12800/29981                 (43%)]\tLoss: 0.375760\n",
      "Training stage for Flod 2 Epoch: 59 [16000/29981                 (53%)]\tLoss: 0.469189\n",
      "Training stage for Flod 2 Epoch: 59 [19200/29981                 (64%)]\tLoss: 0.439815\n",
      "Training stage for Flod 2 Epoch: 59 [22400/29981                 (75%)]\tLoss: 0.466510\n",
      "Training stage for Flod 2 Epoch: 59 [25600/29981                 (85%)]\tLoss: 0.344514\n",
      "Training stage for Flod 2 Epoch: 59 [28800/29981                 (96%)]\tLoss: 0.314714\n",
      "Test set for fold2: Average Loss:           711343.2037, Accuracy: 6804/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 60 [0/29981                 (0%)]\tLoss: 0.425808\n",
      "Training stage for Flod 2 Epoch: 60 [3200/29981                 (11%)]\tLoss: 0.344599\n",
      "Training stage for Flod 2 Epoch: 60 [6400/29981                 (21%)]\tLoss: 0.404694\n",
      "Training stage for Flod 2 Epoch: 60 [9600/29981                 (32%)]\tLoss: 0.345514\n",
      "Training stage for Flod 2 Epoch: 60 [12800/29981                 (43%)]\tLoss: 0.438314\n",
      "Training stage for Flod 2 Epoch: 60 [16000/29981                 (53%)]\tLoss: 0.388108\n",
      "Training stage for Flod 2 Epoch: 60 [19200/29981                 (64%)]\tLoss: 0.345024\n",
      "Training stage for Flod 2 Epoch: 60 [22400/29981                 (75%)]\tLoss: 0.356014\n",
      "Training stage for Flod 2 Epoch: 60 [25600/29981                 (85%)]\tLoss: 0.313264\n",
      "Training stage for Flod 2 Epoch: 60 [28800/29981                 (96%)]\tLoss: 0.346992\n",
      "Test set for fold2: Average Loss:           785756.4820, Accuracy: 6503/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 61 [0/29981                 (0%)]\tLoss: 0.438182\n",
      "Training stage for Flod 2 Epoch: 61 [3200/29981                 (11%)]\tLoss: 0.375836\n",
      "Training stage for Flod 2 Epoch: 61 [6400/29981                 (21%)]\tLoss: 0.403337\n",
      "Training stage for Flod 2 Epoch: 61 [9600/29981                 (32%)]\tLoss: 0.313277\n",
      "Training stage for Flod 2 Epoch: 61 [12800/29981                 (43%)]\tLoss: 0.439126\n",
      "Training stage for Flod 2 Epoch: 61 [16000/29981                 (53%)]\tLoss: 0.313766\n",
      "Training stage for Flod 2 Epoch: 61 [19200/29981                 (64%)]\tLoss: 0.438301\n",
      "Training stage for Flod 2 Epoch: 61 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 61 [25600/29981                 (85%)]\tLoss: 0.347379\n",
      "Training stage for Flod 2 Epoch: 61 [28800/29981                 (96%)]\tLoss: 0.345048\n",
      "Test set for fold2: Average Loss:           728334.4055, Accuracy: 6713/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 62 [0/29981                 (0%)]\tLoss: 0.344627\n",
      "Training stage for Flod 2 Epoch: 62 [3200/29981                 (11%)]\tLoss: 0.375713\n",
      "Training stage for Flod 2 Epoch: 62 [6400/29981                 (21%)]\tLoss: 0.344598\n",
      "Training stage for Flod 2 Epoch: 62 [9600/29981                 (32%)]\tLoss: 0.327193\n",
      "Training stage for Flod 2 Epoch: 62 [12800/29981                 (43%)]\tLoss: 0.355134\n",
      "Training stage for Flod 2 Epoch: 62 [16000/29981                 (53%)]\tLoss: 0.344638\n",
      "Training stage for Flod 2 Epoch: 62 [19200/29981                 (64%)]\tLoss: 0.375477\n",
      "Training stage for Flod 2 Epoch: 62 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 62 [25600/29981                 (85%)]\tLoss: 0.344520\n",
      "Training stage for Flod 2 Epoch: 62 [28800/29981                 (96%)]\tLoss: 0.313421\n",
      "Test set for fold2: Average Loss:           727049.3916, Accuracy: 6715/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 63 [0/29981                 (0%)]\tLoss: 0.406961\n",
      "Training stage for Flod 2 Epoch: 63 [3200/29981                 (11%)]\tLoss: 0.377932\n",
      "Training stage for Flod 2 Epoch: 63 [6400/29981                 (21%)]\tLoss: 0.362100\n",
      "Training stage for Flod 2 Epoch: 63 [9600/29981                 (32%)]\tLoss: 0.344550\n",
      "Training stage for Flod 2 Epoch: 63 [12800/29981                 (43%)]\tLoss: 0.380189\n",
      "Training stage for Flod 2 Epoch: 63 [16000/29981                 (53%)]\tLoss: 0.337684\n",
      "Training stage for Flod 2 Epoch: 63 [19200/29981                 (64%)]\tLoss: 0.375767\n",
      "Training stage for Flod 2 Epoch: 63 [22400/29981                 (75%)]\tLoss: 0.346187\n",
      "Training stage for Flod 2 Epoch: 63 [25600/29981                 (85%)]\tLoss: 0.344513\n",
      "Training stage for Flod 2 Epoch: 63 [28800/29981                 (96%)]\tLoss: 0.313272\n",
      "Test set for fold2: Average Loss:           711130.2425, Accuracy: 6789/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 64 [0/29981                 (0%)]\tLoss: 0.365706\n",
      "Training stage for Flod 2 Epoch: 64 [3200/29981                 (11%)]\tLoss: 0.393568\n",
      "Training stage for Flod 2 Epoch: 64 [6400/29981                 (21%)]\tLoss: 0.376393\n",
      "Training stage for Flod 2 Epoch: 64 [9600/29981                 (32%)]\tLoss: 0.377569\n",
      "Training stage for Flod 2 Epoch: 64 [12800/29981                 (43%)]\tLoss: 0.411114\n",
      "Training stage for Flod 2 Epoch: 64 [16000/29981                 (53%)]\tLoss: 0.345532\n",
      "Training stage for Flod 2 Epoch: 64 [19200/29981                 (64%)]\tLoss: 0.375743\n",
      "Training stage for Flod 2 Epoch: 64 [22400/29981                 (75%)]\tLoss: 0.313655\n",
      "Training stage for Flod 2 Epoch: 64 [25600/29981                 (85%)]\tLoss: 0.377165\n",
      "Training stage for Flod 2 Epoch: 64 [28800/29981                 (96%)]\tLoss: 0.361490\n",
      "Test set for fold2: Average Loss:           711932.2094, Accuracy: 6794/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 65 [0/29981                 (0%)]\tLoss: 0.344527\n",
      "Training stage for Flod 2 Epoch: 65 [3200/29981                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 65 [6400/29981                 (21%)]\tLoss: 0.407047\n",
      "Training stage for Flod 2 Epoch: 65 [9600/29981                 (32%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 65 [12800/29981                 (43%)]\tLoss: 0.408166\n",
      "Training stage for Flod 2 Epoch: 65 [16000/29981                 (53%)]\tLoss: 0.344515\n",
      "Training stage for Flod 2 Epoch: 65 [19200/29981                 (64%)]\tLoss: 0.345954\n",
      "Training stage for Flod 2 Epoch: 65 [22400/29981                 (75%)]\tLoss: 0.321068\n",
      "Training stage for Flod 2 Epoch: 65 [25600/29981                 (85%)]\tLoss: 0.314263\n",
      "Training stage for Flod 2 Epoch: 65 [28800/29981                 (96%)]\tLoss: 0.344514\n",
      "Test set for fold2: Average Loss:           746504.0987, Accuracy: 6639/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 66 [0/29981                 (0%)]\tLoss: 0.313621\n",
      "Training stage for Flod 2 Epoch: 66 [3200/29981                 (11%)]\tLoss: 0.408639\n",
      "Training stage for Flod 2 Epoch: 66 [6400/29981                 (21%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 66 [9600/29981                 (32%)]\tLoss: 0.447391\n",
      "Training stage for Flod 2 Epoch: 66 [12800/29981                 (43%)]\tLoss: 0.449866\n",
      "Training stage for Flod 2 Epoch: 66 [16000/29981                 (53%)]\tLoss: 0.489989\n",
      "Training stage for Flod 2 Epoch: 66 [19200/29981                 (64%)]\tLoss: 0.438261\n",
      "Training stage for Flod 2 Epoch: 66 [22400/29981                 (75%)]\tLoss: 0.380362\n",
      "Training stage for Flod 2 Epoch: 66 [25600/29981                 (85%)]\tLoss: 0.344528\n",
      "Training stage for Flod 2 Epoch: 66 [28800/29981                 (96%)]\tLoss: 0.459389\n",
      "Test set for fold2: Average Loss:           750172.3032, Accuracy: 6638/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 67 [0/29981                 (0%)]\tLoss: 0.407179\n",
      "Training stage for Flod 2 Epoch: 67 [3200/29981                 (11%)]\tLoss: 0.421680\n",
      "Training stage for Flod 2 Epoch: 67 [6400/29981                 (21%)]\tLoss: 0.352747\n",
      "Training stage for Flod 2 Epoch: 67 [9600/29981                 (32%)]\tLoss: 0.451791\n",
      "Training stage for Flod 2 Epoch: 67 [12800/29981                 (43%)]\tLoss: 0.411270\n",
      "Training stage for Flod 2 Epoch: 67 [16000/29981                 (53%)]\tLoss: 0.413963\n",
      "Training stage for Flod 2 Epoch: 67 [19200/29981                 (64%)]\tLoss: 0.344571\n",
      "Training stage for Flod 2 Epoch: 67 [22400/29981                 (75%)]\tLoss: 0.407067\n",
      "Training stage for Flod 2 Epoch: 67 [25600/29981                 (85%)]\tLoss: 0.412795\n",
      "Training stage for Flod 2 Epoch: 67 [28800/29981                 (96%)]\tLoss: 0.346800\n",
      "Test set for fold2: Average Loss:           801462.7939, Accuracy: 6409/7495           (86%)\n",
      "Training stage for Flod 2 Epoch: 68 [0/29981                 (0%)]\tLoss: 0.356365\n",
      "Training stage for Flod 2 Epoch: 68 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 68 [6400/29981                 (21%)]\tLoss: 0.344558\n",
      "Training stage for Flod 2 Epoch: 68 [9600/29981                 (32%)]\tLoss: 0.313763\n",
      "Training stage for Flod 2 Epoch: 68 [12800/29981                 (43%)]\tLoss: 0.344506\n",
      "Training stage for Flod 2 Epoch: 68 [16000/29981                 (53%)]\tLoss: 0.357079\n",
      "Training stage for Flod 2 Epoch: 68 [19200/29981                 (64%)]\tLoss: 0.453223\n",
      "Training stage for Flod 2 Epoch: 68 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 68 [25600/29981                 (85%)]\tLoss: 0.375173\n",
      "Training stage for Flod 2 Epoch: 68 [28800/29981                 (96%)]\tLoss: 0.322438\n",
      "Test set for fold2: Average Loss:           756460.0148, Accuracy: 6589/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 69 [0/29981                 (0%)]\tLoss: 0.381032\n",
      "Training stage for Flod 2 Epoch: 69 [3200/29981                 (11%)]\tLoss: 0.362411\n",
      "Training stage for Flod 2 Epoch: 69 [6400/29981                 (21%)]\tLoss: 0.375854\n",
      "Training stage for Flod 2 Epoch: 69 [9600/29981                 (32%)]\tLoss: 0.313264\n",
      "Training stage for Flod 2 Epoch: 69 [12800/29981                 (43%)]\tLoss: 0.375977\n",
      "Training stage for Flod 2 Epoch: 69 [16000/29981                 (53%)]\tLoss: 0.407542\n",
      "Training stage for Flod 2 Epoch: 69 [19200/29981                 (64%)]\tLoss: 0.345800\n",
      "Training stage for Flod 2 Epoch: 69 [22400/29981                 (75%)]\tLoss: 0.377827\n",
      "Training stage for Flod 2 Epoch: 69 [25600/29981                 (85%)]\tLoss: 0.313602\n",
      "Training stage for Flod 2 Epoch: 69 [28800/29981                 (96%)]\tLoss: 0.383394\n",
      "Test set for fold2: Average Loss:           726292.1396, Accuracy: 6735/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 70 [0/29981                 (0%)]\tLoss: 0.375765\n",
      "Training stage for Flod 2 Epoch: 70 [3200/29981                 (11%)]\tLoss: 0.344548\n",
      "Training stage for Flod 2 Epoch: 70 [6400/29981                 (21%)]\tLoss: 0.375765\n",
      "Training stage for Flod 2 Epoch: 70 [9600/29981                 (32%)]\tLoss: 0.314723\n",
      "Training stage for Flod 2 Epoch: 70 [12800/29981                 (43%)]\tLoss: 0.363431\n",
      "Training stage for Flod 2 Epoch: 70 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 70 [19200/29981                 (64%)]\tLoss: 0.472371\n",
      "Training stage for Flod 2 Epoch: 70 [22400/29981                 (75%)]\tLoss: 0.375308\n",
      "Training stage for Flod 2 Epoch: 70 [25600/29981                 (85%)]\tLoss: 0.313266\n",
      "Training stage for Flod 2 Epoch: 70 [28800/29981                 (96%)]\tLoss: 0.344760\n",
      "Test set for fold2: Average Loss:           755092.6242, Accuracy: 6596/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 71 [0/29981                 (0%)]\tLoss: 0.375841\n",
      "Training stage for Flod 2 Epoch: 71 [3200/29981                 (11%)]\tLoss: 0.344556\n",
      "Training stage for Flod 2 Epoch: 71 [6400/29981                 (21%)]\tLoss: 0.375764\n",
      "Training stage for Flod 2 Epoch: 71 [9600/29981                 (32%)]\tLoss: 0.344548\n",
      "Training stage for Flod 2 Epoch: 71 [12800/29981                 (43%)]\tLoss: 0.323083\n",
      "Training stage for Flod 2 Epoch: 71 [16000/29981                 (53%)]\tLoss: 0.344513\n",
      "Training stage for Flod 2 Epoch: 71 [19200/29981                 (64%)]\tLoss: 0.343881\n",
      "Training stage for Flod 2 Epoch: 71 [22400/29981                 (75%)]\tLoss: 0.344523\n",
      "Training stage for Flod 2 Epoch: 71 [25600/29981                 (85%)]\tLoss: 0.383143\n",
      "Training stage for Flod 2 Epoch: 71 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold2: Average Loss:           736086.4632, Accuracy: 6678/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 72 [0/29981                 (0%)]\tLoss: 0.407294\n",
      "Training stage for Flod 2 Epoch: 72 [3200/29981                 (11%)]\tLoss: 0.500069\n",
      "Training stage for Flod 2 Epoch: 72 [6400/29981                 (21%)]\tLoss: 0.326783\n",
      "Training stage for Flod 2 Epoch: 72 [9600/29981                 (32%)]\tLoss: 0.344528\n",
      "Training stage for Flod 2 Epoch: 72 [12800/29981                 (43%)]\tLoss: 0.349005\n",
      "Training stage for Flod 2 Epoch: 72 [16000/29981                 (53%)]\tLoss: 0.344598\n",
      "Training stage for Flod 2 Epoch: 72 [19200/29981                 (64%)]\tLoss: 0.376125\n",
      "Training stage for Flod 2 Epoch: 72 [22400/29981                 (75%)]\tLoss: 0.374222\n",
      "Training stage for Flod 2 Epoch: 72 [25600/29981                 (85%)]\tLoss: 0.313364\n",
      "Training stage for Flod 2 Epoch: 72 [28800/29981                 (96%)]\tLoss: 0.375764\n",
      "Test set for fold2: Average Loss:           780654.0853, Accuracy: 6480/7495           (86%)\n",
      "Training stage for Flod 2 Epoch: 73 [0/29981                 (0%)]\tLoss: 0.462356\n",
      "Training stage for Flod 2 Epoch: 73 [3200/29981                 (11%)]\tLoss: 0.313263\n",
      "Training stage for Flod 2 Epoch: 73 [6400/29981                 (21%)]\tLoss: 0.402766\n",
      "Training stage for Flod 2 Epoch: 73 [9600/29981                 (32%)]\tLoss: 0.363573\n",
      "Training stage for Flod 2 Epoch: 73 [12800/29981                 (43%)]\tLoss: 0.320713\n",
      "Training stage for Flod 2 Epoch: 73 [16000/29981                 (53%)]\tLoss: 0.344628\n",
      "Training stage for Flod 2 Epoch: 73 [19200/29981                 (64%)]\tLoss: 0.349402\n",
      "Training stage for Flod 2 Epoch: 73 [22400/29981                 (75%)]\tLoss: 0.366994\n",
      "Training stage for Flod 2 Epoch: 73 [25600/29981                 (85%)]\tLoss: 0.407573\n",
      "Training stage for Flod 2 Epoch: 73 [28800/29981                 (96%)]\tLoss: 0.511794\n",
      "Test set for fold2: Average Loss:           764334.7109, Accuracy: 6544/7495           (87%)\n",
      "Training stage for Flod 2 Epoch: 74 [0/29981                 (0%)]\tLoss: 0.313264\n",
      "Training stage for Flod 2 Epoch: 74 [3200/29981                 (11%)]\tLoss: 0.378755\n",
      "Training stage for Flod 2 Epoch: 74 [6400/29981                 (21%)]\tLoss: 0.344884\n",
      "Training stage for Flod 2 Epoch: 74 [9600/29981                 (32%)]\tLoss: 0.321088\n",
      "Training stage for Flod 2 Epoch: 74 [12800/29981                 (43%)]\tLoss: 0.344725\n",
      "Training stage for Flod 2 Epoch: 74 [16000/29981                 (53%)]\tLoss: 0.438262\n",
      "Training stage for Flod 2 Epoch: 74 [19200/29981                 (64%)]\tLoss: 0.344652\n",
      "Training stage for Flod 2 Epoch: 74 [22400/29981                 (75%)]\tLoss: 0.344667\n",
      "Training stage for Flod 2 Epoch: 74 [25600/29981                 (85%)]\tLoss: 0.338146\n",
      "Training stage for Flod 2 Epoch: 74 [28800/29981                 (96%)]\tLoss: 0.345559\n",
      "Test set for fold2: Average Loss:           707354.8210, Accuracy: 6781/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 75 [0/29981                 (0%)]\tLoss: 0.320544\n",
      "Training stage for Flod 2 Epoch: 75 [3200/29981                 (11%)]\tLoss: 0.313759\n",
      "Training stage for Flod 2 Epoch: 75 [6400/29981                 (21%)]\tLoss: 0.344524\n",
      "Training stage for Flod 2 Epoch: 75 [9600/29981                 (32%)]\tLoss: 0.322655\n",
      "Training stage for Flod 2 Epoch: 75 [12800/29981                 (43%)]\tLoss: 0.344516\n",
      "Training stage for Flod 2 Epoch: 75 [16000/29981                 (53%)]\tLoss: 0.344671\n",
      "Training stage for Flod 2 Epoch: 75 [19200/29981                 (64%)]\tLoss: 0.345035\n",
      "Training stage for Flod 2 Epoch: 75 [22400/29981                 (75%)]\tLoss: 0.313316\n",
      "Training stage for Flod 2 Epoch: 75 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 75 [28800/29981                 (96%)]\tLoss: 0.352791\n",
      "Test set for fold2: Average Loss:           705582.9895, Accuracy: 6823/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 76 [0/29981                 (0%)]\tLoss: 0.318978\n",
      "Training stage for Flod 2 Epoch: 76 [3200/29981                 (11%)]\tLoss: 0.375263\n",
      "Training stage for Flod 2 Epoch: 76 [6400/29981                 (21%)]\tLoss: 0.373100\n",
      "Training stage for Flod 2 Epoch: 76 [9600/29981                 (32%)]\tLoss: 0.337577\n",
      "Training stage for Flod 2 Epoch: 76 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 76 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 76 [19200/29981                 (64%)]\tLoss: 0.375456\n",
      "Training stage for Flod 2 Epoch: 76 [22400/29981                 (75%)]\tLoss: 0.344513\n",
      "Training stage for Flod 2 Epoch: 76 [25600/29981                 (85%)]\tLoss: 0.364381\n",
      "Training stage for Flod 2 Epoch: 76 [28800/29981                 (96%)]\tLoss: 0.344736\n",
      "Test set for fold2: Average Loss:           745707.7012, Accuracy: 6633/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 77 [0/29981                 (0%)]\tLoss: 0.375085\n",
      "Training stage for Flod 2 Epoch: 77 [3200/29981                 (11%)]\tLoss: 0.377582\n",
      "Training stage for Flod 2 Epoch: 77 [6400/29981                 (21%)]\tLoss: 0.360091\n",
      "Training stage for Flod 2 Epoch: 77 [9600/29981                 (32%)]\tLoss: 0.387258\n",
      "Training stage for Flod 2 Epoch: 77 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 77 [16000/29981                 (53%)]\tLoss: 0.313289\n",
      "Training stage for Flod 2 Epoch: 77 [19200/29981                 (64%)]\tLoss: 0.325910\n",
      "Training stage for Flod 2 Epoch: 77 [22400/29981                 (75%)]\tLoss: 0.375793\n",
      "Training stage for Flod 2 Epoch: 77 [25600/29981                 (85%)]\tLoss: 0.405557\n",
      "Training stage for Flod 2 Epoch: 77 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold2: Average Loss:           795251.9194, Accuracy: 6418/7495           (86%)\n",
      "Training stage for Flod 2 Epoch: 78 [0/29981                 (0%)]\tLoss: 0.526959\n",
      "Training stage for Flod 2 Epoch: 78 [3200/29981                 (11%)]\tLoss: 0.494827\n",
      "Training stage for Flod 2 Epoch: 78 [6400/29981                 (21%)]\tLoss: 0.407012\n",
      "Training stage for Flod 2 Epoch: 78 [9600/29981                 (32%)]\tLoss: 0.344529\n",
      "Training stage for Flod 2 Epoch: 78 [12800/29981                 (43%)]\tLoss: 0.344532\n",
      "Training stage for Flod 2 Epoch: 78 [16000/29981                 (53%)]\tLoss: 0.407157\n",
      "Training stage for Flod 2 Epoch: 78 [19200/29981                 (64%)]\tLoss: 0.313496\n",
      "Training stage for Flod 2 Epoch: 78 [22400/29981                 (75%)]\tLoss: 0.376273\n",
      "Training stage for Flod 2 Epoch: 78 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 78 [28800/29981                 (96%)]\tLoss: 0.406857\n",
      "Test set for fold2: Average Loss:           753386.9665, Accuracy: 6613/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 79 [0/29981                 (0%)]\tLoss: 0.408402\n",
      "Training stage for Flod 2 Epoch: 79 [3200/29981                 (11%)]\tLoss: 0.375372\n",
      "Training stage for Flod 2 Epoch: 79 [6400/29981                 (21%)]\tLoss: 0.396550\n",
      "Training stage for Flod 2 Epoch: 79 [9600/29981                 (32%)]\tLoss: 0.438076\n",
      "Training stage for Flod 2 Epoch: 79 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 79 [16000/29981                 (53%)]\tLoss: 0.375082\n",
      "Training stage for Flod 2 Epoch: 79 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 79 [22400/29981                 (75%)]\tLoss: 0.344540\n",
      "Training stage for Flod 2 Epoch: 79 [25600/29981                 (85%)]\tLoss: 0.375957\n",
      "Training stage for Flod 2 Epoch: 79 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold2: Average Loss:           736290.6306, Accuracy: 6672/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 80 [0/29981                 (0%)]\tLoss: 0.436796\n",
      "Training stage for Flod 2 Epoch: 80 [3200/29981                 (11%)]\tLoss: 0.377995\n",
      "Training stage for Flod 2 Epoch: 80 [6400/29981                 (21%)]\tLoss: 0.344519\n",
      "Training stage for Flod 2 Epoch: 80 [9600/29981                 (32%)]\tLoss: 0.346750\n",
      "Training stage for Flod 2 Epoch: 80 [12800/29981                 (43%)]\tLoss: 0.344683\n",
      "Training stage for Flod 2 Epoch: 80 [16000/29981                 (53%)]\tLoss: 0.315436\n",
      "Training stage for Flod 2 Epoch: 80 [19200/29981                 (64%)]\tLoss: 0.407592\n",
      "Training stage for Flod 2 Epoch: 80 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 80 [25600/29981                 (85%)]\tLoss: 0.425541\n",
      "Training stage for Flod 2 Epoch: 80 [28800/29981                 (96%)]\tLoss: 0.378186\n",
      "Test set for fold2: Average Loss:           717524.0916, Accuracy: 6766/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 81 [0/29981                 (0%)]\tLoss: 0.404668\n",
      "Training stage for Flod 2 Epoch: 81 [3200/29981                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 81 [6400/29981                 (21%)]\tLoss: 0.322970\n",
      "Training stage for Flod 2 Epoch: 81 [9600/29981                 (32%)]\tLoss: 0.313266\n",
      "Training stage for Flod 2 Epoch: 81 [12800/29981                 (43%)]\tLoss: 0.368666\n",
      "Training stage for Flod 2 Epoch: 81 [16000/29981                 (53%)]\tLoss: 0.407749\n",
      "Training stage for Flod 2 Epoch: 81 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 81 [22400/29981                 (75%)]\tLoss: 0.313272\n",
      "Training stage for Flod 2 Epoch: 81 [25600/29981                 (85%)]\tLoss: 0.344101\n",
      "Training stage for Flod 2 Epoch: 81 [28800/29981                 (96%)]\tLoss: 0.457322\n",
      "Test set for fold2: Average Loss:           754415.0156, Accuracy: 6608/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 82 [0/29981                 (0%)]\tLoss: 0.406611\n",
      "Training stage for Flod 2 Epoch: 82 [3200/29981                 (11%)]\tLoss: 0.375958\n",
      "Training stage for Flod 2 Epoch: 82 [6400/29981                 (21%)]\tLoss: 0.313525\n",
      "Training stage for Flod 2 Epoch: 82 [9600/29981                 (32%)]\tLoss: 0.343158\n",
      "Training stage for Flod 2 Epoch: 82 [12800/29981                 (43%)]\tLoss: 0.345367\n",
      "Training stage for Flod 2 Epoch: 82 [16000/29981                 (53%)]\tLoss: 0.357935\n",
      "Training stage for Flod 2 Epoch: 82 [19200/29981                 (64%)]\tLoss: 0.393911\n",
      "Training stage for Flod 2 Epoch: 82 [22400/29981                 (75%)]\tLoss: 0.344509\n",
      "Training stage for Flod 2 Epoch: 82 [25600/29981                 (85%)]\tLoss: 0.344553\n",
      "Training stage for Flod 2 Epoch: 82 [28800/29981                 (96%)]\tLoss: 0.313313\n",
      "Test set for fold2: Average Loss:           725048.5275, Accuracy: 6735/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 83 [0/29981                 (0%)]\tLoss: 0.438160\n",
      "Training stage for Flod 2 Epoch: 83 [3200/29981                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 83 [6400/29981                 (21%)]\tLoss: 0.374269\n",
      "Training stage for Flod 2 Epoch: 83 [9600/29981                 (32%)]\tLoss: 0.438441\n",
      "Training stage for Flod 2 Epoch: 83 [12800/29981                 (43%)]\tLoss: 0.502801\n",
      "Training stage for Flod 2 Epoch: 83 [16000/29981                 (53%)]\tLoss: 0.383205\n",
      "Training stage for Flod 2 Epoch: 83 [19200/29981                 (64%)]\tLoss: 0.438150\n",
      "Training stage for Flod 2 Epoch: 83 [22400/29981                 (75%)]\tLoss: 0.345086\n",
      "Training stage for Flod 2 Epoch: 83 [25600/29981                 (85%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 83 [28800/29981                 (96%)]\tLoss: 0.345831\n",
      "Test set for fold2: Average Loss:           737712.4034, Accuracy: 6662/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 84 [0/29981                 (0%)]\tLoss: 0.418218\n",
      "Training stage for Flod 2 Epoch: 84 [3200/29981                 (11%)]\tLoss: 0.337098\n",
      "Training stage for Flod 2 Epoch: 84 [6400/29981                 (21%)]\tLoss: 0.448490\n",
      "Training stage for Flod 2 Epoch: 84 [9600/29981                 (32%)]\tLoss: 0.438369\n",
      "Training stage for Flod 2 Epoch: 84 [12800/29981                 (43%)]\tLoss: 0.321764\n",
      "Training stage for Flod 2 Epoch: 84 [16000/29981                 (53%)]\tLoss: 0.407032\n",
      "Training stage for Flod 2 Epoch: 84 [19200/29981                 (64%)]\tLoss: 0.334063\n",
      "Training stage for Flod 2 Epoch: 84 [22400/29981                 (75%)]\tLoss: 0.375820\n",
      "Training stage for Flod 2 Epoch: 84 [25600/29981                 (85%)]\tLoss: 0.369356\n",
      "Training stage for Flod 2 Epoch: 84 [28800/29981                 (96%)]\tLoss: 0.352412\n",
      "Test set for fold2: Average Loss:           716210.5214, Accuracy: 6770/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 85 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 85 [3200/29981                 (11%)]\tLoss: 0.348371\n",
      "Training stage for Flod 2 Epoch: 85 [6400/29981                 (21%)]\tLoss: 0.378619\n",
      "Training stage for Flod 2 Epoch: 85 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 85 [12800/29981                 (43%)]\tLoss: 0.346386\n",
      "Training stage for Flod 2 Epoch: 85 [16000/29981                 (53%)]\tLoss: 0.365459\n",
      "Training stage for Flod 2 Epoch: 85 [19200/29981                 (64%)]\tLoss: 0.363941\n",
      "Training stage for Flod 2 Epoch: 85 [22400/29981                 (75%)]\tLoss: 0.400246\n",
      "Training stage for Flod 2 Epoch: 85 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 85 [28800/29981                 (96%)]\tLoss: 0.313903\n",
      "Test set for fold2: Average Loss:           746244.4397, Accuracy: 6627/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 86 [0/29981                 (0%)]\tLoss: 0.407853\n",
      "Training stage for Flod 2 Epoch: 86 [3200/29981                 (11%)]\tLoss: 0.344552\n",
      "Training stage for Flod 2 Epoch: 86 [6400/29981                 (21%)]\tLoss: 0.375728\n",
      "Training stage for Flod 2 Epoch: 86 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 86 [12800/29981                 (43%)]\tLoss: 0.375776\n",
      "Training stage for Flod 2 Epoch: 86 [16000/29981                 (53%)]\tLoss: 0.408309\n",
      "Training stage for Flod 2 Epoch: 86 [19200/29981                 (64%)]\tLoss: 0.376832\n",
      "Training stage for Flod 2 Epoch: 86 [22400/29981                 (75%)]\tLoss: 0.470868\n",
      "Training stage for Flod 2 Epoch: 86 [25600/29981                 (85%)]\tLoss: 0.376588\n",
      "Training stage for Flod 2 Epoch: 86 [28800/29981                 (96%)]\tLoss: 0.313264\n",
      "Test set for fold2: Average Loss:           759743.6566, Accuracy: 6578/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 87 [0/29981                 (0%)]\tLoss: 0.407162\n",
      "Training stage for Flod 2 Epoch: 87 [3200/29981                 (11%)]\tLoss: 0.313419\n",
      "Training stage for Flod 2 Epoch: 87 [6400/29981                 (21%)]\tLoss: 0.344604\n",
      "Training stage for Flod 2 Epoch: 87 [9600/29981                 (32%)]\tLoss: 0.367520\n",
      "Training stage for Flod 2 Epoch: 87 [12800/29981                 (43%)]\tLoss: 0.406195\n",
      "Training stage for Flod 2 Epoch: 87 [16000/29981                 (53%)]\tLoss: 0.314433\n",
      "Training stage for Flod 2 Epoch: 87 [19200/29981                 (64%)]\tLoss: 0.344510\n",
      "Training stage for Flod 2 Epoch: 87 [22400/29981                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 87 [25600/29981                 (85%)]\tLoss: 0.371557\n",
      "Training stage for Flod 2 Epoch: 87 [28800/29981                 (96%)]\tLoss: 0.314621\n",
      "Test set for fold2: Average Loss:           751499.3179, Accuracy: 6622/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 88 [0/29981                 (0%)]\tLoss: 0.376118\n",
      "Training stage for Flod 2 Epoch: 88 [3200/29981                 (11%)]\tLoss: 0.344519\n",
      "Training stage for Flod 2 Epoch: 88 [6400/29981                 (21%)]\tLoss: 0.344675\n",
      "Training stage for Flod 2 Epoch: 88 [9600/29981                 (32%)]\tLoss: 0.352188\n",
      "Training stage for Flod 2 Epoch: 88 [12800/29981                 (43%)]\tLoss: 0.344897\n",
      "Training stage for Flod 2 Epoch: 88 [16000/29981                 (53%)]\tLoss: 0.344579\n",
      "Training stage for Flod 2 Epoch: 88 [19200/29981                 (64%)]\tLoss: 0.347661\n",
      "Training stage for Flod 2 Epoch: 88 [22400/29981                 (75%)]\tLoss: 0.344523\n",
      "Training stage for Flod 2 Epoch: 88 [25600/29981                 (85%)]\tLoss: 0.315270\n",
      "Training stage for Flod 2 Epoch: 88 [28800/29981                 (96%)]\tLoss: 0.313273\n",
      "Test set for fold2: Average Loss:           737273.7943, Accuracy: 6684/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 89 [0/29981                 (0%)]\tLoss: 0.313950\n",
      "Training stage for Flod 2 Epoch: 89 [3200/29981                 (11%)]\tLoss: 0.334615\n",
      "Training stage for Flod 2 Epoch: 89 [6400/29981                 (21%)]\tLoss: 0.346883\n",
      "Training stage for Flod 2 Epoch: 89 [9600/29981                 (32%)]\tLoss: 0.375834\n",
      "Training stage for Flod 2 Epoch: 89 [12800/29981                 (43%)]\tLoss: 0.400330\n",
      "Training stage for Flod 2 Epoch: 89 [16000/29981                 (53%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 89 [19200/29981                 (64%)]\tLoss: 0.314611\n",
      "Training stage for Flod 2 Epoch: 89 [22400/29981                 (75%)]\tLoss: 0.438297\n",
      "Training stage for Flod 2 Epoch: 89 [25600/29981                 (85%)]\tLoss: 0.347730\n",
      "Training stage for Flod 2 Epoch: 89 [28800/29981                 (96%)]\tLoss: 0.469512\n",
      "Test set for fold2: Average Loss:           744488.2259, Accuracy: 6641/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 90 [0/29981                 (0%)]\tLoss: 0.344546\n",
      "Training stage for Flod 2 Epoch: 90 [3200/29981                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 90 [6400/29981                 (21%)]\tLoss: 0.407843\n",
      "Training stage for Flod 2 Epoch: 90 [9600/29981                 (32%)]\tLoss: 0.344725\n",
      "Training stage for Flod 2 Epoch: 90 [12800/29981                 (43%)]\tLoss: 0.375609\n",
      "Training stage for Flod 2 Epoch: 90 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 90 [19200/29981                 (64%)]\tLoss: 0.407012\n",
      "Training stage for Flod 2 Epoch: 90 [22400/29981                 (75%)]\tLoss: 0.519513\n",
      "Training stage for Flod 2 Epoch: 90 [25600/29981                 (85%)]\tLoss: 0.344554\n",
      "Training stage for Flod 2 Epoch: 90 [28800/29981                 (96%)]\tLoss: 0.375770\n",
      "Test set for fold2: Average Loss:           734343.6911, Accuracy: 6696/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 91 [0/29981                 (0%)]\tLoss: 0.313265\n",
      "Training stage for Flod 2 Epoch: 91 [3200/29981                 (11%)]\tLoss: 0.313432\n",
      "Training stage for Flod 2 Epoch: 91 [6400/29981                 (21%)]\tLoss: 0.344517\n",
      "Training stage for Flod 2 Epoch: 91 [9600/29981                 (32%)]\tLoss: 0.313269\n",
      "Training stage for Flod 2 Epoch: 91 [12800/29981                 (43%)]\tLoss: 0.374123\n",
      "Training stage for Flod 2 Epoch: 91 [16000/29981                 (53%)]\tLoss: 0.344523\n",
      "Training stage for Flod 2 Epoch: 91 [19200/29981                 (64%)]\tLoss: 0.469010\n",
      "Training stage for Flod 2 Epoch: 91 [22400/29981                 (75%)]\tLoss: 0.362754\n",
      "Training stage for Flod 2 Epoch: 91 [25600/29981                 (85%)]\tLoss: 0.344940\n",
      "Training stage for Flod 2 Epoch: 91 [28800/29981                 (96%)]\tLoss: 0.376382\n",
      "Test set for fold2: Average Loss:           728946.4657, Accuracy: 6738/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 92 [0/29981                 (0%)]\tLoss: 0.406939\n",
      "Training stage for Flod 2 Epoch: 92 [3200/29981                 (11%)]\tLoss: 0.313268\n",
      "Training stage for Flod 2 Epoch: 92 [6400/29981                 (21%)]\tLoss: 0.344516\n",
      "Training stage for Flod 2 Epoch: 92 [9600/29981                 (32%)]\tLoss: 0.438262\n",
      "Training stage for Flod 2 Epoch: 92 [12800/29981                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 92 [16000/29981                 (53%)]\tLoss: 0.319883\n",
      "Training stage for Flod 2 Epoch: 92 [19200/29981                 (64%)]\tLoss: 0.375772\n",
      "Training stage for Flod 2 Epoch: 92 [22400/29981                 (75%)]\tLoss: 0.344528\n",
      "Training stage for Flod 2 Epoch: 92 [25600/29981                 (85%)]\tLoss: 0.375747\n",
      "Training stage for Flod 2 Epoch: 92 [28800/29981                 (96%)]\tLoss: 0.407105\n",
      "Test set for fold2: Average Loss:           720797.1616, Accuracy: 6736/7495           (90%)\n",
      "Training stage for Flod 2 Epoch: 93 [0/29981                 (0%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 93 [3200/29981                 (11%)]\tLoss: 0.405621\n",
      "Training stage for Flod 2 Epoch: 93 [6400/29981                 (21%)]\tLoss: 0.438689\n",
      "Training stage for Flod 2 Epoch: 93 [9600/29981                 (32%)]\tLoss: 0.371488\n",
      "Training stage for Flod 2 Epoch: 93 [12800/29981                 (43%)]\tLoss: 0.339975\n",
      "Training stage for Flod 2 Epoch: 93 [16000/29981                 (53%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 93 [19200/29981                 (64%)]\tLoss: 0.354487\n",
      "Training stage for Flod 2 Epoch: 93 [22400/29981                 (75%)]\tLoss: 0.344524\n",
      "Training stage for Flod 2 Epoch: 93 [25600/29981                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 93 [28800/29981                 (96%)]\tLoss: 0.375872\n",
      "Test set for fold2: Average Loss:           762513.8048, Accuracy: 6577/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 94 [0/29981                 (0%)]\tLoss: 0.313537\n",
      "Training stage for Flod 2 Epoch: 94 [3200/29981                 (11%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 94 [6400/29981                 (21%)]\tLoss: 0.316545\n",
      "Training stage for Flod 2 Epoch: 94 [9600/29981                 (32%)]\tLoss: 0.407012\n",
      "Training stage for Flod 2 Epoch: 94 [12800/29981                 (43%)]\tLoss: 0.432878\n",
      "Training stage for Flod 2 Epoch: 94 [16000/29981                 (53%)]\tLoss: 0.344518\n",
      "Training stage for Flod 2 Epoch: 94 [19200/29981                 (64%)]\tLoss: 0.375791\n",
      "Training stage for Flod 2 Epoch: 94 [22400/29981                 (75%)]\tLoss: 0.313995\n",
      "Training stage for Flod 2 Epoch: 94 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 94 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold2: Average Loss:           759418.7974, Accuracy: 6592/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 95 [0/29981                 (0%)]\tLoss: 0.313292\n",
      "Training stage for Flod 2 Epoch: 95 [3200/29981                 (11%)]\tLoss: 0.443239\n",
      "Training stage for Flod 2 Epoch: 95 [6400/29981                 (21%)]\tLoss: 0.375760\n",
      "Training stage for Flod 2 Epoch: 95 [9600/29981                 (32%)]\tLoss: 0.344511\n",
      "Training stage for Flod 2 Epoch: 95 [12800/29981                 (43%)]\tLoss: 0.337793\n",
      "Training stage for Flod 2 Epoch: 95 [16000/29981                 (53%)]\tLoss: 0.344544\n",
      "Training stage for Flod 2 Epoch: 95 [19200/29981                 (64%)]\tLoss: 0.442410\n",
      "Training stage for Flod 2 Epoch: 95 [22400/29981                 (75%)]\tLoss: 0.352728\n",
      "Training stage for Flod 2 Epoch: 95 [25600/29981                 (85%)]\tLoss: 0.313543\n",
      "Training stage for Flod 2 Epoch: 95 [28800/29981                 (96%)]\tLoss: 0.375766\n",
      "Test set for fold2: Average Loss:           747037.4933, Accuracy: 6656/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 96 [0/29981                 (0%)]\tLoss: 0.417514\n",
      "Training stage for Flod 2 Epoch: 96 [3200/29981                 (11%)]\tLoss: 0.313263\n",
      "Training stage for Flod 2 Epoch: 96 [6400/29981                 (21%)]\tLoss: 0.407064\n",
      "Training stage for Flod 2 Epoch: 96 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 96 [12800/29981                 (43%)]\tLoss: 0.347227\n",
      "Training stage for Flod 2 Epoch: 96 [16000/29981                 (53%)]\tLoss: 0.438262\n",
      "Training stage for Flod 2 Epoch: 96 [19200/29981                 (64%)]\tLoss: 0.344714\n",
      "Training stage for Flod 2 Epoch: 96 [22400/29981                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 96 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 96 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold2: Average Loss:           738006.6812, Accuracy: 6647/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 97 [0/29981                 (0%)]\tLoss: 0.315239\n",
      "Training stage for Flod 2 Epoch: 97 [3200/29981                 (11%)]\tLoss: 0.406151\n",
      "Training stage for Flod 2 Epoch: 97 [6400/29981                 (21%)]\tLoss: 0.313470\n",
      "Training stage for Flod 2 Epoch: 97 [9600/29981                 (32%)]\tLoss: 0.342057\n",
      "Training stage for Flod 2 Epoch: 97 [12800/29981                 (43%)]\tLoss: 0.407012\n",
      "Training stage for Flod 2 Epoch: 97 [16000/29981                 (53%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 97 [19200/29981                 (64%)]\tLoss: 0.407051\n",
      "Training stage for Flod 2 Epoch: 97 [22400/29981                 (75%)]\tLoss: 0.344897\n",
      "Training stage for Flod 2 Epoch: 97 [25600/29981                 (85%)]\tLoss: 0.344535\n",
      "Training stage for Flod 2 Epoch: 97 [28800/29981                 (96%)]\tLoss: 0.375580\n",
      "Test set for fold2: Average Loss:           758347.5225, Accuracy: 6581/7495           (88%)\n",
      "Training stage for Flod 2 Epoch: 98 [0/29981                 (0%)]\tLoss: 0.407037\n",
      "Training stage for Flod 2 Epoch: 98 [3200/29981                 (11%)]\tLoss: 0.378483\n",
      "Training stage for Flod 2 Epoch: 98 [6400/29981                 (21%)]\tLoss: 0.375767\n",
      "Training stage for Flod 2 Epoch: 98 [9600/29981                 (32%)]\tLoss: 0.484551\n",
      "Training stage for Flod 2 Epoch: 98 [12800/29981                 (43%)]\tLoss: 0.411990\n",
      "Training stage for Flod 2 Epoch: 98 [16000/29981                 (53%)]\tLoss: 0.354751\n",
      "Training stage for Flod 2 Epoch: 98 [19200/29981                 (64%)]\tLoss: 0.408258\n",
      "Training stage for Flod 2 Epoch: 98 [22400/29981                 (75%)]\tLoss: 0.313264\n",
      "Training stage for Flod 2 Epoch: 98 [25600/29981                 (85%)]\tLoss: 0.342981\n",
      "Training stage for Flod 2 Epoch: 98 [28800/29981                 (96%)]\tLoss: 0.374708\n",
      "Test set for fold2: Average Loss:           744002.7637, Accuracy: 6651/7495           (89%)\n",
      "Training stage for Flod 2 Epoch: 99 [0/29981                 (0%)]\tLoss: 0.369110\n",
      "Training stage for Flod 2 Epoch: 99 [3200/29981                 (11%)]\tLoss: 0.375404\n",
      "Training stage for Flod 2 Epoch: 99 [6400/29981                 (21%)]\tLoss: 0.375766\n",
      "Training stage for Flod 2 Epoch: 99 [9600/29981                 (32%)]\tLoss: 0.345332\n",
      "Training stage for Flod 2 Epoch: 99 [12800/29981                 (43%)]\tLoss: 0.375763\n",
      "Training stage for Flod 2 Epoch: 99 [16000/29981                 (53%)]\tLoss: 0.353049\n",
      "Training stage for Flod 2 Epoch: 99 [19200/29981                 (64%)]\tLoss: 0.380628\n",
      "Training stage for Flod 2 Epoch: 99 [22400/29981                 (75%)]\tLoss: 0.469512\n",
      "Training stage for Flod 2 Epoch: 99 [25600/29981                 (85%)]\tLoss: 0.521976\n",
      "Training stage for Flod 2 Epoch: 99 [28800/29981                 (96%)]\tLoss: 0.368337\n",
      "Test set for fold2: Average Loss:           714747.7295, Accuracy: 6794/7495           (91%)\n",
      "Training stage for Flod 2 Epoch: 100 [0/29981                 (0%)]\tLoss: 0.378103\n",
      "Training stage for Flod 2 Epoch: 100 [3200/29981                 (11%)]\tLoss: 0.374066\n",
      "Training stage for Flod 2 Epoch: 100 [6400/29981                 (21%)]\tLoss: 0.375763\n",
      "Training stage for Flod 2 Epoch: 100 [9600/29981                 (32%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 100 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 2 Epoch: 100 [16000/29981                 (53%)]\tLoss: 0.375762\n",
      "Training stage for Flod 2 Epoch: 100 [19200/29981                 (64%)]\tLoss: 0.367235\n",
      "Training stage for Flod 2 Epoch: 100 [22400/29981                 (75%)]\tLoss: 0.344513\n",
      "Training stage for Flod 2 Epoch: 100 [25600/29981                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 2 Epoch: 100 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold2: Average Loss:           723253.1912, Accuracy: 6728/7495           (90%)\n",
      "-------------------Fold 3-------------------\n",
      "Training stage for Flod 3 Epoch: 1 [0/29981                 (0%)]\tLoss: 0.693064\n",
      "Training stage for Flod 3 Epoch: 1 [3200/29981                 (11%)]\tLoss: 0.454837\n",
      "Training stage for Flod 3 Epoch: 1 [6400/29981                 (21%)]\tLoss: 0.461584\n",
      "Training stage for Flod 3 Epoch: 1 [9600/29981                 (32%)]\tLoss: 0.572751\n",
      "Training stage for Flod 3 Epoch: 1 [12800/29981                 (43%)]\tLoss: 0.424117\n",
      "Training stage for Flod 3 Epoch: 1 [16000/29981                 (53%)]\tLoss: 0.431521\n",
      "Training stage for Flod 3 Epoch: 1 [19200/29981                 (64%)]\tLoss: 0.462345\n",
      "Training stage for Flod 3 Epoch: 1 [22400/29981                 (75%)]\tLoss: 0.473661\n",
      "Training stage for Flod 3 Epoch: 1 [25600/29981                 (85%)]\tLoss: 0.436106\n",
      "Training stage for Flod 3 Epoch: 1 [28800/29981                 (96%)]\tLoss: 0.482382\n",
      "Test set for fold3: Average Loss:           730346.0629, Accuracy: 6700/7495           (89%)\n",
      "Training stage for Flod 3 Epoch: 2 [0/29981                 (0%)]\tLoss: 0.422167\n",
      "Training stage for Flod 3 Epoch: 2 [3200/29981                 (11%)]\tLoss: 0.364960\n",
      "Training stage for Flod 3 Epoch: 2 [6400/29981                 (21%)]\tLoss: 0.426732\n",
      "Training stage for Flod 3 Epoch: 2 [9600/29981                 (32%)]\tLoss: 0.410368\n",
      "Training stage for Flod 3 Epoch: 2 [12800/29981                 (43%)]\tLoss: 0.445676\n",
      "Training stage for Flod 3 Epoch: 2 [16000/29981                 (53%)]\tLoss: 0.468114\n",
      "Training stage for Flod 3 Epoch: 2 [19200/29981                 (64%)]\tLoss: 0.474869\n",
      "Training stage for Flod 3 Epoch: 2 [22400/29981                 (75%)]\tLoss: 0.488652\n",
      "Training stage for Flod 3 Epoch: 2 [25600/29981                 (85%)]\tLoss: 0.404587\n",
      "Training stage for Flod 3 Epoch: 2 [28800/29981                 (96%)]\tLoss: 0.487010\n",
      "Test set for fold3: Average Loss:           706466.4401, Accuracy: 6808/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 3 [0/29981                 (0%)]\tLoss: 0.465372\n",
      "Training stage for Flod 3 Epoch: 3 [3200/29981                 (11%)]\tLoss: 0.379098\n",
      "Training stage for Flod 3 Epoch: 3 [6400/29981                 (21%)]\tLoss: 0.604716\n",
      "Training stage for Flod 3 Epoch: 3 [9600/29981                 (32%)]\tLoss: 0.437634\n",
      "Training stage for Flod 3 Epoch: 3 [12800/29981                 (43%)]\tLoss: 0.397204\n",
      "Training stage for Flod 3 Epoch: 3 [16000/29981                 (53%)]\tLoss: 0.399293\n",
      "Training stage for Flod 3 Epoch: 3 [19200/29981                 (64%)]\tLoss: 0.487367\n",
      "Training stage for Flod 3 Epoch: 3 [22400/29981                 (75%)]\tLoss: 0.464849\n",
      "Training stage for Flod 3 Epoch: 3 [25600/29981                 (85%)]\tLoss: 0.436754\n",
      "Training stage for Flod 3 Epoch: 3 [28800/29981                 (96%)]\tLoss: 0.466039\n",
      "Test set for fold3: Average Loss:           702416.4258, Accuracy: 6826/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 4 [0/29981                 (0%)]\tLoss: 0.362187\n",
      "Training stage for Flod 3 Epoch: 4 [3200/29981                 (11%)]\tLoss: 0.393306\n",
      "Training stage for Flod 3 Epoch: 4 [6400/29981                 (21%)]\tLoss: 0.502477\n",
      "Training stage for Flod 3 Epoch: 4 [9600/29981                 (32%)]\tLoss: 0.484453\n",
      "Training stage for Flod 3 Epoch: 4 [12800/29981                 (43%)]\tLoss: 0.477855\n",
      "Training stage for Flod 3 Epoch: 4 [16000/29981                 (53%)]\tLoss: 0.404031\n",
      "Training stage for Flod 3 Epoch: 4 [19200/29981                 (64%)]\tLoss: 0.397177\n",
      "Training stage for Flod 3 Epoch: 4 [22400/29981                 (75%)]\tLoss: 0.349782\n",
      "Training stage for Flod 3 Epoch: 4 [25600/29981                 (85%)]\tLoss: 0.466068\n",
      "Training stage for Flod 3 Epoch: 4 [28800/29981                 (96%)]\tLoss: 0.408836\n",
      "Test set for fold3: Average Loss:           702907.2625, Accuracy: 6811/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 5 [0/29981                 (0%)]\tLoss: 0.438324\n",
      "Training stage for Flod 3 Epoch: 5 [3200/29981                 (11%)]\tLoss: 0.448108\n",
      "Training stage for Flod 3 Epoch: 5 [6400/29981                 (21%)]\tLoss: 0.400327\n",
      "Training stage for Flod 3 Epoch: 5 [9600/29981                 (32%)]\tLoss: 0.399754\n",
      "Training stage for Flod 3 Epoch: 5 [12800/29981                 (43%)]\tLoss: 0.389784\n",
      "Training stage for Flod 3 Epoch: 5 [16000/29981                 (53%)]\tLoss: 0.455361\n",
      "Training stage for Flod 3 Epoch: 5 [19200/29981                 (64%)]\tLoss: 0.407559\n",
      "Training stage for Flod 3 Epoch: 5 [22400/29981                 (75%)]\tLoss: 0.538419\n",
      "Training stage for Flod 3 Epoch: 5 [25600/29981                 (85%)]\tLoss: 0.426614\n",
      "Training stage for Flod 3 Epoch: 5 [28800/29981                 (96%)]\tLoss: 0.444179\n",
      "Test set for fold3: Average Loss:           700157.0175, Accuracy: 6841/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 6 [0/29981                 (0%)]\tLoss: 0.388086\n",
      "Training stage for Flod 3 Epoch: 6 [3200/29981                 (11%)]\tLoss: 0.422169\n",
      "Training stage for Flod 3 Epoch: 6 [6400/29981                 (21%)]\tLoss: 0.374432\n",
      "Training stage for Flod 3 Epoch: 6 [9600/29981                 (32%)]\tLoss: 0.488862\n",
      "Training stage for Flod 3 Epoch: 6 [12800/29981                 (43%)]\tLoss: 0.380214\n",
      "Training stage for Flod 3 Epoch: 6 [16000/29981                 (53%)]\tLoss: 0.347790\n",
      "Training stage for Flod 3 Epoch: 6 [19200/29981                 (64%)]\tLoss: 0.437057\n",
      "Training stage for Flod 3 Epoch: 6 [22400/29981                 (75%)]\tLoss: 0.358115\n",
      "Training stage for Flod 3 Epoch: 6 [25600/29981                 (85%)]\tLoss: 0.414031\n",
      "Training stage for Flod 3 Epoch: 6 [28800/29981                 (96%)]\tLoss: 0.426280\n",
      "Test set for fold3: Average Loss:           711454.6989, Accuracy: 6766/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 7 [0/29981                 (0%)]\tLoss: 0.347171\n",
      "Training stage for Flod 3 Epoch: 7 [3200/29981                 (11%)]\tLoss: 0.457183\n",
      "Training stage for Flod 3 Epoch: 7 [6400/29981                 (21%)]\tLoss: 0.493782\n",
      "Training stage for Flod 3 Epoch: 7 [9600/29981                 (32%)]\tLoss: 0.410794\n",
      "Training stage for Flod 3 Epoch: 7 [12800/29981                 (43%)]\tLoss: 0.446723\n",
      "Training stage for Flod 3 Epoch: 7 [16000/29981                 (53%)]\tLoss: 0.417747\n",
      "Training stage for Flod 3 Epoch: 7 [19200/29981                 (64%)]\tLoss: 0.469867\n",
      "Training stage for Flod 3 Epoch: 7 [22400/29981                 (75%)]\tLoss: 0.455081\n",
      "Training stage for Flod 3 Epoch: 7 [25600/29981                 (85%)]\tLoss: 0.443813\n",
      "Training stage for Flod 3 Epoch: 7 [28800/29981                 (96%)]\tLoss: 0.384252\n",
      "Test set for fold3: Average Loss:           696336.1180, Accuracy: 6879/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 8 [0/29981                 (0%)]\tLoss: 0.408728\n",
      "Training stage for Flod 3 Epoch: 8 [3200/29981                 (11%)]\tLoss: 0.406498\n",
      "Training stage for Flod 3 Epoch: 8 [6400/29981                 (21%)]\tLoss: 0.375944\n",
      "Training stage for Flod 3 Epoch: 8 [9600/29981                 (32%)]\tLoss: 0.412249\n",
      "Training stage for Flod 3 Epoch: 8 [12800/29981                 (43%)]\tLoss: 0.380084\n",
      "Training stage for Flod 3 Epoch: 8 [16000/29981                 (53%)]\tLoss: 0.469224\n",
      "Training stage for Flod 3 Epoch: 8 [19200/29981                 (64%)]\tLoss: 0.433558\n",
      "Training stage for Flod 3 Epoch: 8 [22400/29981                 (75%)]\tLoss: 0.430799\n",
      "Training stage for Flod 3 Epoch: 8 [25600/29981                 (85%)]\tLoss: 0.358734\n",
      "Training stage for Flod 3 Epoch: 8 [28800/29981                 (96%)]\tLoss: 0.548612\n",
      "Test set for fold3: Average Loss:           744636.6305, Accuracy: 6652/7495           (89%)\n",
      "Training stage for Flod 3 Epoch: 9 [0/29981                 (0%)]\tLoss: 0.472644\n",
      "Training stage for Flod 3 Epoch: 9 [3200/29981                 (11%)]\tLoss: 0.378911\n",
      "Training stage for Flod 3 Epoch: 9 [6400/29981                 (21%)]\tLoss: 0.387676\n",
      "Training stage for Flod 3 Epoch: 9 [9600/29981                 (32%)]\tLoss: 0.419230\n",
      "Training stage for Flod 3 Epoch: 9 [12800/29981                 (43%)]\tLoss: 0.387890\n",
      "Training stage for Flod 3 Epoch: 9 [16000/29981                 (53%)]\tLoss: 0.351291\n",
      "Training stage for Flod 3 Epoch: 9 [19200/29981                 (64%)]\tLoss: 0.390135\n",
      "Training stage for Flod 3 Epoch: 9 [22400/29981                 (75%)]\tLoss: 0.453692\n",
      "Training stage for Flod 3 Epoch: 9 [25600/29981                 (85%)]\tLoss: 0.377683\n",
      "Training stage for Flod 3 Epoch: 9 [28800/29981                 (96%)]\tLoss: 0.418511\n",
      "Test set for fold3: Average Loss:           708973.8724, Accuracy: 6794/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 10 [0/29981                 (0%)]\tLoss: 0.475396\n",
      "Training stage for Flod 3 Epoch: 10 [3200/29981                 (11%)]\tLoss: 0.406421\n",
      "Training stage for Flod 3 Epoch: 10 [6400/29981                 (21%)]\tLoss: 0.392065\n",
      "Training stage for Flod 3 Epoch: 10 [9600/29981                 (32%)]\tLoss: 0.389529\n",
      "Training stage for Flod 3 Epoch: 10 [12800/29981                 (43%)]\tLoss: 0.432273\n",
      "Training stage for Flod 3 Epoch: 10 [16000/29981                 (53%)]\tLoss: 0.371180\n",
      "Training stage for Flod 3 Epoch: 10 [19200/29981                 (64%)]\tLoss: 0.397186\n",
      "Training stage for Flod 3 Epoch: 10 [22400/29981                 (75%)]\tLoss: 0.406211\n",
      "Training stage for Flod 3 Epoch: 10 [25600/29981                 (85%)]\tLoss: 0.469843\n",
      "Training stage for Flod 3 Epoch: 10 [28800/29981                 (96%)]\tLoss: 0.405118\n",
      "Test set for fold3: Average Loss:           697942.2023, Accuracy: 6853/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 11 [0/29981                 (0%)]\tLoss: 0.483972\n",
      "Training stage for Flod 3 Epoch: 11 [3200/29981                 (11%)]\tLoss: 0.406423\n",
      "Training stage for Flod 3 Epoch: 11 [6400/29981                 (21%)]\tLoss: 0.483819\n",
      "Training stage for Flod 3 Epoch: 11 [9600/29981                 (32%)]\tLoss: 0.432216\n",
      "Training stage for Flod 3 Epoch: 11 [12800/29981                 (43%)]\tLoss: 0.401260\n",
      "Training stage for Flod 3 Epoch: 11 [16000/29981                 (53%)]\tLoss: 0.407941\n",
      "Training stage for Flod 3 Epoch: 11 [19200/29981                 (64%)]\tLoss: 0.408212\n",
      "Training stage for Flod 3 Epoch: 11 [22400/29981                 (75%)]\tLoss: 0.458558\n",
      "Training stage for Flod 3 Epoch: 11 [25600/29981                 (85%)]\tLoss: 0.373284\n",
      "Training stage for Flod 3 Epoch: 11 [28800/29981                 (96%)]\tLoss: 0.371582\n",
      "Test set for fold3: Average Loss:           713087.0388, Accuracy: 6769/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 12 [0/29981                 (0%)]\tLoss: 0.512397\n",
      "Training stage for Flod 3 Epoch: 12 [3200/29981                 (11%)]\tLoss: 0.475647\n",
      "Training stage for Flod 3 Epoch: 12 [6400/29981                 (21%)]\tLoss: 0.363649\n",
      "Training stage for Flod 3 Epoch: 12 [9600/29981                 (32%)]\tLoss: 0.388910\n",
      "Training stage for Flod 3 Epoch: 12 [12800/29981                 (43%)]\tLoss: 0.376327\n",
      "Training stage for Flod 3 Epoch: 12 [16000/29981                 (53%)]\tLoss: 0.326307\n",
      "Training stage for Flod 3 Epoch: 12 [19200/29981                 (64%)]\tLoss: 0.402150\n",
      "Training stage for Flod 3 Epoch: 12 [22400/29981                 (75%)]\tLoss: 0.420145\n",
      "Training stage for Flod 3 Epoch: 12 [25600/29981                 (85%)]\tLoss: 0.376057\n",
      "Training stage for Flod 3 Epoch: 12 [28800/29981                 (96%)]\tLoss: 0.427460\n",
      "Test set for fold3: Average Loss:           688203.9695, Accuracy: 6861/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 13 [0/29981                 (0%)]\tLoss: 0.416304\n",
      "Training stage for Flod 3 Epoch: 13 [3200/29981                 (11%)]\tLoss: 0.396648\n",
      "Training stage for Flod 3 Epoch: 13 [6400/29981                 (21%)]\tLoss: 0.496101\n",
      "Training stage for Flod 3 Epoch: 13 [9600/29981                 (32%)]\tLoss: 0.438306\n",
      "Training stage for Flod 3 Epoch: 13 [12800/29981                 (43%)]\tLoss: 0.407630\n",
      "Training stage for Flod 3 Epoch: 13 [16000/29981                 (53%)]\tLoss: 0.442823\n",
      "Training stage for Flod 3 Epoch: 13 [19200/29981                 (64%)]\tLoss: 0.405994\n",
      "Training stage for Flod 3 Epoch: 13 [22400/29981                 (75%)]\tLoss: 0.402667\n",
      "Training stage for Flod 3 Epoch: 13 [25600/29981                 (85%)]\tLoss: 0.412449\n",
      "Training stage for Flod 3 Epoch: 13 [28800/29981                 (96%)]\tLoss: 0.386335\n",
      "Test set for fold3: Average Loss:           703012.4004, Accuracy: 6829/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 14 [0/29981                 (0%)]\tLoss: 0.376232\n",
      "Training stage for Flod 3 Epoch: 14 [3200/29981                 (11%)]\tLoss: 0.398164\n",
      "Training stage for Flod 3 Epoch: 14 [6400/29981                 (21%)]\tLoss: 0.412709\n",
      "Training stage for Flod 3 Epoch: 14 [9600/29981                 (32%)]\tLoss: 0.354004\n",
      "Training stage for Flod 3 Epoch: 14 [12800/29981                 (43%)]\tLoss: 0.406940\n",
      "Training stage for Flod 3 Epoch: 14 [16000/29981                 (53%)]\tLoss: 0.376725\n",
      "Training stage for Flod 3 Epoch: 14 [19200/29981                 (64%)]\tLoss: 0.395338\n",
      "Training stage for Flod 3 Epoch: 14 [22400/29981                 (75%)]\tLoss: 0.404163\n",
      "Training stage for Flod 3 Epoch: 14 [25600/29981                 (85%)]\tLoss: 0.438366\n",
      "Training stage for Flod 3 Epoch: 14 [28800/29981                 (96%)]\tLoss: 0.409178\n",
      "Test set for fold3: Average Loss:           690906.3899, Accuracy: 6886/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 15 [0/29981                 (0%)]\tLoss: 0.344563\n",
      "Training stage for Flod 3 Epoch: 15 [3200/29981                 (11%)]\tLoss: 0.417687\n",
      "Training stage for Flod 3 Epoch: 15 [6400/29981                 (21%)]\tLoss: 0.423867\n",
      "Training stage for Flod 3 Epoch: 15 [9600/29981                 (32%)]\tLoss: 0.378268\n",
      "Training stage for Flod 3 Epoch: 15 [12800/29981                 (43%)]\tLoss: 0.388509\n",
      "Training stage for Flod 3 Epoch: 15 [16000/29981                 (53%)]\tLoss: 0.458417\n",
      "Training stage for Flod 3 Epoch: 15 [19200/29981                 (64%)]\tLoss: 0.375792\n",
      "Training stage for Flod 3 Epoch: 15 [22400/29981                 (75%)]\tLoss: 0.341803\n",
      "Training stage for Flod 3 Epoch: 15 [25600/29981                 (85%)]\tLoss: 0.407022\n",
      "Training stage for Flod 3 Epoch: 15 [28800/29981                 (96%)]\tLoss: 0.336725\n",
      "Test set for fold3: Average Loss:           690678.6191, Accuracy: 6875/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 16 [0/29981                 (0%)]\tLoss: 0.517833\n",
      "Training stage for Flod 3 Epoch: 16 [3200/29981                 (11%)]\tLoss: 0.445698\n",
      "Training stage for Flod 3 Epoch: 16 [6400/29981                 (21%)]\tLoss: 0.438761\n",
      "Training stage for Flod 3 Epoch: 16 [9600/29981                 (32%)]\tLoss: 0.500673\n",
      "Training stage for Flod 3 Epoch: 16 [12800/29981                 (43%)]\tLoss: 0.383867\n",
      "Training stage for Flod 3 Epoch: 16 [16000/29981                 (53%)]\tLoss: 0.365484\n",
      "Training stage for Flod 3 Epoch: 16 [19200/29981                 (64%)]\tLoss: 0.438166\n",
      "Training stage for Flod 3 Epoch: 16 [22400/29981                 (75%)]\tLoss: 0.453933\n",
      "Training stage for Flod 3 Epoch: 16 [25600/29981                 (85%)]\tLoss: 0.404149\n",
      "Training stage for Flod 3 Epoch: 16 [28800/29981                 (96%)]\tLoss: 0.430398\n",
      "Test set for fold3: Average Loss:           696779.0124, Accuracy: 6869/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 17 [0/29981                 (0%)]\tLoss: 0.407730\n",
      "Training stage for Flod 3 Epoch: 17 [3200/29981                 (11%)]\tLoss: 0.358703\n",
      "Training stage for Flod 3 Epoch: 17 [6400/29981                 (21%)]\tLoss: 0.345205\n",
      "Training stage for Flod 3 Epoch: 17 [9600/29981                 (32%)]\tLoss: 0.415362\n",
      "Training stage for Flod 3 Epoch: 17 [12800/29981                 (43%)]\tLoss: 0.401407\n",
      "Training stage for Flod 3 Epoch: 17 [16000/29981                 (53%)]\tLoss: 0.423627\n",
      "Training stage for Flod 3 Epoch: 17 [19200/29981                 (64%)]\tLoss: 0.358104\n",
      "Training stage for Flod 3 Epoch: 17 [22400/29981                 (75%)]\tLoss: 0.441933\n",
      "Training stage for Flod 3 Epoch: 17 [25600/29981                 (85%)]\tLoss: 0.400938\n",
      "Training stage for Flod 3 Epoch: 17 [28800/29981                 (96%)]\tLoss: 0.375819\n",
      "Test set for fold3: Average Loss:           687508.2778, Accuracy: 6905/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 18 [0/29981                 (0%)]\tLoss: 0.357384\n",
      "Training stage for Flod 3 Epoch: 18 [3200/29981                 (11%)]\tLoss: 0.344574\n",
      "Training stage for Flod 3 Epoch: 18 [6400/29981                 (21%)]\tLoss: 0.389569\n",
      "Training stage for Flod 3 Epoch: 18 [9600/29981                 (32%)]\tLoss: 0.376616\n",
      "Training stage for Flod 3 Epoch: 18 [12800/29981                 (43%)]\tLoss: 0.381235\n",
      "Training stage for Flod 3 Epoch: 18 [16000/29981                 (53%)]\tLoss: 0.405940\n",
      "Training stage for Flod 3 Epoch: 18 [19200/29981                 (64%)]\tLoss: 0.428414\n",
      "Training stage for Flod 3 Epoch: 18 [22400/29981                 (75%)]\tLoss: 0.320038\n",
      "Training stage for Flod 3 Epoch: 18 [25600/29981                 (85%)]\tLoss: 0.379826\n",
      "Training stage for Flod 3 Epoch: 18 [28800/29981                 (96%)]\tLoss: 0.331055\n",
      "Test set for fold3: Average Loss:           687331.7049, Accuracy: 6883/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 19 [0/29981                 (0%)]\tLoss: 0.465159\n",
      "Training stage for Flod 3 Epoch: 19 [3200/29981                 (11%)]\tLoss: 0.443736\n",
      "Training stage for Flod 3 Epoch: 19 [6400/29981                 (21%)]\tLoss: 0.376263\n",
      "Training stage for Flod 3 Epoch: 19 [9600/29981                 (32%)]\tLoss: 0.397076\n",
      "Training stage for Flod 3 Epoch: 19 [12800/29981                 (43%)]\tLoss: 0.375596\n",
      "Training stage for Flod 3 Epoch: 19 [16000/29981                 (53%)]\tLoss: 0.408273\n",
      "Training stage for Flod 3 Epoch: 19 [19200/29981                 (64%)]\tLoss: 0.313413\n",
      "Training stage for Flod 3 Epoch: 19 [22400/29981                 (75%)]\tLoss: 0.407020\n",
      "Training stage for Flod 3 Epoch: 19 [25600/29981                 (85%)]\tLoss: 0.377200\n",
      "Training stage for Flod 3 Epoch: 19 [28800/29981                 (96%)]\tLoss: 0.434095\n",
      "Test set for fold3: Average Loss:           688898.0937, Accuracy: 6892/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 20 [0/29981                 (0%)]\tLoss: 0.436280\n",
      "Training stage for Flod 3 Epoch: 20 [3200/29981                 (11%)]\tLoss: 0.432340\n",
      "Training stage for Flod 3 Epoch: 20 [6400/29981                 (21%)]\tLoss: 0.408189\n",
      "Training stage for Flod 3 Epoch: 20 [9600/29981                 (32%)]\tLoss: 0.412051\n",
      "Training stage for Flod 3 Epoch: 20 [12800/29981                 (43%)]\tLoss: 0.409360\n",
      "Training stage for Flod 3 Epoch: 20 [16000/29981                 (53%)]\tLoss: 0.435163\n",
      "Training stage for Flod 3 Epoch: 20 [19200/29981                 (64%)]\tLoss: 0.396365\n",
      "Training stage for Flod 3 Epoch: 20 [22400/29981                 (75%)]\tLoss: 0.344598\n",
      "Training stage for Flod 3 Epoch: 20 [25600/29981                 (85%)]\tLoss: 0.407265\n",
      "Training stage for Flod 3 Epoch: 20 [28800/29981                 (96%)]\tLoss: 0.396525\n",
      "Test set for fold3: Average Loss:           683169.3765, Accuracy: 6925/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 21 [0/29981                 (0%)]\tLoss: 0.394968\n",
      "Training stage for Flod 3 Epoch: 21 [3200/29981                 (11%)]\tLoss: 0.407242\n",
      "Training stage for Flod 3 Epoch: 21 [6400/29981                 (21%)]\tLoss: 0.313326\n",
      "Training stage for Flod 3 Epoch: 21 [9600/29981                 (32%)]\tLoss: 0.521971\n",
      "Training stage for Flod 3 Epoch: 21 [12800/29981                 (43%)]\tLoss: 0.376564\n",
      "Training stage for Flod 3 Epoch: 21 [16000/29981                 (53%)]\tLoss: 0.379070\n",
      "Training stage for Flod 3 Epoch: 21 [19200/29981                 (64%)]\tLoss: 0.413779\n",
      "Training stage for Flod 3 Epoch: 21 [22400/29981                 (75%)]\tLoss: 0.475985\n",
      "Training stage for Flod 3 Epoch: 21 [25600/29981                 (85%)]\tLoss: 0.453597\n",
      "Training stage for Flod 3 Epoch: 21 [28800/29981                 (96%)]\tLoss: 0.397784\n",
      "Test set for fold3: Average Loss:           694998.9291, Accuracy: 6851/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 22 [0/29981                 (0%)]\tLoss: 0.377109\n",
      "Training stage for Flod 3 Epoch: 22 [3200/29981                 (11%)]\tLoss: 0.437759\n",
      "Training stage for Flod 3 Epoch: 22 [6400/29981                 (21%)]\tLoss: 0.424858\n",
      "Training stage for Flod 3 Epoch: 22 [9600/29981                 (32%)]\tLoss: 0.348819\n",
      "Training stage for Flod 3 Epoch: 22 [12800/29981                 (43%)]\tLoss: 0.371931\n",
      "Training stage for Flod 3 Epoch: 22 [16000/29981                 (53%)]\tLoss: 0.392782\n",
      "Training stage for Flod 3 Epoch: 22 [19200/29981                 (64%)]\tLoss: 0.344642\n",
      "Training stage for Flod 3 Epoch: 22 [22400/29981                 (75%)]\tLoss: 0.390822\n",
      "Training stage for Flod 3 Epoch: 22 [25600/29981                 (85%)]\tLoss: 0.404806\n",
      "Training stage for Flod 3 Epoch: 22 [28800/29981                 (96%)]\tLoss: 0.391962\n",
      "Test set for fold3: Average Loss:           695488.9670, Accuracy: 6869/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 23 [0/29981                 (0%)]\tLoss: 0.344787\n",
      "Training stage for Flod 3 Epoch: 23 [3200/29981                 (11%)]\tLoss: 0.373762\n",
      "Training stage for Flod 3 Epoch: 23 [6400/29981                 (21%)]\tLoss: 0.400096\n",
      "Training stage for Flod 3 Epoch: 23 [9600/29981                 (32%)]\tLoss: 0.399627\n",
      "Training stage for Flod 3 Epoch: 23 [12800/29981                 (43%)]\tLoss: 0.385176\n",
      "Training stage for Flod 3 Epoch: 23 [16000/29981                 (53%)]\tLoss: 0.456131\n",
      "Training stage for Flod 3 Epoch: 23 [19200/29981                 (64%)]\tLoss: 0.313276\n",
      "Training stage for Flod 3 Epoch: 23 [22400/29981                 (75%)]\tLoss: 0.408616\n",
      "Training stage for Flod 3 Epoch: 23 [25600/29981                 (85%)]\tLoss: 0.313754\n",
      "Training stage for Flod 3 Epoch: 23 [28800/29981                 (96%)]\tLoss: 0.459493\n",
      "Test set for fold3: Average Loss:           711973.6134, Accuracy: 6794/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 24 [0/29981                 (0%)]\tLoss: 0.344519\n",
      "Training stage for Flod 3 Epoch: 24 [3200/29981                 (11%)]\tLoss: 0.323362\n",
      "Training stage for Flod 3 Epoch: 24 [6400/29981                 (21%)]\tLoss: 0.376139\n",
      "Training stage for Flod 3 Epoch: 24 [9600/29981                 (32%)]\tLoss: 0.344702\n",
      "Training stage for Flod 3 Epoch: 24 [12800/29981                 (43%)]\tLoss: 0.345807\n",
      "Training stage for Flod 3 Epoch: 24 [16000/29981                 (53%)]\tLoss: 0.416652\n",
      "Training stage for Flod 3 Epoch: 24 [19200/29981                 (64%)]\tLoss: 0.419497\n",
      "Training stage for Flod 3 Epoch: 24 [22400/29981                 (75%)]\tLoss: 0.478258\n",
      "Training stage for Flod 3 Epoch: 24 [25600/29981                 (85%)]\tLoss: 0.501224\n",
      "Training stage for Flod 3 Epoch: 24 [28800/29981                 (96%)]\tLoss: 0.384323\n",
      "Test set for fold3: Average Loss:           707273.4436, Accuracy: 6765/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 25 [0/29981                 (0%)]\tLoss: 0.409240\n",
      "Training stage for Flod 3 Epoch: 25 [3200/29981                 (11%)]\tLoss: 0.452564\n",
      "Training stage for Flod 3 Epoch: 25 [6400/29981                 (21%)]\tLoss: 0.469409\n",
      "Training stage for Flod 3 Epoch: 25 [9600/29981                 (32%)]\tLoss: 0.407333\n",
      "Training stage for Flod 3 Epoch: 25 [12800/29981                 (43%)]\tLoss: 0.344673\n",
      "Training stage for Flod 3 Epoch: 25 [16000/29981                 (53%)]\tLoss: 0.415694\n",
      "Training stage for Flod 3 Epoch: 25 [19200/29981                 (64%)]\tLoss: 0.397111\n",
      "Training stage for Flod 3 Epoch: 25 [22400/29981                 (75%)]\tLoss: 0.378587\n",
      "Training stage for Flod 3 Epoch: 25 [25600/29981                 (85%)]\tLoss: 0.406998\n",
      "Training stage for Flod 3 Epoch: 25 [28800/29981                 (96%)]\tLoss: 0.387241\n",
      "Test set for fold3: Average Loss:           691019.3303, Accuracy: 6851/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 26 [0/29981                 (0%)]\tLoss: 0.344549\n",
      "Training stage for Flod 3 Epoch: 26 [3200/29981                 (11%)]\tLoss: 0.369963\n",
      "Training stage for Flod 3 Epoch: 26 [6400/29981                 (21%)]\tLoss: 0.376580\n",
      "Training stage for Flod 3 Epoch: 26 [9600/29981                 (32%)]\tLoss: 0.344530\n",
      "Training stage for Flod 3 Epoch: 26 [12800/29981                 (43%)]\tLoss: 0.412889\n",
      "Training stage for Flod 3 Epoch: 26 [16000/29981                 (53%)]\tLoss: 0.314624\n",
      "Training stage for Flod 3 Epoch: 26 [19200/29981                 (64%)]\tLoss: 0.315639\n",
      "Training stage for Flod 3 Epoch: 26 [22400/29981                 (75%)]\tLoss: 0.375764\n",
      "Training stage for Flod 3 Epoch: 26 [25600/29981                 (85%)]\tLoss: 0.505531\n",
      "Training stage for Flod 3 Epoch: 26 [28800/29981                 (96%)]\tLoss: 0.349103\n",
      "Test set for fold3: Average Loss:           688036.1718, Accuracy: 6883/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 27 [0/29981                 (0%)]\tLoss: 0.438272\n",
      "Training stage for Flod 3 Epoch: 27 [3200/29981                 (11%)]\tLoss: 0.365502\n",
      "Training stage for Flod 3 Epoch: 27 [6400/29981                 (21%)]\tLoss: 0.375557\n",
      "Training stage for Flod 3 Epoch: 27 [9600/29981                 (32%)]\tLoss: 0.396822\n",
      "Training stage for Flod 3 Epoch: 27 [12800/29981                 (43%)]\tLoss: 0.348835\n",
      "Training stage for Flod 3 Epoch: 27 [16000/29981                 (53%)]\tLoss: 0.345000\n",
      "Training stage for Flod 3 Epoch: 27 [19200/29981                 (64%)]\tLoss: 0.406217\n",
      "Training stage for Flod 3 Epoch: 27 [22400/29981                 (75%)]\tLoss: 0.340017\n",
      "Training stage for Flod 3 Epoch: 27 [25600/29981                 (85%)]\tLoss: 0.467876\n",
      "Training stage for Flod 3 Epoch: 27 [28800/29981                 (96%)]\tLoss: 0.384659\n",
      "Test set for fold3: Average Loss:           713921.0044, Accuracy: 6775/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 28 [0/29981                 (0%)]\tLoss: 0.460717\n",
      "Training stage for Flod 3 Epoch: 28 [3200/29981                 (11%)]\tLoss: 0.385606\n",
      "Training stage for Flod 3 Epoch: 28 [6400/29981                 (21%)]\tLoss: 0.541775\n",
      "Training stage for Flod 3 Epoch: 28 [9600/29981                 (32%)]\tLoss: 0.344310\n",
      "Training stage for Flod 3 Epoch: 28 [12800/29981                 (43%)]\tLoss: 0.375513\n",
      "Training stage for Flod 3 Epoch: 28 [16000/29981                 (53%)]\tLoss: 0.376843\n",
      "Training stage for Flod 3 Epoch: 28 [19200/29981                 (64%)]\tLoss: 0.376886\n",
      "Training stage for Flod 3 Epoch: 28 [22400/29981                 (75%)]\tLoss: 0.344162\n",
      "Training stage for Flod 3 Epoch: 28 [25600/29981                 (85%)]\tLoss: 0.345603\n",
      "Training stage for Flod 3 Epoch: 28 [28800/29981                 (96%)]\tLoss: 0.313628\n",
      "Test set for fold3: Average Loss:           707141.2718, Accuracy: 6807/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 29 [0/29981                 (0%)]\tLoss: 0.374602\n",
      "Training stage for Flod 3 Epoch: 29 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 29 [6400/29981                 (21%)]\tLoss: 0.407811\n",
      "Training stage for Flod 3 Epoch: 29 [9600/29981                 (32%)]\tLoss: 0.399893\n",
      "Training stage for Flod 3 Epoch: 29 [12800/29981                 (43%)]\tLoss: 0.444360\n",
      "Training stage for Flod 3 Epoch: 29 [16000/29981                 (53%)]\tLoss: 0.469241\n",
      "Training stage for Flod 3 Epoch: 29 [19200/29981                 (64%)]\tLoss: 0.420932\n",
      "Training stage for Flod 3 Epoch: 29 [22400/29981                 (75%)]\tLoss: 0.346820\n",
      "Training stage for Flod 3 Epoch: 29 [25600/29981                 (85%)]\tLoss: 0.375783\n",
      "Training stage for Flod 3 Epoch: 29 [28800/29981                 (96%)]\tLoss: 0.402352\n",
      "Test set for fold3: Average Loss:           698401.3952, Accuracy: 6857/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 30 [0/29981                 (0%)]\tLoss: 0.355382\n",
      "Training stage for Flod 3 Epoch: 30 [3200/29981                 (11%)]\tLoss: 0.382534\n",
      "Training stage for Flod 3 Epoch: 30 [6400/29981                 (21%)]\tLoss: 0.324045\n",
      "Training stage for Flod 3 Epoch: 30 [9600/29981                 (32%)]\tLoss: 0.375880\n",
      "Training stage for Flod 3 Epoch: 30 [12800/29981                 (43%)]\tLoss: 0.433528\n",
      "Training stage for Flod 3 Epoch: 30 [16000/29981                 (53%)]\tLoss: 0.350961\n",
      "Training stage for Flod 3 Epoch: 30 [19200/29981                 (64%)]\tLoss: 0.380107\n",
      "Training stage for Flod 3 Epoch: 30 [22400/29981                 (75%)]\tLoss: 0.384120\n",
      "Training stage for Flod 3 Epoch: 30 [25600/29981                 (85%)]\tLoss: 0.361978\n",
      "Training stage for Flod 3 Epoch: 30 [28800/29981                 (96%)]\tLoss: 0.344535\n",
      "Test set for fold3: Average Loss:           696564.5859, Accuracy: 6830/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 31 [0/29981                 (0%)]\tLoss: 0.410011\n",
      "Training stage for Flod 3 Epoch: 31 [3200/29981                 (11%)]\tLoss: 0.342970\n",
      "Training stage for Flod 3 Epoch: 31 [6400/29981                 (21%)]\tLoss: 0.375891\n",
      "Training stage for Flod 3 Epoch: 31 [9600/29981                 (32%)]\tLoss: 0.375764\n",
      "Training stage for Flod 3 Epoch: 31 [12800/29981                 (43%)]\tLoss: 0.467380\n",
      "Training stage for Flod 3 Epoch: 31 [16000/29981                 (53%)]\tLoss: 0.423829\n",
      "Training stage for Flod 3 Epoch: 31 [19200/29981                 (64%)]\tLoss: 0.347966\n",
      "Training stage for Flod 3 Epoch: 31 [22400/29981                 (75%)]\tLoss: 0.351139\n",
      "Training stage for Flod 3 Epoch: 31 [25600/29981                 (85%)]\tLoss: 0.415205\n",
      "Training stage for Flod 3 Epoch: 31 [28800/29981                 (96%)]\tLoss: 0.375617\n",
      "Test set for fold3: Average Loss:           703950.3181, Accuracy: 6781/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 32 [0/29981                 (0%)]\tLoss: 0.435824\n",
      "Training stage for Flod 3 Epoch: 32 [3200/29981                 (11%)]\tLoss: 0.353486\n",
      "Training stage for Flod 3 Epoch: 32 [6400/29981                 (21%)]\tLoss: 0.404807\n",
      "Training stage for Flod 3 Epoch: 32 [9600/29981                 (32%)]\tLoss: 0.362433\n",
      "Training stage for Flod 3 Epoch: 32 [12800/29981                 (43%)]\tLoss: 0.330869\n",
      "Training stage for Flod 3 Epoch: 32 [16000/29981                 (53%)]\tLoss: 0.377905\n",
      "Training stage for Flod 3 Epoch: 32 [19200/29981                 (64%)]\tLoss: 0.410308\n",
      "Training stage for Flod 3 Epoch: 32 [22400/29981                 (75%)]\tLoss: 0.322813\n",
      "Training stage for Flod 3 Epoch: 32 [25600/29981                 (85%)]\tLoss: 0.407216\n",
      "Training stage for Flod 3 Epoch: 32 [28800/29981                 (96%)]\tLoss: 0.397779\n",
      "Test set for fold3: Average Loss:           679338.8473, Accuracy: 6913/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 33 [0/29981                 (0%)]\tLoss: 0.377358\n",
      "Training stage for Flod 3 Epoch: 33 [3200/29981                 (11%)]\tLoss: 0.317713\n",
      "Training stage for Flod 3 Epoch: 33 [6400/29981                 (21%)]\tLoss: 0.428331\n",
      "Training stage for Flod 3 Epoch: 33 [9600/29981                 (32%)]\tLoss: 0.344576\n",
      "Training stage for Flod 3 Epoch: 33 [12800/29981                 (43%)]\tLoss: 0.475924\n",
      "Training stage for Flod 3 Epoch: 33 [16000/29981                 (53%)]\tLoss: 0.321616\n",
      "Training stage for Flod 3 Epoch: 33 [19200/29981                 (64%)]\tLoss: 0.378066\n",
      "Training stage for Flod 3 Epoch: 33 [22400/29981                 (75%)]\tLoss: 0.375946\n",
      "Training stage for Flod 3 Epoch: 33 [25600/29981                 (85%)]\tLoss: 0.442400\n",
      "Training stage for Flod 3 Epoch: 33 [28800/29981                 (96%)]\tLoss: 0.375919\n",
      "Test set for fold3: Average Loss:           683761.9469, Accuracy: 6892/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 34 [0/29981                 (0%)]\tLoss: 0.496793\n",
      "Training stage for Flod 3 Epoch: 34 [3200/29981                 (11%)]\tLoss: 0.412025\n",
      "Training stage for Flod 3 Epoch: 34 [6400/29981                 (21%)]\tLoss: 0.375781\n",
      "Training stage for Flod 3 Epoch: 34 [9600/29981                 (32%)]\tLoss: 0.433070\n",
      "Training stage for Flod 3 Epoch: 34 [12800/29981                 (43%)]\tLoss: 0.406455\n",
      "Training stage for Flod 3 Epoch: 34 [16000/29981                 (53%)]\tLoss: 0.375769\n",
      "Training stage for Flod 3 Epoch: 34 [19200/29981                 (64%)]\tLoss: 0.438266\n",
      "Training stage for Flod 3 Epoch: 34 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 34 [25600/29981                 (85%)]\tLoss: 0.391175\n",
      "Training stage for Flod 3 Epoch: 34 [28800/29981                 (96%)]\tLoss: 0.457126\n",
      "Test set for fold3: Average Loss:           671446.7867, Accuracy: 6961/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 35 [0/29981                 (0%)]\tLoss: 0.355320\n",
      "Training stage for Flod 3 Epoch: 35 [3200/29981                 (11%)]\tLoss: 0.332294\n",
      "Training stage for Flod 3 Epoch: 35 [6400/29981                 (21%)]\tLoss: 0.351160\n",
      "Training stage for Flod 3 Epoch: 35 [9600/29981                 (32%)]\tLoss: 0.403007\n",
      "Training stage for Flod 3 Epoch: 35 [12800/29981                 (43%)]\tLoss: 0.438071\n",
      "Training stage for Flod 3 Epoch: 35 [16000/29981                 (53%)]\tLoss: 0.373079\n",
      "Training stage for Flod 3 Epoch: 35 [19200/29981                 (64%)]\tLoss: 0.344519\n",
      "Training stage for Flod 3 Epoch: 35 [22400/29981                 (75%)]\tLoss: 0.376703\n",
      "Training stage for Flod 3 Epoch: 35 [25600/29981                 (85%)]\tLoss: 0.359038\n",
      "Training stage for Flod 3 Epoch: 35 [28800/29981                 (96%)]\tLoss: 0.313278\n",
      "Test set for fold3: Average Loss:           686371.8051, Accuracy: 6894/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 36 [0/29981                 (0%)]\tLoss: 0.437309\n",
      "Training stage for Flod 3 Epoch: 36 [3200/29981                 (11%)]\tLoss: 0.452018\n",
      "Training stage for Flod 3 Epoch: 36 [6400/29981                 (21%)]\tLoss: 0.346956\n",
      "Training stage for Flod 3 Epoch: 36 [9600/29981                 (32%)]\tLoss: 0.348597\n",
      "Training stage for Flod 3 Epoch: 36 [12800/29981                 (43%)]\tLoss: 0.406188\n",
      "Training stage for Flod 3 Epoch: 36 [16000/29981                 (53%)]\tLoss: 0.348595\n",
      "Training stage for Flod 3 Epoch: 36 [19200/29981                 (64%)]\tLoss: 0.428795\n",
      "Training stage for Flod 3 Epoch: 36 [22400/29981                 (75%)]\tLoss: 0.357774\n",
      "Training stage for Flod 3 Epoch: 36 [25600/29981                 (85%)]\tLoss: 0.407445\n",
      "Training stage for Flod 3 Epoch: 36 [28800/29981                 (96%)]\tLoss: 0.437548\n",
      "Test set for fold3: Average Loss:           702657.7982, Accuracy: 6828/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 37 [0/29981                 (0%)]\tLoss: 0.441968\n",
      "Training stage for Flod 3 Epoch: 37 [3200/29981                 (11%)]\tLoss: 0.409574\n",
      "Training stage for Flod 3 Epoch: 37 [6400/29981                 (21%)]\tLoss: 0.377747\n",
      "Training stage for Flod 3 Epoch: 37 [9600/29981                 (32%)]\tLoss: 0.379943\n",
      "Training stage for Flod 3 Epoch: 37 [12800/29981                 (43%)]\tLoss: 0.384900\n",
      "Training stage for Flod 3 Epoch: 37 [16000/29981                 (53%)]\tLoss: 0.387277\n",
      "Training stage for Flod 3 Epoch: 37 [19200/29981                 (64%)]\tLoss: 0.314283\n",
      "Training stage for Flod 3 Epoch: 37 [22400/29981                 (75%)]\tLoss: 0.366958\n",
      "Training stage for Flod 3 Epoch: 37 [25600/29981                 (85%)]\tLoss: 0.315651\n",
      "Training stage for Flod 3 Epoch: 37 [28800/29981                 (96%)]\tLoss: 0.375692\n",
      "Test set for fold3: Average Loss:           747737.5422, Accuracy: 6624/7495           (88%)\n",
      "Training stage for Flod 3 Epoch: 38 [0/29981                 (0%)]\tLoss: 0.407174\n",
      "Training stage for Flod 3 Epoch: 38 [3200/29981                 (11%)]\tLoss: 0.344639\n",
      "Training stage for Flod 3 Epoch: 38 [6400/29981                 (21%)]\tLoss: 0.504704\n",
      "Training stage for Flod 3 Epoch: 38 [9600/29981                 (32%)]\tLoss: 0.549661\n",
      "Training stage for Flod 3 Epoch: 38 [12800/29981                 (43%)]\tLoss: 0.335225\n",
      "Training stage for Flod 3 Epoch: 38 [16000/29981                 (53%)]\tLoss: 0.475592\n",
      "Training stage for Flod 3 Epoch: 38 [19200/29981                 (64%)]\tLoss: 0.364633\n",
      "Training stage for Flod 3 Epoch: 38 [22400/29981                 (75%)]\tLoss: 0.394080\n",
      "Training stage for Flod 3 Epoch: 38 [25600/29981                 (85%)]\tLoss: 0.362714\n",
      "Training stage for Flod 3 Epoch: 38 [28800/29981                 (96%)]\tLoss: 0.374672\n",
      "Test set for fold3: Average Loss:           695883.9980, Accuracy: 6826/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 39 [0/29981                 (0%)]\tLoss: 0.375627\n",
      "Training stage for Flod 3 Epoch: 39 [3200/29981                 (11%)]\tLoss: 0.375985\n",
      "Training stage for Flod 3 Epoch: 39 [6400/29981                 (21%)]\tLoss: 0.367593\n",
      "Training stage for Flod 3 Epoch: 39 [9600/29981                 (32%)]\tLoss: 0.347747\n",
      "Training stage for Flod 3 Epoch: 39 [12800/29981                 (43%)]\tLoss: 0.378414\n",
      "Training stage for Flod 3 Epoch: 39 [16000/29981                 (53%)]\tLoss: 0.383246\n",
      "Training stage for Flod 3 Epoch: 39 [19200/29981                 (64%)]\tLoss: 0.407478\n",
      "Training stage for Flod 3 Epoch: 39 [22400/29981                 (75%)]\tLoss: 0.359546\n",
      "Training stage for Flod 3 Epoch: 39 [25600/29981                 (85%)]\tLoss: 0.320958\n",
      "Training stage for Flod 3 Epoch: 39 [28800/29981                 (96%)]\tLoss: 0.425149\n",
      "Test set for fold3: Average Loss:           693190.2184, Accuracy: 6864/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 40 [0/29981                 (0%)]\tLoss: 0.344526\n",
      "Training stage for Flod 3 Epoch: 40 [3200/29981                 (11%)]\tLoss: 0.404226\n",
      "Training stage for Flod 3 Epoch: 40 [6400/29981                 (21%)]\tLoss: 0.344830\n",
      "Training stage for Flod 3 Epoch: 40 [9600/29981                 (32%)]\tLoss: 0.381491\n",
      "Training stage for Flod 3 Epoch: 40 [12800/29981                 (43%)]\tLoss: 0.344580\n",
      "Training stage for Flod 3 Epoch: 40 [16000/29981                 (53%)]\tLoss: 0.329758\n",
      "Training stage for Flod 3 Epoch: 40 [19200/29981                 (64%)]\tLoss: 0.415240\n",
      "Training stage for Flod 3 Epoch: 40 [22400/29981                 (75%)]\tLoss: 0.376411\n",
      "Training stage for Flod 3 Epoch: 40 [25600/29981                 (85%)]\tLoss: 0.314077\n",
      "Training stage for Flod 3 Epoch: 40 [28800/29981                 (96%)]\tLoss: 0.373374\n",
      "Test set for fold3: Average Loss:           673540.4280, Accuracy: 6945/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 41 [0/29981                 (0%)]\tLoss: 0.313350\n",
      "Training stage for Flod 3 Epoch: 41 [3200/29981                 (11%)]\tLoss: 0.376064\n",
      "Training stage for Flod 3 Epoch: 41 [6400/29981                 (21%)]\tLoss: 0.384130\n",
      "Training stage for Flod 3 Epoch: 41 [9600/29981                 (32%)]\tLoss: 0.344592\n",
      "Training stage for Flod 3 Epoch: 41 [12800/29981                 (43%)]\tLoss: 0.343917\n",
      "Training stage for Flod 3 Epoch: 41 [16000/29981                 (53%)]\tLoss: 0.346423\n",
      "Training stage for Flod 3 Epoch: 41 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 41 [22400/29981                 (75%)]\tLoss: 0.471484\n",
      "Training stage for Flod 3 Epoch: 41 [25600/29981                 (85%)]\tLoss: 0.344533\n",
      "Training stage for Flod 3 Epoch: 41 [28800/29981                 (96%)]\tLoss: 0.376788\n",
      "Test set for fold3: Average Loss:           746146.4126, Accuracy: 6615/7495           (88%)\n",
      "Training stage for Flod 3 Epoch: 42 [0/29981                 (0%)]\tLoss: 0.375743\n",
      "Training stage for Flod 3 Epoch: 42 [3200/29981                 (11%)]\tLoss: 0.375356\n",
      "Training stage for Flod 3 Epoch: 42 [6400/29981                 (21%)]\tLoss: 0.408485\n",
      "Training stage for Flod 3 Epoch: 42 [9600/29981                 (32%)]\tLoss: 0.364476\n",
      "Training stage for Flod 3 Epoch: 42 [12800/29981                 (43%)]\tLoss: 0.343039\n",
      "Training stage for Flod 3 Epoch: 42 [16000/29981                 (53%)]\tLoss: 0.376055\n",
      "Training stage for Flod 3 Epoch: 42 [19200/29981                 (64%)]\tLoss: 0.346391\n",
      "Training stage for Flod 3 Epoch: 42 [22400/29981                 (75%)]\tLoss: 0.381945\n",
      "Training stage for Flod 3 Epoch: 42 [25600/29981                 (85%)]\tLoss: 0.360853\n",
      "Training stage for Flod 3 Epoch: 42 [28800/29981                 (96%)]\tLoss: 0.376337\n",
      "Test set for fold3: Average Loss:           684780.0054, Accuracy: 6893/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 43 [0/29981                 (0%)]\tLoss: 0.375750\n",
      "Training stage for Flod 3 Epoch: 43 [3200/29981                 (11%)]\tLoss: 0.377147\n",
      "Training stage for Flod 3 Epoch: 43 [6400/29981                 (21%)]\tLoss: 0.398820\n",
      "Training stage for Flod 3 Epoch: 43 [9600/29981                 (32%)]\tLoss: 0.341130\n",
      "Training stage for Flod 3 Epoch: 43 [12800/29981                 (43%)]\tLoss: 0.497323\n",
      "Training stage for Flod 3 Epoch: 43 [16000/29981                 (53%)]\tLoss: 0.346402\n",
      "Training stage for Flod 3 Epoch: 43 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 43 [22400/29981                 (75%)]\tLoss: 0.377006\n",
      "Training stage for Flod 3 Epoch: 43 [25600/29981                 (85%)]\tLoss: 0.347684\n",
      "Training stage for Flod 3 Epoch: 43 [28800/29981                 (96%)]\tLoss: 0.313263\n",
      "Test set for fold3: Average Loss:           666583.3549, Accuracy: 6982/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 44 [0/29981                 (0%)]\tLoss: 0.343512\n",
      "Training stage for Flod 3 Epoch: 44 [3200/29981                 (11%)]\tLoss: 0.407629\n",
      "Training stage for Flod 3 Epoch: 44 [6400/29981                 (21%)]\tLoss: 0.344893\n",
      "Training stage for Flod 3 Epoch: 44 [9600/29981                 (32%)]\tLoss: 0.375391\n",
      "Training stage for Flod 3 Epoch: 44 [12800/29981                 (43%)]\tLoss: 0.376921\n",
      "Training stage for Flod 3 Epoch: 44 [16000/29981                 (53%)]\tLoss: 0.350082\n",
      "Training stage for Flod 3 Epoch: 44 [19200/29981                 (64%)]\tLoss: 0.379177\n",
      "Training stage for Flod 3 Epoch: 44 [22400/29981                 (75%)]\tLoss: 0.405654\n",
      "Training stage for Flod 3 Epoch: 44 [25600/29981                 (85%)]\tLoss: 0.364915\n",
      "Training stage for Flod 3 Epoch: 44 [28800/29981                 (96%)]\tLoss: 0.366721\n",
      "Test set for fold3: Average Loss:           696321.9098, Accuracy: 6838/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 45 [0/29981                 (0%)]\tLoss: 0.376732\n",
      "Training stage for Flod 3 Epoch: 45 [3200/29981                 (11%)]\tLoss: 0.375724\n",
      "Training stage for Flod 3 Epoch: 45 [6400/29981                 (21%)]\tLoss: 0.478165\n",
      "Training stage for Flod 3 Epoch: 45 [9600/29981                 (32%)]\tLoss: 0.344739\n",
      "Training stage for Flod 3 Epoch: 45 [12800/29981                 (43%)]\tLoss: 0.407416\n",
      "Training stage for Flod 3 Epoch: 45 [16000/29981                 (53%)]\tLoss: 0.345144\n",
      "Training stage for Flod 3 Epoch: 45 [19200/29981                 (64%)]\tLoss: 0.313498\n",
      "Training stage for Flod 3 Epoch: 45 [22400/29981                 (75%)]\tLoss: 0.358240\n",
      "Training stage for Flod 3 Epoch: 45 [25600/29981                 (85%)]\tLoss: 0.394075\n",
      "Training stage for Flod 3 Epoch: 45 [28800/29981                 (96%)]\tLoss: 0.378835\n",
      "Test set for fold3: Average Loss:           689547.5628, Accuracy: 6873/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 46 [0/29981                 (0%)]\tLoss: 0.375784\n",
      "Training stage for Flod 3 Epoch: 46 [3200/29981                 (11%)]\tLoss: 0.453859\n",
      "Training stage for Flod 3 Epoch: 46 [6400/29981                 (21%)]\tLoss: 0.439379\n",
      "Training stage for Flod 3 Epoch: 46 [9600/29981                 (32%)]\tLoss: 0.375152\n",
      "Training stage for Flod 3 Epoch: 46 [12800/29981                 (43%)]\tLoss: 0.449835\n",
      "Training stage for Flod 3 Epoch: 46 [16000/29981                 (53%)]\tLoss: 0.379987\n",
      "Training stage for Flod 3 Epoch: 46 [19200/29981                 (64%)]\tLoss: 0.362534\n",
      "Training stage for Flod 3 Epoch: 46 [22400/29981                 (75%)]\tLoss: 0.405023\n",
      "Training stage for Flod 3 Epoch: 46 [25600/29981                 (85%)]\tLoss: 0.410818\n",
      "Training stage for Flod 3 Epoch: 46 [28800/29981                 (96%)]\tLoss: 0.432387\n",
      "Test set for fold3: Average Loss:           677116.1867, Accuracy: 6931/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 47 [0/29981                 (0%)]\tLoss: 0.344951\n",
      "Training stage for Flod 3 Epoch: 47 [3200/29981                 (11%)]\tLoss: 0.316445\n",
      "Training stage for Flod 3 Epoch: 47 [6400/29981                 (21%)]\tLoss: 0.342680\n",
      "Training stage for Flod 3 Epoch: 47 [9600/29981                 (32%)]\tLoss: 0.408351\n",
      "Training stage for Flod 3 Epoch: 47 [12800/29981                 (43%)]\tLoss: 0.321447\n",
      "Training stage for Flod 3 Epoch: 47 [16000/29981                 (53%)]\tLoss: 0.444227\n",
      "Training stage for Flod 3 Epoch: 47 [19200/29981                 (64%)]\tLoss: 0.361180\n",
      "Training stage for Flod 3 Epoch: 47 [22400/29981                 (75%)]\tLoss: 0.344530\n",
      "Training stage for Flod 3 Epoch: 47 [25600/29981                 (85%)]\tLoss: 0.417529\n",
      "Training stage for Flod 3 Epoch: 47 [28800/29981                 (96%)]\tLoss: 0.407063\n",
      "Test set for fold3: Average Loss:           690433.0121, Accuracy: 6860/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 48 [0/29981                 (0%)]\tLoss: 0.437194\n",
      "Training stage for Flod 3 Epoch: 48 [3200/29981                 (11%)]\tLoss: 0.344510\n",
      "Training stage for Flod 3 Epoch: 48 [6400/29981                 (21%)]\tLoss: 0.314022\n",
      "Training stage for Flod 3 Epoch: 48 [9600/29981                 (32%)]\tLoss: 0.405517\n",
      "Training stage for Flod 3 Epoch: 48 [12800/29981                 (43%)]\tLoss: 0.375932\n",
      "Training stage for Flod 3 Epoch: 48 [16000/29981                 (53%)]\tLoss: 0.488026\n",
      "Training stage for Flod 3 Epoch: 48 [19200/29981                 (64%)]\tLoss: 0.418323\n",
      "Training stage for Flod 3 Epoch: 48 [22400/29981                 (75%)]\tLoss: 0.344933\n",
      "Training stage for Flod 3 Epoch: 48 [25600/29981                 (85%)]\tLoss: 0.344281\n",
      "Training stage for Flod 3 Epoch: 48 [28800/29981                 (96%)]\tLoss: 0.327167\n",
      "Test set for fold3: Average Loss:           675419.2317, Accuracy: 6943/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 49 [0/29981                 (0%)]\tLoss: 0.375180\n",
      "Training stage for Flod 3 Epoch: 49 [3200/29981                 (11%)]\tLoss: 0.485424\n",
      "Training stage for Flod 3 Epoch: 49 [6400/29981                 (21%)]\tLoss: 0.316340\n",
      "Training stage for Flod 3 Epoch: 49 [9600/29981                 (32%)]\tLoss: 0.345026\n",
      "Training stage for Flod 3 Epoch: 49 [12800/29981                 (43%)]\tLoss: 0.415855\n",
      "Training stage for Flod 3 Epoch: 49 [16000/29981                 (53%)]\tLoss: 0.343629\n",
      "Training stage for Flod 3 Epoch: 49 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 49 [22400/29981                 (75%)]\tLoss: 0.375764\n",
      "Training stage for Flod 3 Epoch: 49 [25600/29981                 (85%)]\tLoss: 0.375763\n",
      "Training stage for Flod 3 Epoch: 49 [28800/29981                 (96%)]\tLoss: 0.375762\n",
      "Test set for fold3: Average Loss:           748005.4940, Accuracy: 6628/7495           (88%)\n",
      "Training stage for Flod 3 Epoch: 50 [0/29981                 (0%)]\tLoss: 0.560554\n",
      "Training stage for Flod 3 Epoch: 50 [3200/29981                 (11%)]\tLoss: 0.375831\n",
      "Training stage for Flod 3 Epoch: 50 [6400/29981                 (21%)]\tLoss: 0.420024\n",
      "Training stage for Flod 3 Epoch: 50 [9600/29981                 (32%)]\tLoss: 0.344552\n",
      "Training stage for Flod 3 Epoch: 50 [12800/29981                 (43%)]\tLoss: 0.373967\n",
      "Training stage for Flod 3 Epoch: 50 [16000/29981                 (53%)]\tLoss: 0.407015\n",
      "Training stage for Flod 3 Epoch: 50 [19200/29981                 (64%)]\tLoss: 0.313666\n",
      "Training stage for Flod 3 Epoch: 50 [22400/29981                 (75%)]\tLoss: 0.344391\n",
      "Training stage for Flod 3 Epoch: 50 [25600/29981                 (85%)]\tLoss: 0.398028\n",
      "Training stage for Flod 3 Epoch: 50 [28800/29981                 (96%)]\tLoss: 0.375632\n",
      "Test set for fold3: Average Loss:           666394.0471, Accuracy: 6991/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 51 [0/29981                 (0%)]\tLoss: 0.375687\n",
      "Training stage for Flod 3 Epoch: 51 [3200/29981                 (11%)]\tLoss: 0.411168\n",
      "Training stage for Flod 3 Epoch: 51 [6400/29981                 (21%)]\tLoss: 0.349860\n",
      "Training stage for Flod 3 Epoch: 51 [9600/29981                 (32%)]\tLoss: 0.344215\n",
      "Training stage for Flod 3 Epoch: 51 [12800/29981                 (43%)]\tLoss: 0.454421\n",
      "Training stage for Flod 3 Epoch: 51 [16000/29981                 (53%)]\tLoss: 0.344572\n",
      "Training stage for Flod 3 Epoch: 51 [19200/29981                 (64%)]\tLoss: 0.344562\n",
      "Training stage for Flod 3 Epoch: 51 [22400/29981                 (75%)]\tLoss: 0.404860\n",
      "Training stage for Flod 3 Epoch: 51 [25600/29981                 (85%)]\tLoss: 0.348116\n",
      "Training stage for Flod 3 Epoch: 51 [28800/29981                 (96%)]\tLoss: 0.424921\n",
      "Test set for fold3: Average Loss:           675516.5254, Accuracy: 6923/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 52 [0/29981                 (0%)]\tLoss: 0.344524\n",
      "Training stage for Flod 3 Epoch: 52 [3200/29981                 (11%)]\tLoss: 0.405939\n",
      "Training stage for Flod 3 Epoch: 52 [6400/29981                 (21%)]\tLoss: 0.345906\n",
      "Training stage for Flod 3 Epoch: 52 [9600/29981                 (32%)]\tLoss: 0.345899\n",
      "Training stage for Flod 3 Epoch: 52 [12800/29981                 (43%)]\tLoss: 0.340997\n",
      "Training stage for Flod 3 Epoch: 52 [16000/29981                 (53%)]\tLoss: 0.375784\n",
      "Training stage for Flod 3 Epoch: 52 [19200/29981                 (64%)]\tLoss: 0.375950\n",
      "Training stage for Flod 3 Epoch: 52 [22400/29981                 (75%)]\tLoss: 0.376514\n",
      "Training stage for Flod 3 Epoch: 52 [25600/29981                 (85%)]\tLoss: 0.421082\n",
      "Training stage for Flod 3 Epoch: 52 [28800/29981                 (96%)]\tLoss: 0.344546\n",
      "Test set for fold3: Average Loss:           672608.0060, Accuracy: 6955/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 53 [0/29981                 (0%)]\tLoss: 0.325394\n",
      "Training stage for Flod 3 Epoch: 53 [3200/29981                 (11%)]\tLoss: 0.394434\n",
      "Training stage for Flod 3 Epoch: 53 [6400/29981                 (21%)]\tLoss: 0.500747\n",
      "Training stage for Flod 3 Epoch: 53 [9600/29981                 (32%)]\tLoss: 0.313303\n",
      "Training stage for Flod 3 Epoch: 53 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 53 [16000/29981                 (53%)]\tLoss: 0.375777\n",
      "Training stage for Flod 3 Epoch: 53 [19200/29981                 (64%)]\tLoss: 0.471006\n",
      "Training stage for Flod 3 Epoch: 53 [22400/29981                 (75%)]\tLoss: 0.346857\n",
      "Training stage for Flod 3 Epoch: 53 [25600/29981                 (85%)]\tLoss: 0.456414\n",
      "Training stage for Flod 3 Epoch: 53 [28800/29981                 (96%)]\tLoss: 0.403507\n",
      "Test set for fold3: Average Loss:           683721.0940, Accuracy: 6900/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 54 [0/29981                 (0%)]\tLoss: 0.344532\n",
      "Training stage for Flod 3 Epoch: 54 [3200/29981                 (11%)]\tLoss: 0.371452\n",
      "Training stage for Flod 3 Epoch: 54 [6400/29981                 (21%)]\tLoss: 0.406802\n",
      "Training stage for Flod 3 Epoch: 54 [9600/29981                 (32%)]\tLoss: 0.347288\n",
      "Training stage for Flod 3 Epoch: 54 [12800/29981                 (43%)]\tLoss: 0.375785\n",
      "Training stage for Flod 3 Epoch: 54 [16000/29981                 (53%)]\tLoss: 0.410893\n",
      "Training stage for Flod 3 Epoch: 54 [19200/29981                 (64%)]\tLoss: 0.348114\n",
      "Training stage for Flod 3 Epoch: 54 [22400/29981                 (75%)]\tLoss: 0.472336\n",
      "Training stage for Flod 3 Epoch: 54 [25600/29981                 (85%)]\tLoss: 0.313309\n",
      "Training stage for Flod 3 Epoch: 54 [28800/29981                 (96%)]\tLoss: 0.344804\n",
      "Test set for fold3: Average Loss:           696866.1100, Accuracy: 6859/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 55 [0/29981                 (0%)]\tLoss: 0.402996\n",
      "Training stage for Flod 3 Epoch: 55 [3200/29981                 (11%)]\tLoss: 0.350753\n",
      "Training stage for Flod 3 Epoch: 55 [6400/29981                 (21%)]\tLoss: 0.406978\n",
      "Training stage for Flod 3 Epoch: 55 [9600/29981                 (32%)]\tLoss: 0.344518\n",
      "Training stage for Flod 3 Epoch: 55 [12800/29981                 (43%)]\tLoss: 0.341950\n",
      "Training stage for Flod 3 Epoch: 55 [16000/29981                 (53%)]\tLoss: 0.476961\n",
      "Training stage for Flod 3 Epoch: 55 [19200/29981                 (64%)]\tLoss: 0.349515\n",
      "Training stage for Flod 3 Epoch: 55 [22400/29981                 (75%)]\tLoss: 0.339916\n",
      "Training stage for Flod 3 Epoch: 55 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 55 [28800/29981                 (96%)]\tLoss: 0.352886\n",
      "Test set for fold3: Average Loss:           706505.4237, Accuracy: 6805/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 56 [0/29981                 (0%)]\tLoss: 0.378586\n",
      "Training stage for Flod 3 Epoch: 56 [3200/29981                 (11%)]\tLoss: 0.390177\n",
      "Training stage for Flod 3 Epoch: 56 [6400/29981                 (21%)]\tLoss: 0.344580\n",
      "Training stage for Flod 3 Epoch: 56 [9600/29981                 (32%)]\tLoss: 0.382891\n",
      "Training stage for Flod 3 Epoch: 56 [12800/29981                 (43%)]\tLoss: 0.375757\n",
      "Training stage for Flod 3 Epoch: 56 [16000/29981                 (53%)]\tLoss: 0.313416\n",
      "Training stage for Flod 3 Epoch: 56 [19200/29981                 (64%)]\tLoss: 0.375832\n",
      "Training stage for Flod 3 Epoch: 56 [22400/29981                 (75%)]\tLoss: 0.344554\n",
      "Training stage for Flod 3 Epoch: 56 [25600/29981                 (85%)]\tLoss: 0.407115\n",
      "Training stage for Flod 3 Epoch: 56 [28800/29981                 (96%)]\tLoss: 0.313437\n",
      "Test set for fold3: Average Loss:           682630.7247, Accuracy: 6916/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 57 [0/29981                 (0%)]\tLoss: 0.376670\n",
      "Training stage for Flod 3 Epoch: 57 [3200/29981                 (11%)]\tLoss: 0.500367\n",
      "Training stage for Flod 3 Epoch: 57 [6400/29981                 (21%)]\tLoss: 0.375478\n",
      "Training stage for Flod 3 Epoch: 57 [9600/29981                 (32%)]\tLoss: 0.344947\n",
      "Training stage for Flod 3 Epoch: 57 [12800/29981                 (43%)]\tLoss: 0.313263\n",
      "Training stage for Flod 3 Epoch: 57 [16000/29981                 (53%)]\tLoss: 0.313361\n",
      "Training stage for Flod 3 Epoch: 57 [19200/29981                 (64%)]\tLoss: 0.438230\n",
      "Training stage for Flod 3 Epoch: 57 [22400/29981                 (75%)]\tLoss: 0.344558\n",
      "Training stage for Flod 3 Epoch: 57 [25600/29981                 (85%)]\tLoss: 0.344253\n",
      "Training stage for Flod 3 Epoch: 57 [28800/29981                 (96%)]\tLoss: 0.375913\n",
      "Test set for fold3: Average Loss:           692555.0620, Accuracy: 6876/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 58 [0/29981                 (0%)]\tLoss: 0.375792\n",
      "Training stage for Flod 3 Epoch: 58 [3200/29981                 (11%)]\tLoss: 0.375596\n",
      "Training stage for Flod 3 Epoch: 58 [6400/29981                 (21%)]\tLoss: 0.365163\n",
      "Training stage for Flod 3 Epoch: 58 [9600/29981                 (32%)]\tLoss: 0.375767\n",
      "Training stage for Flod 3 Epoch: 58 [12800/29981                 (43%)]\tLoss: 0.344636\n",
      "Training stage for Flod 3 Epoch: 58 [16000/29981                 (53%)]\tLoss: 0.406500\n",
      "Training stage for Flod 3 Epoch: 58 [19200/29981                 (64%)]\tLoss: 0.424392\n",
      "Training stage for Flod 3 Epoch: 58 [22400/29981                 (75%)]\tLoss: 0.339185\n",
      "Training stage for Flod 3 Epoch: 58 [25600/29981                 (85%)]\tLoss: 0.346103\n",
      "Training stage for Flod 3 Epoch: 58 [28800/29981                 (96%)]\tLoss: 0.344537\n",
      "Test set for fold3: Average Loss:           663330.5829, Accuracy: 7012/7495           (94%)\n",
      "Training stage for Flod 3 Epoch: 59 [0/29981                 (0%)]\tLoss: 0.344516\n",
      "Training stage for Flod 3 Epoch: 59 [3200/29981                 (11%)]\tLoss: 0.328327\n",
      "Training stage for Flod 3 Epoch: 59 [6400/29981                 (21%)]\tLoss: 0.379853\n",
      "Training stage for Flod 3 Epoch: 59 [9600/29981                 (32%)]\tLoss: 0.369264\n",
      "Training stage for Flod 3 Epoch: 59 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 59 [16000/29981                 (53%)]\tLoss: 0.438262\n",
      "Training stage for Flod 3 Epoch: 59 [19200/29981                 (64%)]\tLoss: 0.375767\n",
      "Training stage for Flod 3 Epoch: 59 [22400/29981                 (75%)]\tLoss: 0.390998\n",
      "Training stage for Flod 3 Epoch: 59 [25600/29981                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 59 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold3: Average Loss:           685468.6216, Accuracy: 6901/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 60 [0/29981                 (0%)]\tLoss: 0.313334\n",
      "Training stage for Flod 3 Epoch: 60 [3200/29981                 (11%)]\tLoss: 0.375786\n",
      "Training stage for Flod 3 Epoch: 60 [6400/29981                 (21%)]\tLoss: 0.313264\n",
      "Training stage for Flod 3 Epoch: 60 [9600/29981                 (32%)]\tLoss: 0.344620\n",
      "Training stage for Flod 3 Epoch: 60 [12800/29981                 (43%)]\tLoss: 0.408834\n",
      "Training stage for Flod 3 Epoch: 60 [16000/29981                 (53%)]\tLoss: 0.376522\n",
      "Training stage for Flod 3 Epoch: 60 [19200/29981                 (64%)]\tLoss: 0.404681\n",
      "Training stage for Flod 3 Epoch: 60 [22400/29981                 (75%)]\tLoss: 0.344416\n",
      "Training stage for Flod 3 Epoch: 60 [25600/29981                 (85%)]\tLoss: 0.375787\n",
      "Training stage for Flod 3 Epoch: 60 [28800/29981                 (96%)]\tLoss: 0.407012\n",
      "Test set for fold3: Average Loss:           690488.1734, Accuracy: 6875/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 61 [0/29981                 (0%)]\tLoss: 0.376835\n",
      "Training stage for Flod 3 Epoch: 61 [3200/29981                 (11%)]\tLoss: 0.419483\n",
      "Training stage for Flod 3 Epoch: 61 [6400/29981                 (21%)]\tLoss: 0.403048\n",
      "Training stage for Flod 3 Epoch: 61 [9600/29981                 (32%)]\tLoss: 0.380701\n",
      "Training stage for Flod 3 Epoch: 61 [12800/29981                 (43%)]\tLoss: 0.323093\n",
      "Training stage for Flod 3 Epoch: 61 [16000/29981                 (53%)]\tLoss: 0.408167\n",
      "Training stage for Flod 3 Epoch: 61 [19200/29981                 (64%)]\tLoss: 0.376395\n",
      "Training stage for Flod 3 Epoch: 61 [22400/29981                 (75%)]\tLoss: 0.408548\n",
      "Training stage for Flod 3 Epoch: 61 [25600/29981                 (85%)]\tLoss: 0.407018\n",
      "Training stage for Flod 3 Epoch: 61 [28800/29981                 (96%)]\tLoss: 0.344515\n",
      "Test set for fold3: Average Loss:           692310.0848, Accuracy: 6875/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 62 [0/29981                 (0%)]\tLoss: 0.377650\n",
      "Training stage for Flod 3 Epoch: 62 [3200/29981                 (11%)]\tLoss: 0.375814\n",
      "Training stage for Flod 3 Epoch: 62 [6400/29981                 (21%)]\tLoss: 0.412166\n",
      "Training stage for Flod 3 Epoch: 62 [9600/29981                 (32%)]\tLoss: 0.375759\n",
      "Training stage for Flod 3 Epoch: 62 [12800/29981                 (43%)]\tLoss: 0.344624\n",
      "Training stage for Flod 3 Epoch: 62 [16000/29981                 (53%)]\tLoss: 0.544157\n",
      "Training stage for Flod 3 Epoch: 62 [19200/29981                 (64%)]\tLoss: 0.313299\n",
      "Training stage for Flod 3 Epoch: 62 [22400/29981                 (75%)]\tLoss: 0.380469\n",
      "Training stage for Flod 3 Epoch: 62 [25600/29981                 (85%)]\tLoss: 0.317134\n",
      "Training stage for Flod 3 Epoch: 62 [28800/29981                 (96%)]\tLoss: 0.500959\n",
      "Test set for fold3: Average Loss:           680975.4920, Accuracy: 6903/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 63 [0/29981                 (0%)]\tLoss: 0.374751\n",
      "Training stage for Flod 3 Epoch: 63 [3200/29981                 (11%)]\tLoss: 0.444680\n",
      "Training stage for Flod 3 Epoch: 63 [6400/29981                 (21%)]\tLoss: 0.345109\n",
      "Training stage for Flod 3 Epoch: 63 [9600/29981                 (32%)]\tLoss: 0.313311\n",
      "Training stage for Flod 3 Epoch: 63 [12800/29981                 (43%)]\tLoss: 0.313264\n",
      "Training stage for Flod 3 Epoch: 63 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 63 [19200/29981                 (64%)]\tLoss: 0.414024\n",
      "Training stage for Flod 3 Epoch: 63 [22400/29981                 (75%)]\tLoss: 0.407745\n",
      "Training stage for Flod 3 Epoch: 63 [25600/29981                 (85%)]\tLoss: 0.375935\n",
      "Training stage for Flod 3 Epoch: 63 [28800/29981                 (96%)]\tLoss: 0.344584\n",
      "Test set for fold3: Average Loss:           684164.1383, Accuracy: 6908/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 64 [0/29981                 (0%)]\tLoss: 0.469513\n",
      "Training stage for Flod 3 Epoch: 64 [3200/29981                 (11%)]\tLoss: 0.313371\n",
      "Training stage for Flod 3 Epoch: 64 [6400/29981                 (21%)]\tLoss: 0.470126\n",
      "Training stage for Flod 3 Epoch: 64 [9600/29981                 (32%)]\tLoss: 0.490510\n",
      "Training stage for Flod 3 Epoch: 64 [12800/29981                 (43%)]\tLoss: 0.472358\n",
      "Training stage for Flod 3 Epoch: 64 [16000/29981                 (53%)]\tLoss: 0.405925\n",
      "Training stage for Flod 3 Epoch: 64 [19200/29981                 (64%)]\tLoss: 0.375815\n",
      "Training stage for Flod 3 Epoch: 64 [22400/29981                 (75%)]\tLoss: 0.380980\n",
      "Training stage for Flod 3 Epoch: 64 [25600/29981                 (85%)]\tLoss: 0.375780\n",
      "Training stage for Flod 3 Epoch: 64 [28800/29981                 (96%)]\tLoss: 0.413975\n",
      "Test set for fold3: Average Loss:           767615.5052, Accuracy: 6539/7495           (87%)\n",
      "Training stage for Flod 3 Epoch: 65 [0/29981                 (0%)]\tLoss: 0.466349\n",
      "Training stage for Flod 3 Epoch: 65 [3200/29981                 (11%)]\tLoss: 0.346085\n",
      "Training stage for Flod 3 Epoch: 65 [6400/29981                 (21%)]\tLoss: 0.344513\n",
      "Training stage for Flod 3 Epoch: 65 [9600/29981                 (32%)]\tLoss: 0.407036\n",
      "Training stage for Flod 3 Epoch: 65 [12800/29981                 (43%)]\tLoss: 0.407501\n",
      "Training stage for Flod 3 Epoch: 65 [16000/29981                 (53%)]\tLoss: 0.335207\n",
      "Training stage for Flod 3 Epoch: 65 [19200/29981                 (64%)]\tLoss: 0.313306\n",
      "Training stage for Flod 3 Epoch: 65 [22400/29981                 (75%)]\tLoss: 0.407644\n",
      "Training stage for Flod 3 Epoch: 65 [25600/29981                 (85%)]\tLoss: 0.367935\n",
      "Training stage for Flod 3 Epoch: 65 [28800/29981                 (96%)]\tLoss: 0.427825\n",
      "Test set for fold3: Average Loss:           693853.6471, Accuracy: 6862/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 66 [0/29981                 (0%)]\tLoss: 0.328612\n",
      "Training stage for Flod 3 Epoch: 66 [3200/29981                 (11%)]\tLoss: 0.407012\n",
      "Training stage for Flod 3 Epoch: 66 [6400/29981                 (21%)]\tLoss: 0.384105\n",
      "Training stage for Flod 3 Epoch: 66 [9600/29981                 (32%)]\tLoss: 0.466577\n",
      "Training stage for Flod 3 Epoch: 66 [12800/29981                 (43%)]\tLoss: 0.438259\n",
      "Training stage for Flod 3 Epoch: 66 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 66 [19200/29981                 (64%)]\tLoss: 0.393111\n",
      "Training stage for Flod 3 Epoch: 66 [22400/29981                 (75%)]\tLoss: 0.344696\n",
      "Training stage for Flod 3 Epoch: 66 [25600/29981                 (85%)]\tLoss: 0.405758\n",
      "Training stage for Flod 3 Epoch: 66 [28800/29981                 (96%)]\tLoss: 0.407023\n",
      "Test set for fold3: Average Loss:           696261.6086, Accuracy: 6845/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 67 [0/29981                 (0%)]\tLoss: 0.344630\n",
      "Training stage for Flod 3 Epoch: 67 [3200/29981                 (11%)]\tLoss: 0.345056\n",
      "Training stage for Flod 3 Epoch: 67 [6400/29981                 (21%)]\tLoss: 0.438231\n",
      "Training stage for Flod 3 Epoch: 67 [9600/29981                 (32%)]\tLoss: 0.375834\n",
      "Training stage for Flod 3 Epoch: 67 [12800/29981                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 67 [16000/29981                 (53%)]\tLoss: 0.407892\n",
      "Training stage for Flod 3 Epoch: 67 [19200/29981                 (64%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 67 [22400/29981                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 67 [25600/29981                 (85%)]\tLoss: 0.531887\n",
      "Training stage for Flod 3 Epoch: 67 [28800/29981                 (96%)]\tLoss: 0.375803\n",
      "Test set for fold3: Average Loss:           682196.4487, Accuracy: 6917/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 68 [0/29981                 (0%)]\tLoss: 0.344514\n",
      "Training stage for Flod 3 Epoch: 68 [3200/29981                 (11%)]\tLoss: 0.403893\n",
      "Training stage for Flod 3 Epoch: 68 [6400/29981                 (21%)]\tLoss: 0.344516\n",
      "Training stage for Flod 3 Epoch: 68 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 68 [12800/29981                 (43%)]\tLoss: 0.357999\n",
      "Training stage for Flod 3 Epoch: 68 [16000/29981                 (53%)]\tLoss: 0.406821\n",
      "Training stage for Flod 3 Epoch: 68 [19200/29981                 (64%)]\tLoss: 0.344618\n",
      "Training stage for Flod 3 Epoch: 68 [22400/29981                 (75%)]\tLoss: 0.345355\n",
      "Training stage for Flod 3 Epoch: 68 [25600/29981                 (85%)]\tLoss: 0.345412\n",
      "Training stage for Flod 3 Epoch: 68 [28800/29981                 (96%)]\tLoss: 0.376095\n",
      "Test set for fold3: Average Loss:           703097.5887, Accuracy: 6823/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 69 [0/29981                 (0%)]\tLoss: 0.372147\n",
      "Training stage for Flod 3 Epoch: 69 [3200/29981                 (11%)]\tLoss: 0.342900\n",
      "Training stage for Flod 3 Epoch: 69 [6400/29981                 (21%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 69 [9600/29981                 (32%)]\tLoss: 0.406694\n",
      "Training stage for Flod 3 Epoch: 69 [12800/29981                 (43%)]\tLoss: 0.407248\n",
      "Training stage for Flod 3 Epoch: 69 [16000/29981                 (53%)]\tLoss: 0.314600\n",
      "Training stage for Flod 3 Epoch: 69 [19200/29981                 (64%)]\tLoss: 0.313428\n",
      "Training stage for Flod 3 Epoch: 69 [22400/29981                 (75%)]\tLoss: 0.429725\n",
      "Training stage for Flod 3 Epoch: 69 [25600/29981                 (85%)]\tLoss: 0.376017\n",
      "Training stage for Flod 3 Epoch: 69 [28800/29981                 (96%)]\tLoss: 0.375762\n",
      "Test set for fold3: Average Loss:           695769.7321, Accuracy: 6857/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 70 [0/29981                 (0%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 70 [3200/29981                 (11%)]\tLoss: 0.407106\n",
      "Training stage for Flod 3 Epoch: 70 [6400/29981                 (21%)]\tLoss: 0.323513\n",
      "Training stage for Flod 3 Epoch: 70 [9600/29981                 (32%)]\tLoss: 0.313285\n",
      "Training stage for Flod 3 Epoch: 70 [12800/29981                 (43%)]\tLoss: 0.344815\n",
      "Training stage for Flod 3 Epoch: 70 [16000/29981                 (53%)]\tLoss: 0.313263\n",
      "Training stage for Flod 3 Epoch: 70 [19200/29981                 (64%)]\tLoss: 0.407119\n",
      "Training stage for Flod 3 Epoch: 70 [22400/29981                 (75%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 70 [25600/29981                 (85%)]\tLoss: 0.368114\n",
      "Training stage for Flod 3 Epoch: 70 [28800/29981                 (96%)]\tLoss: 0.313300\n",
      "Test set for fold3: Average Loss:           718854.2368, Accuracy: 6758/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 71 [0/29981                 (0%)]\tLoss: 0.469438\n",
      "Training stage for Flod 3 Epoch: 71 [3200/29981                 (11%)]\tLoss: 0.438278\n",
      "Training stage for Flod 3 Epoch: 71 [6400/29981                 (21%)]\tLoss: 0.409004\n",
      "Training stage for Flod 3 Epoch: 71 [9600/29981                 (32%)]\tLoss: 0.438262\n",
      "Training stage for Flod 3 Epoch: 71 [12800/29981                 (43%)]\tLoss: 0.314643\n",
      "Training stage for Flod 3 Epoch: 71 [16000/29981                 (53%)]\tLoss: 0.438262\n",
      "Training stage for Flod 3 Epoch: 71 [19200/29981                 (64%)]\tLoss: 0.326190\n",
      "Training stage for Flod 3 Epoch: 71 [22400/29981                 (75%)]\tLoss: 0.344511\n",
      "Training stage for Flod 3 Epoch: 71 [25600/29981                 (85%)]\tLoss: 0.383341\n",
      "Training stage for Flod 3 Epoch: 71 [28800/29981                 (96%)]\tLoss: 0.375796\n",
      "Test set for fold3: Average Loss:           702779.6144, Accuracy: 6835/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 72 [0/29981                 (0%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 72 [3200/29981                 (11%)]\tLoss: 0.320238\n",
      "Training stage for Flod 3 Epoch: 72 [6400/29981                 (21%)]\tLoss: 0.375838\n",
      "Training stage for Flod 3 Epoch: 72 [9600/29981                 (32%)]\tLoss: 0.320356\n",
      "Training stage for Flod 3 Epoch: 72 [12800/29981                 (43%)]\tLoss: 0.407011\n",
      "Training stage for Flod 3 Epoch: 72 [16000/29981                 (53%)]\tLoss: 0.342765\n",
      "Training stage for Flod 3 Epoch: 72 [19200/29981                 (64%)]\tLoss: 0.407442\n",
      "Training stage for Flod 3 Epoch: 72 [22400/29981                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 72 [25600/29981                 (85%)]\tLoss: 0.376028\n",
      "Training stage for Flod 3 Epoch: 72 [28800/29981                 (96%)]\tLoss: 0.343987\n",
      "Test set for fold3: Average Loss:           680317.7084, Accuracy: 6930/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 73 [0/29981                 (0%)]\tLoss: 0.353684\n",
      "Training stage for Flod 3 Epoch: 73 [3200/29981                 (11%)]\tLoss: 0.438977\n",
      "Training stage for Flod 3 Epoch: 73 [6400/29981                 (21%)]\tLoss: 0.313263\n",
      "Training stage for Flod 3 Epoch: 73 [9600/29981                 (32%)]\tLoss: 0.375913\n",
      "Training stage for Flod 3 Epoch: 73 [12800/29981                 (43%)]\tLoss: 0.376221\n",
      "Training stage for Flod 3 Epoch: 73 [16000/29981                 (53%)]\tLoss: 0.334669\n",
      "Training stage for Flod 3 Epoch: 73 [19200/29981                 (64%)]\tLoss: 0.406107\n",
      "Training stage for Flod 3 Epoch: 73 [22400/29981                 (75%)]\tLoss: 0.314359\n",
      "Training stage for Flod 3 Epoch: 73 [25600/29981                 (85%)]\tLoss: 0.344525\n",
      "Training stage for Flod 3 Epoch: 73 [28800/29981                 (96%)]\tLoss: 0.375762\n",
      "Test set for fold3: Average Loss:           672274.4732, Accuracy: 6971/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 74 [0/29981                 (0%)]\tLoss: 0.344479\n",
      "Training stage for Flod 3 Epoch: 74 [3200/29981                 (11%)]\tLoss: 0.375903\n",
      "Training stage for Flod 3 Epoch: 74 [6400/29981                 (21%)]\tLoss: 0.344541\n",
      "Training stage for Flod 3 Epoch: 74 [9600/29981                 (32%)]\tLoss: 0.386752\n",
      "Training stage for Flod 3 Epoch: 74 [12800/29981                 (43%)]\tLoss: 0.368890\n",
      "Training stage for Flod 3 Epoch: 74 [16000/29981                 (53%)]\tLoss: 0.313287\n",
      "Training stage for Flod 3 Epoch: 74 [19200/29981                 (64%)]\tLoss: 0.343673\n",
      "Training stage for Flod 3 Epoch: 74 [22400/29981                 (75%)]\tLoss: 0.407015\n",
      "Training stage for Flod 3 Epoch: 74 [25600/29981                 (85%)]\tLoss: 0.313294\n",
      "Training stage for Flod 3 Epoch: 74 [28800/29981                 (96%)]\tLoss: 0.324673\n",
      "Test set for fold3: Average Loss:           690170.7278, Accuracy: 6866/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 75 [0/29981                 (0%)]\tLoss: 0.313998\n",
      "Training stage for Flod 3 Epoch: 75 [3200/29981                 (11%)]\tLoss: 0.376724\n",
      "Training stage for Flod 3 Epoch: 75 [6400/29981                 (21%)]\tLoss: 0.344864\n",
      "Training stage for Flod 3 Epoch: 75 [9600/29981                 (32%)]\tLoss: 0.407300\n",
      "Training stage for Flod 3 Epoch: 75 [12800/29981                 (43%)]\tLoss: 0.376916\n",
      "Training stage for Flod 3 Epoch: 75 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 75 [19200/29981                 (64%)]\tLoss: 0.405832\n",
      "Training stage for Flod 3 Epoch: 75 [22400/29981                 (75%)]\tLoss: 0.344609\n",
      "Training stage for Flod 3 Epoch: 75 [25600/29981                 (85%)]\tLoss: 0.332114\n",
      "Training stage for Flod 3 Epoch: 75 [28800/29981                 (96%)]\tLoss: 0.375659\n",
      "Test set for fold3: Average Loss:           670310.4292, Accuracy: 6967/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 76 [0/29981                 (0%)]\tLoss: 0.424968\n",
      "Training stage for Flod 3 Epoch: 76 [3200/29981                 (11%)]\tLoss: 0.344505\n",
      "Training stage for Flod 3 Epoch: 76 [6400/29981                 (21%)]\tLoss: 0.345405\n",
      "Training stage for Flod 3 Epoch: 76 [9600/29981                 (32%)]\tLoss: 0.353558\n",
      "Training stage for Flod 3 Epoch: 76 [12800/29981                 (43%)]\tLoss: 0.402510\n",
      "Training stage for Flod 3 Epoch: 76 [16000/29981                 (53%)]\tLoss: 0.346080\n",
      "Training stage for Flod 3 Epoch: 76 [19200/29981                 (64%)]\tLoss: 0.396694\n",
      "Training stage for Flod 3 Epoch: 76 [22400/29981                 (75%)]\tLoss: 0.313292\n",
      "Training stage for Flod 3 Epoch: 76 [25600/29981                 (85%)]\tLoss: 0.313271\n",
      "Training stage for Flod 3 Epoch: 76 [28800/29981                 (96%)]\tLoss: 0.313300\n",
      "Test set for fold3: Average Loss:           651363.7512, Accuracy: 7057/7495           (94%)\n",
      "Training stage for Flod 3 Epoch: 77 [0/29981                 (0%)]\tLoss: 0.408332\n",
      "Training stage for Flod 3 Epoch: 77 [3200/29981                 (11%)]\tLoss: 0.317753\n",
      "Training stage for Flod 3 Epoch: 77 [6400/29981                 (21%)]\tLoss: 0.436126\n",
      "Training stage for Flod 3 Epoch: 77 [9600/29981                 (32%)]\tLoss: 0.501958\n",
      "Training stage for Flod 3 Epoch: 77 [12800/29981                 (43%)]\tLoss: 0.313349\n",
      "Training stage for Flod 3 Epoch: 77 [16000/29981                 (53%)]\tLoss: 0.375772\n",
      "Training stage for Flod 3 Epoch: 77 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 77 [22400/29981                 (75%)]\tLoss: 0.407317\n",
      "Training stage for Flod 3 Epoch: 77 [25600/29981                 (85%)]\tLoss: 0.404512\n",
      "Training stage for Flod 3 Epoch: 77 [28800/29981                 (96%)]\tLoss: 0.375770\n",
      "Test set for fold3: Average Loss:           677327.6860, Accuracy: 6945/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 78 [0/29981                 (0%)]\tLoss: 0.379198\n",
      "Training stage for Flod 3 Epoch: 78 [3200/29981                 (11%)]\tLoss: 0.344542\n",
      "Training stage for Flod 3 Epoch: 78 [6400/29981                 (21%)]\tLoss: 0.411943\n",
      "Training stage for Flod 3 Epoch: 78 [9600/29981                 (32%)]\tLoss: 0.375879\n",
      "Training stage for Flod 3 Epoch: 78 [12800/29981                 (43%)]\tLoss: 0.375839\n",
      "Training stage for Flod 3 Epoch: 78 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 78 [19200/29981                 (64%)]\tLoss: 0.344585\n",
      "Training stage for Flod 3 Epoch: 78 [22400/29981                 (75%)]\tLoss: 0.344529\n",
      "Training stage for Flod 3 Epoch: 78 [25600/29981                 (85%)]\tLoss: 0.315431\n",
      "Training stage for Flod 3 Epoch: 78 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold3: Average Loss:           697158.9818, Accuracy: 6834/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 79 [0/29981                 (0%)]\tLoss: 0.345884\n",
      "Training stage for Flod 3 Epoch: 79 [3200/29981                 (11%)]\tLoss: 0.313273\n",
      "Training stage for Flod 3 Epoch: 79 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 79 [9600/29981                 (32%)]\tLoss: 0.428929\n",
      "Training stage for Flod 3 Epoch: 79 [12800/29981                 (43%)]\tLoss: 0.364052\n",
      "Training stage for Flod 3 Epoch: 79 [16000/29981                 (53%)]\tLoss: 0.407037\n",
      "Training stage for Flod 3 Epoch: 79 [19200/29981                 (64%)]\tLoss: 0.407012\n",
      "Training stage for Flod 3 Epoch: 79 [22400/29981                 (75%)]\tLoss: 0.392761\n",
      "Training stage for Flod 3 Epoch: 79 [25600/29981                 (85%)]\tLoss: 0.353250\n",
      "Training stage for Flod 3 Epoch: 79 [28800/29981                 (96%)]\tLoss: 0.318877\n",
      "Test set for fold3: Average Loss:           677297.8252, Accuracy: 6935/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 80 [0/29981                 (0%)]\tLoss: 0.407032\n",
      "Training stage for Flod 3 Epoch: 80 [3200/29981                 (11%)]\tLoss: 0.318397\n",
      "Training stage for Flod 3 Epoch: 80 [6400/29981                 (21%)]\tLoss: 0.404467\n",
      "Training stage for Flod 3 Epoch: 80 [9600/29981                 (32%)]\tLoss: 0.375821\n",
      "Training stage for Flod 3 Epoch: 80 [12800/29981                 (43%)]\tLoss: 0.407012\n",
      "Training stage for Flod 3 Epoch: 80 [16000/29981                 (53%)]\tLoss: 0.314141\n",
      "Training stage for Flod 3 Epoch: 80 [19200/29981                 (64%)]\tLoss: 0.375771\n",
      "Training stage for Flod 3 Epoch: 80 [22400/29981                 (75%)]\tLoss: 0.375781\n",
      "Training stage for Flod 3 Epoch: 80 [25600/29981                 (85%)]\tLoss: 0.375981\n",
      "Training stage for Flod 3 Epoch: 80 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold3: Average Loss:           715987.5055, Accuracy: 6770/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 81 [0/29981                 (0%)]\tLoss: 0.344520\n",
      "Training stage for Flod 3 Epoch: 81 [3200/29981                 (11%)]\tLoss: 0.363636\n",
      "Training stage for Flod 3 Epoch: 81 [6400/29981                 (21%)]\tLoss: 0.352794\n",
      "Training stage for Flod 3 Epoch: 81 [9600/29981                 (32%)]\tLoss: 0.375773\n",
      "Training stage for Flod 3 Epoch: 81 [12800/29981                 (43%)]\tLoss: 0.375741\n",
      "Training stage for Flod 3 Epoch: 81 [16000/29981                 (53%)]\tLoss: 0.438266\n",
      "Training stage for Flod 3 Epoch: 81 [19200/29981                 (64%)]\tLoss: 0.313288\n",
      "Training stage for Flod 3 Epoch: 81 [22400/29981                 (75%)]\tLoss: 0.375761\n",
      "Training stage for Flod 3 Epoch: 81 [25600/29981                 (85%)]\tLoss: 0.320204\n",
      "Training stage for Flod 3 Epoch: 81 [28800/29981                 (96%)]\tLoss: 0.314987\n",
      "Test set for fold3: Average Loss:           699394.3965, Accuracy: 6855/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 82 [0/29981                 (0%)]\tLoss: 0.407543\n",
      "Training stage for Flod 3 Epoch: 82 [3200/29981                 (11%)]\tLoss: 0.440579\n",
      "Training stage for Flod 3 Epoch: 82 [6400/29981                 (21%)]\tLoss: 0.313263\n",
      "Training stage for Flod 3 Epoch: 82 [9600/29981                 (32%)]\tLoss: 0.438228\n",
      "Training stage for Flod 3 Epoch: 82 [12800/29981                 (43%)]\tLoss: 0.345596\n",
      "Training stage for Flod 3 Epoch: 82 [16000/29981                 (53%)]\tLoss: 0.407888\n",
      "Training stage for Flod 3 Epoch: 82 [19200/29981                 (64%)]\tLoss: 0.375820\n",
      "Training stage for Flod 3 Epoch: 82 [22400/29981                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 82 [25600/29981                 (85%)]\tLoss: 0.469512\n",
      "Training stage for Flod 3 Epoch: 82 [28800/29981                 (96%)]\tLoss: 0.387722\n",
      "Test set for fold3: Average Loss:           699850.6573, Accuracy: 6843/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 83 [0/29981                 (0%)]\tLoss: 0.384060\n",
      "Training stage for Flod 3 Epoch: 83 [3200/29981                 (11%)]\tLoss: 0.313268\n",
      "Training stage for Flod 3 Epoch: 83 [6400/29981                 (21%)]\tLoss: 0.375773\n",
      "Training stage for Flod 3 Epoch: 83 [9600/29981                 (32%)]\tLoss: 0.380912\n",
      "Training stage for Flod 3 Epoch: 83 [12800/29981                 (43%)]\tLoss: 0.437932\n",
      "Training stage for Flod 3 Epoch: 83 [16000/29981                 (53%)]\tLoss: 0.407063\n",
      "Training stage for Flod 3 Epoch: 83 [19200/29981                 (64%)]\tLoss: 0.375741\n",
      "Training stage for Flod 3 Epoch: 83 [22400/29981                 (75%)]\tLoss: 0.451539\n",
      "Training stage for Flod 3 Epoch: 83 [25600/29981                 (85%)]\tLoss: 0.344576\n",
      "Training stage for Flod 3 Epoch: 83 [28800/29981                 (96%)]\tLoss: 0.352354\n",
      "Test set for fold3: Average Loss:           687259.4521, Accuracy: 6886/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 84 [0/29981                 (0%)]\tLoss: 0.313575\n",
      "Training stage for Flod 3 Epoch: 84 [3200/29981                 (11%)]\tLoss: 0.335424\n",
      "Training stage for Flod 3 Epoch: 84 [6400/29981                 (21%)]\tLoss: 0.431346\n",
      "Training stage for Flod 3 Epoch: 84 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 84 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 84 [16000/29981                 (53%)]\tLoss: 0.374220\n",
      "Training stage for Flod 3 Epoch: 84 [19200/29981                 (64%)]\tLoss: 0.313345\n",
      "Training stage for Flod 3 Epoch: 84 [22400/29981                 (75%)]\tLoss: 0.416775\n",
      "Training stage for Flod 3 Epoch: 84 [25600/29981                 (85%)]\tLoss: 0.415167\n",
      "Training stage for Flod 3 Epoch: 84 [28800/29981                 (96%)]\tLoss: 0.386779\n",
      "Test set for fold3: Average Loss:           732518.5143, Accuracy: 6684/7495           (89%)\n",
      "Training stage for Flod 3 Epoch: 85 [0/29981                 (0%)]\tLoss: 0.344607\n",
      "Training stage for Flod 3 Epoch: 85 [3200/29981                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 85 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 85 [9600/29981                 (32%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 85 [12800/29981                 (43%)]\tLoss: 0.313291\n",
      "Training stage for Flod 3 Epoch: 85 [16000/29981                 (53%)]\tLoss: 0.406587\n",
      "Training stage for Flod 3 Epoch: 85 [19200/29981                 (64%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 85 [22400/29981                 (75%)]\tLoss: 0.377571\n",
      "Training stage for Flod 3 Epoch: 85 [25600/29981                 (85%)]\tLoss: 0.383309\n",
      "Training stage for Flod 3 Epoch: 85 [28800/29981                 (96%)]\tLoss: 0.320758\n",
      "Test set for fold3: Average Loss:           715028.0743, Accuracy: 6781/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 86 [0/29981                 (0%)]\tLoss: 0.375802\n",
      "Training stage for Flod 3 Epoch: 86 [3200/29981                 (11%)]\tLoss: 0.384338\n",
      "Training stage for Flod 3 Epoch: 86 [6400/29981                 (21%)]\tLoss: 0.328013\n",
      "Training stage for Flod 3 Epoch: 86 [9600/29981                 (32%)]\tLoss: 0.375634\n",
      "Training stage for Flod 3 Epoch: 86 [12800/29981                 (43%)]\tLoss: 0.375854\n",
      "Training stage for Flod 3 Epoch: 86 [16000/29981                 (53%)]\tLoss: 0.364450\n",
      "Training stage for Flod 3 Epoch: 86 [19200/29981                 (64%)]\tLoss: 0.344549\n",
      "Training stage for Flod 3 Epoch: 86 [22400/29981                 (75%)]\tLoss: 0.363036\n",
      "Training stage for Flod 3 Epoch: 86 [25600/29981                 (85%)]\tLoss: 0.363297\n",
      "Training stage for Flod 3 Epoch: 86 [28800/29981                 (96%)]\tLoss: 0.343900\n",
      "Test set for fold3: Average Loss:           684114.6617, Accuracy: 6916/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 87 [0/29981                 (0%)]\tLoss: 0.313279\n",
      "Training stage for Flod 3 Epoch: 87 [3200/29981                 (11%)]\tLoss: 0.321319\n",
      "Training stage for Flod 3 Epoch: 87 [6400/29981                 (21%)]\tLoss: 0.313335\n",
      "Training stage for Flod 3 Epoch: 87 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 87 [12800/29981                 (43%)]\tLoss: 0.406035\n",
      "Training stage for Flod 3 Epoch: 87 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 87 [19200/29981                 (64%)]\tLoss: 0.407017\n",
      "Training stage for Flod 3 Epoch: 87 [22400/29981                 (75%)]\tLoss: 0.344521\n",
      "Training stage for Flod 3 Epoch: 87 [25600/29981                 (85%)]\tLoss: 0.334780\n",
      "Training stage for Flod 3 Epoch: 87 [28800/29981                 (96%)]\tLoss: 0.407011\n",
      "Test set for fold3: Average Loss:           719572.8514, Accuracy: 6756/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 88 [0/29981                 (0%)]\tLoss: 0.459543\n",
      "Training stage for Flod 3 Epoch: 88 [3200/29981                 (11%)]\tLoss: 0.380672\n",
      "Training stage for Flod 3 Epoch: 88 [6400/29981                 (21%)]\tLoss: 0.407469\n",
      "Training stage for Flod 3 Epoch: 88 [9600/29981                 (32%)]\tLoss: 0.336601\n",
      "Training stage for Flod 3 Epoch: 88 [12800/29981                 (43%)]\tLoss: 0.313269\n",
      "Training stage for Flod 3 Epoch: 88 [16000/29981                 (53%)]\tLoss: 0.316352\n",
      "Training stage for Flod 3 Epoch: 88 [19200/29981                 (64%)]\tLoss: 0.344558\n",
      "Training stage for Flod 3 Epoch: 88 [22400/29981                 (75%)]\tLoss: 0.407039\n",
      "Training stage for Flod 3 Epoch: 88 [25600/29981                 (85%)]\tLoss: 0.406901\n",
      "Training stage for Flod 3 Epoch: 88 [28800/29981                 (96%)]\tLoss: 0.313268\n",
      "Test set for fold3: Average Loss:           715471.1568, Accuracy: 6766/7495           (90%)\n",
      "Training stage for Flod 3 Epoch: 89 [0/29981                 (0%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 89 [3200/29981                 (11%)]\tLoss: 0.344561\n",
      "Training stage for Flod 3 Epoch: 89 [6400/29981                 (21%)]\tLoss: 0.313263\n",
      "Training stage for Flod 3 Epoch: 89 [9600/29981                 (32%)]\tLoss: 0.338361\n",
      "Training stage for Flod 3 Epoch: 89 [12800/29981                 (43%)]\tLoss: 0.469293\n",
      "Training stage for Flod 3 Epoch: 89 [16000/29981                 (53%)]\tLoss: 0.407013\n",
      "Training stage for Flod 3 Epoch: 89 [19200/29981                 (64%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 89 [22400/29981                 (75%)]\tLoss: 0.313265\n",
      "Training stage for Flod 3 Epoch: 89 [25600/29981                 (85%)]\tLoss: 0.369989\n",
      "Training stage for Flod 3 Epoch: 89 [28800/29981                 (96%)]\tLoss: 0.377909\n",
      "Test set for fold3: Average Loss:           690634.8072, Accuracy: 6872/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 90 [0/29981                 (0%)]\tLoss: 0.407326\n",
      "Training stage for Flod 3 Epoch: 90 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 90 [6400/29981                 (21%)]\tLoss: 0.376638\n",
      "Training stage for Flod 3 Epoch: 90 [9600/29981                 (32%)]\tLoss: 0.407317\n",
      "Training stage for Flod 3 Epoch: 90 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 90 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 90 [19200/29981                 (64%)]\tLoss: 0.387732\n",
      "Training stage for Flod 3 Epoch: 90 [22400/29981                 (75%)]\tLoss: 0.407032\n",
      "Training stage for Flod 3 Epoch: 90 [25600/29981                 (85%)]\tLoss: 0.359677\n",
      "Training stage for Flod 3 Epoch: 90 [28800/29981                 (96%)]\tLoss: 0.375762\n",
      "Test set for fold3: Average Loss:           699879.0545, Accuracy: 6830/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 91 [0/29981                 (0%)]\tLoss: 0.313922\n",
      "Training stage for Flod 3 Epoch: 91 [3200/29981                 (11%)]\tLoss: 0.377384\n",
      "Training stage for Flod 3 Epoch: 91 [6400/29981                 (21%)]\tLoss: 0.411227\n",
      "Training stage for Flod 3 Epoch: 91 [9600/29981                 (32%)]\tLoss: 0.416518\n",
      "Training stage for Flod 3 Epoch: 91 [12800/29981                 (43%)]\tLoss: 0.351164\n",
      "Training stage for Flod 3 Epoch: 91 [16000/29981                 (53%)]\tLoss: 0.406972\n",
      "Training stage for Flod 3 Epoch: 91 [19200/29981                 (64%)]\tLoss: 0.344530\n",
      "Training stage for Flod 3 Epoch: 91 [22400/29981                 (75%)]\tLoss: 0.407006\n",
      "Training stage for Flod 3 Epoch: 91 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 91 [28800/29981                 (96%)]\tLoss: 0.377106\n",
      "Test set for fold3: Average Loss:           696553.6062, Accuracy: 6829/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 92 [0/29981                 (0%)]\tLoss: 0.407608\n",
      "Training stage for Flod 3 Epoch: 92 [3200/29981                 (11%)]\tLoss: 0.315710\n",
      "Training stage for Flod 3 Epoch: 92 [6400/29981                 (21%)]\tLoss: 0.408182\n",
      "Training stage for Flod 3 Epoch: 92 [9600/29981                 (32%)]\tLoss: 0.344518\n",
      "Training stage for Flod 3 Epoch: 92 [12800/29981                 (43%)]\tLoss: 0.313635\n",
      "Training stage for Flod 3 Epoch: 92 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 92 [19200/29981                 (64%)]\tLoss: 0.344589\n",
      "Training stage for Flod 3 Epoch: 92 [22400/29981                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 92 [25600/29981                 (85%)]\tLoss: 0.408753\n",
      "Training stage for Flod 3 Epoch: 92 [28800/29981                 (96%)]\tLoss: 0.345709\n",
      "Test set for fold3: Average Loss:           684808.9700, Accuracy: 6898/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 93 [0/29981                 (0%)]\tLoss: 0.406518\n",
      "Training stage for Flod 3 Epoch: 93 [3200/29981                 (11%)]\tLoss: 0.313279\n",
      "Training stage for Flod 3 Epoch: 93 [6400/29981                 (21%)]\tLoss: 0.313275\n",
      "Training stage for Flod 3 Epoch: 93 [9600/29981                 (32%)]\tLoss: 0.313281\n",
      "Training stage for Flod 3 Epoch: 93 [12800/29981                 (43%)]\tLoss: 0.313264\n",
      "Training stage for Flod 3 Epoch: 93 [16000/29981                 (53%)]\tLoss: 0.344541\n",
      "Training stage for Flod 3 Epoch: 93 [19200/29981                 (64%)]\tLoss: 0.375978\n",
      "Training stage for Flod 3 Epoch: 93 [22400/29981                 (75%)]\tLoss: 0.315275\n",
      "Training stage for Flod 3 Epoch: 93 [25600/29981                 (85%)]\tLoss: 0.366620\n",
      "Training stage for Flod 3 Epoch: 93 [28800/29981                 (96%)]\tLoss: 0.455263\n",
      "Test set for fold3: Average Loss:           682418.4927, Accuracy: 6913/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 94 [0/29981                 (0%)]\tLoss: 0.344543\n",
      "Training stage for Flod 3 Epoch: 94 [3200/29981                 (11%)]\tLoss: 0.409681\n",
      "Training stage for Flod 3 Epoch: 94 [6400/29981                 (21%)]\tLoss: 0.344716\n",
      "Training stage for Flod 3 Epoch: 94 [9600/29981                 (32%)]\tLoss: 0.373737\n",
      "Training stage for Flod 3 Epoch: 94 [12800/29981                 (43%)]\tLoss: 0.371505\n",
      "Training stage for Flod 3 Epoch: 94 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 94 [19200/29981                 (64%)]\tLoss: 0.314428\n",
      "Training stage for Flod 3 Epoch: 94 [22400/29981                 (75%)]\tLoss: 0.407012\n",
      "Training stage for Flod 3 Epoch: 94 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 94 [28800/29981                 (96%)]\tLoss: 0.375764\n",
      "Test set for fold3: Average Loss:           667045.2301, Accuracy: 6984/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 95 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 95 [3200/29981                 (11%)]\tLoss: 0.344531\n",
      "Training stage for Flod 3 Epoch: 95 [6400/29981                 (21%)]\tLoss: 0.382441\n",
      "Training stage for Flod 3 Epoch: 95 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 95 [12800/29981                 (43%)]\tLoss: 0.450435\n",
      "Training stage for Flod 3 Epoch: 95 [16000/29981                 (53%)]\tLoss: 0.377498\n",
      "Training stage for Flod 3 Epoch: 95 [19200/29981                 (64%)]\tLoss: 0.376137\n",
      "Training stage for Flod 3 Epoch: 95 [22400/29981                 (75%)]\tLoss: 0.344672\n",
      "Training stage for Flod 3 Epoch: 95 [25600/29981                 (85%)]\tLoss: 0.347046\n",
      "Training stage for Flod 3 Epoch: 95 [28800/29981                 (96%)]\tLoss: 0.344606\n",
      "Test set for fold3: Average Loss:           710381.3800, Accuracy: 6787/7495           (91%)\n",
      "Training stage for Flod 3 Epoch: 96 [0/29981                 (0%)]\tLoss: 0.313281\n",
      "Training stage for Flod 3 Epoch: 96 [3200/29981                 (11%)]\tLoss: 0.344527\n",
      "Training stage for Flod 3 Epoch: 96 [6400/29981                 (21%)]\tLoss: 0.407039\n",
      "Training stage for Flod 3 Epoch: 96 [9600/29981                 (32%)]\tLoss: 0.563158\n",
      "Training stage for Flod 3 Epoch: 96 [12800/29981                 (43%)]\tLoss: 0.344583\n",
      "Training stage for Flod 3 Epoch: 96 [16000/29981                 (53%)]\tLoss: 0.407016\n",
      "Training stage for Flod 3 Epoch: 96 [19200/29981                 (64%)]\tLoss: 0.314161\n",
      "Training stage for Flod 3 Epoch: 96 [22400/29981                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 96 [25600/29981                 (85%)]\tLoss: 0.344081\n",
      "Training stage for Flod 3 Epoch: 96 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold3: Average Loss:           684753.0084, Accuracy: 6903/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 97 [0/29981                 (0%)]\tLoss: 0.344513\n",
      "Training stage for Flod 3 Epoch: 97 [3200/29981                 (11%)]\tLoss: 0.313269\n",
      "Training stage for Flod 3 Epoch: 97 [6400/29981                 (21%)]\tLoss: 0.336797\n",
      "Training stage for Flod 3 Epoch: 97 [9600/29981                 (32%)]\tLoss: 0.313270\n",
      "Training stage for Flod 3 Epoch: 97 [12800/29981                 (43%)]\tLoss: 0.345649\n",
      "Training stage for Flod 3 Epoch: 97 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 97 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 97 [22400/29981                 (75%)]\tLoss: 0.346778\n",
      "Training stage for Flod 3 Epoch: 97 [25600/29981                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 97 [28800/29981                 (96%)]\tLoss: 0.381591\n",
      "Test set for fold3: Average Loss:           685218.0402, Accuracy: 6901/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 98 [0/29981                 (0%)]\tLoss: 0.375767\n",
      "Training stage for Flod 3 Epoch: 98 [3200/29981                 (11%)]\tLoss: 0.407030\n",
      "Training stage for Flod 3 Epoch: 98 [6400/29981                 (21%)]\tLoss: 0.407013\n",
      "Training stage for Flod 3 Epoch: 98 [9600/29981                 (32%)]\tLoss: 0.347000\n",
      "Training stage for Flod 3 Epoch: 98 [12800/29981                 (43%)]\tLoss: 0.365550\n",
      "Training stage for Flod 3 Epoch: 98 [16000/29981                 (53%)]\tLoss: 0.405666\n",
      "Training stage for Flod 3 Epoch: 98 [19200/29981                 (64%)]\tLoss: 0.375109\n",
      "Training stage for Flod 3 Epoch: 98 [22400/29981                 (75%)]\tLoss: 0.407127\n",
      "Training stage for Flod 3 Epoch: 98 [25600/29981                 (85%)]\tLoss: 0.313295\n",
      "Training stage for Flod 3 Epoch: 98 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold3: Average Loss:           676621.7824, Accuracy: 6936/7495           (93%)\n",
      "Training stage for Flod 3 Epoch: 99 [0/29981                 (0%)]\tLoss: 0.313264\n",
      "Training stage for Flod 3 Epoch: 99 [3200/29981                 (11%)]\tLoss: 0.375765\n",
      "Training stage for Flod 3 Epoch: 99 [6400/29981                 (21%)]\tLoss: 0.344329\n",
      "Training stage for Flod 3 Epoch: 99 [9600/29981                 (32%)]\tLoss: 0.406735\n",
      "Training stage for Flod 3 Epoch: 99 [12800/29981                 (43%)]\tLoss: 0.372566\n",
      "Training stage for Flod 3 Epoch: 99 [16000/29981                 (53%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 99 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 99 [22400/29981                 (75%)]\tLoss: 0.344486\n",
      "Training stage for Flod 3 Epoch: 99 [25600/29981                 (85%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 99 [28800/29981                 (96%)]\tLoss: 0.344515\n",
      "Test set for fold3: Average Loss:           681570.2722, Accuracy: 6931/7495           (92%)\n",
      "Training stage for Flod 3 Epoch: 100 [0/29981                 (0%)]\tLoss: 0.372493\n",
      "Training stage for Flod 3 Epoch: 100 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 3 Epoch: 100 [6400/29981                 (21%)]\tLoss: 0.463533\n",
      "Training stage for Flod 3 Epoch: 100 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 3 Epoch: 100 [12800/29981                 (43%)]\tLoss: 0.438262\n",
      "Training stage for Flod 3 Epoch: 100 [16000/29981                 (53%)]\tLoss: 0.530139\n",
      "Training stage for Flod 3 Epoch: 100 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 3 Epoch: 100 [22400/29981                 (75%)]\tLoss: 0.377696\n",
      "Training stage for Flod 3 Epoch: 100 [25600/29981                 (85%)]\tLoss: 0.313940\n",
      "Training stage for Flod 3 Epoch: 100 [28800/29981                 (96%)]\tLoss: 0.375762\n",
      "Test set for fold3: Average Loss:           651174.7105, Accuracy: 7043/7495           (94%)\n",
      "-------------------Fold 4-------------------\n",
      "Training stage for Flod 4 Epoch: 1 [0/29981                 (0%)]\tLoss: 0.684356\n",
      "Training stage for Flod 4 Epoch: 1 [3200/29981                 (11%)]\tLoss: 0.487371\n",
      "Training stage for Flod 4 Epoch: 1 [6400/29981                 (21%)]\tLoss: 0.515196\n",
      "Training stage for Flod 4 Epoch: 1 [9600/29981                 (32%)]\tLoss: 0.366803\n",
      "Training stage for Flod 4 Epoch: 1 [12800/29981                 (43%)]\tLoss: 0.376027\n",
      "Training stage for Flod 4 Epoch: 1 [16000/29981                 (53%)]\tLoss: 0.481838\n",
      "Training stage for Flod 4 Epoch: 1 [19200/29981                 (64%)]\tLoss: 0.380512\n",
      "Training stage for Flod 4 Epoch: 1 [22400/29981                 (75%)]\tLoss: 0.480471\n",
      "Training stage for Flod 4 Epoch: 1 [25600/29981                 (85%)]\tLoss: 0.426773\n",
      "Training stage for Flod 4 Epoch: 1 [28800/29981                 (96%)]\tLoss: 0.355794\n",
      "Test set for fold4: Average Loss:           801228.7641, Accuracy: 6379/7495           (85%)\n",
      "Training stage for Flod 4 Epoch: 2 [0/29981                 (0%)]\tLoss: 0.406246\n",
      "Training stage for Flod 4 Epoch: 2 [3200/29981                 (11%)]\tLoss: 0.376939\n",
      "Training stage for Flod 4 Epoch: 2 [6400/29981                 (21%)]\tLoss: 0.383343\n",
      "Training stage for Flod 4 Epoch: 2 [9600/29981                 (32%)]\tLoss: 0.485134\n",
      "Training stage for Flod 4 Epoch: 2 [12800/29981                 (43%)]\tLoss: 0.377959\n",
      "Training stage for Flod 4 Epoch: 2 [16000/29981                 (53%)]\tLoss: 0.422295\n",
      "Training stage for Flod 4 Epoch: 2 [19200/29981                 (64%)]\tLoss: 0.388062\n",
      "Training stage for Flod 4 Epoch: 2 [22400/29981                 (75%)]\tLoss: 0.352270\n",
      "Training stage for Flod 4 Epoch: 2 [25600/29981                 (85%)]\tLoss: 0.381491\n",
      "Training stage for Flod 4 Epoch: 2 [28800/29981                 (96%)]\tLoss: 0.411389\n",
      "Test set for fold4: Average Loss:           780483.4466, Accuracy: 6478/7495           (86%)\n",
      "Training stage for Flod 4 Epoch: 3 [0/29981                 (0%)]\tLoss: 0.432324\n",
      "Training stage for Flod 4 Epoch: 3 [3200/29981                 (11%)]\tLoss: 0.444874\n",
      "Training stage for Flod 4 Epoch: 3 [6400/29981                 (21%)]\tLoss: 0.377730\n",
      "Training stage for Flod 4 Epoch: 3 [9600/29981                 (32%)]\tLoss: 0.394074\n",
      "Training stage for Flod 4 Epoch: 3 [12800/29981                 (43%)]\tLoss: 0.484904\n",
      "Training stage for Flod 4 Epoch: 3 [16000/29981                 (53%)]\tLoss: 0.463054\n",
      "Training stage for Flod 4 Epoch: 3 [19200/29981                 (64%)]\tLoss: 0.358263\n",
      "Training stage for Flod 4 Epoch: 3 [22400/29981                 (75%)]\tLoss: 0.337765\n",
      "Training stage for Flod 4 Epoch: 3 [25600/29981                 (85%)]\tLoss: 0.432100\n",
      "Training stage for Flod 4 Epoch: 3 [28800/29981                 (96%)]\tLoss: 0.420579\n",
      "Test set for fold4: Average Loss:           752224.5062, Accuracy: 6571/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 4 [0/29981                 (0%)]\tLoss: 0.368769\n",
      "Training stage for Flod 4 Epoch: 4 [3200/29981                 (11%)]\tLoss: 0.506274\n",
      "Training stage for Flod 4 Epoch: 4 [6400/29981                 (21%)]\tLoss: 0.456542\n",
      "Training stage for Flod 4 Epoch: 4 [9600/29981                 (32%)]\tLoss: 0.406399\n",
      "Training stage for Flod 4 Epoch: 4 [12800/29981                 (43%)]\tLoss: 0.451935\n",
      "Training stage for Flod 4 Epoch: 4 [16000/29981                 (53%)]\tLoss: 0.434828\n",
      "Training stage for Flod 4 Epoch: 4 [19200/29981                 (64%)]\tLoss: 0.338321\n",
      "Training stage for Flod 4 Epoch: 4 [22400/29981                 (75%)]\tLoss: 0.411470\n",
      "Training stage for Flod 4 Epoch: 4 [25600/29981                 (85%)]\tLoss: 0.333828\n",
      "Training stage for Flod 4 Epoch: 4 [28800/29981                 (96%)]\tLoss: 0.338202\n",
      "Test set for fold4: Average Loss:           758596.0254, Accuracy: 6544/7495           (87%)\n",
      "Training stage for Flod 4 Epoch: 5 [0/29981                 (0%)]\tLoss: 0.452646\n",
      "Training stage for Flod 4 Epoch: 5 [3200/29981                 (11%)]\tLoss: 0.344986\n",
      "Training stage for Flod 4 Epoch: 5 [6400/29981                 (21%)]\tLoss: 0.511497\n",
      "Training stage for Flod 4 Epoch: 5 [9600/29981                 (32%)]\tLoss: 0.424267\n",
      "Training stage for Flod 4 Epoch: 5 [12800/29981                 (43%)]\tLoss: 0.441725\n",
      "Training stage for Flod 4 Epoch: 5 [16000/29981                 (53%)]\tLoss: 0.349576\n",
      "Training stage for Flod 4 Epoch: 5 [19200/29981                 (64%)]\tLoss: 0.445994\n",
      "Training stage for Flod 4 Epoch: 5 [22400/29981                 (75%)]\tLoss: 0.378993\n",
      "Training stage for Flod 4 Epoch: 5 [25600/29981                 (85%)]\tLoss: 0.349228\n",
      "Training stage for Flod 4 Epoch: 5 [28800/29981                 (96%)]\tLoss: 0.346963\n",
      "Test set for fold4: Average Loss:           779967.0816, Accuracy: 6481/7495           (86%)\n",
      "Training stage for Flod 4 Epoch: 6 [0/29981                 (0%)]\tLoss: 0.401509\n",
      "Training stage for Flod 4 Epoch: 6 [3200/29981                 (11%)]\tLoss: 0.392790\n",
      "Training stage for Flod 4 Epoch: 6 [6400/29981                 (21%)]\tLoss: 0.344831\n",
      "Training stage for Flod 4 Epoch: 6 [9600/29981                 (32%)]\tLoss: 0.398888\n",
      "Training stage for Flod 4 Epoch: 6 [12800/29981                 (43%)]\tLoss: 0.432014\n",
      "Training stage for Flod 4 Epoch: 6 [16000/29981                 (53%)]\tLoss: 0.444662\n",
      "Training stage for Flod 4 Epoch: 6 [19200/29981                 (64%)]\tLoss: 0.378215\n",
      "Training stage for Flod 4 Epoch: 6 [22400/29981                 (75%)]\tLoss: 0.392521\n",
      "Training stage for Flod 4 Epoch: 6 [25600/29981                 (85%)]\tLoss: 0.378174\n",
      "Training stage for Flod 4 Epoch: 6 [28800/29981                 (96%)]\tLoss: 0.426576\n",
      "Test set for fold4: Average Loss:           729268.6000, Accuracy: 6695/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 7 [0/29981                 (0%)]\tLoss: 0.325155\n",
      "Training stage for Flod 4 Epoch: 7 [3200/29981                 (11%)]\tLoss: 0.425401\n",
      "Training stage for Flod 4 Epoch: 7 [6400/29981                 (21%)]\tLoss: 0.362896\n",
      "Training stage for Flod 4 Epoch: 7 [9600/29981                 (32%)]\tLoss: 0.485662\n",
      "Training stage for Flod 4 Epoch: 7 [12800/29981                 (43%)]\tLoss: 0.469254\n",
      "Training stage for Flod 4 Epoch: 7 [16000/29981                 (53%)]\tLoss: 0.383144\n",
      "Training stage for Flod 4 Epoch: 7 [19200/29981                 (64%)]\tLoss: 0.316825\n",
      "Training stage for Flod 4 Epoch: 7 [22400/29981                 (75%)]\tLoss: 0.348767\n",
      "Training stage for Flod 4 Epoch: 7 [25600/29981                 (85%)]\tLoss: 0.355182\n",
      "Training stage for Flod 4 Epoch: 7 [28800/29981                 (96%)]\tLoss: 0.456435\n",
      "Test set for fold4: Average Loss:           779360.8517, Accuracy: 6460/7495           (86%)\n",
      "Training stage for Flod 4 Epoch: 8 [0/29981                 (0%)]\tLoss: 0.314136\n",
      "Training stage for Flod 4 Epoch: 8 [3200/29981                 (11%)]\tLoss: 0.440933\n",
      "Training stage for Flod 4 Epoch: 8 [6400/29981                 (21%)]\tLoss: 0.315495\n",
      "Training stage for Flod 4 Epoch: 8 [9600/29981                 (32%)]\tLoss: 0.441349\n",
      "Training stage for Flod 4 Epoch: 8 [12800/29981                 (43%)]\tLoss: 0.370085\n",
      "Training stage for Flod 4 Epoch: 8 [16000/29981                 (53%)]\tLoss: 0.418355\n",
      "Training stage for Flod 4 Epoch: 8 [19200/29981                 (64%)]\tLoss: 0.464777\n",
      "Training stage for Flod 4 Epoch: 8 [22400/29981                 (75%)]\tLoss: 0.444184\n",
      "Training stage for Flod 4 Epoch: 8 [25600/29981                 (85%)]\tLoss: 0.386482\n",
      "Training stage for Flod 4 Epoch: 8 [28800/29981                 (96%)]\tLoss: 0.330245\n",
      "Test set for fold4: Average Loss:           733022.1477, Accuracy: 6703/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 9 [0/29981                 (0%)]\tLoss: 0.390089\n",
      "Training stage for Flod 4 Epoch: 9 [3200/29981                 (11%)]\tLoss: 0.429092\n",
      "Training stage for Flod 4 Epoch: 9 [6400/29981                 (21%)]\tLoss: 0.371302\n",
      "Training stage for Flod 4 Epoch: 9 [9600/29981                 (32%)]\tLoss: 0.426594\n",
      "Training stage for Flod 4 Epoch: 9 [12800/29981                 (43%)]\tLoss: 0.388978\n",
      "Training stage for Flod 4 Epoch: 9 [16000/29981                 (53%)]\tLoss: 0.319471\n",
      "Training stage for Flod 4 Epoch: 9 [19200/29981                 (64%)]\tLoss: 0.378385\n",
      "Training stage for Flod 4 Epoch: 9 [22400/29981                 (75%)]\tLoss: 0.450865\n",
      "Training stage for Flod 4 Epoch: 9 [25600/29981                 (85%)]\tLoss: 0.468648\n",
      "Training stage for Flod 4 Epoch: 9 [28800/29981                 (96%)]\tLoss: 0.389099\n",
      "Test set for fold4: Average Loss:           724084.9354, Accuracy: 6739/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 10 [0/29981                 (0%)]\tLoss: 0.314143\n",
      "Training stage for Flod 4 Epoch: 10 [3200/29981                 (11%)]\tLoss: 0.390913\n",
      "Training stage for Flod 4 Epoch: 10 [6400/29981                 (21%)]\tLoss: 0.413824\n",
      "Training stage for Flod 4 Epoch: 10 [9600/29981                 (32%)]\tLoss: 0.358485\n",
      "Training stage for Flod 4 Epoch: 10 [12800/29981                 (43%)]\tLoss: 0.316032\n",
      "Training stage for Flod 4 Epoch: 10 [16000/29981                 (53%)]\tLoss: 0.352660\n",
      "Training stage for Flod 4 Epoch: 10 [19200/29981                 (64%)]\tLoss: 0.428791\n",
      "Training stage for Flod 4 Epoch: 10 [22400/29981                 (75%)]\tLoss: 0.399359\n",
      "Training stage for Flod 4 Epoch: 10 [25600/29981                 (85%)]\tLoss: 0.515947\n",
      "Training stage for Flod 4 Epoch: 10 [28800/29981                 (96%)]\tLoss: 0.368550\n",
      "Test set for fold4: Average Loss:           751329.7837, Accuracy: 6588/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 11 [0/29981                 (0%)]\tLoss: 0.361222\n",
      "Training stage for Flod 4 Epoch: 11 [3200/29981                 (11%)]\tLoss: 0.563357\n",
      "Training stage for Flod 4 Epoch: 11 [6400/29981                 (21%)]\tLoss: 0.380072\n",
      "Training stage for Flod 4 Epoch: 11 [9600/29981                 (32%)]\tLoss: 0.394892\n",
      "Training stage for Flod 4 Epoch: 11 [12800/29981                 (43%)]\tLoss: 0.377455\n",
      "Training stage for Flod 4 Epoch: 11 [16000/29981                 (53%)]\tLoss: 0.418258\n",
      "Training stage for Flod 4 Epoch: 11 [19200/29981                 (64%)]\tLoss: 0.346816\n",
      "Training stage for Flod 4 Epoch: 11 [22400/29981                 (75%)]\tLoss: 0.406382\n",
      "Training stage for Flod 4 Epoch: 11 [25600/29981                 (85%)]\tLoss: 0.313442\n",
      "Training stage for Flod 4 Epoch: 11 [28800/29981                 (96%)]\tLoss: 0.361695\n",
      "Test set for fold4: Average Loss:           721554.8750, Accuracy: 6759/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 12 [0/29981                 (0%)]\tLoss: 0.411704\n",
      "Training stage for Flod 4 Epoch: 12 [3200/29981                 (11%)]\tLoss: 0.399187\n",
      "Training stage for Flod 4 Epoch: 12 [6400/29981                 (21%)]\tLoss: 0.364392\n",
      "Training stage for Flod 4 Epoch: 12 [9600/29981                 (32%)]\tLoss: 0.346166\n",
      "Training stage for Flod 4 Epoch: 12 [12800/29981                 (43%)]\tLoss: 0.346035\n",
      "Training stage for Flod 4 Epoch: 12 [16000/29981                 (53%)]\tLoss: 0.314401\n",
      "Training stage for Flod 4 Epoch: 12 [19200/29981                 (64%)]\tLoss: 0.457486\n",
      "Training stage for Flod 4 Epoch: 12 [22400/29981                 (75%)]\tLoss: 0.392521\n",
      "Training stage for Flod 4 Epoch: 12 [25600/29981                 (85%)]\tLoss: 0.378013\n",
      "Training stage for Flod 4 Epoch: 12 [28800/29981                 (96%)]\tLoss: 0.365973\n",
      "Test set for fold4: Average Loss:           754920.1018, Accuracy: 6610/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 13 [0/29981                 (0%)]\tLoss: 0.431736\n",
      "Training stage for Flod 4 Epoch: 13 [3200/29981                 (11%)]\tLoss: 0.423685\n",
      "Training stage for Flod 4 Epoch: 13 [6400/29981                 (21%)]\tLoss: 0.442258\n",
      "Training stage for Flod 4 Epoch: 13 [9600/29981                 (32%)]\tLoss: 0.387074\n",
      "Training stage for Flod 4 Epoch: 13 [12800/29981                 (43%)]\tLoss: 0.313394\n",
      "Training stage for Flod 4 Epoch: 13 [16000/29981                 (53%)]\tLoss: 0.466286\n",
      "Training stage for Flod 4 Epoch: 13 [19200/29981                 (64%)]\tLoss: 0.407707\n",
      "Training stage for Flod 4 Epoch: 13 [22400/29981                 (75%)]\tLoss: 0.418660\n",
      "Training stage for Flod 4 Epoch: 13 [25600/29981                 (85%)]\tLoss: 0.443425\n",
      "Training stage for Flod 4 Epoch: 13 [28800/29981                 (96%)]\tLoss: 0.408387\n",
      "Test set for fold4: Average Loss:           725745.2701, Accuracy: 6707/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 14 [0/29981                 (0%)]\tLoss: 0.314092\n",
      "Training stage for Flod 4 Epoch: 14 [3200/29981                 (11%)]\tLoss: 0.402563\n",
      "Training stage for Flod 4 Epoch: 14 [6400/29981                 (21%)]\tLoss: 0.375962\n",
      "Training stage for Flod 4 Epoch: 14 [9600/29981                 (32%)]\tLoss: 0.381521\n",
      "Training stage for Flod 4 Epoch: 14 [12800/29981                 (43%)]\tLoss: 0.407662\n",
      "Training stage for Flod 4 Epoch: 14 [16000/29981                 (53%)]\tLoss: 0.377730\n",
      "Training stage for Flod 4 Epoch: 14 [19200/29981                 (64%)]\tLoss: 0.346256\n",
      "Training stage for Flod 4 Epoch: 14 [22400/29981                 (75%)]\tLoss: 0.378413\n",
      "Training stage for Flod 4 Epoch: 14 [25600/29981                 (85%)]\tLoss: 0.347806\n",
      "Training stage for Flod 4 Epoch: 14 [28800/29981                 (96%)]\tLoss: 0.426380\n",
      "Test set for fold4: Average Loss:           726586.6142, Accuracy: 6688/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 15 [0/29981                 (0%)]\tLoss: 0.407218\n",
      "Training stage for Flod 4 Epoch: 15 [3200/29981                 (11%)]\tLoss: 0.381465\n",
      "Training stage for Flod 4 Epoch: 15 [6400/29981                 (21%)]\tLoss: 0.425380\n",
      "Training stage for Flod 4 Epoch: 15 [9600/29981                 (32%)]\tLoss: 0.393036\n",
      "Training stage for Flod 4 Epoch: 15 [12800/29981                 (43%)]\tLoss: 0.415258\n",
      "Training stage for Flod 4 Epoch: 15 [16000/29981                 (53%)]\tLoss: 0.445529\n",
      "Training stage for Flod 4 Epoch: 15 [19200/29981                 (64%)]\tLoss: 0.344569\n",
      "Training stage for Flod 4 Epoch: 15 [22400/29981                 (75%)]\tLoss: 0.361297\n",
      "Training stage for Flod 4 Epoch: 15 [25600/29981                 (85%)]\tLoss: 0.467833\n",
      "Training stage for Flod 4 Epoch: 15 [28800/29981                 (96%)]\tLoss: 0.327373\n",
      "Test set for fold4: Average Loss:           706529.5073, Accuracy: 6808/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 16 [0/29981                 (0%)]\tLoss: 0.404226\n",
      "Training stage for Flod 4 Epoch: 16 [3200/29981                 (11%)]\tLoss: 0.347626\n",
      "Training stage for Flod 4 Epoch: 16 [6400/29981                 (21%)]\tLoss: 0.412401\n",
      "Training stage for Flod 4 Epoch: 16 [9600/29981                 (32%)]\tLoss: 0.455678\n",
      "Training stage for Flod 4 Epoch: 16 [12800/29981                 (43%)]\tLoss: 0.395484\n",
      "Training stage for Flod 4 Epoch: 16 [16000/29981                 (53%)]\tLoss: 0.466664\n",
      "Training stage for Flod 4 Epoch: 16 [19200/29981                 (64%)]\tLoss: 0.360198\n",
      "Training stage for Flod 4 Epoch: 16 [22400/29981                 (75%)]\tLoss: 0.367856\n",
      "Training stage for Flod 4 Epoch: 16 [25600/29981                 (85%)]\tLoss: 0.344817\n",
      "Training stage for Flod 4 Epoch: 16 [28800/29981                 (96%)]\tLoss: 0.369928\n",
      "Test set for fold4: Average Loss:           737626.6600, Accuracy: 6690/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 17 [0/29981                 (0%)]\tLoss: 0.399184\n",
      "Training stage for Flod 4 Epoch: 17 [3200/29981                 (11%)]\tLoss: 0.422804\n",
      "Training stage for Flod 4 Epoch: 17 [6400/29981                 (21%)]\tLoss: 0.416695\n",
      "Training stage for Flod 4 Epoch: 17 [9600/29981                 (32%)]\tLoss: 0.350683\n",
      "Training stage for Flod 4 Epoch: 17 [12800/29981                 (43%)]\tLoss: 0.342458\n",
      "Training stage for Flod 4 Epoch: 17 [16000/29981                 (53%)]\tLoss: 0.453695\n",
      "Training stage for Flod 4 Epoch: 17 [19200/29981                 (64%)]\tLoss: 0.379220\n",
      "Training stage for Flod 4 Epoch: 17 [22400/29981                 (75%)]\tLoss: 0.345116\n",
      "Training stage for Flod 4 Epoch: 17 [25600/29981                 (85%)]\tLoss: 0.375535\n",
      "Training stage for Flod 4 Epoch: 17 [28800/29981                 (96%)]\tLoss: 0.344530\n",
      "Test set for fold4: Average Loss:           747584.1417, Accuracy: 6623/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 18 [0/29981                 (0%)]\tLoss: 0.341519\n",
      "Training stage for Flod 4 Epoch: 18 [3200/29981                 (11%)]\tLoss: 0.436937\n",
      "Training stage for Flod 4 Epoch: 18 [6400/29981                 (21%)]\tLoss: 0.474080\n",
      "Training stage for Flod 4 Epoch: 18 [9600/29981                 (32%)]\tLoss: 0.409412\n",
      "Training stage for Flod 4 Epoch: 18 [12800/29981                 (43%)]\tLoss: 0.385540\n",
      "Training stage for Flod 4 Epoch: 18 [16000/29981                 (53%)]\tLoss: 0.413644\n",
      "Training stage for Flod 4 Epoch: 18 [19200/29981                 (64%)]\tLoss: 0.329306\n",
      "Training stage for Flod 4 Epoch: 18 [22400/29981                 (75%)]\tLoss: 0.358565\n",
      "Training stage for Flod 4 Epoch: 18 [25600/29981                 (85%)]\tLoss: 0.382201\n",
      "Training stage for Flod 4 Epoch: 18 [28800/29981                 (96%)]\tLoss: 0.347544\n",
      "Test set for fold4: Average Loss:           732518.1111, Accuracy: 6697/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 19 [0/29981                 (0%)]\tLoss: 0.382057\n",
      "Training stage for Flod 4 Epoch: 19 [3200/29981                 (11%)]\tLoss: 0.375773\n",
      "Training stage for Flod 4 Epoch: 19 [6400/29981                 (21%)]\tLoss: 0.407038\n",
      "Training stage for Flod 4 Epoch: 19 [9600/29981                 (32%)]\tLoss: 0.345005\n",
      "Training stage for Flod 4 Epoch: 19 [12800/29981                 (43%)]\tLoss: 0.438305\n",
      "Training stage for Flod 4 Epoch: 19 [16000/29981                 (53%)]\tLoss: 0.396260\n",
      "Training stage for Flod 4 Epoch: 19 [19200/29981                 (64%)]\tLoss: 0.371959\n",
      "Training stage for Flod 4 Epoch: 19 [22400/29981                 (75%)]\tLoss: 0.407510\n",
      "Training stage for Flod 4 Epoch: 19 [25600/29981                 (85%)]\tLoss: 0.429528\n",
      "Training stage for Flod 4 Epoch: 19 [28800/29981                 (96%)]\tLoss: 0.425256\n",
      "Test set for fold4: Average Loss:           719410.6122, Accuracy: 6766/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 20 [0/29981                 (0%)]\tLoss: 0.346334\n",
      "Training stage for Flod 4 Epoch: 20 [3200/29981                 (11%)]\tLoss: 0.437535\n",
      "Training stage for Flod 4 Epoch: 20 [6400/29981                 (21%)]\tLoss: 0.379396\n",
      "Training stage for Flod 4 Epoch: 20 [9600/29981                 (32%)]\tLoss: 0.313368\n",
      "Training stage for Flod 4 Epoch: 20 [12800/29981                 (43%)]\tLoss: 0.314435\n",
      "Training stage for Flod 4 Epoch: 20 [16000/29981                 (53%)]\tLoss: 0.407025\n",
      "Training stage for Flod 4 Epoch: 20 [19200/29981                 (64%)]\tLoss: 0.397379\n",
      "Training stage for Flod 4 Epoch: 20 [22400/29981                 (75%)]\tLoss: 0.353940\n",
      "Training stage for Flod 4 Epoch: 20 [25600/29981                 (85%)]\tLoss: 0.387222\n",
      "Training stage for Flod 4 Epoch: 20 [28800/29981                 (96%)]\tLoss: 0.501059\n",
      "Test set for fold4: Average Loss:           724702.7675, Accuracy: 6751/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 21 [0/29981                 (0%)]\tLoss: 0.499367\n",
      "Training stage for Flod 4 Epoch: 21 [3200/29981                 (11%)]\tLoss: 0.383404\n",
      "Training stage for Flod 4 Epoch: 21 [6400/29981                 (21%)]\tLoss: 0.344513\n",
      "Training stage for Flod 4 Epoch: 21 [9600/29981                 (32%)]\tLoss: 0.439514\n",
      "Training stage for Flod 4 Epoch: 21 [12800/29981                 (43%)]\tLoss: 0.413058\n",
      "Training stage for Flod 4 Epoch: 21 [16000/29981                 (53%)]\tLoss: 0.368493\n",
      "Training stage for Flod 4 Epoch: 21 [19200/29981                 (64%)]\tLoss: 0.331908\n",
      "Training stage for Flod 4 Epoch: 21 [22400/29981                 (75%)]\tLoss: 0.343012\n",
      "Training stage for Flod 4 Epoch: 21 [25600/29981                 (85%)]\tLoss: 0.349981\n",
      "Training stage for Flod 4 Epoch: 21 [28800/29981                 (96%)]\tLoss: 0.488903\n",
      "Test set for fold4: Average Loss:           742786.0458, Accuracy: 6628/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 22 [0/29981                 (0%)]\tLoss: 0.389089\n",
      "Training stage for Flod 4 Epoch: 22 [3200/29981                 (11%)]\tLoss: 0.393367\n",
      "Training stage for Flod 4 Epoch: 22 [6400/29981                 (21%)]\tLoss: 0.436161\n",
      "Training stage for Flod 4 Epoch: 22 [9600/29981                 (32%)]\tLoss: 0.475422\n",
      "Training stage for Flod 4 Epoch: 22 [12800/29981                 (43%)]\tLoss: 0.376165\n",
      "Training stage for Flod 4 Epoch: 22 [16000/29981                 (53%)]\tLoss: 0.379506\n",
      "Training stage for Flod 4 Epoch: 22 [19200/29981                 (64%)]\tLoss: 0.370512\n",
      "Training stage for Flod 4 Epoch: 22 [22400/29981                 (75%)]\tLoss: 0.458718\n",
      "Training stage for Flod 4 Epoch: 22 [25600/29981                 (85%)]\tLoss: 0.408812\n",
      "Training stage for Flod 4 Epoch: 22 [28800/29981                 (96%)]\tLoss: 0.377176\n",
      "Test set for fold4: Average Loss:           706218.9466, Accuracy: 6819/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 23 [0/29981                 (0%)]\tLoss: 0.412172\n",
      "Training stage for Flod 4 Epoch: 23 [3200/29981                 (11%)]\tLoss: 0.351810\n",
      "Training stage for Flod 4 Epoch: 23 [6400/29981                 (21%)]\tLoss: 0.373797\n",
      "Training stage for Flod 4 Epoch: 23 [9600/29981                 (32%)]\tLoss: 0.378299\n",
      "Training stage for Flod 4 Epoch: 23 [12800/29981                 (43%)]\tLoss: 0.360675\n",
      "Training stage for Flod 4 Epoch: 23 [16000/29981                 (53%)]\tLoss: 0.352361\n",
      "Training stage for Flod 4 Epoch: 23 [19200/29981                 (64%)]\tLoss: 0.352680\n",
      "Training stage for Flod 4 Epoch: 23 [22400/29981                 (75%)]\tLoss: 0.473046\n",
      "Training stage for Flod 4 Epoch: 23 [25600/29981                 (85%)]\tLoss: 0.373995\n",
      "Training stage for Flod 4 Epoch: 23 [28800/29981                 (96%)]\tLoss: 0.367274\n",
      "Test set for fold4: Average Loss:           741717.3379, Accuracy: 6661/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 24 [0/29981                 (0%)]\tLoss: 0.344603\n",
      "Training stage for Flod 4 Epoch: 24 [3200/29981                 (11%)]\tLoss: 0.417903\n",
      "Training stage for Flod 4 Epoch: 24 [6400/29981                 (21%)]\tLoss: 0.332556\n",
      "Training stage for Flod 4 Epoch: 24 [9600/29981                 (32%)]\tLoss: 0.381758\n",
      "Training stage for Flod 4 Epoch: 24 [12800/29981                 (43%)]\tLoss: 0.316796\n",
      "Training stage for Flod 4 Epoch: 24 [16000/29981                 (53%)]\tLoss: 0.441979\n",
      "Training stage for Flod 4 Epoch: 24 [19200/29981                 (64%)]\tLoss: 0.343850\n",
      "Training stage for Flod 4 Epoch: 24 [22400/29981                 (75%)]\tLoss: 0.396269\n",
      "Training stage for Flod 4 Epoch: 24 [25600/29981                 (85%)]\tLoss: 0.408813\n",
      "Training stage for Flod 4 Epoch: 24 [28800/29981                 (96%)]\tLoss: 0.376142\n",
      "Test set for fold4: Average Loss:           748245.3524, Accuracy: 6637/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 25 [0/29981                 (0%)]\tLoss: 0.398757\n",
      "Training stage for Flod 4 Epoch: 25 [3200/29981                 (11%)]\tLoss: 0.385701\n",
      "Training stage for Flod 4 Epoch: 25 [6400/29981                 (21%)]\tLoss: 0.511981\n",
      "Training stage for Flod 4 Epoch: 25 [9600/29981                 (32%)]\tLoss: 0.379780\n",
      "Training stage for Flod 4 Epoch: 25 [12800/29981                 (43%)]\tLoss: 0.415857\n",
      "Training stage for Flod 4 Epoch: 25 [16000/29981                 (53%)]\tLoss: 0.500762\n",
      "Training stage for Flod 4 Epoch: 25 [19200/29981                 (64%)]\tLoss: 0.407557\n",
      "Training stage for Flod 4 Epoch: 25 [22400/29981                 (75%)]\tLoss: 0.367519\n",
      "Training stage for Flod 4 Epoch: 25 [25600/29981                 (85%)]\tLoss: 0.375842\n",
      "Training stage for Flod 4 Epoch: 25 [28800/29981                 (96%)]\tLoss: 0.326442\n",
      "Test set for fold4: Average Loss:           704147.9337, Accuracy: 6798/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 26 [0/29981                 (0%)]\tLoss: 0.336131\n",
      "Training stage for Flod 4 Epoch: 26 [3200/29981                 (11%)]\tLoss: 0.344513\n",
      "Training stage for Flod 4 Epoch: 26 [6400/29981                 (21%)]\tLoss: 0.344755\n",
      "Training stage for Flod 4 Epoch: 26 [9600/29981                 (32%)]\tLoss: 0.385838\n",
      "Training stage for Flod 4 Epoch: 26 [12800/29981                 (43%)]\tLoss: 0.375482\n",
      "Training stage for Flod 4 Epoch: 26 [16000/29981                 (53%)]\tLoss: 0.346749\n",
      "Training stage for Flod 4 Epoch: 26 [19200/29981                 (64%)]\tLoss: 0.376225\n",
      "Training stage for Flod 4 Epoch: 26 [22400/29981                 (75%)]\tLoss: 0.411007\n",
      "Training stage for Flod 4 Epoch: 26 [25600/29981                 (85%)]\tLoss: 0.395079\n",
      "Training stage for Flod 4 Epoch: 26 [28800/29981                 (96%)]\tLoss: 0.423652\n",
      "Test set for fold4: Average Loss:           739905.7167, Accuracy: 6636/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 27 [0/29981                 (0%)]\tLoss: 0.379806\n",
      "Training stage for Flod 4 Epoch: 27 [3200/29981                 (11%)]\tLoss: 0.408589\n",
      "Training stage for Flod 4 Epoch: 27 [6400/29981                 (21%)]\tLoss: 0.437484\n",
      "Training stage for Flod 4 Epoch: 27 [9600/29981                 (32%)]\tLoss: 0.468186\n",
      "Training stage for Flod 4 Epoch: 27 [12800/29981                 (43%)]\tLoss: 0.408935\n",
      "Training stage for Flod 4 Epoch: 27 [16000/29981                 (53%)]\tLoss: 0.376931\n",
      "Training stage for Flod 4 Epoch: 27 [19200/29981                 (64%)]\tLoss: 0.365156\n",
      "Training stage for Flod 4 Epoch: 27 [22400/29981                 (75%)]\tLoss: 0.359796\n",
      "Training stage for Flod 4 Epoch: 27 [25600/29981                 (85%)]\tLoss: 0.372802\n",
      "Training stage for Flod 4 Epoch: 27 [28800/29981                 (96%)]\tLoss: 0.376625\n",
      "Test set for fold4: Average Loss:           714999.7224, Accuracy: 6775/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 28 [0/29981                 (0%)]\tLoss: 0.367566\n",
      "Training stage for Flod 4 Epoch: 28 [3200/29981                 (11%)]\tLoss: 0.441250\n",
      "Training stage for Flod 4 Epoch: 28 [6400/29981                 (21%)]\tLoss: 0.372726\n",
      "Training stage for Flod 4 Epoch: 28 [9600/29981                 (32%)]\tLoss: 0.400958\n",
      "Training stage for Flod 4 Epoch: 28 [12800/29981                 (43%)]\tLoss: 0.405286\n",
      "Training stage for Flod 4 Epoch: 28 [16000/29981                 (53%)]\tLoss: 0.407054\n",
      "Training stage for Flod 4 Epoch: 28 [19200/29981                 (64%)]\tLoss: 0.384430\n",
      "Training stage for Flod 4 Epoch: 28 [22400/29981                 (75%)]\tLoss: 0.426354\n",
      "Training stage for Flod 4 Epoch: 28 [25600/29981                 (85%)]\tLoss: 0.438891\n",
      "Training stage for Flod 4 Epoch: 28 [28800/29981                 (96%)]\tLoss: 0.345990\n",
      "Test set for fold4: Average Loss:           708365.7725, Accuracy: 6794/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 29 [0/29981                 (0%)]\tLoss: 0.453354\n",
      "Training stage for Flod 4 Epoch: 29 [3200/29981                 (11%)]\tLoss: 0.346184\n",
      "Training stage for Flod 4 Epoch: 29 [6400/29981                 (21%)]\tLoss: 0.383227\n",
      "Training stage for Flod 4 Epoch: 29 [9600/29981                 (32%)]\tLoss: 0.437712\n",
      "Training stage for Flod 4 Epoch: 29 [12800/29981                 (43%)]\tLoss: 0.473036\n",
      "Training stage for Flod 4 Epoch: 29 [16000/29981                 (53%)]\tLoss: 0.344550\n",
      "Training stage for Flod 4 Epoch: 29 [19200/29981                 (64%)]\tLoss: 0.313579\n",
      "Training stage for Flod 4 Epoch: 29 [22400/29981                 (75%)]\tLoss: 0.408929\n",
      "Training stage for Flod 4 Epoch: 29 [25600/29981                 (85%)]\tLoss: 0.325301\n",
      "Training stage for Flod 4 Epoch: 29 [28800/29981                 (96%)]\tLoss: 0.417726\n",
      "Test set for fold4: Average Loss:           727762.4226, Accuracy: 6698/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 30 [0/29981                 (0%)]\tLoss: 0.375738\n",
      "Training stage for Flod 4 Epoch: 30 [3200/29981                 (11%)]\tLoss: 0.315029\n",
      "Training stage for Flod 4 Epoch: 30 [6400/29981                 (21%)]\tLoss: 0.440548\n",
      "Training stage for Flod 4 Epoch: 30 [9600/29981                 (32%)]\tLoss: 0.407089\n",
      "Training stage for Flod 4 Epoch: 30 [12800/29981                 (43%)]\tLoss: 0.407020\n",
      "Training stage for Flod 4 Epoch: 30 [16000/29981                 (53%)]\tLoss: 0.364566\n",
      "Training stage for Flod 4 Epoch: 30 [19200/29981                 (64%)]\tLoss: 0.469536\n",
      "Training stage for Flod 4 Epoch: 30 [22400/29981                 (75%)]\tLoss: 0.360284\n",
      "Training stage for Flod 4 Epoch: 30 [25600/29981                 (85%)]\tLoss: 0.345141\n",
      "Training stage for Flod 4 Epoch: 30 [28800/29981                 (96%)]\tLoss: 0.411841\n",
      "Test set for fold4: Average Loss:           706245.5813, Accuracy: 6804/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 31 [0/29981                 (0%)]\tLoss: 0.375771\n",
      "Training stage for Flod 4 Epoch: 31 [3200/29981                 (11%)]\tLoss: 0.438482\n",
      "Training stage for Flod 4 Epoch: 31 [6400/29981                 (21%)]\tLoss: 0.455587\n",
      "Training stage for Flod 4 Epoch: 31 [9600/29981                 (32%)]\tLoss: 0.407196\n",
      "Training stage for Flod 4 Epoch: 31 [12800/29981                 (43%)]\tLoss: 0.407409\n",
      "Training stage for Flod 4 Epoch: 31 [16000/29981                 (53%)]\tLoss: 0.313428\n",
      "Training stage for Flod 4 Epoch: 31 [19200/29981                 (64%)]\tLoss: 0.344522\n",
      "Training stage for Flod 4 Epoch: 31 [22400/29981                 (75%)]\tLoss: 0.407030\n",
      "Training stage for Flod 4 Epoch: 31 [25600/29981                 (85%)]\tLoss: 0.438497\n",
      "Training stage for Flod 4 Epoch: 31 [28800/29981                 (96%)]\tLoss: 0.355376\n",
      "Test set for fold4: Average Loss:           738459.0869, Accuracy: 6669/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 32 [0/29981                 (0%)]\tLoss: 0.319924\n",
      "Training stage for Flod 4 Epoch: 32 [3200/29981                 (11%)]\tLoss: 0.418909\n",
      "Training stage for Flod 4 Epoch: 32 [6400/29981                 (21%)]\tLoss: 0.339084\n",
      "Training stage for Flod 4 Epoch: 32 [9600/29981                 (32%)]\tLoss: 0.377735\n",
      "Training stage for Flod 4 Epoch: 32 [12800/29981                 (43%)]\tLoss: 0.469417\n",
      "Training stage for Flod 4 Epoch: 32 [16000/29981                 (53%)]\tLoss: 0.340819\n",
      "Training stage for Flod 4 Epoch: 32 [19200/29981                 (64%)]\tLoss: 0.345670\n",
      "Training stage for Flod 4 Epoch: 32 [22400/29981                 (75%)]\tLoss: 0.344656\n",
      "Training stage for Flod 4 Epoch: 32 [25600/29981                 (85%)]\tLoss: 0.346411\n",
      "Training stage for Flod 4 Epoch: 32 [28800/29981                 (96%)]\tLoss: 0.314418\n",
      "Test set for fold4: Average Loss:           757611.4452, Accuracy: 6574/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 33 [0/29981                 (0%)]\tLoss: 0.345244\n",
      "Training stage for Flod 4 Epoch: 33 [3200/29981                 (11%)]\tLoss: 0.344559\n",
      "Training stage for Flod 4 Epoch: 33 [6400/29981                 (21%)]\tLoss: 0.407308\n",
      "Training stage for Flod 4 Epoch: 33 [9600/29981                 (32%)]\tLoss: 0.375853\n",
      "Training stage for Flod 4 Epoch: 33 [12800/29981                 (43%)]\tLoss: 0.407915\n",
      "Training stage for Flod 4 Epoch: 33 [16000/29981                 (53%)]\tLoss: 0.444373\n",
      "Training stage for Flod 4 Epoch: 33 [19200/29981                 (64%)]\tLoss: 0.381882\n",
      "Training stage for Flod 4 Epoch: 33 [22400/29981                 (75%)]\tLoss: 0.341495\n",
      "Training stage for Flod 4 Epoch: 33 [25600/29981                 (85%)]\tLoss: 0.464934\n",
      "Training stage for Flod 4 Epoch: 33 [28800/29981                 (96%)]\tLoss: 0.376382\n",
      "Test set for fold4: Average Loss:           733720.2022, Accuracy: 6699/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 34 [0/29981                 (0%)]\tLoss: 0.313399\n",
      "Training stage for Flod 4 Epoch: 34 [3200/29981                 (11%)]\tLoss: 0.344620\n",
      "Training stage for Flod 4 Epoch: 34 [6400/29981                 (21%)]\tLoss: 0.333760\n",
      "Training stage for Flod 4 Epoch: 34 [9600/29981                 (32%)]\tLoss: 0.412370\n",
      "Training stage for Flod 4 Epoch: 34 [12800/29981                 (43%)]\tLoss: 0.375674\n",
      "Training stage for Flod 4 Epoch: 34 [16000/29981                 (53%)]\tLoss: 0.437413\n",
      "Training stage for Flod 4 Epoch: 34 [19200/29981                 (64%)]\tLoss: 0.387244\n",
      "Training stage for Flod 4 Epoch: 34 [22400/29981                 (75%)]\tLoss: 0.438285\n",
      "Training stage for Flod 4 Epoch: 34 [25600/29981                 (85%)]\tLoss: 0.503897\n",
      "Training stage for Flod 4 Epoch: 34 [28800/29981                 (96%)]\tLoss: 0.349515\n",
      "Test set for fold4: Average Loss:           734365.4862, Accuracy: 6661/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 35 [0/29981                 (0%)]\tLoss: 0.337964\n",
      "Training stage for Flod 4 Epoch: 35 [3200/29981                 (11%)]\tLoss: 0.374114\n",
      "Training stage for Flod 4 Epoch: 35 [6400/29981                 (21%)]\tLoss: 0.417686\n",
      "Training stage for Flod 4 Epoch: 35 [9600/29981                 (32%)]\tLoss: 0.313304\n",
      "Training stage for Flod 4 Epoch: 35 [12800/29981                 (43%)]\tLoss: 0.344665\n",
      "Training stage for Flod 4 Epoch: 35 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 35 [19200/29981                 (64%)]\tLoss: 0.348811\n",
      "Training stage for Flod 4 Epoch: 35 [22400/29981                 (75%)]\tLoss: 0.345088\n",
      "Training stage for Flod 4 Epoch: 35 [25600/29981                 (85%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 35 [28800/29981                 (96%)]\tLoss: 0.344960\n",
      "Test set for fold4: Average Loss:           720628.9891, Accuracy: 6772/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 36 [0/29981                 (0%)]\tLoss: 0.360011\n",
      "Training stage for Flod 4 Epoch: 36 [3200/29981                 (11%)]\tLoss: 0.344981\n",
      "Training stage for Flod 4 Epoch: 36 [6400/29981                 (21%)]\tLoss: 0.389075\n",
      "Training stage for Flod 4 Epoch: 36 [9600/29981                 (32%)]\tLoss: 0.375788\n",
      "Training stage for Flod 4 Epoch: 36 [12800/29981                 (43%)]\tLoss: 0.465201\n",
      "Training stage for Flod 4 Epoch: 36 [16000/29981                 (53%)]\tLoss: 0.411957\n",
      "Training stage for Flod 4 Epoch: 36 [19200/29981                 (64%)]\tLoss: 0.398642\n",
      "Training stage for Flod 4 Epoch: 36 [22400/29981                 (75%)]\tLoss: 0.408817\n",
      "Training stage for Flod 4 Epoch: 36 [25600/29981                 (85%)]\tLoss: 0.344783\n",
      "Training stage for Flod 4 Epoch: 36 [28800/29981                 (96%)]\tLoss: 0.406998\n",
      "Test set for fold4: Average Loss:           722462.9333, Accuracy: 6747/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 37 [0/29981                 (0%)]\tLoss: 0.317732\n",
      "Training stage for Flod 4 Epoch: 37 [3200/29981                 (11%)]\tLoss: 0.344533\n",
      "Training stage for Flod 4 Epoch: 37 [6400/29981                 (21%)]\tLoss: 0.377201\n",
      "Training stage for Flod 4 Epoch: 37 [9600/29981                 (32%)]\tLoss: 0.373490\n",
      "Training stage for Flod 4 Epoch: 37 [12800/29981                 (43%)]\tLoss: 0.324350\n",
      "Training stage for Flod 4 Epoch: 37 [16000/29981                 (53%)]\tLoss: 0.407346\n",
      "Training stage for Flod 4 Epoch: 37 [19200/29981                 (64%)]\tLoss: 0.350060\n",
      "Training stage for Flod 4 Epoch: 37 [22400/29981                 (75%)]\tLoss: 0.344514\n",
      "Training stage for Flod 4 Epoch: 37 [25600/29981                 (85%)]\tLoss: 0.344531\n",
      "Training stage for Flod 4 Epoch: 37 [28800/29981                 (96%)]\tLoss: 0.372842\n",
      "Test set for fold4: Average Loss:           721040.7810, Accuracy: 6720/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 38 [0/29981                 (0%)]\tLoss: 0.438263\n",
      "Training stage for Flod 4 Epoch: 38 [3200/29981                 (11%)]\tLoss: 0.389320\n",
      "Training stage for Flod 4 Epoch: 38 [6400/29981                 (21%)]\tLoss: 0.411284\n",
      "Training stage for Flod 4 Epoch: 38 [9600/29981                 (32%)]\tLoss: 0.314515\n",
      "Training stage for Flod 4 Epoch: 38 [12800/29981                 (43%)]\tLoss: 0.470713\n",
      "Training stage for Flod 4 Epoch: 38 [16000/29981                 (53%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 38 [19200/29981                 (64%)]\tLoss: 0.376514\n",
      "Training stage for Flod 4 Epoch: 38 [22400/29981                 (75%)]\tLoss: 0.387863\n",
      "Training stage for Flod 4 Epoch: 38 [25600/29981                 (85%)]\tLoss: 0.341251\n",
      "Training stage for Flod 4 Epoch: 38 [28800/29981                 (96%)]\tLoss: 0.360149\n",
      "Test set for fold4: Average Loss:           708523.2361, Accuracy: 6798/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 39 [0/29981                 (0%)]\tLoss: 0.375867\n",
      "Training stage for Flod 4 Epoch: 39 [3200/29981                 (11%)]\tLoss: 0.313460\n",
      "Training stage for Flod 4 Epoch: 39 [6400/29981                 (21%)]\tLoss: 0.363544\n",
      "Training stage for Flod 4 Epoch: 39 [9600/29981                 (32%)]\tLoss: 0.384754\n",
      "Training stage for Flod 4 Epoch: 39 [12800/29981                 (43%)]\tLoss: 0.402499\n",
      "Training stage for Flod 4 Epoch: 39 [16000/29981                 (53%)]\tLoss: 0.344516\n",
      "Training stage for Flod 4 Epoch: 39 [19200/29981                 (64%)]\tLoss: 0.323067\n",
      "Training stage for Flod 4 Epoch: 39 [22400/29981                 (75%)]\tLoss: 0.313403\n",
      "Training stage for Flod 4 Epoch: 39 [25600/29981                 (85%)]\tLoss: 0.376534\n",
      "Training stage for Flod 4 Epoch: 39 [28800/29981                 (96%)]\tLoss: 0.375868\n",
      "Test set for fold4: Average Loss:           724902.7537, Accuracy: 6738/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 40 [0/29981                 (0%)]\tLoss: 0.344607\n",
      "Training stage for Flod 4 Epoch: 40 [3200/29981                 (11%)]\tLoss: 0.334008\n",
      "Training stage for Flod 4 Epoch: 40 [6400/29981                 (21%)]\tLoss: 0.377662\n",
      "Training stage for Flod 4 Epoch: 40 [9600/29981                 (32%)]\tLoss: 0.343347\n",
      "Training stage for Flod 4 Epoch: 40 [12800/29981                 (43%)]\tLoss: 0.344953\n",
      "Training stage for Flod 4 Epoch: 40 [16000/29981                 (53%)]\tLoss: 0.395414\n",
      "Training stage for Flod 4 Epoch: 40 [19200/29981                 (64%)]\tLoss: 0.313973\n",
      "Training stage for Flod 4 Epoch: 40 [22400/29981                 (75%)]\tLoss: 0.393877\n",
      "Training stage for Flod 4 Epoch: 40 [25600/29981                 (85%)]\tLoss: 0.454827\n",
      "Training stage for Flod 4 Epoch: 40 [28800/29981                 (96%)]\tLoss: 0.350945\n",
      "Test set for fold4: Average Loss:           726021.8243, Accuracy: 6744/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 41 [0/29981                 (0%)]\tLoss: 0.407079\n",
      "Training stage for Flod 4 Epoch: 41 [3200/29981                 (11%)]\tLoss: 0.344747\n",
      "Training stage for Flod 4 Epoch: 41 [6400/29981                 (21%)]\tLoss: 0.344538\n",
      "Training stage for Flod 4 Epoch: 41 [9600/29981                 (32%)]\tLoss: 0.469864\n",
      "Training stage for Flod 4 Epoch: 41 [12800/29981                 (43%)]\tLoss: 0.382391\n",
      "Training stage for Flod 4 Epoch: 41 [16000/29981                 (53%)]\tLoss: 0.355439\n",
      "Training stage for Flod 4 Epoch: 41 [19200/29981                 (64%)]\tLoss: 0.438407\n",
      "Training stage for Flod 4 Epoch: 41 [22400/29981                 (75%)]\tLoss: 0.437312\n",
      "Training stage for Flod 4 Epoch: 41 [25600/29981                 (85%)]\tLoss: 0.433352\n",
      "Training stage for Flod 4 Epoch: 41 [28800/29981                 (96%)]\tLoss: 0.315371\n",
      "Test set for fold4: Average Loss:           736383.5503, Accuracy: 6661/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 42 [0/29981                 (0%)]\tLoss: 0.321121\n",
      "Training stage for Flod 4 Epoch: 42 [3200/29981                 (11%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 42 [6400/29981                 (21%)]\tLoss: 0.407792\n",
      "Training stage for Flod 4 Epoch: 42 [9600/29981                 (32%)]\tLoss: 0.344514\n",
      "Training stage for Flod 4 Epoch: 42 [12800/29981                 (43%)]\tLoss: 0.370906\n",
      "Training stage for Flod 4 Epoch: 42 [16000/29981                 (53%)]\tLoss: 0.375752\n",
      "Training stage for Flod 4 Epoch: 42 [19200/29981                 (64%)]\tLoss: 0.337743\n",
      "Training stage for Flod 4 Epoch: 42 [22400/29981                 (75%)]\tLoss: 0.419098\n",
      "Training stage for Flod 4 Epoch: 42 [25600/29981                 (85%)]\tLoss: 0.360399\n",
      "Training stage for Flod 4 Epoch: 42 [28800/29981                 (96%)]\tLoss: 0.345195\n",
      "Test set for fold4: Average Loss:           732350.6692, Accuracy: 6700/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 43 [0/29981                 (0%)]\tLoss: 0.375799\n",
      "Training stage for Flod 4 Epoch: 43 [3200/29981                 (11%)]\tLoss: 0.376412\n",
      "Training stage for Flod 4 Epoch: 43 [6400/29981                 (21%)]\tLoss: 0.376341\n",
      "Training stage for Flod 4 Epoch: 43 [9600/29981                 (32%)]\tLoss: 0.403435\n",
      "Training stage for Flod 4 Epoch: 43 [12800/29981                 (43%)]\tLoss: 0.464584\n",
      "Training stage for Flod 4 Epoch: 43 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 43 [19200/29981                 (64%)]\tLoss: 0.372165\n",
      "Training stage for Flod 4 Epoch: 43 [22400/29981                 (75%)]\tLoss: 0.344873\n",
      "Training stage for Flod 4 Epoch: 43 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 43 [28800/29981                 (96%)]\tLoss: 0.389606\n",
      "Test set for fold4: Average Loss:           732951.1724, Accuracy: 6697/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 44 [0/29981                 (0%)]\tLoss: 0.379336\n",
      "Training stage for Flod 4 Epoch: 44 [3200/29981                 (11%)]\tLoss: 0.435242\n",
      "Training stage for Flod 4 Epoch: 44 [6400/29981                 (21%)]\tLoss: 0.392592\n",
      "Training stage for Flod 4 Epoch: 44 [9600/29981                 (32%)]\tLoss: 0.313270\n",
      "Training stage for Flod 4 Epoch: 44 [12800/29981                 (43%)]\tLoss: 0.380042\n",
      "Training stage for Flod 4 Epoch: 44 [16000/29981                 (53%)]\tLoss: 0.406967\n",
      "Training stage for Flod 4 Epoch: 44 [19200/29981                 (64%)]\tLoss: 0.313353\n",
      "Training stage for Flod 4 Epoch: 44 [22400/29981                 (75%)]\tLoss: 0.376239\n",
      "Training stage for Flod 4 Epoch: 44 [25600/29981                 (85%)]\tLoss: 0.347988\n",
      "Training stage for Flod 4 Epoch: 44 [28800/29981                 (96%)]\tLoss: 0.316284\n",
      "Test set for fold4: Average Loss:           766074.2473, Accuracy: 6557/7495           (87%)\n",
      "Training stage for Flod 4 Epoch: 45 [0/29981                 (0%)]\tLoss: 0.346216\n",
      "Training stage for Flod 4 Epoch: 45 [3200/29981                 (11%)]\tLoss: 0.445902\n",
      "Training stage for Flod 4 Epoch: 45 [6400/29981                 (21%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 45 [9600/29981                 (32%)]\tLoss: 0.438761\n",
      "Training stage for Flod 4 Epoch: 45 [12800/29981                 (43%)]\tLoss: 0.425310\n",
      "Training stage for Flod 4 Epoch: 45 [16000/29981                 (53%)]\tLoss: 0.313275\n",
      "Training stage for Flod 4 Epoch: 45 [19200/29981                 (64%)]\tLoss: 0.438237\n",
      "Training stage for Flod 4 Epoch: 45 [22400/29981                 (75%)]\tLoss: 0.447896\n",
      "Training stage for Flod 4 Epoch: 45 [25600/29981                 (85%)]\tLoss: 0.353345\n",
      "Training stage for Flod 4 Epoch: 45 [28800/29981                 (96%)]\tLoss: 0.439033\n",
      "Test set for fold4: Average Loss:           729123.4133, Accuracy: 6712/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 46 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 46 [3200/29981                 (11%)]\tLoss: 0.411182\n",
      "Training stage for Flod 4 Epoch: 46 [6400/29981                 (21%)]\tLoss: 0.322220\n",
      "Training stage for Flod 4 Epoch: 46 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 46 [12800/29981                 (43%)]\tLoss: 0.375768\n",
      "Training stage for Flod 4 Epoch: 46 [16000/29981                 (53%)]\tLoss: 0.376475\n",
      "Training stage for Flod 4 Epoch: 46 [19200/29981                 (64%)]\tLoss: 0.347910\n",
      "Training stage for Flod 4 Epoch: 46 [22400/29981                 (75%)]\tLoss: 0.375626\n",
      "Training stage for Flod 4 Epoch: 46 [25600/29981                 (85%)]\tLoss: 0.423067\n",
      "Training stage for Flod 4 Epoch: 46 [28800/29981                 (96%)]\tLoss: 0.437788\n",
      "Test set for fold4: Average Loss:           759613.5277, Accuracy: 6588/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 47 [0/29981                 (0%)]\tLoss: 0.355553\n",
      "Training stage for Flod 4 Epoch: 47 [3200/29981                 (11%)]\tLoss: 0.438309\n",
      "Training stage for Flod 4 Epoch: 47 [6400/29981                 (21%)]\tLoss: 0.437816\n",
      "Training stage for Flod 4 Epoch: 47 [9600/29981                 (32%)]\tLoss: 0.440027\n",
      "Training stage for Flod 4 Epoch: 47 [12800/29981                 (43%)]\tLoss: 0.406935\n",
      "Training stage for Flod 4 Epoch: 47 [16000/29981                 (53%)]\tLoss: 0.366149\n",
      "Training stage for Flod 4 Epoch: 47 [19200/29981                 (64%)]\tLoss: 0.374981\n",
      "Training stage for Flod 4 Epoch: 47 [22400/29981                 (75%)]\tLoss: 0.363269\n",
      "Training stage for Flod 4 Epoch: 47 [25600/29981                 (85%)]\tLoss: 0.345487\n",
      "Training stage for Flod 4 Epoch: 47 [28800/29981                 (96%)]\tLoss: 0.313910\n",
      "Test set for fold4: Average Loss:           720293.7424, Accuracy: 6769/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 48 [0/29981                 (0%)]\tLoss: 0.375777\n",
      "Training stage for Flod 4 Epoch: 48 [3200/29981                 (11%)]\tLoss: 0.468364\n",
      "Training stage for Flod 4 Epoch: 48 [6400/29981                 (21%)]\tLoss: 0.374676\n",
      "Training stage for Flod 4 Epoch: 48 [9600/29981                 (32%)]\tLoss: 0.377940\n",
      "Training stage for Flod 4 Epoch: 48 [12800/29981                 (43%)]\tLoss: 0.406977\n",
      "Training stage for Flod 4 Epoch: 48 [16000/29981                 (53%)]\tLoss: 0.401547\n",
      "Training stage for Flod 4 Epoch: 48 [19200/29981                 (64%)]\tLoss: 0.341801\n",
      "Training stage for Flod 4 Epoch: 48 [22400/29981                 (75%)]\tLoss: 0.406755\n",
      "Training stage for Flod 4 Epoch: 48 [25600/29981                 (85%)]\tLoss: 0.378356\n",
      "Training stage for Flod 4 Epoch: 48 [28800/29981                 (96%)]\tLoss: 0.376469\n",
      "Test set for fold4: Average Loss:           723091.2853, Accuracy: 6734/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 49 [0/29981                 (0%)]\tLoss: 0.375763\n",
      "Training stage for Flod 4 Epoch: 49 [3200/29981                 (11%)]\tLoss: 0.344584\n",
      "Training stage for Flod 4 Epoch: 49 [6400/29981                 (21%)]\tLoss: 0.313417\n",
      "Training stage for Flod 4 Epoch: 49 [9600/29981                 (32%)]\tLoss: 0.313523\n",
      "Training stage for Flod 4 Epoch: 49 [12800/29981                 (43%)]\tLoss: 0.378027\n",
      "Training stage for Flod 4 Epoch: 49 [16000/29981                 (53%)]\tLoss: 0.440890\n",
      "Training stage for Flod 4 Epoch: 49 [19200/29981                 (64%)]\tLoss: 0.360925\n",
      "Training stage for Flod 4 Epoch: 49 [22400/29981                 (75%)]\tLoss: 0.344520\n",
      "Training stage for Flod 4 Epoch: 49 [25600/29981                 (85%)]\tLoss: 0.410537\n",
      "Training stage for Flod 4 Epoch: 49 [28800/29981                 (96%)]\tLoss: 0.318332\n",
      "Test set for fold4: Average Loss:           724877.2520, Accuracy: 6722/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 50 [0/29981                 (0%)]\tLoss: 0.344707\n",
      "Training stage for Flod 4 Epoch: 50 [3200/29981                 (11%)]\tLoss: 0.352248\n",
      "Training stage for Flod 4 Epoch: 50 [6400/29981                 (21%)]\tLoss: 0.407896\n",
      "Training stage for Flod 4 Epoch: 50 [9600/29981                 (32%)]\tLoss: 0.344523\n",
      "Training stage for Flod 4 Epoch: 50 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 50 [16000/29981                 (53%)]\tLoss: 0.375772\n",
      "Training stage for Flod 4 Epoch: 50 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 50 [22400/29981                 (75%)]\tLoss: 0.407047\n",
      "Training stage for Flod 4 Epoch: 50 [25600/29981                 (85%)]\tLoss: 0.406691\n",
      "Training stage for Flod 4 Epoch: 50 [28800/29981                 (96%)]\tLoss: 0.375771\n",
      "Test set for fold4: Average Loss:           742474.6922, Accuracy: 6638/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 51 [0/29981                 (0%)]\tLoss: 0.313406\n",
      "Training stage for Flod 4 Epoch: 51 [3200/29981                 (11%)]\tLoss: 0.445959\n",
      "Training stage for Flod 4 Epoch: 51 [6400/29981                 (21%)]\tLoss: 0.344537\n",
      "Training stage for Flod 4 Epoch: 51 [9600/29981                 (32%)]\tLoss: 0.344563\n",
      "Training stage for Flod 4 Epoch: 51 [12800/29981                 (43%)]\tLoss: 0.414920\n",
      "Training stage for Flod 4 Epoch: 51 [16000/29981                 (53%)]\tLoss: 0.407224\n",
      "Training stage for Flod 4 Epoch: 51 [19200/29981                 (64%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 51 [22400/29981                 (75%)]\tLoss: 0.375930\n",
      "Training stage for Flod 4 Epoch: 51 [25600/29981                 (85%)]\tLoss: 0.375936\n",
      "Training stage for Flod 4 Epoch: 51 [28800/29981                 (96%)]\tLoss: 0.376550\n",
      "Test set for fold4: Average Loss:           770647.5091, Accuracy: 6526/7495           (87%)\n",
      "Training stage for Flod 4 Epoch: 52 [0/29981                 (0%)]\tLoss: 0.447855\n",
      "Training stage for Flod 4 Epoch: 52 [3200/29981                 (11%)]\tLoss: 0.395443\n",
      "Training stage for Flod 4 Epoch: 52 [6400/29981                 (21%)]\tLoss: 0.407010\n",
      "Training stage for Flod 4 Epoch: 52 [9600/29981                 (32%)]\tLoss: 0.348902\n",
      "Training stage for Flod 4 Epoch: 52 [12800/29981                 (43%)]\tLoss: 0.320325\n",
      "Training stage for Flod 4 Epoch: 52 [16000/29981                 (53%)]\tLoss: 0.395037\n",
      "Training stage for Flod 4 Epoch: 52 [19200/29981                 (64%)]\tLoss: 0.378659\n",
      "Training stage for Flod 4 Epoch: 52 [22400/29981                 (75%)]\tLoss: 0.430796\n",
      "Training stage for Flod 4 Epoch: 52 [25600/29981                 (85%)]\tLoss: 0.382991\n",
      "Training stage for Flod 4 Epoch: 52 [28800/29981                 (96%)]\tLoss: 0.355463\n",
      "Test set for fold4: Average Loss:           743382.8127, Accuracy: 6659/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 53 [0/29981                 (0%)]\tLoss: 0.390357\n",
      "Training stage for Flod 4 Epoch: 53 [3200/29981                 (11%)]\tLoss: 0.313269\n",
      "Training stage for Flod 4 Epoch: 53 [6400/29981                 (21%)]\tLoss: 0.375944\n",
      "Training stage for Flod 4 Epoch: 53 [9600/29981                 (32%)]\tLoss: 0.409782\n",
      "Training stage for Flod 4 Epoch: 53 [12800/29981                 (43%)]\tLoss: 0.375757\n",
      "Training stage for Flod 4 Epoch: 53 [16000/29981                 (53%)]\tLoss: 0.402747\n",
      "Training stage for Flod 4 Epoch: 53 [19200/29981                 (64%)]\tLoss: 0.419662\n",
      "Training stage for Flod 4 Epoch: 53 [22400/29981                 (75%)]\tLoss: 0.314200\n",
      "Training stage for Flod 4 Epoch: 53 [25600/29981                 (85%)]\tLoss: 0.407331\n",
      "Training stage for Flod 4 Epoch: 53 [28800/29981                 (96%)]\tLoss: 0.313416\n",
      "Test set for fold4: Average Loss:           725773.4956, Accuracy: 6738/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 54 [0/29981                 (0%)]\tLoss: 0.404610\n",
      "Training stage for Flod 4 Epoch: 54 [3200/29981                 (11%)]\tLoss: 0.375769\n",
      "Training stage for Flod 4 Epoch: 54 [6400/29981                 (21%)]\tLoss: 0.345825\n",
      "Training stage for Flod 4 Epoch: 54 [9600/29981                 (32%)]\tLoss: 0.313264\n",
      "Training stage for Flod 4 Epoch: 54 [12800/29981                 (43%)]\tLoss: 0.473677\n",
      "Training stage for Flod 4 Epoch: 54 [16000/29981                 (53%)]\tLoss: 0.380739\n",
      "Training stage for Flod 4 Epoch: 54 [19200/29981                 (64%)]\tLoss: 0.371086\n",
      "Training stage for Flod 4 Epoch: 54 [22400/29981                 (75%)]\tLoss: 0.378728\n",
      "Training stage for Flod 4 Epoch: 54 [25600/29981                 (85%)]\tLoss: 0.429570\n",
      "Training stage for Flod 4 Epoch: 54 [28800/29981                 (96%)]\tLoss: 0.374117\n",
      "Test set for fold4: Average Loss:           735502.4428, Accuracy: 6679/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 55 [0/29981                 (0%)]\tLoss: 0.345883\n",
      "Training stage for Flod 4 Epoch: 55 [3200/29981                 (11%)]\tLoss: 0.357643\n",
      "Training stage for Flod 4 Epoch: 55 [6400/29981                 (21%)]\tLoss: 0.344622\n",
      "Training stage for Flod 4 Epoch: 55 [9600/29981                 (32%)]\tLoss: 0.327908\n",
      "Training stage for Flod 4 Epoch: 55 [12800/29981                 (43%)]\tLoss: 0.374389\n",
      "Training stage for Flod 4 Epoch: 55 [16000/29981                 (53%)]\tLoss: 0.344576\n",
      "Training stage for Flod 4 Epoch: 55 [19200/29981                 (64%)]\tLoss: 0.354582\n",
      "Training stage for Flod 4 Epoch: 55 [22400/29981                 (75%)]\tLoss: 0.375701\n",
      "Training stage for Flod 4 Epoch: 55 [25600/29981                 (85%)]\tLoss: 0.344603\n",
      "Training stage for Flod 4 Epoch: 55 [28800/29981                 (96%)]\tLoss: 0.373321\n",
      "Test set for fold4: Average Loss:           735337.8245, Accuracy: 6670/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 56 [0/29981                 (0%)]\tLoss: 0.375760\n",
      "Training stage for Flod 4 Epoch: 56 [3200/29981                 (11%)]\tLoss: 0.376966\n",
      "Training stage for Flod 4 Epoch: 56 [6400/29981                 (21%)]\tLoss: 0.404486\n",
      "Training stage for Flod 4 Epoch: 56 [9600/29981                 (32%)]\tLoss: 0.436251\n",
      "Training stage for Flod 4 Epoch: 56 [12800/29981                 (43%)]\tLoss: 0.315468\n",
      "Training stage for Flod 4 Epoch: 56 [16000/29981                 (53%)]\tLoss: 0.435983\n",
      "Training stage for Flod 4 Epoch: 56 [19200/29981                 (64%)]\tLoss: 0.408141\n",
      "Training stage for Flod 4 Epoch: 56 [22400/29981                 (75%)]\tLoss: 0.405265\n",
      "Training stage for Flod 4 Epoch: 56 [25600/29981                 (85%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 56 [28800/29981                 (96%)]\tLoss: 0.313308\n",
      "Test set for fold4: Average Loss:           728763.5527, Accuracy: 6708/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 57 [0/29981                 (0%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 57 [3200/29981                 (11%)]\tLoss: 0.375801\n",
      "Training stage for Flod 4 Epoch: 57 [6400/29981                 (21%)]\tLoss: 0.313280\n",
      "Training stage for Flod 4 Epoch: 57 [9600/29981                 (32%)]\tLoss: 0.437614\n",
      "Training stage for Flod 4 Epoch: 57 [12800/29981                 (43%)]\tLoss: 0.315702\n",
      "Training stage for Flod 4 Epoch: 57 [16000/29981                 (53%)]\tLoss: 0.375179\n",
      "Training stage for Flod 4 Epoch: 57 [19200/29981                 (64%)]\tLoss: 0.375938\n",
      "Training stage for Flod 4 Epoch: 57 [22400/29981                 (75%)]\tLoss: 0.408076\n",
      "Training stage for Flod 4 Epoch: 57 [25600/29981                 (85%)]\tLoss: 0.346092\n",
      "Training stage for Flod 4 Epoch: 57 [28800/29981                 (96%)]\tLoss: 0.376524\n",
      "Test set for fold4: Average Loss:           721828.2940, Accuracy: 6720/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 58 [0/29981                 (0%)]\tLoss: 0.344642\n",
      "Training stage for Flod 4 Epoch: 58 [3200/29981                 (11%)]\tLoss: 0.376526\n",
      "Training stage for Flod 4 Epoch: 58 [6400/29981                 (21%)]\tLoss: 0.313295\n",
      "Training stage for Flod 4 Epoch: 58 [9600/29981                 (32%)]\tLoss: 0.344908\n",
      "Training stage for Flod 4 Epoch: 58 [12800/29981                 (43%)]\tLoss: 0.431833\n",
      "Training stage for Flod 4 Epoch: 58 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 58 [19200/29981                 (64%)]\tLoss: 0.344587\n",
      "Training stage for Flod 4 Epoch: 58 [22400/29981                 (75%)]\tLoss: 0.387188\n",
      "Training stage for Flod 4 Epoch: 58 [25600/29981                 (85%)]\tLoss: 0.414710\n",
      "Training stage for Flod 4 Epoch: 58 [28800/29981                 (96%)]\tLoss: 0.375458\n",
      "Test set for fold4: Average Loss:           755475.3501, Accuracy: 6588/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 59 [0/29981                 (0%)]\tLoss: 0.321699\n",
      "Training stage for Flod 4 Epoch: 59 [3200/29981                 (11%)]\tLoss: 0.313280\n",
      "Training stage for Flod 4 Epoch: 59 [6400/29981                 (21%)]\tLoss: 0.374307\n",
      "Training stage for Flod 4 Epoch: 59 [9600/29981                 (32%)]\tLoss: 0.445639\n",
      "Training stage for Flod 4 Epoch: 59 [12800/29981                 (43%)]\tLoss: 0.397819\n",
      "Training stage for Flod 4 Epoch: 59 [16000/29981                 (53%)]\tLoss: 0.344843\n",
      "Training stage for Flod 4 Epoch: 59 [19200/29981                 (64%)]\tLoss: 0.407014\n",
      "Training stage for Flod 4 Epoch: 59 [22400/29981                 (75%)]\tLoss: 0.345577\n",
      "Training stage for Flod 4 Epoch: 59 [25600/29981                 (85%)]\tLoss: 0.313340\n",
      "Training stage for Flod 4 Epoch: 59 [28800/29981                 (96%)]\tLoss: 0.342684\n",
      "Test set for fold4: Average Loss:           734823.4308, Accuracy: 6695/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 60 [0/29981                 (0%)]\tLoss: 0.346941\n",
      "Training stage for Flod 4 Epoch: 60 [3200/29981                 (11%)]\tLoss: 0.347251\n",
      "Training stage for Flod 4 Epoch: 60 [6400/29981                 (21%)]\tLoss: 0.344514\n",
      "Training stage for Flod 4 Epoch: 60 [9600/29981                 (32%)]\tLoss: 0.344516\n",
      "Training stage for Flod 4 Epoch: 60 [12800/29981                 (43%)]\tLoss: 0.344513\n",
      "Training stage for Flod 4 Epoch: 60 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 60 [19200/29981                 (64%)]\tLoss: 0.379833\n",
      "Training stage for Flod 4 Epoch: 60 [22400/29981                 (75%)]\tLoss: 0.376159\n",
      "Training stage for Flod 4 Epoch: 60 [25600/29981                 (85%)]\tLoss: 0.407016\n",
      "Training stage for Flod 4 Epoch: 60 [28800/29981                 (96%)]\tLoss: 0.319887\n",
      "Test set for fold4: Average Loss:           704358.7749, Accuracy: 6812/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 61 [0/29981                 (0%)]\tLoss: 0.348360\n",
      "Training stage for Flod 4 Epoch: 61 [3200/29981                 (11%)]\tLoss: 0.313393\n",
      "Training stage for Flod 4 Epoch: 61 [6400/29981                 (21%)]\tLoss: 0.373955\n",
      "Training stage for Flod 4 Epoch: 61 [9600/29981                 (32%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 61 [12800/29981                 (43%)]\tLoss: 0.375867\n",
      "Training stage for Flod 4 Epoch: 61 [16000/29981                 (53%)]\tLoss: 0.322749\n",
      "Training stage for Flod 4 Epoch: 61 [19200/29981                 (64%)]\tLoss: 0.339270\n",
      "Training stage for Flod 4 Epoch: 61 [22400/29981                 (75%)]\tLoss: 0.346986\n",
      "Training stage for Flod 4 Epoch: 61 [25600/29981                 (85%)]\tLoss: 0.396046\n",
      "Training stage for Flod 4 Epoch: 61 [28800/29981                 (96%)]\tLoss: 0.408727\n",
      "Test set for fold4: Average Loss:           716887.7762, Accuracy: 6768/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 62 [0/29981                 (0%)]\tLoss: 0.438433\n",
      "Training stage for Flod 4 Epoch: 62 [3200/29981                 (11%)]\tLoss: 0.344978\n",
      "Training stage for Flod 4 Epoch: 62 [6400/29981                 (21%)]\tLoss: 0.379706\n",
      "Training stage for Flod 4 Epoch: 62 [9600/29981                 (32%)]\tLoss: 0.315274\n",
      "Training stage for Flod 4 Epoch: 62 [12800/29981                 (43%)]\tLoss: 0.345134\n",
      "Training stage for Flod 4 Epoch: 62 [16000/29981                 (53%)]\tLoss: 0.407493\n",
      "Training stage for Flod 4 Epoch: 62 [19200/29981                 (64%)]\tLoss: 0.348067\n",
      "Training stage for Flod 4 Epoch: 62 [22400/29981                 (75%)]\tLoss: 0.341919\n",
      "Training stage for Flod 4 Epoch: 62 [25600/29981                 (85%)]\tLoss: 0.357441\n",
      "Training stage for Flod 4 Epoch: 62 [28800/29981                 (96%)]\tLoss: 0.429094\n",
      "Test set for fold4: Average Loss:           720227.9791, Accuracy: 6741/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 63 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 63 [3200/29981                 (11%)]\tLoss: 0.344332\n",
      "Training stage for Flod 4 Epoch: 63 [6400/29981                 (21%)]\tLoss: 0.346976\n",
      "Training stage for Flod 4 Epoch: 63 [9600/29981                 (32%)]\tLoss: 0.351270\n",
      "Training stage for Flod 4 Epoch: 63 [12800/29981                 (43%)]\tLoss: 0.386550\n",
      "Training stage for Flod 4 Epoch: 63 [16000/29981                 (53%)]\tLoss: 0.313335\n",
      "Training stage for Flod 4 Epoch: 63 [19200/29981                 (64%)]\tLoss: 0.375773\n",
      "Training stage for Flod 4 Epoch: 63 [22400/29981                 (75%)]\tLoss: 0.313300\n",
      "Training stage for Flod 4 Epoch: 63 [25600/29981                 (85%)]\tLoss: 0.375812\n",
      "Training stage for Flod 4 Epoch: 63 [28800/29981                 (96%)]\tLoss: 0.406704\n",
      "Test set for fold4: Average Loss:           716074.2231, Accuracy: 6782/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 64 [0/29981                 (0%)]\tLoss: 0.346126\n",
      "Training stage for Flod 4 Epoch: 64 [3200/29981                 (11%)]\tLoss: 0.314216\n",
      "Training stage for Flod 4 Epoch: 64 [6400/29981                 (21%)]\tLoss: 0.410316\n",
      "Training stage for Flod 4 Epoch: 64 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 64 [12800/29981                 (43%)]\tLoss: 0.346363\n",
      "Training stage for Flod 4 Epoch: 64 [16000/29981                 (53%)]\tLoss: 0.344541\n",
      "Training stage for Flod 4 Epoch: 64 [19200/29981                 (64%)]\tLoss: 0.345551\n",
      "Training stage for Flod 4 Epoch: 64 [22400/29981                 (75%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 64 [25600/29981                 (85%)]\tLoss: 0.376379\n",
      "Training stage for Flod 4 Epoch: 64 [28800/29981                 (96%)]\tLoss: 0.407061\n",
      "Test set for fold4: Average Loss:           725453.0135, Accuracy: 6716/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 65 [0/29981                 (0%)]\tLoss: 0.466053\n",
      "Training stage for Flod 4 Epoch: 65 [3200/29981                 (11%)]\tLoss: 0.345393\n",
      "Training stage for Flod 4 Epoch: 65 [6400/29981                 (21%)]\tLoss: 0.345296\n",
      "Training stage for Flod 4 Epoch: 65 [9600/29981                 (32%)]\tLoss: 0.375900\n",
      "Training stage for Flod 4 Epoch: 65 [12800/29981                 (43%)]\tLoss: 0.404814\n",
      "Training stage for Flod 4 Epoch: 65 [16000/29981                 (53%)]\tLoss: 0.375764\n",
      "Training stage for Flod 4 Epoch: 65 [19200/29981                 (64%)]\tLoss: 0.358730\n",
      "Training stage for Flod 4 Epoch: 65 [22400/29981                 (75%)]\tLoss: 0.344544\n",
      "Training stage for Flod 4 Epoch: 65 [25600/29981                 (85%)]\tLoss: 0.376954\n",
      "Training stage for Flod 4 Epoch: 65 [28800/29981                 (96%)]\tLoss: 0.381011\n",
      "Test set for fold4: Average Loss:           718191.7739, Accuracy: 6756/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 66 [0/29981                 (0%)]\tLoss: 0.408205\n",
      "Training stage for Flod 4 Epoch: 66 [3200/29981                 (11%)]\tLoss: 0.407935\n",
      "Training stage for Flod 4 Epoch: 66 [6400/29981                 (21%)]\tLoss: 0.313264\n",
      "Training stage for Flod 4 Epoch: 66 [9600/29981                 (32%)]\tLoss: 0.395834\n",
      "Training stage for Flod 4 Epoch: 66 [12800/29981                 (43%)]\tLoss: 0.375764\n",
      "Training stage for Flod 4 Epoch: 66 [16000/29981                 (53%)]\tLoss: 0.334764\n",
      "Training stage for Flod 4 Epoch: 66 [19200/29981                 (64%)]\tLoss: 0.319310\n",
      "Training stage for Flod 4 Epoch: 66 [22400/29981                 (75%)]\tLoss: 0.375832\n",
      "Training stage for Flod 4 Epoch: 66 [25600/29981                 (85%)]\tLoss: 0.375767\n",
      "Training stage for Flod 4 Epoch: 66 [28800/29981                 (96%)]\tLoss: 0.410351\n",
      "Test set for fold4: Average Loss:           727985.1824, Accuracy: 6712/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 67 [0/29981                 (0%)]\tLoss: 0.344839\n",
      "Training stage for Flod 4 Epoch: 67 [3200/29981                 (11%)]\tLoss: 0.314474\n",
      "Training stage for Flod 4 Epoch: 67 [6400/29981                 (21%)]\tLoss: 0.318646\n",
      "Training stage for Flod 4 Epoch: 67 [9600/29981                 (32%)]\tLoss: 0.344517\n",
      "Training stage for Flod 4 Epoch: 67 [12800/29981                 (43%)]\tLoss: 0.398972\n",
      "Training stage for Flod 4 Epoch: 67 [16000/29981                 (53%)]\tLoss: 0.368492\n",
      "Training stage for Flod 4 Epoch: 67 [19200/29981                 (64%)]\tLoss: 0.396611\n",
      "Training stage for Flod 4 Epoch: 67 [22400/29981                 (75%)]\tLoss: 0.344001\n",
      "Training stage for Flod 4 Epoch: 67 [25600/29981                 (85%)]\tLoss: 0.344558\n",
      "Training stage for Flod 4 Epoch: 67 [28800/29981                 (96%)]\tLoss: 0.407289\n",
      "Test set for fold4: Average Loss:           724650.6666, Accuracy: 6740/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 68 [0/29981                 (0%)]\tLoss: 0.396004\n",
      "Training stage for Flod 4 Epoch: 68 [3200/29981                 (11%)]\tLoss: 0.467346\n",
      "Training stage for Flod 4 Epoch: 68 [6400/29981                 (21%)]\tLoss: 0.344518\n",
      "Training stage for Flod 4 Epoch: 68 [9600/29981                 (32%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 68 [12800/29981                 (43%)]\tLoss: 0.383963\n",
      "Training stage for Flod 4 Epoch: 68 [16000/29981                 (53%)]\tLoss: 0.345549\n",
      "Training stage for Flod 4 Epoch: 68 [19200/29981                 (64%)]\tLoss: 0.375793\n",
      "Training stage for Flod 4 Epoch: 68 [22400/29981                 (75%)]\tLoss: 0.313280\n",
      "Training stage for Flod 4 Epoch: 68 [25600/29981                 (85%)]\tLoss: 0.395268\n",
      "Training stage for Flod 4 Epoch: 68 [28800/29981                 (96%)]\tLoss: 0.313315\n",
      "Test set for fold4: Average Loss:           749792.5798, Accuracy: 6627/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 69 [0/29981                 (0%)]\tLoss: 0.375719\n",
      "Training stage for Flod 4 Epoch: 69 [3200/29981                 (11%)]\tLoss: 0.378118\n",
      "Training stage for Flod 4 Epoch: 69 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 69 [9600/29981                 (32%)]\tLoss: 0.403080\n",
      "Training stage for Flod 4 Epoch: 69 [12800/29981                 (43%)]\tLoss: 0.376694\n",
      "Training stage for Flod 4 Epoch: 69 [16000/29981                 (53%)]\tLoss: 0.344595\n",
      "Training stage for Flod 4 Epoch: 69 [19200/29981                 (64%)]\tLoss: 0.313263\n",
      "Training stage for Flod 4 Epoch: 69 [22400/29981                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 69 [25600/29981                 (85%)]\tLoss: 0.375740\n",
      "Training stage for Flod 4 Epoch: 69 [28800/29981                 (96%)]\tLoss: 0.451967\n",
      "Test set for fold4: Average Loss:           729591.2023, Accuracy: 6708/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 70 [0/29981                 (0%)]\tLoss: 0.366360\n",
      "Training stage for Flod 4 Epoch: 70 [3200/29981                 (11%)]\tLoss: 0.407376\n",
      "Training stage for Flod 4 Epoch: 70 [6400/29981                 (21%)]\tLoss: 0.352577\n",
      "Training stage for Flod 4 Epoch: 70 [9600/29981                 (32%)]\tLoss: 0.475011\n",
      "Training stage for Flod 4 Epoch: 70 [12800/29981                 (43%)]\tLoss: 0.437979\n",
      "Training stage for Flod 4 Epoch: 70 [16000/29981                 (53%)]\tLoss: 0.374995\n",
      "Training stage for Flod 4 Epoch: 70 [19200/29981                 (64%)]\tLoss: 0.343281\n",
      "Training stage for Flod 4 Epoch: 70 [22400/29981                 (75%)]\tLoss: 0.378350\n",
      "Training stage for Flod 4 Epoch: 70 [25600/29981                 (85%)]\tLoss: 0.472288\n",
      "Training stage for Flod 4 Epoch: 70 [28800/29981                 (96%)]\tLoss: 0.367039\n",
      "Test set for fold4: Average Loss:           705438.7190, Accuracy: 6821/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 71 [0/29981                 (0%)]\tLoss: 0.367807\n",
      "Training stage for Flod 4 Epoch: 71 [3200/29981                 (11%)]\tLoss: 0.316511\n",
      "Training stage for Flod 4 Epoch: 71 [6400/29981                 (21%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 71 [9600/29981                 (32%)]\tLoss: 0.331511\n",
      "Training stage for Flod 4 Epoch: 71 [12800/29981                 (43%)]\tLoss: 0.374573\n",
      "Training stage for Flod 4 Epoch: 71 [16000/29981                 (53%)]\tLoss: 0.469981\n",
      "Training stage for Flod 4 Epoch: 71 [19200/29981                 (64%)]\tLoss: 0.313267\n",
      "Training stage for Flod 4 Epoch: 71 [22400/29981                 (75%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 71 [25600/29981                 (85%)]\tLoss: 0.336477\n",
      "Training stage for Flod 4 Epoch: 71 [28800/29981                 (96%)]\tLoss: 0.313640\n",
      "Test set for fold4: Average Loss:           729458.7317, Accuracy: 6713/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 72 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 72 [3200/29981                 (11%)]\tLoss: 0.344733\n",
      "Training stage for Flod 4 Epoch: 72 [6400/29981                 (21%)]\tLoss: 0.344604\n",
      "Training stage for Flod 4 Epoch: 72 [9600/29981                 (32%)]\tLoss: 0.438201\n",
      "Training stage for Flod 4 Epoch: 72 [12800/29981                 (43%)]\tLoss: 0.355139\n",
      "Training stage for Flod 4 Epoch: 72 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 72 [19200/29981                 (64%)]\tLoss: 0.377650\n",
      "Training stage for Flod 4 Epoch: 72 [22400/29981                 (75%)]\tLoss: 0.313499\n",
      "Training stage for Flod 4 Epoch: 72 [25600/29981                 (85%)]\tLoss: 0.375451\n",
      "Training stage for Flod 4 Epoch: 72 [28800/29981                 (96%)]\tLoss: 0.438262\n",
      "Test set for fold4: Average Loss:           724014.0497, Accuracy: 6755/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 73 [0/29981                 (0%)]\tLoss: 0.391736\n",
      "Training stage for Flod 4 Epoch: 73 [3200/29981                 (11%)]\tLoss: 0.375633\n",
      "Training stage for Flod 4 Epoch: 73 [6400/29981                 (21%)]\tLoss: 0.344513\n",
      "Training stage for Flod 4 Epoch: 73 [9600/29981                 (32%)]\tLoss: 0.414142\n",
      "Training stage for Flod 4 Epoch: 73 [12800/29981                 (43%)]\tLoss: 0.411988\n",
      "Training stage for Flod 4 Epoch: 73 [16000/29981                 (53%)]\tLoss: 0.347404\n",
      "Training stage for Flod 4 Epoch: 73 [19200/29981                 (64%)]\tLoss: 0.375806\n",
      "Training stage for Flod 4 Epoch: 73 [22400/29981                 (75%)]\tLoss: 0.344612\n",
      "Training stage for Flod 4 Epoch: 73 [25600/29981                 (85%)]\tLoss: 0.343078\n",
      "Training stage for Flod 4 Epoch: 73 [28800/29981                 (96%)]\tLoss: 0.375873\n",
      "Test set for fold4: Average Loss:           721628.3668, Accuracy: 6745/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 74 [0/29981                 (0%)]\tLoss: 0.463755\n",
      "Training stage for Flod 4 Epoch: 74 [3200/29981                 (11%)]\tLoss: 0.443299\n",
      "Training stage for Flod 4 Epoch: 74 [6400/29981                 (21%)]\tLoss: 0.379959\n",
      "Training stage for Flod 4 Epoch: 74 [9600/29981                 (32%)]\tLoss: 0.469937\n",
      "Training stage for Flod 4 Epoch: 74 [12800/29981                 (43%)]\tLoss: 0.395348\n",
      "Training stage for Flod 4 Epoch: 74 [16000/29981                 (53%)]\tLoss: 0.375764\n",
      "Training stage for Flod 4 Epoch: 74 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 74 [22400/29981                 (75%)]\tLoss: 0.371684\n",
      "Training stage for Flod 4 Epoch: 74 [25600/29981                 (85%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 74 [28800/29981                 (96%)]\tLoss: 0.344945\n",
      "Test set for fold4: Average Loss:           731003.8936, Accuracy: 6685/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 75 [0/29981                 (0%)]\tLoss: 0.344517\n",
      "Training stage for Flod 4 Epoch: 75 [3200/29981                 (11%)]\tLoss: 0.344514\n",
      "Training stage for Flod 4 Epoch: 75 [6400/29981                 (21%)]\tLoss: 0.314082\n",
      "Training stage for Flod 4 Epoch: 75 [9600/29981                 (32%)]\tLoss: 0.313299\n",
      "Training stage for Flod 4 Epoch: 75 [12800/29981                 (43%)]\tLoss: 0.376598\n",
      "Training stage for Flod 4 Epoch: 75 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 75 [19200/29981                 (64%)]\tLoss: 0.345782\n",
      "Training stage for Flod 4 Epoch: 75 [22400/29981                 (75%)]\tLoss: 0.349974\n",
      "Training stage for Flod 4 Epoch: 75 [25600/29981                 (85%)]\tLoss: 0.434764\n",
      "Training stage for Flod 4 Epoch: 75 [28800/29981                 (96%)]\tLoss: 0.313851\n",
      "Test set for fold4: Average Loss:           718203.1921, Accuracy: 6777/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 76 [0/29981                 (0%)]\tLoss: 0.336104\n",
      "Training stage for Flod 4 Epoch: 76 [3200/29981                 (11%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 76 [6400/29981                 (21%)]\tLoss: 0.406814\n",
      "Training stage for Flod 4 Epoch: 76 [9600/29981                 (32%)]\tLoss: 0.351631\n",
      "Training stage for Flod 4 Epoch: 76 [12800/29981                 (43%)]\tLoss: 0.344803\n",
      "Training stage for Flod 4 Epoch: 76 [16000/29981                 (53%)]\tLoss: 0.375773\n",
      "Training stage for Flod 4 Epoch: 76 [19200/29981                 (64%)]\tLoss: 0.313290\n",
      "Training stage for Flod 4 Epoch: 76 [22400/29981                 (75%)]\tLoss: 0.354157\n",
      "Training stage for Flod 4 Epoch: 76 [25600/29981                 (85%)]\tLoss: 0.313297\n",
      "Training stage for Flod 4 Epoch: 76 [28800/29981                 (96%)]\tLoss: 0.329075\n",
      "Test set for fold4: Average Loss:           717683.6466, Accuracy: 6763/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 77 [0/29981                 (0%)]\tLoss: 0.344566\n",
      "Training stage for Flod 4 Epoch: 77 [3200/29981                 (11%)]\tLoss: 0.344519\n",
      "Training stage for Flod 4 Epoch: 77 [6400/29981                 (21%)]\tLoss: 0.313583\n",
      "Training stage for Flod 4 Epoch: 77 [9600/29981                 (32%)]\tLoss: 0.409187\n",
      "Training stage for Flod 4 Epoch: 77 [12800/29981                 (43%)]\tLoss: 0.320310\n",
      "Training stage for Flod 4 Epoch: 77 [16000/29981                 (53%)]\tLoss: 0.403915\n",
      "Training stage for Flod 4 Epoch: 77 [19200/29981                 (64%)]\tLoss: 0.349456\n",
      "Training stage for Flod 4 Epoch: 77 [22400/29981                 (75%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 77 [25600/29981                 (85%)]\tLoss: 0.345860\n",
      "Training stage for Flod 4 Epoch: 77 [28800/29981                 (96%)]\tLoss: 0.375958\n",
      "Test set for fold4: Average Loss:           728547.4907, Accuracy: 6710/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 78 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 78 [3200/29981                 (11%)]\tLoss: 0.344602\n",
      "Training stage for Flod 4 Epoch: 78 [6400/29981                 (21%)]\tLoss: 0.424885\n",
      "Training stage for Flod 4 Epoch: 78 [9600/29981                 (32%)]\tLoss: 0.477539\n",
      "Training stage for Flod 4 Epoch: 78 [12800/29981                 (43%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 78 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 78 [19200/29981                 (64%)]\tLoss: 0.365202\n",
      "Training stage for Flod 4 Epoch: 78 [22400/29981                 (75%)]\tLoss: 0.344090\n",
      "Training stage for Flod 4 Epoch: 78 [25600/29981                 (85%)]\tLoss: 0.344950\n",
      "Training stage for Flod 4 Epoch: 78 [28800/29981                 (96%)]\tLoss: 0.313265\n",
      "Test set for fold4: Average Loss:           717662.7826, Accuracy: 6755/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 79 [0/29981                 (0%)]\tLoss: 0.375714\n",
      "Training stage for Flod 4 Epoch: 79 [3200/29981                 (11%)]\tLoss: 0.344419\n",
      "Training stage for Flod 4 Epoch: 79 [6400/29981                 (21%)]\tLoss: 0.313335\n",
      "Training stage for Flod 4 Epoch: 79 [9600/29981                 (32%)]\tLoss: 0.462791\n",
      "Training stage for Flod 4 Epoch: 79 [12800/29981                 (43%)]\tLoss: 0.375719\n",
      "Training stage for Flod 4 Epoch: 79 [16000/29981                 (53%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 79 [19200/29981                 (64%)]\tLoss: 0.313662\n",
      "Training stage for Flod 4 Epoch: 79 [22400/29981                 (75%)]\tLoss: 0.356171\n",
      "Training stage for Flod 4 Epoch: 79 [25600/29981                 (85%)]\tLoss: 0.407065\n",
      "Training stage for Flod 4 Epoch: 79 [28800/29981                 (96%)]\tLoss: 0.514639\n",
      "Test set for fold4: Average Loss:           719616.5354, Accuracy: 6757/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 80 [0/29981                 (0%)]\tLoss: 0.396596\n",
      "Training stage for Flod 4 Epoch: 80 [3200/29981                 (11%)]\tLoss: 0.366584\n",
      "Training stage for Flod 4 Epoch: 80 [6400/29981                 (21%)]\tLoss: 0.375896\n",
      "Training stage for Flod 4 Epoch: 80 [9600/29981                 (32%)]\tLoss: 0.344518\n",
      "Training stage for Flod 4 Epoch: 80 [12800/29981                 (43%)]\tLoss: 0.344528\n",
      "Training stage for Flod 4 Epoch: 80 [16000/29981                 (53%)]\tLoss: 0.407316\n",
      "Training stage for Flod 4 Epoch: 80 [19200/29981                 (64%)]\tLoss: 0.313277\n",
      "Training stage for Flod 4 Epoch: 80 [22400/29981                 (75%)]\tLoss: 0.317068\n",
      "Training stage for Flod 4 Epoch: 80 [25600/29981                 (85%)]\tLoss: 0.344515\n",
      "Training stage for Flod 4 Epoch: 80 [28800/29981                 (96%)]\tLoss: 0.438154\n",
      "Test set for fold4: Average Loss:           720667.2510, Accuracy: 6743/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 81 [0/29981                 (0%)]\tLoss: 0.345658\n",
      "Training stage for Flod 4 Epoch: 81 [3200/29981                 (11%)]\tLoss: 0.313304\n",
      "Training stage for Flod 4 Epoch: 81 [6400/29981                 (21%)]\tLoss: 0.315335\n",
      "Training stage for Flod 4 Epoch: 81 [9600/29981                 (32%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 81 [12800/29981                 (43%)]\tLoss: 0.392308\n",
      "Training stage for Flod 4 Epoch: 81 [16000/29981                 (53%)]\tLoss: 0.406618\n",
      "Training stage for Flod 4 Epoch: 81 [19200/29981                 (64%)]\tLoss: 0.336354\n",
      "Training stage for Flod 4 Epoch: 81 [22400/29981                 (75%)]\tLoss: 0.317118\n",
      "Training stage for Flod 4 Epoch: 81 [25600/29981                 (85%)]\tLoss: 0.375448\n",
      "Training stage for Flod 4 Epoch: 81 [28800/29981                 (96%)]\tLoss: 0.313263\n",
      "Test set for fold4: Average Loss:           742805.2593, Accuracy: 6647/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 82 [0/29981                 (0%)]\tLoss: 0.403916\n",
      "Training stage for Flod 4 Epoch: 82 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 82 [6400/29981                 (21%)]\tLoss: 0.313269\n",
      "Training stage for Flod 4 Epoch: 82 [9600/29981                 (32%)]\tLoss: 0.344987\n",
      "Training stage for Flod 4 Epoch: 82 [12800/29981                 (43%)]\tLoss: 0.376168\n",
      "Training stage for Flod 4 Epoch: 82 [16000/29981                 (53%)]\tLoss: 0.473410\n",
      "Training stage for Flod 4 Epoch: 82 [19200/29981                 (64%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 82 [22400/29981                 (75%)]\tLoss: 0.376114\n",
      "Training stage for Flod 4 Epoch: 82 [25600/29981                 (85%)]\tLoss: 0.375808\n",
      "Training stage for Flod 4 Epoch: 82 [28800/29981                 (96%)]\tLoss: 0.313594\n",
      "Test set for fold4: Average Loss:           758900.3519, Accuracy: 6589/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 83 [0/29981                 (0%)]\tLoss: 0.417483\n",
      "Training stage for Flod 4 Epoch: 83 [3200/29981                 (11%)]\tLoss: 0.469686\n",
      "Training stage for Flod 4 Epoch: 83 [6400/29981                 (21%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 83 [9600/29981                 (32%)]\tLoss: 0.313270\n",
      "Training stage for Flod 4 Epoch: 83 [12800/29981                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 83 [16000/29981                 (53%)]\tLoss: 0.406265\n",
      "Training stage for Flod 4 Epoch: 83 [19200/29981                 (64%)]\tLoss: 0.407150\n",
      "Training stage for Flod 4 Epoch: 83 [22400/29981                 (75%)]\tLoss: 0.376553\n",
      "Training stage for Flod 4 Epoch: 83 [25600/29981                 (85%)]\tLoss: 0.344781\n",
      "Training stage for Flod 4 Epoch: 83 [28800/29981                 (96%)]\tLoss: 0.385107\n",
      "Test set for fold4: Average Loss:           757534.2835, Accuracy: 6573/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 84 [0/29981                 (0%)]\tLoss: 0.375969\n",
      "Training stage for Flod 4 Epoch: 84 [3200/29981                 (11%)]\tLoss: 0.353509\n",
      "Training stage for Flod 4 Epoch: 84 [6400/29981                 (21%)]\tLoss: 0.494127\n",
      "Training stage for Flod 4 Epoch: 84 [9600/29981                 (32%)]\tLoss: 0.441513\n",
      "Training stage for Flod 4 Epoch: 84 [12800/29981                 (43%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 84 [16000/29981                 (53%)]\tLoss: 0.377983\n",
      "Training stage for Flod 4 Epoch: 84 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 84 [22400/29981                 (75%)]\tLoss: 0.399751\n",
      "Training stage for Flod 4 Epoch: 84 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 84 [28800/29981                 (96%)]\tLoss: 0.438262\n",
      "Test set for fold4: Average Loss:           748148.3464, Accuracy: 6626/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 85 [0/29981                 (0%)]\tLoss: 0.344513\n",
      "Training stage for Flod 4 Epoch: 85 [3200/29981                 (11%)]\tLoss: 0.313480\n",
      "Training stage for Flod 4 Epoch: 85 [6400/29981                 (21%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 85 [9600/29981                 (32%)]\tLoss: 0.438675\n",
      "Training stage for Flod 4 Epoch: 85 [12800/29981                 (43%)]\tLoss: 0.405846\n",
      "Training stage for Flod 4 Epoch: 85 [16000/29981                 (53%)]\tLoss: 0.344519\n",
      "Training stage for Flod 4 Epoch: 85 [19200/29981                 (64%)]\tLoss: 0.368756\n",
      "Training stage for Flod 4 Epoch: 85 [22400/29981                 (75%)]\tLoss: 0.473970\n",
      "Training stage for Flod 4 Epoch: 85 [25600/29981                 (85%)]\tLoss: 0.407011\n",
      "Training stage for Flod 4 Epoch: 85 [28800/29981                 (96%)]\tLoss: 0.347952\n",
      "Test set for fold4: Average Loss:           721116.6177, Accuracy: 6737/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 86 [0/29981                 (0%)]\tLoss: 0.407011\n",
      "Training stage for Flod 4 Epoch: 86 [3200/29981                 (11%)]\tLoss: 0.407013\n",
      "Training stage for Flod 4 Epoch: 86 [6400/29981                 (21%)]\tLoss: 0.407145\n",
      "Training stage for Flod 4 Epoch: 86 [9600/29981                 (32%)]\tLoss: 0.313267\n",
      "Training stage for Flod 4 Epoch: 86 [12800/29981                 (43%)]\tLoss: 0.347626\n",
      "Training stage for Flod 4 Epoch: 86 [16000/29981                 (53%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 86 [19200/29981                 (64%)]\tLoss: 0.344587\n",
      "Training stage for Flod 4 Epoch: 86 [22400/29981                 (75%)]\tLoss: 0.370790\n",
      "Training stage for Flod 4 Epoch: 86 [25600/29981                 (85%)]\tLoss: 0.344727\n",
      "Training stage for Flod 4 Epoch: 86 [28800/29981                 (96%)]\tLoss: 0.469503\n",
      "Test set for fold4: Average Loss:           710830.0603, Accuracy: 6799/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 87 [0/29981                 (0%)]\tLoss: 0.375772\n",
      "Training stage for Flod 4 Epoch: 87 [3200/29981                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 87 [6400/29981                 (21%)]\tLoss: 0.375855\n",
      "Training stage for Flod 4 Epoch: 87 [9600/29981                 (32%)]\tLoss: 0.407028\n",
      "Training stage for Flod 4 Epoch: 87 [12800/29981                 (43%)]\tLoss: 0.344609\n",
      "Training stage for Flod 4 Epoch: 87 [16000/29981                 (53%)]\tLoss: 0.432170\n",
      "Training stage for Flod 4 Epoch: 87 [19200/29981                 (64%)]\tLoss: 0.406927\n",
      "Training stage for Flod 4 Epoch: 87 [22400/29981                 (75%)]\tLoss: 0.344514\n",
      "Training stage for Flod 4 Epoch: 87 [25600/29981                 (85%)]\tLoss: 0.344096\n",
      "Training stage for Flod 4 Epoch: 87 [28800/29981                 (96%)]\tLoss: 0.372620\n",
      "Test set for fold4: Average Loss:           738036.2049, Accuracy: 6661/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 88 [0/29981                 (0%)]\tLoss: 0.374323\n",
      "Training stage for Flod 4 Epoch: 88 [3200/29981                 (11%)]\tLoss: 0.345238\n",
      "Training stage for Flod 4 Epoch: 88 [6400/29981                 (21%)]\tLoss: 0.405206\n",
      "Training stage for Flod 4 Epoch: 88 [9600/29981                 (32%)]\tLoss: 0.376877\n",
      "Training stage for Flod 4 Epoch: 88 [12800/29981                 (43%)]\tLoss: 0.436172\n",
      "Training stage for Flod 4 Epoch: 88 [16000/29981                 (53%)]\tLoss: 0.375843\n",
      "Training stage for Flod 4 Epoch: 88 [19200/29981                 (64%)]\tLoss: 0.314843\n",
      "Training stage for Flod 4 Epoch: 88 [22400/29981                 (75%)]\tLoss: 0.375794\n",
      "Training stage for Flod 4 Epoch: 88 [25600/29981                 (85%)]\tLoss: 0.372047\n",
      "Training stage for Flod 4 Epoch: 88 [28800/29981                 (96%)]\tLoss: 0.313269\n",
      "Test set for fold4: Average Loss:           705615.4581, Accuracy: 6824/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 89 [0/29981                 (0%)]\tLoss: 0.347878\n",
      "Training stage for Flod 4 Epoch: 89 [3200/29981                 (11%)]\tLoss: 0.313348\n",
      "Training stage for Flod 4 Epoch: 89 [6400/29981                 (21%)]\tLoss: 0.313298\n",
      "Training stage for Flod 4 Epoch: 89 [9600/29981                 (32%)]\tLoss: 0.313358\n",
      "Training stage for Flod 4 Epoch: 89 [12800/29981                 (43%)]\tLoss: 0.347935\n",
      "Training stage for Flod 4 Epoch: 89 [16000/29981                 (53%)]\tLoss: 0.344520\n",
      "Training stage for Flod 4 Epoch: 89 [19200/29981                 (64%)]\tLoss: 0.344600\n",
      "Training stage for Flod 4 Epoch: 89 [22400/29981                 (75%)]\tLoss: 0.375799\n",
      "Training stage for Flod 4 Epoch: 89 [25600/29981                 (85%)]\tLoss: 0.344601\n",
      "Training stage for Flod 4 Epoch: 89 [28800/29981                 (96%)]\tLoss: 0.407155\n",
      "Test set for fold4: Average Loss:           740648.5639, Accuracy: 6652/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 90 [0/29981                 (0%)]\tLoss: 0.407017\n",
      "Training stage for Flod 4 Epoch: 90 [3200/29981                 (11%)]\tLoss: 0.313280\n",
      "Training stage for Flod 4 Epoch: 90 [6400/29981                 (21%)]\tLoss: 0.345977\n",
      "Training stage for Flod 4 Epoch: 90 [9600/29981                 (32%)]\tLoss: 0.377102\n",
      "Training stage for Flod 4 Epoch: 90 [12800/29981                 (43%)]\tLoss: 0.407041\n",
      "Training stage for Flod 4 Epoch: 90 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 90 [19200/29981                 (64%)]\tLoss: 0.438141\n",
      "Training stage for Flod 4 Epoch: 90 [22400/29981                 (75%)]\tLoss: 0.344942\n",
      "Training stage for Flod 4 Epoch: 90 [25600/29981                 (85%)]\tLoss: 0.313589\n",
      "Training stage for Flod 4 Epoch: 90 [28800/29981                 (96%)]\tLoss: 0.313268\n",
      "Test set for fold4: Average Loss:           708226.4239, Accuracy: 6801/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 91 [0/29981                 (0%)]\tLoss: 0.316851\n",
      "Training stage for Flod 4 Epoch: 91 [3200/29981                 (11%)]\tLoss: 0.313336\n",
      "Training stage for Flod 4 Epoch: 91 [6400/29981                 (21%)]\tLoss: 0.407018\n",
      "Training stage for Flod 4 Epoch: 91 [9600/29981                 (32%)]\tLoss: 0.337480\n",
      "Training stage for Flod 4 Epoch: 91 [12800/29981                 (43%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 91 [16000/29981                 (53%)]\tLoss: 0.344596\n",
      "Training stage for Flod 4 Epoch: 91 [19200/29981                 (64%)]\tLoss: 0.407021\n",
      "Training stage for Flod 4 Epoch: 91 [22400/29981                 (75%)]\tLoss: 0.313501\n",
      "Training stage for Flod 4 Epoch: 91 [25600/29981                 (85%)]\tLoss: 0.420551\n",
      "Training stage for Flod 4 Epoch: 91 [28800/29981                 (96%)]\tLoss: 0.313262\n",
      "Test set for fold4: Average Loss:           712181.0650, Accuracy: 6781/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 92 [0/29981                 (0%)]\tLoss: 0.344486\n",
      "Training stage for Flod 4 Epoch: 92 [3200/29981                 (11%)]\tLoss: 0.375792\n",
      "Training stage for Flod 4 Epoch: 92 [6400/29981                 (21%)]\tLoss: 0.500681\n",
      "Training stage for Flod 4 Epoch: 92 [9600/29981                 (32%)]\tLoss: 0.357503\n",
      "Training stage for Flod 4 Epoch: 92 [12800/29981                 (43%)]\tLoss: 0.407810\n",
      "Training stage for Flod 4 Epoch: 92 [16000/29981                 (53%)]\tLoss: 0.383413\n",
      "Training stage for Flod 4 Epoch: 92 [19200/29981                 (64%)]\tLoss: 0.344555\n",
      "Training stage for Flod 4 Epoch: 92 [22400/29981                 (75%)]\tLoss: 0.347416\n",
      "Training stage for Flod 4 Epoch: 92 [25600/29981                 (85%)]\tLoss: 0.416541\n",
      "Training stage for Flod 4 Epoch: 92 [28800/29981                 (96%)]\tLoss: 0.407012\n",
      "Test set for fold4: Average Loss:           709060.0285, Accuracy: 6792/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 93 [0/29981                 (0%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 93 [3200/29981                 (11%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 93 [6400/29981                 (21%)]\tLoss: 0.375863\n",
      "Training stage for Flod 4 Epoch: 93 [9600/29981                 (32%)]\tLoss: 0.375100\n",
      "Training stage for Flod 4 Epoch: 93 [12800/29981                 (43%)]\tLoss: 0.313400\n",
      "Training stage for Flod 4 Epoch: 93 [16000/29981                 (53%)]\tLoss: 0.374141\n",
      "Training stage for Flod 4 Epoch: 93 [19200/29981                 (64%)]\tLoss: 0.434344\n",
      "Training stage for Flod 4 Epoch: 93 [22400/29981                 (75%)]\tLoss: 0.407262\n",
      "Training stage for Flod 4 Epoch: 93 [25600/29981                 (85%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 93 [28800/29981                 (96%)]\tLoss: 0.313355\n",
      "Test set for fold4: Average Loss:           734158.3044, Accuracy: 6683/7495           (89%)\n",
      "Training stage for Flod 4 Epoch: 94 [0/29981                 (0%)]\tLoss: 0.356453\n",
      "Training stage for Flod 4 Epoch: 94 [3200/29981                 (11%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 94 [6400/29981                 (21%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 94 [9600/29981                 (32%)]\tLoss: 0.375066\n",
      "Training stage for Flod 4 Epoch: 94 [12800/29981                 (43%)]\tLoss: 0.326505\n",
      "Training stage for Flod 4 Epoch: 94 [16000/29981                 (53%)]\tLoss: 0.352090\n",
      "Training stage for Flod 4 Epoch: 94 [19200/29981                 (64%)]\tLoss: 0.375842\n",
      "Training stage for Flod 4 Epoch: 94 [22400/29981                 (75%)]\tLoss: 0.344663\n",
      "Training stage for Flod 4 Epoch: 94 [25600/29981                 (85%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 94 [28800/29981                 (96%)]\tLoss: 0.344678\n",
      "Test set for fold4: Average Loss:           720510.6174, Accuracy: 6767/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 95 [0/29981                 (0%)]\tLoss: 0.370616\n",
      "Training stage for Flod 4 Epoch: 95 [3200/29981                 (11%)]\tLoss: 0.348325\n",
      "Training stage for Flod 4 Epoch: 95 [6400/29981                 (21%)]\tLoss: 0.403742\n",
      "Training stage for Flod 4 Epoch: 95 [9600/29981                 (32%)]\tLoss: 0.313287\n",
      "Training stage for Flod 4 Epoch: 95 [12800/29981                 (43%)]\tLoss: 0.435530\n",
      "Training stage for Flod 4 Epoch: 95 [16000/29981                 (53%)]\tLoss: 0.378225\n",
      "Training stage for Flod 4 Epoch: 95 [19200/29981                 (64%)]\tLoss: 0.375800\n",
      "Training stage for Flod 4 Epoch: 95 [22400/29981                 (75%)]\tLoss: 0.469512\n",
      "Training stage for Flod 4 Epoch: 95 [25600/29981                 (85%)]\tLoss: 0.407607\n",
      "Training stage for Flod 4 Epoch: 95 [28800/29981                 (96%)]\tLoss: 0.375804\n",
      "Test set for fold4: Average Loss:           706829.2626, Accuracy: 6818/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 96 [0/29981                 (0%)]\tLoss: 0.438262\n",
      "Training stage for Flod 4 Epoch: 96 [3200/29981                 (11%)]\tLoss: 0.375762\n",
      "Training stage for Flod 4 Epoch: 96 [6400/29981                 (21%)]\tLoss: 0.469012\n",
      "Training stage for Flod 4 Epoch: 96 [9600/29981                 (32%)]\tLoss: 0.313374\n",
      "Training stage for Flod 4 Epoch: 96 [12800/29981                 (43%)]\tLoss: 0.375780\n",
      "Training stage for Flod 4 Epoch: 96 [16000/29981                 (53%)]\tLoss: 0.363514\n",
      "Training stage for Flod 4 Epoch: 96 [19200/29981                 (64%)]\tLoss: 0.314127\n",
      "Training stage for Flod 4 Epoch: 96 [22400/29981                 (75%)]\tLoss: 0.313330\n",
      "Training stage for Flod 4 Epoch: 96 [25600/29981                 (85%)]\tLoss: 0.346045\n",
      "Training stage for Flod 4 Epoch: 96 [28800/29981                 (96%)]\tLoss: 0.407135\n",
      "Test set for fold4: Average Loss:           711238.5196, Accuracy: 6787/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 97 [0/29981                 (0%)]\tLoss: 0.343849\n",
      "Training stage for Flod 4 Epoch: 97 [3200/29981                 (11%)]\tLoss: 0.313283\n",
      "Training stage for Flod 4 Epoch: 97 [6400/29981                 (21%)]\tLoss: 0.375775\n",
      "Training stage for Flod 4 Epoch: 97 [9600/29981                 (32%)]\tLoss: 0.373545\n",
      "Training stage for Flod 4 Epoch: 97 [12800/29981                 (43%)]\tLoss: 0.375769\n",
      "Training stage for Flod 4 Epoch: 97 [16000/29981                 (53%)]\tLoss: 0.345092\n",
      "Training stage for Flod 4 Epoch: 97 [19200/29981                 (64%)]\tLoss: 0.314338\n",
      "Training stage for Flod 4 Epoch: 97 [22400/29981                 (75%)]\tLoss: 0.376831\n",
      "Training stage for Flod 4 Epoch: 97 [25600/29981                 (85%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 97 [28800/29981                 (96%)]\tLoss: 0.380948\n",
      "Test set for fold4: Average Loss:           759852.4683, Accuracy: 6579/7495           (88%)\n",
      "Training stage for Flod 4 Epoch: 98 [0/29981                 (0%)]\tLoss: 0.375766\n",
      "Training stage for Flod 4 Epoch: 98 [3200/29981                 (11%)]\tLoss: 0.420145\n",
      "Training stage for Flod 4 Epoch: 98 [6400/29981                 (21%)]\tLoss: 0.438429\n",
      "Training stage for Flod 4 Epoch: 98 [9600/29981                 (32%)]\tLoss: 0.345754\n",
      "Training stage for Flod 4 Epoch: 98 [12800/29981                 (43%)]\tLoss: 0.476498\n",
      "Training stage for Flod 4 Epoch: 98 [16000/29981                 (53%)]\tLoss: 0.345008\n",
      "Training stage for Flod 4 Epoch: 98 [19200/29981                 (64%)]\tLoss: 0.407401\n",
      "Training stage for Flod 4 Epoch: 98 [22400/29981                 (75%)]\tLoss: 0.435214\n",
      "Training stage for Flod 4 Epoch: 98 [25600/29981                 (85%)]\tLoss: 0.345834\n",
      "Training stage for Flod 4 Epoch: 98 [28800/29981                 (96%)]\tLoss: 0.344733\n",
      "Test set for fold4: Average Loss:           726880.0904, Accuracy: 6723/7495           (90%)\n",
      "Training stage for Flod 4 Epoch: 99 [0/29981                 (0%)]\tLoss: 0.376330\n",
      "Training stage for Flod 4 Epoch: 99 [3200/29981                 (11%)]\tLoss: 0.375951\n",
      "Training stage for Flod 4 Epoch: 99 [6400/29981                 (21%)]\tLoss: 0.343877\n",
      "Training stage for Flod 4 Epoch: 99 [9600/29981                 (32%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 99 [12800/29981                 (43%)]\tLoss: 0.320797\n",
      "Training stage for Flod 4 Epoch: 99 [16000/29981                 (53%)]\tLoss: 0.313262\n",
      "Training stage for Flod 4 Epoch: 99 [19200/29981                 (64%)]\tLoss: 0.344512\n",
      "Training stage for Flod 4 Epoch: 99 [22400/29981                 (75%)]\tLoss: 0.378984\n",
      "Training stage for Flod 4 Epoch: 99 [25600/29981                 (85%)]\tLoss: 0.406848\n",
      "Training stage for Flod 4 Epoch: 99 [28800/29981                 (96%)]\tLoss: 0.344512\n",
      "Test set for fold4: Average Loss:           703153.2154, Accuracy: 6831/7495           (91%)\n",
      "Training stage for Flod 4 Epoch: 100 [0/29981                 (0%)]\tLoss: 0.344521\n",
      "Training stage for Flod 4 Epoch: 100 [3200/29981                 (11%)]\tLoss: 0.377214\n",
      "Training stage for Flod 4 Epoch: 100 [6400/29981                 (21%)]\tLoss: 0.344684\n",
      "Training stage for Flod 4 Epoch: 100 [9600/29981                 (32%)]\tLoss: 0.313265\n",
      "Training stage for Flod 4 Epoch: 100 [12800/29981                 (43%)]\tLoss: 0.407012\n",
      "Training stage for Flod 4 Epoch: 100 [16000/29981                 (53%)]\tLoss: 0.438264\n",
      "Training stage for Flod 4 Epoch: 100 [19200/29981                 (64%)]\tLoss: 0.313365\n",
      "Training stage for Flod 4 Epoch: 100 [22400/29981                 (75%)]\tLoss: 0.407021\n",
      "Training stage for Flod 4 Epoch: 100 [25600/29981                 (85%)]\tLoss: 0.376724\n",
      "Training stage for Flod 4 Epoch: 100 [28800/29981                 (96%)]\tLoss: 0.344899\n",
      "Test set for fold4: Average Loss:           701586.6140, Accuracy: 6820/7495           (91%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1cff74c460>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X_features, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    # Randomly select the train and test samples to generate train and test datasets\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "    test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "    \n",
    "    # weighted random sampler for train and test datasets\n",
    "    y_train = [TCRData.y[i] for i in train_idx]\n",
    "    class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "    weights = 1. / class_sample_count\n",
    "    samples_weight = torch.from_numpy(np.array([weights[t] for t in y_train]))\n",
    "    train_sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "    \n",
    "    y_test = [TCRData.y[i] for i in test_idx]\n",
    "    class_sample_count = np.array([len(np.where(y_test == t)[0]) for t in np.unique(y_test)])\n",
    "    weights = 1. / class_sample_count\n",
    "    samples_weight = torch.from_numpy(np.array([weights[t] for t in y_test]))\n",
    "    test_sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler, drop_last=True)\n",
    "\n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_dataloader, optimizer, epoch, criterion)\n",
    "        test_losses, test_correct = test(fold, model, device, test_dataloader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    ax[0].plot(train_losses_history, label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, label=f\"test accuracy fold{fold}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n",
      "-------------------Fold 0-------------------\n",
      "Training stage for Flod 0 Epoch: 1 [0/37476                 (0%)]\tLoss: 0.934957\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[0.3114, 0.6886],\n",
      "        [0.3008, 0.6992],\n",
      "        [0.2090, 0.7910],\n",
      "        [0.3380, 0.6620],\n",
      "        [0.2680, 0.7320],\n",
      "        [0.2953, 0.7047],\n",
      "        [0.2819, 0.7181],\n",
      "        [0.2746, 0.7254],\n",
      "        [0.2777, 0.7223],\n",
      "        [0.2991, 0.7009],\n",
      "        [0.3069, 0.6931],\n",
      "        [0.2814, 0.7186],\n",
      "        [0.2875, 0.7125],\n",
      "        [0.2563, 0.7437],\n",
      "        [0.2455, 0.7545],\n",
      "        [0.2621, 0.7379],\n",
      "        [0.2777, 0.7223],\n",
      "        [0.2705, 0.7295],\n",
      "        [0.2712, 0.7288],\n",
      "        [0.2896, 0.7104],\n",
      "        [0.2732, 0.7268],\n",
      "        [0.2787, 0.7213],\n",
      "        [0.3176, 0.6824],\n",
      "        [0.2810, 0.7190],\n",
      "        [0.2971, 0.7029],\n",
      "        [0.2939, 0.7061],\n",
      "        [0.3021, 0.6979],\n",
      "        [0.2684, 0.7316],\n",
      "        [0.2719, 0.7281],\n",
      "        [0.2595, 0.7405],\n",
      "        [0.2884, 0.7116],\n",
      "        [0.2881, 0.7119]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [3200/37476                 (11%)]\tLoss: 0.438262\n",
      "The weight of the batch is tensor([1.0000, 7.0000], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 2.7587e-12],\n",
      "        [1.0000e+00, 6.2420e-11],\n",
      "        [1.0000e+00, 5.1403e-13],\n",
      "        [1.0000e+00, 1.5615e-11],\n",
      "        [1.0000e+00, 7.9632e-12],\n",
      "        [1.0000e+00, 4.4945e-12],\n",
      "        [1.0000e+00, 2.2468e-10],\n",
      "        [1.0000e+00, 5.4548e-10],\n",
      "        [1.0000e+00, 1.7775e-09],\n",
      "        [1.0000e+00, 1.7013e-09],\n",
      "        [1.0000e+00, 1.0911e-12],\n",
      "        [1.0000e+00, 6.8420e-14],\n",
      "        [1.0000e+00, 2.4842e-11],\n",
      "        [1.0000e+00, 6.2941e-13],\n",
      "        [1.0000e+00, 3.2263e-10],\n",
      "        [1.0000e+00, 1.1196e-11],\n",
      "        [1.0000e+00, 4.0254e-12],\n",
      "        [1.0000e+00, 1.7380e-13],\n",
      "        [1.0000e+00, 6.1680e-10],\n",
      "        [1.0000e+00, 4.1866e-10],\n",
      "        [1.0000e+00, 1.3730e-10],\n",
      "        [1.0000e+00, 1.6268e-14],\n",
      "        [1.0000e+00, 6.9063e-10],\n",
      "        [1.0000e+00, 1.8288e-10],\n",
      "        [1.0000e+00, 1.3620e-12],\n",
      "        [1.0000e+00, 2.7763e-16],\n",
      "        [1.0000e+00, 2.7337e-14],\n",
      "        [1.0000e+00, 3.4219e-12],\n",
      "        [1.0000e+00, 6.9217e-15],\n",
      "        [1.0000e+00, 6.5475e-14],\n",
      "        [1.0000e+00, 1.5278e-08],\n",
      "        [1.0000e+00, 5.2899e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [6400/37476                 (21%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 8.9018e-11],\n",
      "        [1.0000e+00, 1.2151e-17],\n",
      "        [1.0000e+00, 8.0806e-12],\n",
      "        [1.0000e+00, 3.0967e-11],\n",
      "        [1.0000e+00, 2.5981e-15],\n",
      "        [1.0000e+00, 2.9788e-10],\n",
      "        [1.0000e+00, 2.1086e-12],\n",
      "        [1.0000e+00, 1.4253e-08],\n",
      "        [1.0000e+00, 4.9011e-12],\n",
      "        [1.0000e+00, 5.2651e-11],\n",
      "        [1.0000e+00, 5.1885e-12],\n",
      "        [1.0000e+00, 7.2429e-13],\n",
      "        [1.0000e+00, 8.9418e-11],\n",
      "        [1.0000e+00, 1.2283e-07],\n",
      "        [1.0000e+00, 7.6285e-12],\n",
      "        [1.0000e+00, 5.7774e-11],\n",
      "        [1.0000e+00, 3.4972e-12],\n",
      "        [1.0000e+00, 3.5872e-16],\n",
      "        [1.0000e+00, 6.4040e-17],\n",
      "        [1.0000e+00, 1.0149e-16],\n",
      "        [1.0000e+00, 5.4141e-13],\n",
      "        [1.0000e+00, 1.1660e-14],\n",
      "        [1.0000e+00, 9.4680e-10],\n",
      "        [1.0000e+00, 2.4127e-12],\n",
      "        [1.0000e+00, 2.6875e-12],\n",
      "        [1.0000e+00, 1.9886e-11],\n",
      "        [1.0000e+00, 4.0658e-10],\n",
      "        [1.0000e+00, 1.1683e-11],\n",
      "        [1.0000e+00, 1.2731e-10],\n",
      "        [1.0000e+00, 8.9546e-12],\n",
      "        [1.0000e+00, 1.8085e-11],\n",
      "        [1.0000e+00, 8.0708e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [9600/37476                 (32%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 6.7697e-17],\n",
      "        [1.0000e+00, 5.1821e-10],\n",
      "        [1.0000e+00, 1.3320e-10],\n",
      "        [1.0000e+00, 1.5089e-13],\n",
      "        [1.0000e+00, 3.9842e-12],\n",
      "        [1.0000e+00, 2.4962e-15],\n",
      "        [1.0000e+00, 3.1357e-10],\n",
      "        [1.0000e+00, 6.2890e-12],\n",
      "        [1.0000e+00, 1.8359e-13],\n",
      "        [1.0000e+00, 2.5017e-15],\n",
      "        [1.0000e+00, 4.2550e-13],\n",
      "        [1.0000e+00, 9.5013e-14],\n",
      "        [1.0000e+00, 4.5910e-10],\n",
      "        [1.0000e+00, 4.5267e-13],\n",
      "        [1.0000e+00, 7.1302e-11],\n",
      "        [1.0000e+00, 5.5136e-10],\n",
      "        [1.0000e+00, 5.6989e-08],\n",
      "        [1.0000e+00, 1.4367e-10],\n",
      "        [1.0000e+00, 1.0178e-11],\n",
      "        [1.0000e+00, 8.5885e-11],\n",
      "        [1.0000e+00, 2.0234e-11],\n",
      "        [1.0000e+00, 1.6635e-17],\n",
      "        [1.0000e+00, 5.9822e-11],\n",
      "        [1.0000e+00, 5.0930e-08],\n",
      "        [1.0000e+00, 5.1734e-10],\n",
      "        [1.0000e+00, 5.9446e-12],\n",
      "        [1.0000e+00, 2.2735e-09],\n",
      "        [1.0000e+00, 2.4513e-11],\n",
      "        [1.0000e+00, 6.4300e-11],\n",
      "        [1.0000e+00, 2.5855e-11],\n",
      "        [1.0000e+00, 3.4595e-07],\n",
      "        [1.0000e+00, 2.6739e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [12800/37476                 (43%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 1.2702e-16],\n",
      "        [1.0000e+00, 3.3281e-16],\n",
      "        [1.0000e+00, 1.4042e-15],\n",
      "        [1.0000e+00, 1.4494e-12],\n",
      "        [1.0000e+00, 1.0190e-09],\n",
      "        [1.0000e+00, 1.3617e-13],\n",
      "        [1.0000e+00, 2.5563e-13],\n",
      "        [1.0000e+00, 1.0298e-11],\n",
      "        [1.0000e+00, 2.0246e-09],\n",
      "        [1.0000e+00, 1.4474e-15],\n",
      "        [1.0000e+00, 4.9878e-13],\n",
      "        [1.0000e+00, 1.3829e-09],\n",
      "        [1.0000e+00, 2.0780e-12],\n",
      "        [1.0000e+00, 7.1440e-15],\n",
      "        [1.0000e+00, 7.2352e-13],\n",
      "        [1.0000e+00, 3.3581e-15],\n",
      "        [1.0000e+00, 1.1281e-12],\n",
      "        [1.0000e+00, 1.2084e-09],\n",
      "        [1.0000e+00, 5.9579e-13],\n",
      "        [1.0000e+00, 2.7423e-14],\n",
      "        [1.0000e+00, 1.3824e-16],\n",
      "        [1.0000e+00, 2.8843e-10],\n",
      "        [1.0000e+00, 2.6105e-14],\n",
      "        [1.0000e+00, 1.1086e-08],\n",
      "        [1.0000e+00, 3.4822e-08],\n",
      "        [1.0000e+00, 3.3314e-08],\n",
      "        [1.0000e+00, 4.7023e-16],\n",
      "        [1.0000e+00, 4.5565e-11],\n",
      "        [1.0000e+00, 1.0937e-11],\n",
      "        [1.0000e+00, 1.9987e-10],\n",
      "        [1.0000e+00, 1.2765e-12],\n",
      "        [1.0000e+00, 1.9069e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [16000/37476                 (53%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 1.0204e-11],\n",
      "        [1.0000e+00, 3.7881e-12],\n",
      "        [1.0000e+00, 3.5693e-16],\n",
      "        [1.0000e+00, 2.6315e-12],\n",
      "        [1.0000e+00, 2.2234e-08],\n",
      "        [1.0000e+00, 4.6302e-10],\n",
      "        [1.0000e+00, 1.9142e-09],\n",
      "        [1.0000e+00, 1.0135e-09],\n",
      "        [1.0000e+00, 1.9506e-11],\n",
      "        [1.0000e+00, 4.1250e-13],\n",
      "        [1.0000e+00, 1.3126e-12],\n",
      "        [1.0000e+00, 1.4177e-11],\n",
      "        [1.0000e+00, 1.5727e-13],\n",
      "        [1.0000e+00, 5.1744e-12],\n",
      "        [1.0000e+00, 2.9664e-15],\n",
      "        [1.0000e+00, 9.5756e-10],\n",
      "        [1.0000e+00, 9.7605e-13],\n",
      "        [1.0000e+00, 4.0798e-11],\n",
      "        [1.0000e+00, 4.1057e-12],\n",
      "        [1.0000e+00, 3.1536e-09],\n",
      "        [1.0000e+00, 1.8113e-09],\n",
      "        [1.0000e+00, 1.0045e-11],\n",
      "        [1.0000e+00, 6.9205e-17],\n",
      "        [1.0000e+00, 3.0929e-10],\n",
      "        [1.0000e+00, 4.3035e-09],\n",
      "        [1.0000e+00, 6.5583e-12],\n",
      "        [1.0000e+00, 2.2672e-16],\n",
      "        [1.0000e+00, 1.4573e-13],\n",
      "        [1.0000e+00, 5.5549e-10],\n",
      "        [1.0000e+00, 4.6517e-12],\n",
      "        [1.0000e+00, 1.4529e-10],\n",
      "        [1.0000e+00, 1.7421e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [19200/37476                 (64%)]\tLoss: 0.407012\n",
      "The weight of the batch is tensor([1.0000, 9.6667], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 8.0952e-11],\n",
      "        [1.0000e+00, 4.3895e-11],\n",
      "        [1.0000e+00, 7.0959e-15],\n",
      "        [1.0000e+00, 7.3732e-17],\n",
      "        [1.0000e+00, 1.3421e-15],\n",
      "        [1.0000e+00, 3.3793e-10],\n",
      "        [1.0000e+00, 5.7675e-11],\n",
      "        [1.0000e+00, 4.4698e-12],\n",
      "        [1.0000e+00, 7.9043e-09],\n",
      "        [1.0000e+00, 2.8755e-16],\n",
      "        [1.0000e+00, 6.3987e-12],\n",
      "        [1.0000e+00, 1.9372e-12],\n",
      "        [1.0000e+00, 5.9482e-11],\n",
      "        [1.0000e+00, 4.6443e-13],\n",
      "        [1.0000e+00, 1.2224e-15],\n",
      "        [1.0000e+00, 3.9227e-11],\n",
      "        [1.0000e+00, 2.7756e-08],\n",
      "        [1.0000e+00, 8.6704e-14],\n",
      "        [1.0000e+00, 1.0262e-12],\n",
      "        [1.0000e+00, 1.3004e-16],\n",
      "        [1.0000e+00, 1.5733e-13],\n",
      "        [1.0000e+00, 1.1714e-12],\n",
      "        [1.0000e+00, 8.8475e-10],\n",
      "        [1.0000e+00, 3.9674e-12],\n",
      "        [1.0000e+00, 8.0085e-11],\n",
      "        [1.0000e+00, 6.3722e-11],\n",
      "        [1.0000e+00, 9.2016e-11],\n",
      "        [1.0000e+00, 2.4596e-14],\n",
      "        [1.0000e+00, 1.4094e-10],\n",
      "        [1.0000e+00, 2.4531e-14],\n",
      "        [1.0000e+00, 1.2151e-16],\n",
      "        [1.0000e+00, 2.5304e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [22400/37476                 (75%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 4.3210e-12],\n",
      "        [1.0000e+00, 4.3463e-11],\n",
      "        [1.0000e+00, 7.4953e-07],\n",
      "        [1.0000e+00, 1.3756e-12],\n",
      "        [1.0000e+00, 4.9359e-12],\n",
      "        [1.0000e+00, 2.4058e-09],\n",
      "        [1.0000e+00, 5.3323e-15],\n",
      "        [1.0000e+00, 5.0707e-13],\n",
      "        [1.0000e+00, 2.2526e-13],\n",
      "        [1.0000e+00, 1.2342e-15],\n",
      "        [1.0000e+00, 9.0140e-13],\n",
      "        [1.0000e+00, 3.4632e-11],\n",
      "        [1.0000e+00, 1.3517e-13],\n",
      "        [1.0000e+00, 2.4103e-12],\n",
      "        [1.0000e+00, 1.1026e-14],\n",
      "        [1.0000e+00, 5.8800e-16],\n",
      "        [1.0000e+00, 7.4953e-07],\n",
      "        [1.0000e+00, 1.1779e-13],\n",
      "        [1.0000e+00, 2.9511e-09],\n",
      "        [1.0000e+00, 1.2014e-14],\n",
      "        [1.0000e+00, 9.1679e-09],\n",
      "        [1.0000e+00, 5.9568e-17],\n",
      "        [1.0000e+00, 8.7931e-13],\n",
      "        [1.0000e+00, 4.6156e-09],\n",
      "        [1.0000e+00, 9.2785e-11],\n",
      "        [1.0000e+00, 1.2269e-11],\n",
      "        [1.0000e+00, 3.4833e-12],\n",
      "        [1.0000e+00, 3.1421e-13],\n",
      "        [1.0000e+00, 3.9942e-12],\n",
      "        [1.0000e+00, 3.6041e-16],\n",
      "        [1.0000e+00, 1.2131e-16],\n",
      "        [1.0000e+00, 1.6567e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [25600/37476                 (85%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 3.1294e-15],\n",
      "        [1.0000e+00, 2.3619e-12],\n",
      "        [1.0000e+00, 4.8248e-12],\n",
      "        [1.0000e+00, 3.8661e-16],\n",
      "        [1.0000e+00, 3.4368e-10],\n",
      "        [1.0000e+00, 7.3623e-11],\n",
      "        [1.0000e+00, 1.4483e-11],\n",
      "        [1.0000e+00, 1.6039e-16],\n",
      "        [1.0000e+00, 1.8419e-09],\n",
      "        [1.0000e+00, 4.5654e-08],\n",
      "        [1.0000e+00, 1.1032e-16],\n",
      "        [1.0000e+00, 7.0061e-17],\n",
      "        [1.0000e+00, 2.2044e-12],\n",
      "        [1.0000e+00, 2.4463e-10],\n",
      "        [1.0000e+00, 2.2446e-12],\n",
      "        [1.0000e+00, 8.6780e-16],\n",
      "        [1.0000e+00, 2.8493e-12],\n",
      "        [1.0000e+00, 6.0535e-11],\n",
      "        [1.0000e+00, 4.1797e-11],\n",
      "        [1.0000e+00, 8.5080e-17],\n",
      "        [1.0000e+00, 6.9579e-12],\n",
      "        [1.0000e+00, 6.8887e-11],\n",
      "        [1.0000e+00, 1.2776e-07],\n",
      "        [1.0000e+00, 6.7091e-09],\n",
      "        [1.0000e+00, 5.7589e-13],\n",
      "        [1.0000e+00, 6.0763e-12],\n",
      "        [1.0000e+00, 4.7764e-15],\n",
      "        [1.0000e+00, 6.0107e-12],\n",
      "        [1.0000e+00, 2.4470e-12],\n",
      "        [1.0000e+00, 4.2069e-08],\n",
      "        [1.0000e+00, 1.3033e-14],\n",
      "        [1.0000e+00, 3.6592e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 1 [28800/37476                 (96%)]\tLoss: 0.375762\n",
      "The weight of the batch is tensor([ 1.0000, 15.0000], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 2.2391e-12],\n",
      "        [1.0000e+00, 1.1640e-11],\n",
      "        [1.0000e+00, 1.6214e-11],\n",
      "        [1.0000e+00, 3.1290e-15],\n",
      "        [1.0000e+00, 6.0979e-12],\n",
      "        [1.0000e+00, 4.1214e-15],\n",
      "        [1.0000e+00, 1.1186e-13],\n",
      "        [1.0000e+00, 1.4076e-12],\n",
      "        [1.0000e+00, 4.4872e-12],\n",
      "        [1.0000e+00, 2.9408e-16],\n",
      "        [1.0000e+00, 4.6324e-10],\n",
      "        [1.0000e+00, 1.1472e-14],\n",
      "        [1.0000e+00, 1.1973e-11],\n",
      "        [1.0000e+00, 1.7402e-10],\n",
      "        [1.0000e+00, 6.8506e-17],\n",
      "        [1.0000e+00, 3.5972e-10],\n",
      "        [1.0000e+00, 3.1080e-12],\n",
      "        [1.0000e+00, 1.4217e-12],\n",
      "        [1.0000e+00, 2.4757e-17],\n",
      "        [1.0000e+00, 2.2232e-12],\n",
      "        [1.0000e+00, 5.1578e-16],\n",
      "        [1.0000e+00, 3.0827e-12],\n",
      "        [1.0000e+00, 9.1215e-10],\n",
      "        [1.0000e+00, 8.7635e-10],\n",
      "        [1.0000e+00, 3.4656e-13],\n",
      "        [1.0000e+00, 4.9866e-12],\n",
      "        [1.0000e+00, 2.6438e-13],\n",
      "        [1.0000e+00, 9.7673e-11],\n",
      "        [1.0000e+00, 3.4323e-16],\n",
      "        [1.0000e+00, 1.8114e-12],\n",
      "        [1.0000e+00, 9.1130e-14],\n",
      "        [1.0000e+00, 1.0987e-09]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Test set for fold0: Average Loss:           79.3657, Accuracy: 7294/37476           (19%)\n",
      "Training stage for Flod 0 Epoch: 2 [0/37476                 (0%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 1.9918e-11],\n",
      "        [1.0000e+00, 2.1789e-13],\n",
      "        [1.0000e+00, 2.0292e-16],\n",
      "        [1.0000e+00, 2.6351e-17],\n",
      "        [1.0000e+00, 1.4401e-15],\n",
      "        [1.0000e+00, 8.4015e-11],\n",
      "        [1.0000e+00, 2.1176e-12],\n",
      "        [1.0000e+00, 7.7818e-15],\n",
      "        [1.0000e+00, 5.9464e-15],\n",
      "        [1.0000e+00, 2.5240e-13],\n",
      "        [1.0000e+00, 1.0269e-10],\n",
      "        [1.0000e+00, 7.6167e-15],\n",
      "        [1.0000e+00, 7.4713e-12],\n",
      "        [1.0000e+00, 2.9544e-12],\n",
      "        [1.0000e+00, 1.2129e-16],\n",
      "        [1.0000e+00, 4.8151e-08],\n",
      "        [1.0000e+00, 8.5765e-11],\n",
      "        [1.0000e+00, 1.3515e-10],\n",
      "        [1.0000e+00, 1.1772e-10],\n",
      "        [1.0000e+00, 2.3424e-12],\n",
      "        [1.0000e+00, 2.6735e-14],\n",
      "        [1.0000e+00, 6.4004e-12],\n",
      "        [1.0000e+00, 1.0632e-10],\n",
      "        [1.0000e+00, 7.0715e-10],\n",
      "        [1.0000e+00, 2.3548e-11],\n",
      "        [1.0000e+00, 3.1605e-12],\n",
      "        [1.0000e+00, 1.8825e-15],\n",
      "        [1.0000e+00, 4.2571e-13],\n",
      "        [1.0000e+00, 1.2506e-12],\n",
      "        [1.0000e+00, 2.4057e-15],\n",
      "        [1.0000e+00, 1.3088e-12],\n",
      "        [1.0000e+00, 6.3021e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [3200/37476                 (11%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 5.0640e-11],\n",
      "        [1.0000e+00, 2.2127e-15],\n",
      "        [1.0000e+00, 1.1373e-16],\n",
      "        [1.0000e+00, 1.5857e-09],\n",
      "        [1.0000e+00, 1.3209e-16],\n",
      "        [1.0000e+00, 3.3179e-16],\n",
      "        [1.0000e+00, 5.1802e-14],\n",
      "        [1.0000e+00, 2.0115e-12],\n",
      "        [1.0000e+00, 1.2711e-16],\n",
      "        [1.0000e+00, 1.4049e-12],\n",
      "        [1.0000e+00, 1.3978e-16],\n",
      "        [1.0000e+00, 4.6002e-10],\n",
      "        [1.0000e+00, 5.6655e-12],\n",
      "        [1.0000e+00, 2.0531e-12],\n",
      "        [1.0000e+00, 6.3811e-15],\n",
      "        [1.0000e+00, 4.7411e-15],\n",
      "        [1.0000e+00, 1.2240e-10],\n",
      "        [1.0000e+00, 1.2700e-16],\n",
      "        [1.0000e+00, 6.4745e-10],\n",
      "        [1.0000e+00, 5.1794e-10],\n",
      "        [1.0000e+00, 6.5351e-15],\n",
      "        [1.0000e+00, 4.9405e-11],\n",
      "        [1.0000e+00, 1.4785e-12],\n",
      "        [1.0000e+00, 6.5246e-11],\n",
      "        [1.0000e+00, 1.2492e-12],\n",
      "        [1.0000e+00, 1.0719e-14],\n",
      "        [1.0000e+00, 4.6877e-15],\n",
      "        [1.0000e+00, 2.6435e-13],\n",
      "        [1.0000e+00, 3.9082e-10],\n",
      "        [1.0000e+00, 2.6542e-13],\n",
      "        [1.0000e+00, 4.0571e-11],\n",
      "        [1.0000e+00, 6.1513e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [6400/37476                 (21%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 6.6253e-15],\n",
      "        [1.0000e+00, 1.2225e-11],\n",
      "        [1.0000e+00, 8.3465e-12],\n",
      "        [1.0000e+00, 3.7310e-11],\n",
      "        [1.0000e+00, 1.7432e-16],\n",
      "        [1.0000e+00, 5.2820e-08],\n",
      "        [1.0000e+00, 5.7328e-09],\n",
      "        [1.0000e+00, 1.0054e-11],\n",
      "        [1.0000e+00, 1.7713e-09],\n",
      "        [1.0000e+00, 7.6394e-09],\n",
      "        [1.0000e+00, 2.9082e-14],\n",
      "        [1.0000e+00, 1.6866e-11],\n",
      "        [1.0000e+00, 3.0250e-11],\n",
      "        [1.0000e+00, 7.3094e-12],\n",
      "        [1.0000e+00, 6.8480e-07],\n",
      "        [1.0000e+00, 2.9892e-12],\n",
      "        [1.0000e+00, 1.8421e-13],\n",
      "        [1.0000e+00, 3.3999e-15],\n",
      "        [1.0000e+00, 1.5064e-10],\n",
      "        [1.0000e+00, 3.5756e-10],\n",
      "        [1.0000e+00, 6.4446e-11],\n",
      "        [1.0000e+00, 1.2212e-12],\n",
      "        [1.0000e+00, 5.4117e-11],\n",
      "        [1.0000e+00, 5.2797e-16],\n",
      "        [1.0000e+00, 1.0210e-08],\n",
      "        [1.0000e+00, 2.7542e-12],\n",
      "        [1.0000e+00, 1.0728e-19],\n",
      "        [1.0000e+00, 5.7664e-11],\n",
      "        [1.0000e+00, 2.6293e-13],\n",
      "        [1.0000e+00, 6.4616e-11],\n",
      "        [1.0000e+00, 1.1489e-12],\n",
      "        [1.0000e+00, 1.1333e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [9600/37476                 (32%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 1.4956e-19],\n",
      "        [1.0000e+00, 4.5929e-13],\n",
      "        [1.0000e+00, 4.4426e-15],\n",
      "        [1.0000e+00, 4.3060e-11],\n",
      "        [1.0000e+00, 4.5247e-13],\n",
      "        [1.0000e+00, 7.9407e-13],\n",
      "        [1.0000e+00, 1.8695e-12],\n",
      "        [1.0000e+00, 6.3485e-15],\n",
      "        [1.0000e+00, 1.9003e-16],\n",
      "        [1.0000e+00, 5.6379e-12],\n",
      "        [1.0000e+00, 1.0351e-13],\n",
      "        [1.0000e+00, 1.0449e-11],\n",
      "        [1.0000e+00, 9.9398e-12],\n",
      "        [1.0000e+00, 7.2738e-15],\n",
      "        [1.0000e+00, 4.9842e-12],\n",
      "        [1.0000e+00, 6.7630e-11],\n",
      "        [1.0000e+00, 1.1657e-10],\n",
      "        [1.0000e+00, 7.9423e-08],\n",
      "        [1.0000e+00, 5.8945e-13],\n",
      "        [1.0000e+00, 2.6726e-11],\n",
      "        [1.0000e+00, 9.5392e-19],\n",
      "        [1.0000e+00, 6.2230e-14],\n",
      "        [1.0000e+00, 2.6701e-13],\n",
      "        [1.0000e+00, 1.2905e-18],\n",
      "        [1.0000e+00, 1.4582e-13],\n",
      "        [1.0000e+00, 1.4492e-10],\n",
      "        [1.0000e+00, 1.8218e-11],\n",
      "        [1.0000e+00, 3.3071e-11],\n",
      "        [1.0000e+00, 8.9992e-17],\n",
      "        [1.0000e+00, 1.4473e-15],\n",
      "        [1.0000e+00, 1.6756e-12],\n",
      "        [1.0000e+00, 1.3147e-16]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [12800/37476                 (43%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 6.5822e-09],\n",
      "        [1.0000e+00, 2.7749e-12],\n",
      "        [1.0000e+00, 9.0480e-11],\n",
      "        [1.0000e+00, 2.8377e-11],\n",
      "        [1.0000e+00, 2.0714e-18],\n",
      "        [1.0000e+00, 3.1750e-11],\n",
      "        [1.0000e+00, 9.9325e-12],\n",
      "        [1.0000e+00, 8.9652e-11],\n",
      "        [1.0000e+00, 4.4085e-06],\n",
      "        [1.0000e+00, 3.2955e-12],\n",
      "        [1.0000e+00, 9.8378e-10],\n",
      "        [1.0000e+00, 2.5467e-13],\n",
      "        [1.0000e+00, 8.4822e-15],\n",
      "        [1.0000e+00, 2.2956e-13],\n",
      "        [1.0000e+00, 1.5933e-11],\n",
      "        [1.0000e+00, 5.3569e-11],\n",
      "        [1.0000e+00, 4.9078e-12],\n",
      "        [1.0000e+00, 2.3771e-12],\n",
      "        [1.0000e+00, 3.7125e-11],\n",
      "        [1.0000e+00, 2.7203e-13],\n",
      "        [1.0000e+00, 1.0315e-10],\n",
      "        [1.0000e+00, 2.4796e-09],\n",
      "        [1.0000e+00, 2.3578e-10],\n",
      "        [1.0000e+00, 5.7418e-15],\n",
      "        [1.0000e+00, 1.2042e-13],\n",
      "        [1.0000e+00, 5.8457e-13],\n",
      "        [1.0000e+00, 9.1016e-13],\n",
      "        [1.0000e+00, 1.5559e-10],\n",
      "        [1.0000e+00, 1.8316e-10],\n",
      "        [1.0000e+00, 3.3830e-19],\n",
      "        [1.0000e+00, 3.7254e-09],\n",
      "        [1.0000e+00, 8.4491e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [16000/37476                 (53%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 5.6131e-09],\n",
      "        [1.0000e+00, 1.0382e-10],\n",
      "        [1.0000e+00, 9.5381e-12],\n",
      "        [1.0000e+00, 2.2927e-10],\n",
      "        [1.0000e+00, 4.3807e-08],\n",
      "        [1.0000e+00, 2.1128e-14],\n",
      "        [1.0000e+00, 2.4247e-09],\n",
      "        [1.0000e+00, 3.8705e-14],\n",
      "        [1.0000e+00, 4.3213e-12],\n",
      "        [1.0000e+00, 1.3943e-12],\n",
      "        [1.0000e+00, 1.2678e-10],\n",
      "        [1.0000e+00, 4.7051e-09],\n",
      "        [1.0000e+00, 1.0279e-09],\n",
      "        [1.0000e+00, 5.0646e-08],\n",
      "        [1.0000e+00, 1.4271e-11],\n",
      "        [1.0000e+00, 8.8486e-15],\n",
      "        [1.0000e+00, 3.8626e-12],\n",
      "        [1.0000e+00, 1.6881e-11],\n",
      "        [1.0000e+00, 1.1187e-15],\n",
      "        [1.0000e+00, 7.2239e-12],\n",
      "        [1.0000e+00, 1.2679e-12],\n",
      "        [1.0000e+00, 2.1066e-12],\n",
      "        [1.0000e+00, 2.1177e-13],\n",
      "        [1.0000e+00, 1.7953e-09],\n",
      "        [1.0000e+00, 1.2148e-07],\n",
      "        [1.0000e+00, 2.5069e-14],\n",
      "        [1.0000e+00, 9.3298e-12],\n",
      "        [1.0000e+00, 9.7890e-15],\n",
      "        [1.0000e+00, 6.4723e-14],\n",
      "        [1.0000e+00, 2.0877e-11],\n",
      "        [1.0000e+00, 1.5402e-10],\n",
      "        [1.0000e+00, 5.8786e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [19200/37476                 (64%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 1.2866e-13],\n",
      "        [1.0000e+00, 1.9134e-16],\n",
      "        [1.0000e+00, 1.5984e-10],\n",
      "        [1.0000e+00, 1.4026e-13],\n",
      "        [1.0000e+00, 2.6855e-11],\n",
      "        [1.0000e+00, 1.2241e-12],\n",
      "        [1.0000e+00, 4.9066e-15],\n",
      "        [1.0000e+00, 2.2545e-14],\n",
      "        [1.0000e+00, 4.4275e-15],\n",
      "        [1.0000e+00, 2.2016e-15],\n",
      "        [1.0000e+00, 8.1024e-11],\n",
      "        [1.0000e+00, 1.6319e-14],\n",
      "        [1.0000e+00, 5.4805e-11],\n",
      "        [1.0000e+00, 1.6541e-11],\n",
      "        [1.0000e+00, 1.3105e-12],\n",
      "        [1.0000e+00, 2.8267e-15],\n",
      "        [1.0000e+00, 5.2666e-08],\n",
      "        [1.0000e+00, 8.3065e-12],\n",
      "        [1.0000e+00, 6.6637e-17],\n",
      "        [1.0000e+00, 3.9613e-15],\n",
      "        [1.0000e+00, 1.0691e-11],\n",
      "        [1.0000e+00, 1.1181e-11],\n",
      "        [1.0000e+00, 6.3796e-11],\n",
      "        [1.0000e+00, 5.5078e-10],\n",
      "        [1.0000e+00, 1.5821e-13],\n",
      "        [1.0000e+00, 3.1644e-13],\n",
      "        [1.0000e+00, 2.4638e-10],\n",
      "        [1.0000e+00, 4.3194e-11],\n",
      "        [1.0000e+00, 7.1005e-13],\n",
      "        [1.0000e+00, 7.0220e-12],\n",
      "        [1.0000e+00, 1.9217e-11],\n",
      "        [1.0000e+00, 2.0315e-14]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [22400/37476                 (75%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 1.7195e-11],\n",
      "        [1.0000e+00, 3.8518e-11],\n",
      "        [1.0000e+00, 4.9793e-12],\n",
      "        [1.0000e+00, 5.4184e-09],\n",
      "        [1.0000e+00, 7.5232e-08],\n",
      "        [1.0000e+00, 4.4121e-11],\n",
      "        [1.0000e+00, 9.4574e-11],\n",
      "        [1.0000e+00, 6.2290e-15],\n",
      "        [1.0000e+00, 1.4943e-10],\n",
      "        [1.0000e+00, 4.4985e-10],\n",
      "        [1.0000e+00, 2.9371e-11],\n",
      "        [1.0000e+00, 3.5717e-12],\n",
      "        [1.0000e+00, 1.3929e-15],\n",
      "        [1.0000e+00, 5.5610e-12],\n",
      "        [1.0000e+00, 9.5180e-13],\n",
      "        [1.0000e+00, 2.1450e-09],\n",
      "        [1.0000e+00, 2.5388e-11],\n",
      "        [1.0000e+00, 4.4123e-12],\n",
      "        [1.0000e+00, 6.3397e-10],\n",
      "        [1.0000e+00, 3.3566e-12],\n",
      "        [1.0000e+00, 1.9942e-15],\n",
      "        [1.0000e+00, 1.6376e-10],\n",
      "        [1.0000e+00, 3.9247e-16],\n",
      "        [1.0000e+00, 5.7642e-13],\n",
      "        [1.0000e+00, 2.5595e-11],\n",
      "        [1.0000e+00, 2.3412e-11],\n",
      "        [1.0000e+00, 2.2336e-12],\n",
      "        [1.0000e+00, 4.9795e-11],\n",
      "        [1.0000e+00, 3.0802e-09],\n",
      "        [1.0000e+00, 1.1840e-12],\n",
      "        [1.0000e+00, 1.4088e-12],\n",
      "        [1.0000e+00, 3.9583e-15]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [25600/37476                 (85%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 4.1182e-10],\n",
      "        [1.0000e+00, 9.3166e-11],\n",
      "        [1.0000e+00, 4.3616e-12],\n",
      "        [1.0000e+00, 7.5227e-16],\n",
      "        [1.0000e+00, 3.7151e-18],\n",
      "        [1.0000e+00, 3.7344e-13],\n",
      "        [1.0000e+00, 1.0403e-14],\n",
      "        [1.0000e+00, 8.4155e-10],\n",
      "        [1.0000e+00, 4.4700e-18],\n",
      "        [1.0000e+00, 3.7952e-11],\n",
      "        [1.0000e+00, 2.1520e-12],\n",
      "        [1.0000e+00, 1.1276e-11],\n",
      "        [1.0000e+00, 1.1435e-16],\n",
      "        [1.0000e+00, 7.1271e-15],\n",
      "        [1.0000e+00, 1.1720e-11],\n",
      "        [1.0000e+00, 7.4375e-14],\n",
      "        [1.0000e+00, 5.4592e-12],\n",
      "        [1.0000e+00, 2.8344e-09],\n",
      "        [1.0000e+00, 1.3769e-16],\n",
      "        [1.0000e+00, 6.1237e-17],\n",
      "        [1.0000e+00, 1.5711e-12],\n",
      "        [1.0000e+00, 1.2629e-11],\n",
      "        [1.0000e+00, 3.6316e-12],\n",
      "        [1.0000e+00, 1.6976e-09],\n",
      "        [1.0000e+00, 7.7590e-17],\n",
      "        [1.0000e+00, 1.8931e-09],\n",
      "        [1.0000e+00, 3.0468e-10],\n",
      "        [1.0000e+00, 2.4012e-13],\n",
      "        [1.0000e+00, 5.5043e-10],\n",
      "        [1.0000e+00, 4.8518e-14],\n",
      "        [1.0000e+00, 7.7213e-12],\n",
      "        [1.0000e+00, 9.8743e-10]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 2 [28800/37476                 (96%)]\tLoss: 0.375762\n",
      "The weight of the batch is tensor([ 1.0000, 15.0000], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 2.6000e-12],\n",
      "        [1.0000e+00, 3.6436e-11],\n",
      "        [1.0000e+00, 1.0747e-10],\n",
      "        [1.0000e+00, 3.3022e-08],\n",
      "        [1.0000e+00, 1.7227e-09],\n",
      "        [1.0000e+00, 2.1487e-12],\n",
      "        [1.0000e+00, 5.1160e-09],\n",
      "        [1.0000e+00, 1.8919e-12],\n",
      "        [1.0000e+00, 1.3941e-11],\n",
      "        [1.0000e+00, 8.6374e-13],\n",
      "        [1.0000e+00, 1.0417e-10],\n",
      "        [1.0000e+00, 1.7701e-11],\n",
      "        [1.0000e+00, 1.0940e-11],\n",
      "        [1.0000e+00, 1.5966e-14],\n",
      "        [1.0000e+00, 1.4204e-11],\n",
      "        [1.0000e+00, 6.4049e-11],\n",
      "        [1.0000e+00, 5.4794e-13],\n",
      "        [1.0000e+00, 3.0342e-16],\n",
      "        [1.0000e+00, 1.3006e-12],\n",
      "        [1.0000e+00, 1.8450e-10],\n",
      "        [1.0000e+00, 4.4640e-08],\n",
      "        [1.0000e+00, 1.0348e-15],\n",
      "        [1.0000e+00, 6.2682e-10],\n",
      "        [1.0000e+00, 6.3984e-12],\n",
      "        [1.0000e+00, 9.8160e-08],\n",
      "        [1.0000e+00, 5.0268e-11],\n",
      "        [1.0000e+00, 1.0736e-14],\n",
      "        [1.0000e+00, 6.6067e-12],\n",
      "        [1.0000e+00, 1.2242e-12],\n",
      "        [1.0000e+00, 2.5502e-09],\n",
      "        [1.0000e+00, 5.8840e-14],\n",
      "        [1.0000e+00, 2.6836e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Test set for fold0: Average Loss:           79.3657, Accuracy: 7294/37476           (19%)\n",
      "Training stage for Flod 0 Epoch: 3 [0/37476                 (0%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 1.0134e-13],\n",
      "        [1.0000e+00, 7.9107e-10],\n",
      "        [1.0000e+00, 5.3436e-13],\n",
      "        [1.0000e+00, 8.4616e-12],\n",
      "        [1.0000e+00, 3.3065e-11],\n",
      "        [1.0000e+00, 3.4451e-10],\n",
      "        [1.0000e+00, 2.2884e-12],\n",
      "        [1.0000e+00, 7.1268e-15],\n",
      "        [1.0000e+00, 1.6033e-12],\n",
      "        [1.0000e+00, 3.0890e-13],\n",
      "        [1.0000e+00, 5.6810e-14],\n",
      "        [1.0000e+00, 5.1859e-11],\n",
      "        [1.0000e+00, 1.8382e-14],\n",
      "        [1.0000e+00, 1.1087e-15],\n",
      "        [1.0000e+00, 1.0433e-15],\n",
      "        [1.0000e+00, 1.3774e-10],\n",
      "        [1.0000e+00, 1.2714e-10],\n",
      "        [1.0000e+00, 3.9324e-12],\n",
      "        [1.0000e+00, 8.6564e-12],\n",
      "        [1.0000e+00, 2.2618e-17],\n",
      "        [1.0000e+00, 1.1765e-11],\n",
      "        [1.0000e+00, 5.7464e-16],\n",
      "        [1.0000e+00, 1.6223e-09],\n",
      "        [1.0000e+00, 2.6203e-13],\n",
      "        [1.0000e+00, 1.2491e-15],\n",
      "        [1.0000e+00, 2.2303e-17],\n",
      "        [1.0000e+00, 6.8205e-12],\n",
      "        [1.0000e+00, 1.2313e-11],\n",
      "        [1.0000e+00, 3.6904e-14],\n",
      "        [1.0000e+00, 5.3009e-11],\n",
      "        [1.0000e+00, 1.0477e-13],\n",
      "        [1.0000e+00, 2.6685e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 3 [3200/37476                 (11%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 5.3698e-15],\n",
      "        [1.0000e+00, 7.6568e-12],\n",
      "        [1.0000e+00, 5.6048e-11],\n",
      "        [1.0000e+00, 7.6174e-17],\n",
      "        [1.0000e+00, 9.4401e-12],\n",
      "        [1.0000e+00, 5.5292e-11],\n",
      "        [1.0000e+00, 1.3254e-16],\n",
      "        [1.0000e+00, 8.4710e-14],\n",
      "        [1.0000e+00, 1.3860e-16],\n",
      "        [1.0000e+00, 1.0036e-16],\n",
      "        [1.0000e+00, 3.2760e-11],\n",
      "        [1.0000e+00, 4.4651e-12],\n",
      "        [1.0000e+00, 1.0605e-14],\n",
      "        [1.0000e+00, 1.6720e-14],\n",
      "        [1.0000e+00, 9.3616e-13],\n",
      "        [1.0000e+00, 8.0483e-13],\n",
      "        [1.0000e+00, 1.0102e-10],\n",
      "        [1.0000e+00, 2.8644e-15],\n",
      "        [1.0000e+00, 8.1703e-12],\n",
      "        [1.0000e+00, 4.4830e-15],\n",
      "        [1.0000e+00, 6.9484e-12],\n",
      "        [1.0000e+00, 1.5580e-12],\n",
      "        [1.0000e+00, 8.5280e-15],\n",
      "        [1.0000e+00, 1.9337e-12],\n",
      "        [1.0000e+00, 4.6549e-14],\n",
      "        [1.0000e+00, 6.4747e-15],\n",
      "        [1.0000e+00, 2.1813e-16],\n",
      "        [1.0000e+00, 2.3862e-08],\n",
      "        [1.0000e+00, 5.8838e-12],\n",
      "        [1.0000e+00, 9.9761e-10],\n",
      "        [1.0000e+00, 5.4490e-16],\n",
      "        [1.0000e+00, 2.1287e-17]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 3 [6400/37476                 (21%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 2.0441e-13],\n",
      "        [1.0000e+00, 2.4937e-18],\n",
      "        [1.0000e+00, 2.2611e-09],\n",
      "        [1.0000e+00, 4.4399e-16],\n",
      "        [1.0000e+00, 2.3641e-14],\n",
      "        [1.0000e+00, 4.2241e-12],\n",
      "        [1.0000e+00, 4.1410e-13],\n",
      "        [1.0000e+00, 7.1917e-12],\n",
      "        [1.0000e+00, 6.8174e-14],\n",
      "        [1.0000e+00, 5.0492e-12],\n",
      "        [1.0000e+00, 1.6701e-09],\n",
      "        [1.0000e+00, 4.6891e-12],\n",
      "        [1.0000e+00, 6.7126e-15],\n",
      "        [1.0000e+00, 3.9852e-15],\n",
      "        [1.0000e+00, 9.9715e-15],\n",
      "        [1.0000e+00, 3.4293e-10],\n",
      "        [1.0000e+00, 3.5336e-11],\n",
      "        [1.0000e+00, 2.1900e-13],\n",
      "        [1.0000e+00, 2.6569e-16],\n",
      "        [1.0000e+00, 2.0371e-13],\n",
      "        [1.0000e+00, 3.9990e-16],\n",
      "        [1.0000e+00, 1.4558e-12],\n",
      "        [1.0000e+00, 8.2495e-13],\n",
      "        [1.0000e+00, 7.4890e-10],\n",
      "        [1.0000e+00, 6.2526e-11],\n",
      "        [1.0000e+00, 3.1913e-13],\n",
      "        [1.0000e+00, 2.0519e-12],\n",
      "        [1.0000e+00, 7.5462e-17],\n",
      "        [1.0000e+00, 7.7294e-11],\n",
      "        [1.0000e+00, 7.8117e-13],\n",
      "        [1.0000e+00, 2.3490e-12],\n",
      "        [1.0000e+00, 5.4917e-13]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 3 [9600/37476                 (32%)]\tLoss: 0.344512\n",
      "The weight of the batch is tensor([ 1., 31.], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 2.9922e-16],\n",
      "        [1.0000e+00, 2.5725e-16],\n",
      "        [1.0000e+00, 5.3329e-11],\n",
      "        [1.0000e+00, 8.1156e-10],\n",
      "        [1.0000e+00, 1.6827e-14],\n",
      "        [1.0000e+00, 6.2121e-12],\n",
      "        [1.0000e+00, 4.0965e-11],\n",
      "        [1.0000e+00, 1.3902e-12],\n",
      "        [1.0000e+00, 2.5573e-13],\n",
      "        [1.0000e+00, 8.8305e-17],\n",
      "        [1.0000e+00, 7.7735e-12],\n",
      "        [1.0000e+00, 1.1871e-11],\n",
      "        [1.0000e+00, 6.7219e-11],\n",
      "        [1.0000e+00, 2.0801e-11],\n",
      "        [1.0000e+00, 1.8251e-13],\n",
      "        [1.0000e+00, 5.6081e-09],\n",
      "        [1.0000e+00, 7.7752e-17],\n",
      "        [1.0000e+00, 6.3311e-11],\n",
      "        [1.0000e+00, 4.0323e-14],\n",
      "        [1.0000e+00, 6.1958e-12],\n",
      "        [1.0000e+00, 9.5217e-11],\n",
      "        [1.0000e+00, 5.6370e-11],\n",
      "        [1.0000e+00, 7.8457e-12],\n",
      "        [1.0000e+00, 3.5580e-18],\n",
      "        [1.0000e+00, 3.1566e-15],\n",
      "        [1.0000e+00, 4.7707e-09],\n",
      "        [1.0000e+00, 1.1089e-26],\n",
      "        [1.0000e+00, 3.7057e-15],\n",
      "        [1.0000e+00, 2.1762e-12],\n",
      "        [1.0000e+00, 6.4826e-10],\n",
      "        [1.0000e+00, 2.4867e-11],\n",
      "        [1.0000e+00, 2.6124e-12]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "Training stage for Flod 0 Epoch: 3 [12800/37476                 (43%)]\tLoss: 0.313262\n",
      "The weight of the batch is tensor([1., inf], device='cuda:0'), \n",
      " The output is tensor([[1.0000e+00, 2.1503e-07],\n",
      "        [1.0000e+00, 8.8228e-11],\n",
      "        [1.0000e+00, 4.6474e-11],\n",
      "        [1.0000e+00, 4.5832e-11],\n",
      "        [1.0000e+00, 3.5940e-14],\n",
      "        [1.0000e+00, 3.5049e-10],\n",
      "        [1.0000e+00, 1.0774e-13],\n",
      "        [1.0000e+00, 1.1760e-15],\n",
      "        [1.0000e+00, 9.5495e-14],\n",
      "        [1.0000e+00, 3.4882e-14],\n",
      "        [1.0000e+00, 2.2965e-16],\n",
      "        [1.0000e+00, 4.1230e-10],\n",
      "        [1.0000e+00, 8.0250e-12],\n",
      "        [1.0000e+00, 6.8337e-07],\n",
      "        [1.0000e+00, 1.1821e-16],\n",
      "        [1.0000e+00, 1.8005e-08],\n",
      "        [1.0000e+00, 3.5447e-10],\n",
      "        [1.0000e+00, 3.0648e-11],\n",
      "        [1.0000e+00, 1.5127e-10],\n",
      "        [1.0000e+00, 8.1898e-09],\n",
      "        [1.0000e+00, 1.2927e-09],\n",
      "        [1.0000e+00, 1.3180e-14],\n",
      "        [1.0000e+00, 2.4065e-14],\n",
      "        [1.0000e+00, 1.0025e-13],\n",
      "        [1.0000e+00, 3.8284e-15],\n",
      "        [1.0000e+00, 6.9670e-11],\n",
      "        [1.0000e+00, 2.8493e-16],\n",
      "        [1.0000e+00, 2.7972e-17],\n",
      "        [1.0000e+00, 1.3235e-09],\n",
      "        [1.0000e+00, 2.6532e-16],\n",
      "        [1.0000e+00, 2.8529e-12],\n",
      "        [1.0000e+00, 5.6710e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>), \n",
      " The one_hot_target is tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m test_accuracy_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     train_losses, train_correct \u001b[39m=\u001b[39m train(fold, model, device, train_loader, optimizer, epoch, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     test_losses, test_correct \u001b[39m=\u001b[39m test(fold, model, device, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     train_losses_history\u001b[39m.\u001b[39mappend(train_losses)\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fold, model, device, train_loader, optimizer, epoch, criterion)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# one-hot encoding the target\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mbool)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb Cell 18\u001b[0m in \u001b[0;36mpMHC_TCRDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_features[idx], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X_features, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    # undersample the training data\n",
    "    # X_res, y_res = rus.fit_resample(TCRData)\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        y_train = [TCRData.y[i] for i in train_idx]\n",
    "        class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "        weights = 1. / class_sample_count\n",
    "        samples_weight = np.array([weights[t] for t in y_train])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "        train_dataloader = DataLoader(TCRData, batch_size=batch_size, sampler=sampler, drop_last=True)\n",
    "\n",
    "        # train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        # test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        # train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "        #     batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        # test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "        #     batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch, criterion)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    # ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    # ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    # ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    # ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "    ax[0].plot(train_losses_history, label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, label=f\"test accuracy fold{fold}\")\n",
    "    break\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# put the legend out of the figure, and adjust the position, prevent the figure from being covered\n",
    "# ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# save the figure\n",
    "# fig.savefig(\"/DATA/User/wuxinchao/project/pMHC-TCR/result/pMHC_without_em_with_encoder_loss_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((32, 92)).to(device)\n",
    "# # a.shape\n",
    "d = model(c).to(device)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "# loss = nn.CrossEntropyLoss()(model(a).to(torch.float32).view(32, 1, 1), torch.ones((32, 1, 1)).to(torch.float32).to(device)) / 32\n",
    "# loss.to(device)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "a = torch.randint(0,2,(32,1)) # mask\n",
    "b = torch.zeros((32,2))\n",
    "# get the value of b where a is 1\n",
    "a = a.to(torch.bool)\n",
    "b[(a==1).squeeze(),1] = 1\n",
    "b[(a==0).squeeze(),0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0259)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss = loss_fn(d, torch.ones((32,2)).to(device))\n",
    "# loss.backward()\n",
    "# train_losses_history\n",
    "# test_losses_history\n",
    "# test_accuracy_history\n",
    "# train_accuracy_history\n",
    "# nn.Softmax(dim=1)(torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float32))\n",
    "# output = nn.Softmax(dim=1)(torch.tensor([[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]], dtype=torch.float32))\n",
    "# nn.CrossEntropyLoss()(output, torch.tensor([[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]))\n",
    "# (b == torch.tensor([1,0])).sum() / (b == torch.tensor([0,1])).sum()\n",
    "# b\n",
    "# 17 / 15\n",
    "\n",
    "# a = torch.tensor([[1,2,3,4,5],[3,3,4,5,6]], dtype=torch.float32) # print(a.shape) shape: 2, 5\n",
    "# a.argmax(dim=1, keepdim=True).shape # shape: 2, 1\n",
    "# a.argmax(dim=1) # shape: 1, 2\n",
    "\n",
    "TCRData.y.sum() / len(TCRData.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# loss.backward()\n",
    "# torch.zeros((32,2))\n",
    "# model(torch.randn((32,92)).to(device))\n",
    "num_dict = {1:2, 2:3, 3:0}\n",
    "# delete the non-zero key\n",
    "num_dict.pop(1)\n",
    "num_dict.pop(2)\n",
    "# print(num_dict.keys())\n",
    "# get the key\n",
    "print(list(num_dict.keys())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, the features are concatanated and used to predict the binding affinity of pMHC-TCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_pred(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        super(pMHC_TCR_pred, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        # use the encoded features to predict the binding affinity through MLP\n",
    "        self.Linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_encode(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        self.input_size = input_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.seq_length, self.input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :] # return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 82, 'BseqCDR3': 24}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "print(len_map)\n",
    "# drop the rows with length == max length, which is much longer than the others\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < len_map[\"AseqCDR3\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.200e+02, 2.288e+03, 2.800e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.100e+01]),\n",
       " array([ 5. , 12.6, 20.2, 27.8, 35.4, 43. , 50.6, 58.2, 65.8, 73.4, 81. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df[\"AseqCDR3\"].value_counts()\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0) # find the longest seq\n",
    "# df.loc[df[\"AseqCDR3\"].str.len() == 83, \"AseqCDR3\"]\n",
    "\n",
    "plt.hist(df[\"AseqCDR3\"].str.len().sort_values(axis=0))\n",
    "# plt.show()\n",
    "# df = df.loc[df[\"AseqCDR3\"].str.len() < 83, :]\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_map\n",
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/20230228.csv\")\n",
    "# df.loc[df[\"AseqCDR3\"].str.contains(\"_\"),]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
