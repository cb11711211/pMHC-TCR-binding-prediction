{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "# check the kernel running in the notebook\n",
    "# !uname -a\n",
    "# find the variables in the notebook\n",
    "# %whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seqCDR[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_encode_data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # drop the rows with duplicate CDR3 sequences\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        \n",
    "        # drop the rows with length == max length, which is much longer than the others\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # If there is any NaN value, drop the row\n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        # concatenate the encoded features\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "        \n",
    "        # discard the duplicate rows, keep the first one\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 49)\n",
    "            nn.Conv1d(5, 10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 25) based on the formula for conv1d: (W + 2P - K)/S + 1 = (49 + 2*1 - 3)/2 + 1 = 25\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 10, 23), 25 - 2 = 23 \n",
    "\n",
    "            nn.Conv1d(10, 15, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12) based on the formula for conv1d: (W + 2P - K)/S + 1 = (23 + 2*1 - 3)/2 + 1 = 12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 10), 12 - 2 = 10\n",
    "\n",
    "            nn.Conv1d(15, 20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 5) based on the formula for conv1d: (W + 2P - K)/S + 1 = (10 + 2*1 - 3)/2 + 1 = 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 3)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 3)\n",
    "            nn.ConvTranspose1d(20, 15, kernel_size=3, stride=3, padding=1),\n",
    "            # (batch_size, 15, 5), based on the formula for convtranspose1d: (W−1)S−2P+F = (3-1)*3-2*1+3= 7\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(15, 10, kernel_size=7, stride=3, padding=1),\n",
    "            # (batch_size, 10, 23) based on the formula for convtranspose1d: (W−1)S−2P+F = (7-1)*3-2*1+7= 23\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(10, 5, kernel_size=7, stride=2, padding=1),\n",
    "            # (batch_size, 5, 49) based on the formula for convtranspose1d: (W−1)S−2P+F = (23-1)*2-2*1+7= 49\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = torch.tensor(x, dtype=np.float32)\n",
    "        # x = torch.tensor(x, dtype=torch.float)\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        # print(f\"encoding shape: {encoded.shape}\")\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2492 (0%)]\tLoss: 1.422211\n",
      "Train Epoch: 1 [1600/2492 (65%)]\tLoss: 1.269276\n",
      "Train Epoch: 2 [0/2492 (0%)]\tLoss: 1.270869\n",
      "Train Epoch: 2 [1600/2492 (65%)]\tLoss: 1.301361\n",
      "Train Epoch: 3 [0/2492 (0%)]\tLoss: 1.353347\n",
      "Train Epoch: 3 [1600/2492 (65%)]\tLoss: 1.198825\n",
      "Train Epoch: 4 [0/2492 (0%)]\tLoss: 1.316884\n",
      "Train Epoch: 4 [1600/2492 (65%)]\tLoss: 1.105351\n",
      "Train Epoch: 5 [0/2492 (0%)]\tLoss: 1.217188\n",
      "Train Epoch: 5 [1600/2492 (65%)]\tLoss: 1.153005\n",
      "Train Epoch: 6 [0/2492 (0%)]\tLoss: 0.985455\n",
      "Train Epoch: 6 [1600/2492 (65%)]\tLoss: 1.065668\n",
      "Train Epoch: 7 [0/2492 (0%)]\tLoss: 1.051921\n",
      "Train Epoch: 7 [1600/2492 (65%)]\tLoss: 1.016856\n",
      "Train Epoch: 8 [0/2492 (0%)]\tLoss: 1.158589\n",
      "Train Epoch: 8 [1600/2492 (65%)]\tLoss: 1.101415\n",
      "Train Epoch: 9 [0/2492 (0%)]\tLoss: 1.090983\n",
      "Train Epoch: 9 [1600/2492 (65%)]\tLoss: 1.065062\n",
      "Train Epoch: 10 [0/2492 (0%)]\tLoss: 1.122251\n",
      "Train Epoch: 10 [1600/2492 (65%)]\tLoss: 1.083244\n",
      "Train Epoch: 11 [0/2492 (0%)]\tLoss: 1.141127\n",
      "Train Epoch: 11 [1600/2492 (65%)]\tLoss: 1.032887\n",
      "Train Epoch: 12 [0/2492 (0%)]\tLoss: 1.072594\n",
      "Train Epoch: 12 [1600/2492 (65%)]\tLoss: 0.974644\n",
      "Train Epoch: 13 [0/2492 (0%)]\tLoss: 1.030072\n",
      "Train Epoch: 13 [1600/2492 (65%)]\tLoss: 0.946859\n",
      "Train Epoch: 14 [0/2492 (0%)]\tLoss: 1.069577\n",
      "Train Epoch: 14 [1600/2492 (65%)]\tLoss: 1.167268\n",
      "Train Epoch: 15 [0/2492 (0%)]\tLoss: 1.114951\n",
      "Train Epoch: 15 [1600/2492 (65%)]\tLoss: 1.155847\n",
      "Train Epoch: 16 [0/2492 (0%)]\tLoss: 0.954150\n",
      "Train Epoch: 16 [1600/2492 (65%)]\tLoss: 0.981289\n",
      "Train Epoch: 17 [0/2492 (0%)]\tLoss: 1.093097\n",
      "Train Epoch: 17 [1600/2492 (65%)]\tLoss: 0.955089\n",
      "Train Epoch: 18 [0/2492 (0%)]\tLoss: 1.086022\n",
      "Train Epoch: 18 [1600/2492 (65%)]\tLoss: 0.995435\n",
      "Train Epoch: 19 [0/2492 (0%)]\tLoss: 0.971045\n",
      "Train Epoch: 19 [1600/2492 (65%)]\tLoss: 1.015016\n",
      "Train Epoch: 20 [0/2492 (0%)]\tLoss: 1.036492\n",
      "Train Epoch: 20 [1600/2492 (65%)]\tLoss: 1.102179\n",
      "Train Epoch: 21 [0/2492 (0%)]\tLoss: 1.119652\n",
      "Train Epoch: 21 [1600/2492 (65%)]\tLoss: 0.969431\n",
      "Train Epoch: 22 [0/2492 (0%)]\tLoss: 1.091771\n",
      "Train Epoch: 22 [1600/2492 (65%)]\tLoss: 0.892847\n",
      "Train Epoch: 23 [0/2492 (0%)]\tLoss: 1.033807\n",
      "Train Epoch: 23 [1600/2492 (65%)]\tLoss: 0.962311\n",
      "Train Epoch: 24 [0/2492 (0%)]\tLoss: 0.925607\n",
      "Train Epoch: 24 [1600/2492 (65%)]\tLoss: 1.076444\n",
      "Train Epoch: 25 [0/2492 (0%)]\tLoss: 0.914540\n",
      "Train Epoch: 25 [1600/2492 (65%)]\tLoss: 0.929081\n",
      "Train Epoch: 26 [0/2492 (0%)]\tLoss: 1.018931\n",
      "Train Epoch: 26 [1600/2492 (65%)]\tLoss: 0.876093\n",
      "Train Epoch: 27 [0/2492 (0%)]\tLoss: 0.947629\n",
      "Train Epoch: 27 [1600/2492 (65%)]\tLoss: 1.014299\n",
      "Train Epoch: 28 [0/2492 (0%)]\tLoss: 0.915245\n",
      "Train Epoch: 28 [1600/2492 (65%)]\tLoss: 1.008010\n",
      "Train Epoch: 29 [0/2492 (0%)]\tLoss: 0.914160\n",
      "Train Epoch: 29 [1600/2492 (65%)]\tLoss: 1.048587\n",
      "Train Epoch: 30 [0/2492 (0%)]\tLoss: 1.006159\n",
      "Train Epoch: 30 [1600/2492 (65%)]\tLoss: 1.031503\n",
      "Train Epoch: 31 [0/2492 (0%)]\tLoss: 0.994206\n",
      "Train Epoch: 31 [1600/2492 (65%)]\tLoss: 0.911761\n",
      "Train Epoch: 32 [0/2492 (0%)]\tLoss: 0.950366\n",
      "Train Epoch: 32 [1600/2492 (65%)]\tLoss: 0.910195\n",
      "Train Epoch: 33 [0/2492 (0%)]\tLoss: 0.905822\n",
      "Train Epoch: 33 [1600/2492 (65%)]\tLoss: 1.010210\n",
      "Train Epoch: 34 [0/2492 (0%)]\tLoss: 0.950278\n",
      "Train Epoch: 34 [1600/2492 (65%)]\tLoss: 0.983163\n",
      "Train Epoch: 35 [0/2492 (0%)]\tLoss: 1.048163\n",
      "Train Epoch: 35 [1600/2492 (65%)]\tLoss: 0.928133\n",
      "Train Epoch: 36 [0/2492 (0%)]\tLoss: 0.911688\n",
      "Train Epoch: 36 [1600/2492 (65%)]\tLoss: 0.957520\n",
      "Train Epoch: 37 [0/2492 (0%)]\tLoss: 0.936063\n",
      "Train Epoch: 37 [1600/2492 (65%)]\tLoss: 0.939569\n",
      "Train Epoch: 38 [0/2492 (0%)]\tLoss: 0.940515\n",
      "Train Epoch: 38 [1600/2492 (65%)]\tLoss: 1.011984\n",
      "Train Epoch: 39 [0/2492 (0%)]\tLoss: 0.997811\n",
      "Train Epoch: 39 [1600/2492 (65%)]\tLoss: 0.863154\n",
      "Train Epoch: 40 [0/2492 (0%)]\tLoss: 0.981072\n",
      "Train Epoch: 40 [1600/2492 (65%)]\tLoss: 0.923593\n",
      "Train Epoch: 41 [0/2492 (0%)]\tLoss: 1.038069\n",
      "Train Epoch: 41 [1600/2492 (65%)]\tLoss: 0.883902\n",
      "Train Epoch: 42 [0/2492 (0%)]\tLoss: 0.912875\n",
      "Train Epoch: 42 [1600/2492 (65%)]\tLoss: 0.912833\n",
      "Train Epoch: 43 [0/2492 (0%)]\tLoss: 1.001985\n",
      "Train Epoch: 43 [1600/2492 (65%)]\tLoss: 0.869817\n",
      "Train Epoch: 44 [0/2492 (0%)]\tLoss: 0.906789\n",
      "Train Epoch: 44 [1600/2492 (65%)]\tLoss: 1.009579\n",
      "Train Epoch: 45 [0/2492 (0%)]\tLoss: 0.936641\n",
      "Train Epoch: 45 [1600/2492 (65%)]\tLoss: 1.073481\n",
      "Train Epoch: 46 [0/2492 (0%)]\tLoss: 1.054508\n",
      "Train Epoch: 46 [1600/2492 (65%)]\tLoss: 0.877092\n",
      "Train Epoch: 47 [0/2492 (0%)]\tLoss: 0.914637\n",
      "Train Epoch: 47 [1600/2492 (65%)]\tLoss: 0.916702\n",
      "Train Epoch: 48 [0/2492 (0%)]\tLoss: 1.088040\n",
      "Train Epoch: 48 [1600/2492 (65%)]\tLoss: 0.891321\n",
      "Train Epoch: 49 [0/2492 (0%)]\tLoss: 0.850180\n",
      "Train Epoch: 49 [1600/2492 (65%)]\tLoss: 0.955480\n",
      "Train Epoch: 50 [0/2492 (0%)]\tLoss: 0.892897\n",
      "Train Epoch: 50 [1600/2492 (65%)]\tLoss: 0.935784\n",
      "Train Epoch: 51 [0/2492 (0%)]\tLoss: 0.947202\n",
      "Train Epoch: 51 [1600/2492 (65%)]\tLoss: 0.836113\n",
      "Train Epoch: 52 [0/2492 (0%)]\tLoss: 1.016443\n",
      "Train Epoch: 52 [1600/2492 (65%)]\tLoss: 0.869260\n",
      "Train Epoch: 53 [0/2492 (0%)]\tLoss: 0.880446\n",
      "Train Epoch: 53 [1600/2492 (65%)]\tLoss: 0.927822\n",
      "Train Epoch: 54 [0/2492 (0%)]\tLoss: 0.883210\n",
      "Train Epoch: 54 [1600/2492 (65%)]\tLoss: 0.923077\n",
      "Train Epoch: 55 [0/2492 (0%)]\tLoss: 0.895967\n",
      "Train Epoch: 55 [1600/2492 (65%)]\tLoss: 0.941418\n",
      "Train Epoch: 56 [0/2492 (0%)]\tLoss: 0.890716\n",
      "Train Epoch: 56 [1600/2492 (65%)]\tLoss: 1.000721\n",
      "Train Epoch: 57 [0/2492 (0%)]\tLoss: 0.855821\n",
      "Train Epoch: 57 [1600/2492 (65%)]\tLoss: 0.851223\n",
      "Train Epoch: 58 [0/2492 (0%)]\tLoss: 0.984846\n",
      "Train Epoch: 58 [1600/2492 (65%)]\tLoss: 0.992998\n",
      "Train Epoch: 59 [0/2492 (0%)]\tLoss: 0.874181\n",
      "Train Epoch: 59 [1600/2492 (65%)]\tLoss: 0.969747\n",
      "Train Epoch: 60 [0/2492 (0%)]\tLoss: 0.973944\n",
      "Train Epoch: 60 [1600/2492 (65%)]\tLoss: 0.912120\n",
      "Train Epoch: 61 [0/2492 (0%)]\tLoss: 0.982802\n",
      "Train Epoch: 61 [1600/2492 (65%)]\tLoss: 0.949310\n",
      "Train Epoch: 62 [0/2492 (0%)]\tLoss: 0.870144\n",
      "Train Epoch: 62 [1600/2492 (65%)]\tLoss: 1.011012\n",
      "Train Epoch: 63 [0/2492 (0%)]\tLoss: 0.914965\n",
      "Train Epoch: 63 [1600/2492 (65%)]\tLoss: 0.960154\n",
      "Train Epoch: 64 [0/2492 (0%)]\tLoss: 0.933336\n",
      "Train Epoch: 64 [1600/2492 (65%)]\tLoss: 0.941942\n",
      "Train Epoch: 65 [0/2492 (0%)]\tLoss: 0.881826\n",
      "Train Epoch: 65 [1600/2492 (65%)]\tLoss: 1.053723\n",
      "Train Epoch: 66 [0/2492 (0%)]\tLoss: 0.891281\n",
      "Train Epoch: 66 [1600/2492 (65%)]\tLoss: 0.912338\n",
      "Train Epoch: 67 [0/2492 (0%)]\tLoss: 0.941636\n",
      "Train Epoch: 67 [1600/2492 (65%)]\tLoss: 0.911273\n",
      "Train Epoch: 68 [0/2492 (0%)]\tLoss: 0.927178\n",
      "Train Epoch: 68 [1600/2492 (65%)]\tLoss: 0.801157\n",
      "Train Epoch: 69 [0/2492 (0%)]\tLoss: 0.937126\n",
      "Train Epoch: 69 [1600/2492 (65%)]\tLoss: 0.969854\n",
      "Train Epoch: 70 [0/2492 (0%)]\tLoss: 0.792439\n",
      "Train Epoch: 70 [1600/2492 (65%)]\tLoss: 0.855129\n",
      "Train Epoch: 71 [0/2492 (0%)]\tLoss: 1.037783\n",
      "Train Epoch: 71 [1600/2492 (65%)]\tLoss: 0.815436\n",
      "Train Epoch: 72 [0/2492 (0%)]\tLoss: 0.873313\n",
      "Train Epoch: 72 [1600/2492 (65%)]\tLoss: 0.772607\n",
      "Train Epoch: 73 [0/2492 (0%)]\tLoss: 0.952622\n",
      "Train Epoch: 73 [1600/2492 (65%)]\tLoss: 0.859415\n",
      "Train Epoch: 74 [0/2492 (0%)]\tLoss: 0.861181\n",
      "Train Epoch: 74 [1600/2492 (65%)]\tLoss: 0.826481\n",
      "Train Epoch: 75 [0/2492 (0%)]\tLoss: 0.754362\n",
      "Train Epoch: 75 [1600/2492 (65%)]\tLoss: 0.786918\n",
      "Train Epoch: 76 [0/2492 (0%)]\tLoss: 0.893129\n",
      "Train Epoch: 76 [1600/2492 (65%)]\tLoss: 0.932158\n",
      "Train Epoch: 77 [0/2492 (0%)]\tLoss: 0.835916\n",
      "Train Epoch: 77 [1600/2492 (65%)]\tLoss: 0.887524\n",
      "Train Epoch: 78 [0/2492 (0%)]\tLoss: 0.889617\n",
      "Train Epoch: 78 [1600/2492 (65%)]\tLoss: 0.898048\n",
      "Train Epoch: 79 [0/2492 (0%)]\tLoss: 0.899574\n",
      "Train Epoch: 79 [1600/2492 (65%)]\tLoss: 0.834428\n",
      "Train Epoch: 80 [0/2492 (0%)]\tLoss: 0.848618\n",
      "Train Epoch: 80 [1600/2492 (65%)]\tLoss: 0.831238\n",
      "Train Epoch: 81 [0/2492 (0%)]\tLoss: 0.856094\n",
      "Train Epoch: 81 [1600/2492 (65%)]\tLoss: 0.880659\n",
      "Train Epoch: 82 [0/2492 (0%)]\tLoss: 0.935261\n",
      "Train Epoch: 82 [1600/2492 (65%)]\tLoss: 0.862267\n",
      "Train Epoch: 83 [0/2492 (0%)]\tLoss: 0.895307\n",
      "Train Epoch: 83 [1600/2492 (65%)]\tLoss: 0.833848\n",
      "Train Epoch: 84 [0/2492 (0%)]\tLoss: 0.890672\n",
      "Train Epoch: 84 [1600/2492 (65%)]\tLoss: 0.916411\n",
      "Train Epoch: 85 [0/2492 (0%)]\tLoss: 0.905846\n",
      "Train Epoch: 85 [1600/2492 (65%)]\tLoss: 0.915573\n",
      "Train Epoch: 86 [0/2492 (0%)]\tLoss: 0.825428\n",
      "Train Epoch: 86 [1600/2492 (65%)]\tLoss: 0.829772\n",
      "Train Epoch: 87 [0/2492 (0%)]\tLoss: 0.912543\n",
      "Train Epoch: 87 [1600/2492 (65%)]\tLoss: 0.855514\n",
      "Train Epoch: 88 [0/2492 (0%)]\tLoss: 0.925518\n",
      "Train Epoch: 88 [1600/2492 (65%)]\tLoss: 0.911232\n",
      "Train Epoch: 89 [0/2492 (0%)]\tLoss: 0.820629\n",
      "Train Epoch: 89 [1600/2492 (65%)]\tLoss: 0.813733\n",
      "Train Epoch: 90 [0/2492 (0%)]\tLoss: 0.839303\n",
      "Train Epoch: 90 [1600/2492 (65%)]\tLoss: 0.888941\n",
      "Train Epoch: 91 [0/2492 (0%)]\tLoss: 0.845602\n",
      "Train Epoch: 91 [1600/2492 (65%)]\tLoss: 0.915797\n",
      "Train Epoch: 92 [0/2492 (0%)]\tLoss: 0.892502\n",
      "Train Epoch: 92 [1600/2492 (65%)]\tLoss: 0.918940\n",
      "Train Epoch: 93 [0/2492 (0%)]\tLoss: 0.967052\n",
      "Train Epoch: 93 [1600/2492 (65%)]\tLoss: 0.829256\n",
      "Train Epoch: 94 [0/2492 (0%)]\tLoss: 0.873799\n",
      "Train Epoch: 94 [1600/2492 (65%)]\tLoss: 0.867253\n",
      "Train Epoch: 95 [0/2492 (0%)]\tLoss: 0.861229\n",
      "Train Epoch: 95 [1600/2492 (65%)]\tLoss: 0.749748\n",
      "Train Epoch: 96 [0/2492 (0%)]\tLoss: 0.879034\n",
      "Train Epoch: 96 [1600/2492 (65%)]\tLoss: 0.849366\n",
      "Train Epoch: 97 [0/2492 (0%)]\tLoss: 0.898900\n",
      "Train Epoch: 97 [1600/2492 (65%)]\tLoss: 0.839055\n",
      "Train Epoch: 98 [0/2492 (0%)]\tLoss: 0.789184\n",
      "Train Epoch: 98 [1600/2492 (65%)]\tLoss: 0.810359\n",
      "Train Epoch: 99 [0/2492 (0%)]\tLoss: 0.813414\n",
      "Train Epoch: 99 [1600/2492 (65%)]\tLoss: 0.880330\n",
      "Train Epoch: 100 [0/2492 (0%)]\tLoss: 0.858247\n",
      "Train Epoch: 100 [1600/2492 (65%)]\tLoss: 0.998067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcd1f7c5ac0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training the autoencoder to encode the TCR sequence\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        # TCR_encode_losses.append(loss.item() / model.batch_size)\n",
    "        # TCR_encode_losses.append(loss.item())\n",
    "        # sum up batch loss\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.set_title(\"TCR encode loss\")\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# load the model\n",
    "# model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "# model.load_state_dict(torch.load(\"/DATA/User/wuxinchao/project/pMHC-TCR/ckpt/TCR_autoencoder.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 3))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    TCR_seq = TCR_seq.view(1, 5, 49).float()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test, not used\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "kernel_size, stride, padding, seq_length\n",
    "# pool of size=3, stride=2\n",
    "# m = nn.MaxPool1d(3, stride=1)\n",
    "# m = nn.Conv1d(16, 33, 3, stride=2, padding=1)\n",
    "m = nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=1)\n",
    "# m = nn.MaxUnpool1d(kernel_size=3, stride=1)\n",
    "input = torch.randn(20, 16, 3)\n",
    "output = m(input)\n",
    "output.shape\n",
    "# TCRData[0][0].shape\n",
    "len(TCRData)\n",
    "# model(TCRData[0:3][0].float().view(3,5,seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TCR_encode(nn.Module):\n",
    "    '''\n",
    "    LSTM for TCR sequence encoding.\n",
    "    The input size of LSTM is (batch_size, seq_length, input_size), the output size is (batch_size, seq_length, hidden_size)\n",
    "    '''\n",
    "    def __init__(self, seq_length, hidden_size, num_layers, device):\n",
    "        super(LSTM_TCR_encode, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(seq_length, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model for TCR sequence encoding, this may not be used in the future\n",
    "# How to use the LSTM model to encode the TCR sequence\n",
    "# The optimization \n",
    "def train_LSTM_TCR_encode(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, seq_length, 5)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training: {batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%) \\\n",
    "                  Loss: {loss.item():.6f}\")\n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the LSTM model\n",
    "model = LSTM_TCR_encode(seq_length, hidden_size, num_layers, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    Here the input is the TCR sequence, neoantigen sequence, and HLA type.\n",
    "    The output should be the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 file_path, \n",
    "                 only_CDR3: bool = False, \n",
    "                 only_experimental: bool = False, \n",
    "                 TCR_encode: str = [\"LSTM\", \"CNN\"],\n",
    "                 encoding_model: nn.Module = None) -> None:\n",
    "        df, HLA_encode, y  = self.basic_io(file_path, only_experimental=only_experimental)\n",
    "\n",
    "        # convert from object to tensor\n",
    "        X_TCR_seq = torch.zeros((len(df), 0))\n",
    "        for region in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            TCR_seq = df.loc[:, region].values\n",
    "            TCR_seq_encode = torch.zeros((0, TCR_seq[0].shape[1]))\n",
    "            for i in range(len(TCR_seq)):\n",
    "                encoding = torch.from_numpy(TCR_seq[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                TCR_seq_encode = torch.cat((TCR_seq_encode, encoding), dim=0)\n",
    "\n",
    "            X_TCR_seq = torch.cat((TCR_seq_encode, X_TCR_seq), dim=1)\n",
    "        \n",
    "        if TCR_encode == \"CNN\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        elif TCR_encode == \"LSTM\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        else:\n",
    "            raise ValueError(\"The TCR encoding method is not supported yet.\")\n",
    "        \n",
    "        # encoding model \n",
    "        X_features, _ = encoding_model(X_TCR_seq)\n",
    "        X_features = X_features.view(-1, 20 * 3).data\n",
    "\n",
    "        # add the neoantigen sequence encoding features\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            neo = df.loc[:, seq].values\n",
    "            neo_encode = torch.zeros((0, neo[0].shape[1]))\n",
    "            for i in range(len(neo)):\n",
    "                encoding = torch.from_numpy(neo[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                neo_encode = torch.cat((neo_encode, encoding), dim=0)\n",
    "            X_features = torch.cat((X_features, neo_encode), dim=1)\n",
    "\n",
    "        X_features = torch.cat((X_features, torch.from_numpy(HLA_encode)), dim=1)\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "            \n",
    "    \n",
    "    def basic_io(self, file_path, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # drop the rows with nan\n",
    "        df = df.dropna()\n",
    "\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        return df, X_HLA_encoded, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, TCR_encode=\"CNN\", only_experimental=True, encoding_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCRData[0][0].shape                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "    length = len_map[chain]\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# drop the rows with nan\n",
    "df = df.dropna()\n",
    "\n",
    "X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "HLAencoder = OneHotEncoder()\n",
    "X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 batch_size=32,) -> None:\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(self.batch_size, self.input_size)\n",
    "        # print(f\"The model input shape is : {input.shape}\")\n",
    "        output = self.linear_layer(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # one-hot encoding the target\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        target = target.to(torch.bool)\n",
    "        one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "        one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "        one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "\n",
    "        data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "        data = data.view(-1, 60+5*6+2).to(torch.float32)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, one_hot_target.data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.sigmoid().round()\n",
    "        correct += pred.eq(one_hot_target.view_as(pred)).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training stage for Flod {fold} Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \\\n",
    "                ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    return train_loss, correct\n",
    "\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            target = target.to(torch.bool)\n",
    "            one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "            one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "            one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "            data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "            data = data.view(-1, 60+5*6+2).to(torch.float32)\n",
    "            output = model(data)\n",
    "            one_hot_target = one_hot_target.to(torch.float32).view(-1, 1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output.reshape(1,-1), one_hot_target.reshape(1,-1)).item()  # sum up loss\n",
    "            # print(test_loss)\n",
    "            pred = output.sigmoid().round()\n",
    "            correct += pred.eq(one_hot_target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n",
      "-------------------Fold 0-------------------\n",
      "Training stage for Flod 0 Epoch: 1 [0/37476                 (0%)]\tLoss: 0.830887\n",
      "Training stage for Flod 0 Epoch: 1 [3200/37476                 (11%)]\tLoss: 0.130566\n",
      "Training stage for Flod 0 Epoch: 1 [6400/37476                 (21%)]\tLoss: 0.032834\n",
      "Training stage for Flod 0 Epoch: 1 [9600/37476                 (32%)]\tLoss: 0.124586\n",
      "Training stage for Flod 0 Epoch: 1 [12800/37476                 (43%)]\tLoss: 0.023386\n",
      "Training stage for Flod 0 Epoch: 1 [16000/37476                 (53%)]\tLoss: 0.266913\n",
      "Training stage for Flod 0 Epoch: 1 [19200/37476                 (64%)]\tLoss: 0.075893\n",
      "Training stage for Flod 0 Epoch: 1 [22400/37476                 (75%)]\tLoss: 0.042342\n",
      "Training stage for Flod 0 Epoch: 1 [25600/37476                 (85%)]\tLoss: 0.001998\n",
      "Training stage for Flod 0 Epoch: 1 [28800/37476                 (96%)]\tLoss: 0.021361\n",
      "Test set for fold0: Average Loss:           0.7816, Accuracy: 14588/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 2 [0/37476                 (0%)]\tLoss: 0.100520\n",
      "Training stage for Flod 0 Epoch: 2 [3200/37476                 (11%)]\tLoss: 0.055564\n",
      "Training stage for Flod 0 Epoch: 2 [6400/37476                 (21%)]\tLoss: 0.079526\n",
      "Training stage for Flod 0 Epoch: 2 [9600/37476                 (32%)]\tLoss: 0.008587\n",
      "Training stage for Flod 0 Epoch: 2 [12800/37476                 (43%)]\tLoss: 0.079582\n",
      "Training stage for Flod 0 Epoch: 2 [16000/37476                 (53%)]\tLoss: 0.117810\n",
      "Training stage for Flod 0 Epoch: 2 [19200/37476                 (64%)]\tLoss: 0.031273\n",
      "Training stage for Flod 0 Epoch: 2 [22400/37476                 (75%)]\tLoss: 0.015212\n",
      "Training stage for Flod 0 Epoch: 2 [25600/37476                 (85%)]\tLoss: 0.051551\n",
      "Training stage for Flod 0 Epoch: 2 [28800/37476                 (96%)]\tLoss: 0.155405\n",
      "Test set for fold0: Average Loss:           0.9398, Accuracy: 14596/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 3 [0/37476                 (0%)]\tLoss: 0.097323\n",
      "Training stage for Flod 0 Epoch: 3 [3200/37476                 (11%)]\tLoss: 0.166180\n",
      "Training stage for Flod 0 Epoch: 3 [6400/37476                 (21%)]\tLoss: 0.104221\n",
      "Training stage for Flod 0 Epoch: 3 [9600/37476                 (32%)]\tLoss: 0.011426\n",
      "Training stage for Flod 0 Epoch: 3 [12800/37476                 (43%)]\tLoss: 0.055441\n",
      "Training stage for Flod 0 Epoch: 3 [16000/37476                 (53%)]\tLoss: 0.265449\n",
      "Training stage for Flod 0 Epoch: 3 [19200/37476                 (64%)]\tLoss: 0.014049\n",
      "Training stage for Flod 0 Epoch: 3 [22400/37476                 (75%)]\tLoss: 0.022517\n",
      "Training stage for Flod 0 Epoch: 3 [25600/37476                 (85%)]\tLoss: 0.093010\n",
      "Training stage for Flod 0 Epoch: 3 [28800/37476                 (96%)]\tLoss: 0.024729\n",
      "Test set for fold0: Average Loss:           0.8871, Accuracy: 14618/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 4 [0/37476                 (0%)]\tLoss: 0.005772\n",
      "Training stage for Flod 0 Epoch: 4 [3200/37476                 (11%)]\tLoss: 0.004717\n",
      "Training stage for Flod 0 Epoch: 4 [6400/37476                 (21%)]\tLoss: 0.002967\n",
      "Training stage for Flod 0 Epoch: 4 [9600/37476                 (32%)]\tLoss: 0.020891\n",
      "Training stage for Flod 0 Epoch: 4 [12800/37476                 (43%)]\tLoss: 0.112427\n",
      "Training stage for Flod 0 Epoch: 4 [16000/37476                 (53%)]\tLoss: 0.139785\n",
      "Training stage for Flod 0 Epoch: 4 [19200/37476                 (64%)]\tLoss: 0.004047\n",
      "Training stage for Flod 0 Epoch: 4 [22400/37476                 (75%)]\tLoss: 0.094599\n",
      "Training stage for Flod 0 Epoch: 4 [25600/37476                 (85%)]\tLoss: 0.098324\n",
      "Training stage for Flod 0 Epoch: 4 [28800/37476                 (96%)]\tLoss: 0.019912\n",
      "Test set for fold0: Average Loss:           0.9678, Accuracy: 14665/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 5 [0/37476                 (0%)]\tLoss: 0.032592\n",
      "Training stage for Flod 0 Epoch: 5 [3200/37476                 (11%)]\tLoss: 0.153518\n",
      "Training stage for Flod 0 Epoch: 5 [6400/37476                 (21%)]\tLoss: 0.011525\n",
      "Training stage for Flod 0 Epoch: 5 [9600/37476                 (32%)]\tLoss: 0.055809\n",
      "Training stage for Flod 0 Epoch: 5 [12800/37476                 (43%)]\tLoss: 0.005897\n",
      "Training stage for Flod 0 Epoch: 5 [16000/37476                 (53%)]\tLoss: 0.063321\n",
      "Training stage for Flod 0 Epoch: 5 [19200/37476                 (64%)]\tLoss: 0.034384\n",
      "Training stage for Flod 0 Epoch: 5 [22400/37476                 (75%)]\tLoss: 0.239818\n",
      "Training stage for Flod 0 Epoch: 5 [25600/37476                 (85%)]\tLoss: 0.021451\n",
      "Training stage for Flod 0 Epoch: 5 [28800/37476                 (96%)]\tLoss: 0.054212\n",
      "Test set for fold0: Average Loss:           0.9102, Accuracy: 14665/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 6 [0/37476                 (0%)]\tLoss: 0.027800\n",
      "Training stage for Flod 0 Epoch: 6 [3200/37476                 (11%)]\tLoss: 0.083382\n",
      "Training stage for Flod 0 Epoch: 6 [6400/37476                 (21%)]\tLoss: 0.125703\n",
      "Training stage for Flod 0 Epoch: 6 [9600/37476                 (32%)]\tLoss: 0.014026\n",
      "Training stage for Flod 0 Epoch: 6 [12800/37476                 (43%)]\tLoss: 0.009174\n",
      "Training stage for Flod 0 Epoch: 6 [16000/37476                 (53%)]\tLoss: 0.044523\n",
      "Training stage for Flod 0 Epoch: 6 [19200/37476                 (64%)]\tLoss: 0.068387\n",
      "Training stage for Flod 0 Epoch: 6 [22400/37476                 (75%)]\tLoss: 0.025725\n",
      "Training stage for Flod 0 Epoch: 6 [25600/37476                 (85%)]\tLoss: 0.380290\n",
      "Training stage for Flod 0 Epoch: 6 [28800/37476                 (96%)]\tLoss: 0.023047\n",
      "Test set for fold0: Average Loss:           0.9260, Accuracy: 14617/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 7 [0/37476                 (0%)]\tLoss: 0.290200\n",
      "Training stage for Flod 0 Epoch: 7 [3200/37476                 (11%)]\tLoss: 0.111341\n",
      "Training stage for Flod 0 Epoch: 7 [6400/37476                 (21%)]\tLoss: 0.043296\n",
      "Training stage for Flod 0 Epoch: 7 [9600/37476                 (32%)]\tLoss: 0.012586\n",
      "Training stage for Flod 0 Epoch: 7 [12800/37476                 (43%)]\tLoss: 0.086661\n",
      "Training stage for Flod 0 Epoch: 7 [16000/37476                 (53%)]\tLoss: 0.001913\n",
      "Training stage for Flod 0 Epoch: 7 [19200/37476                 (64%)]\tLoss: 0.015400\n",
      "Training stage for Flod 0 Epoch: 7 [22400/37476                 (75%)]\tLoss: 0.012496\n",
      "Training stage for Flod 0 Epoch: 7 [25600/37476                 (85%)]\tLoss: 0.026253\n",
      "Training stage for Flod 0 Epoch: 7 [28800/37476                 (96%)]\tLoss: 0.081789\n",
      "Test set for fold0: Average Loss:           0.9295, Accuracy: 14682/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 8 [0/37476                 (0%)]\tLoss: 0.010310\n",
      "Training stage for Flod 0 Epoch: 8 [3200/37476                 (11%)]\tLoss: 0.004547\n",
      "Training stage for Flod 0 Epoch: 8 [6400/37476                 (21%)]\tLoss: 0.089399\n",
      "Training stage for Flod 0 Epoch: 8 [9600/37476                 (32%)]\tLoss: 0.079187\n",
      "Training stage for Flod 0 Epoch: 8 [12800/37476                 (43%)]\tLoss: 0.066909\n",
      "Training stage for Flod 0 Epoch: 8 [16000/37476                 (53%)]\tLoss: 0.003497\n",
      "Training stage for Flod 0 Epoch: 8 [19200/37476                 (64%)]\tLoss: 0.260577\n",
      "Training stage for Flod 0 Epoch: 8 [22400/37476                 (75%)]\tLoss: 0.089988\n",
      "Training stage for Flod 0 Epoch: 8 [25600/37476                 (85%)]\tLoss: 0.000806\n",
      "Training stage for Flod 0 Epoch: 8 [28800/37476                 (96%)]\tLoss: 0.064789\n",
      "Test set for fold0: Average Loss:           1.0802, Accuracy: 14681/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 9 [0/37476                 (0%)]\tLoss: 0.038616\n",
      "Training stage for Flod 0 Epoch: 9 [3200/37476                 (11%)]\tLoss: 0.015017\n",
      "Training stage for Flod 0 Epoch: 9 [6400/37476                 (21%)]\tLoss: 0.002913\n",
      "Training stage for Flod 0 Epoch: 9 [9600/37476                 (32%)]\tLoss: 0.028105\n",
      "Training stage for Flod 0 Epoch: 9 [12800/37476                 (43%)]\tLoss: 0.045794\n",
      "Training stage for Flod 0 Epoch: 9 [16000/37476                 (53%)]\tLoss: 0.065777\n",
      "Training stage for Flod 0 Epoch: 9 [19200/37476                 (64%)]\tLoss: 0.031448\n",
      "Training stage for Flod 0 Epoch: 9 [22400/37476                 (75%)]\tLoss: 0.040612\n",
      "Training stage for Flod 0 Epoch: 9 [25600/37476                 (85%)]\tLoss: 0.112406\n",
      "Training stage for Flod 0 Epoch: 9 [28800/37476                 (96%)]\tLoss: 0.048846\n",
      "Test set for fold0: Average Loss:           0.9279, Accuracy: 14694/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 10 [0/37476                 (0%)]\tLoss: 0.037212\n",
      "Training stage for Flod 0 Epoch: 10 [3200/37476                 (11%)]\tLoss: 0.045044\n",
      "Training stage for Flod 0 Epoch: 10 [6400/37476                 (21%)]\tLoss: 0.012159\n",
      "Training stage for Flod 0 Epoch: 10 [9600/37476                 (32%)]\tLoss: 0.013639\n",
      "Training stage for Flod 0 Epoch: 10 [12800/37476                 (43%)]\tLoss: 0.006656\n",
      "Training stage for Flod 0 Epoch: 10 [16000/37476                 (53%)]\tLoss: 0.004897\n",
      "Training stage for Flod 0 Epoch: 10 [19200/37476                 (64%)]\tLoss: 0.078270\n",
      "Training stage for Flod 0 Epoch: 10 [22400/37476                 (75%)]\tLoss: 0.059063\n",
      "Training stage for Flod 0 Epoch: 10 [25600/37476                 (85%)]\tLoss: 0.068137\n",
      "Training stage for Flod 0 Epoch: 10 [28800/37476                 (96%)]\tLoss: 0.107931\n",
      "Test set for fold0: Average Loss:           0.9726, Accuracy: 14694/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 11 [0/37476                 (0%)]\tLoss: 0.064901\n",
      "Training stage for Flod 0 Epoch: 11 [3200/37476                 (11%)]\tLoss: 0.067041\n",
      "Training stage for Flod 0 Epoch: 11 [6400/37476                 (21%)]\tLoss: 0.015215\n",
      "Training stage for Flod 0 Epoch: 11 [9600/37476                 (32%)]\tLoss: 0.052255\n",
      "Training stage for Flod 0 Epoch: 11 [12800/37476                 (43%)]\tLoss: 0.004566\n",
      "Training stage for Flod 0 Epoch: 11 [16000/37476                 (53%)]\tLoss: 0.054807\n",
      "Training stage for Flod 0 Epoch: 11 [19200/37476                 (64%)]\tLoss: 0.085120\n",
      "Training stage for Flod 0 Epoch: 11 [22400/37476                 (75%)]\tLoss: 0.017138\n",
      "Training stage for Flod 0 Epoch: 11 [25600/37476                 (85%)]\tLoss: 0.034462\n",
      "Training stage for Flod 0 Epoch: 11 [28800/37476                 (96%)]\tLoss: 0.034479\n",
      "Test set for fold0: Average Loss:           1.0266, Accuracy: 14689/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 12 [0/37476                 (0%)]\tLoss: 0.011085\n",
      "Training stage for Flod 0 Epoch: 12 [3200/37476                 (11%)]\tLoss: 0.056677\n",
      "Training stage for Flod 0 Epoch: 12 [6400/37476                 (21%)]\tLoss: 0.151727\n",
      "Training stage for Flod 0 Epoch: 12 [9600/37476                 (32%)]\tLoss: 0.020610\n",
      "Training stage for Flod 0 Epoch: 12 [12800/37476                 (43%)]\tLoss: 0.019617\n",
      "Training stage for Flod 0 Epoch: 12 [16000/37476                 (53%)]\tLoss: 0.039885\n",
      "Training stage for Flod 0 Epoch: 12 [19200/37476                 (64%)]\tLoss: 0.010457\n",
      "Training stage for Flod 0 Epoch: 12 [22400/37476                 (75%)]\tLoss: 0.011079\n",
      "Training stage for Flod 0 Epoch: 12 [25600/37476                 (85%)]\tLoss: 0.030374\n",
      "Training stage for Flod 0 Epoch: 12 [28800/37476                 (96%)]\tLoss: 0.095014\n",
      "Test set for fold0: Average Loss:           0.9858, Accuracy: 14710/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 13 [0/37476                 (0%)]\tLoss: 0.007262\n",
      "Training stage for Flod 0 Epoch: 13 [3200/37476                 (11%)]\tLoss: 0.027893\n",
      "Training stage for Flod 0 Epoch: 13 [6400/37476                 (21%)]\tLoss: 0.027044\n",
      "Training stage for Flod 0 Epoch: 13 [9600/37476                 (32%)]\tLoss: 0.011756\n",
      "Training stage for Flod 0 Epoch: 13 [12800/37476                 (43%)]\tLoss: 0.071084\n",
      "Training stage for Flod 0 Epoch: 13 [16000/37476                 (53%)]\tLoss: 0.038561\n",
      "Training stage for Flod 0 Epoch: 13 [19200/37476                 (64%)]\tLoss: 0.018102\n",
      "Training stage for Flod 0 Epoch: 13 [22400/37476                 (75%)]\tLoss: 0.008506\n",
      "Training stage for Flod 0 Epoch: 13 [25600/37476                 (85%)]\tLoss: 0.035960\n",
      "Training stage for Flod 0 Epoch: 13 [28800/37476                 (96%)]\tLoss: 0.047102\n",
      "Test set for fold0: Average Loss:           1.0711, Accuracy: 14518/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 14 [0/37476                 (0%)]\tLoss: 0.110477\n",
      "Training stage for Flod 0 Epoch: 14 [3200/37476                 (11%)]\tLoss: 0.087506\n",
      "Training stage for Flod 0 Epoch: 14 [6400/37476                 (21%)]\tLoss: 0.039296\n",
      "Training stage for Flod 0 Epoch: 14 [9600/37476                 (32%)]\tLoss: 0.007807\n",
      "Training stage for Flod 0 Epoch: 14 [12800/37476                 (43%)]\tLoss: 0.002262\n",
      "Training stage for Flod 0 Epoch: 14 [16000/37476                 (53%)]\tLoss: 0.000122\n",
      "Training stage for Flod 0 Epoch: 14 [19200/37476                 (64%)]\tLoss: 0.015458\n",
      "Training stage for Flod 0 Epoch: 14 [22400/37476                 (75%)]\tLoss: 0.000235\n",
      "Training stage for Flod 0 Epoch: 14 [25600/37476                 (85%)]\tLoss: 0.030633\n",
      "Training stage for Flod 0 Epoch: 14 [28800/37476                 (96%)]\tLoss: 0.011109\n",
      "Test set for fold0: Average Loss:           1.0449, Accuracy: 14728/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 15 [0/37476                 (0%)]\tLoss: 0.020665\n",
      "Training stage for Flod 0 Epoch: 15 [3200/37476                 (11%)]\tLoss: 0.014281\n",
      "Training stage for Flod 0 Epoch: 15 [6400/37476                 (21%)]\tLoss: 0.039916\n",
      "Training stage for Flod 0 Epoch: 15 [9600/37476                 (32%)]\tLoss: 0.122681\n",
      "Training stage for Flod 0 Epoch: 15 [12800/37476                 (43%)]\tLoss: 0.057425\n",
      "Training stage for Flod 0 Epoch: 15 [16000/37476                 (53%)]\tLoss: 0.018745\n",
      "Training stage for Flod 0 Epoch: 15 [19200/37476                 (64%)]\tLoss: 0.036961\n",
      "Training stage for Flod 0 Epoch: 15 [22400/37476                 (75%)]\tLoss: 0.024442\n",
      "Training stage for Flod 0 Epoch: 15 [25600/37476                 (85%)]\tLoss: 0.025365\n",
      "Training stage for Flod 0 Epoch: 15 [28800/37476                 (96%)]\tLoss: 0.057926\n",
      "Test set for fold0: Average Loss:           1.2212, Accuracy: 14660/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 16 [0/37476                 (0%)]\tLoss: 0.265629\n",
      "Training stage for Flod 0 Epoch: 16 [3200/37476                 (11%)]\tLoss: 0.013018\n",
      "Training stage for Flod 0 Epoch: 16 [6400/37476                 (21%)]\tLoss: 0.049334\n",
      "Training stage for Flod 0 Epoch: 16 [9600/37476                 (32%)]\tLoss: 0.011446\n",
      "Training stage for Flod 0 Epoch: 16 [12800/37476                 (43%)]\tLoss: 0.103884\n",
      "Training stage for Flod 0 Epoch: 16 [16000/37476                 (53%)]\tLoss: 0.015684\n",
      "Training stage for Flod 0 Epoch: 16 [19200/37476                 (64%)]\tLoss: 0.004419\n",
      "Training stage for Flod 0 Epoch: 16 [22400/37476                 (75%)]\tLoss: 0.067572\n",
      "Training stage for Flod 0 Epoch: 16 [25600/37476                 (85%)]\tLoss: 0.062518\n",
      "Training stage for Flod 0 Epoch: 16 [28800/37476                 (96%)]\tLoss: 0.022548\n",
      "Test set for fold0: Average Loss:           1.1235, Accuracy: 14679/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 17 [0/37476                 (0%)]\tLoss: 0.023631\n",
      "Training stage for Flod 0 Epoch: 17 [3200/37476                 (11%)]\tLoss: 0.018194\n",
      "Training stage for Flod 0 Epoch: 17 [6400/37476                 (21%)]\tLoss: 0.017254\n",
      "Training stage for Flod 0 Epoch: 17 [9600/37476                 (32%)]\tLoss: 0.013379\n",
      "Training stage for Flod 0 Epoch: 17 [12800/37476                 (43%)]\tLoss: 0.101876\n",
      "Training stage for Flod 0 Epoch: 17 [16000/37476                 (53%)]\tLoss: 0.078862\n",
      "Training stage for Flod 0 Epoch: 17 [19200/37476                 (64%)]\tLoss: 0.010123\n",
      "Training stage for Flod 0 Epoch: 17 [22400/37476                 (75%)]\tLoss: 0.025230\n",
      "Training stage for Flod 0 Epoch: 17 [25600/37476                 (85%)]\tLoss: 0.069439\n",
      "Training stage for Flod 0 Epoch: 17 [28800/37476                 (96%)]\tLoss: 0.000089\n",
      "Test set for fold0: Average Loss:           1.2443, Accuracy: 14710/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 18 [0/37476                 (0%)]\tLoss: 0.012624\n",
      "Training stage for Flod 0 Epoch: 18 [3200/37476                 (11%)]\tLoss: 0.046882\n",
      "Training stage for Flod 0 Epoch: 18 [6400/37476                 (21%)]\tLoss: 0.001692\n",
      "Training stage for Flod 0 Epoch: 18 [9600/37476                 (32%)]\tLoss: 0.042385\n",
      "Training stage for Flod 0 Epoch: 18 [12800/37476                 (43%)]\tLoss: 0.027241\n",
      "Training stage for Flod 0 Epoch: 18 [16000/37476                 (53%)]\tLoss: 0.007942\n",
      "Training stage for Flod 0 Epoch: 18 [19200/37476                 (64%)]\tLoss: 0.001935\n",
      "Training stage for Flod 0 Epoch: 18 [22400/37476                 (75%)]\tLoss: 0.041019\n",
      "Training stage for Flod 0 Epoch: 18 [25600/37476                 (85%)]\tLoss: 0.014771\n",
      "Training stage for Flod 0 Epoch: 18 [28800/37476                 (96%)]\tLoss: 0.128823\n",
      "Test set for fold0: Average Loss:           1.1037, Accuracy: 14736/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 19 [0/37476                 (0%)]\tLoss: 0.027442\n",
      "Training stage for Flod 0 Epoch: 19 [3200/37476                 (11%)]\tLoss: 0.074899\n",
      "Training stage for Flod 0 Epoch: 19 [6400/37476                 (21%)]\tLoss: 0.048028\n",
      "Training stage for Flod 0 Epoch: 19 [9600/37476                 (32%)]\tLoss: 0.023881\n",
      "Training stage for Flod 0 Epoch: 19 [12800/37476                 (43%)]\tLoss: 0.080642\n",
      "Training stage for Flod 0 Epoch: 19 [16000/37476                 (53%)]\tLoss: 0.136263\n",
      "Training stage for Flod 0 Epoch: 19 [19200/37476                 (64%)]\tLoss: 0.102749\n",
      "Training stage for Flod 0 Epoch: 19 [22400/37476                 (75%)]\tLoss: 0.026100\n",
      "Training stage for Flod 0 Epoch: 19 [25600/37476                 (85%)]\tLoss: 0.005563\n",
      "Training stage for Flod 0 Epoch: 19 [28800/37476                 (96%)]\tLoss: 0.171570\n",
      "Test set for fold0: Average Loss:           1.1364, Accuracy: 14726/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 20 [0/37476                 (0%)]\tLoss: 0.012413\n",
      "Training stage for Flod 0 Epoch: 20 [3200/37476                 (11%)]\tLoss: 0.003295\n",
      "Training stage for Flod 0 Epoch: 20 [6400/37476                 (21%)]\tLoss: 0.023222\n",
      "Training stage for Flod 0 Epoch: 20 [9600/37476                 (32%)]\tLoss: 0.031027\n",
      "Training stage for Flod 0 Epoch: 20 [12800/37476                 (43%)]\tLoss: 0.012854\n",
      "Training stage for Flod 0 Epoch: 20 [16000/37476                 (53%)]\tLoss: 0.003041\n",
      "Training stage for Flod 0 Epoch: 20 [19200/37476                 (64%)]\tLoss: 0.000092\n",
      "Training stage for Flod 0 Epoch: 20 [22400/37476                 (75%)]\tLoss: 0.023756\n",
      "Training stage for Flod 0 Epoch: 20 [25600/37476                 (85%)]\tLoss: 0.013255\n",
      "Training stage for Flod 0 Epoch: 20 [28800/37476                 (96%)]\tLoss: 0.003328\n",
      "Test set for fold0: Average Loss:           1.2529, Accuracy: 14722/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 21 [0/37476                 (0%)]\tLoss: 0.009350\n",
      "Training stage for Flod 0 Epoch: 21 [3200/37476                 (11%)]\tLoss: 0.058853\n",
      "Training stage for Flod 0 Epoch: 21 [6400/37476                 (21%)]\tLoss: 0.044792\n",
      "Training stage for Flod 0 Epoch: 21 [9600/37476                 (32%)]\tLoss: 0.006202\n",
      "Training stage for Flod 0 Epoch: 21 [12800/37476                 (43%)]\tLoss: 0.083513\n",
      "Training stage for Flod 0 Epoch: 21 [16000/37476                 (53%)]\tLoss: 0.112558\n",
      "Training stage for Flod 0 Epoch: 21 [19200/37476                 (64%)]\tLoss: 0.018470\n",
      "Training stage for Flod 0 Epoch: 21 [22400/37476                 (75%)]\tLoss: 0.220484\n",
      "Training stage for Flod 0 Epoch: 21 [25600/37476                 (85%)]\tLoss: 0.015817\n",
      "Training stage for Flod 0 Epoch: 21 [28800/37476                 (96%)]\tLoss: 0.012659\n",
      "Test set for fold0: Average Loss:           1.2013, Accuracy: 14687/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 22 [0/37476                 (0%)]\tLoss: 0.020248\n",
      "Training stage for Flod 0 Epoch: 22 [3200/37476                 (11%)]\tLoss: 0.033576\n",
      "Training stage for Flod 0 Epoch: 22 [6400/37476                 (21%)]\tLoss: 0.129280\n",
      "Training stage for Flod 0 Epoch: 22 [9600/37476                 (32%)]\tLoss: 0.190907\n",
      "Training stage for Flod 0 Epoch: 22 [12800/37476                 (43%)]\tLoss: 0.020287\n",
      "Training stage for Flod 0 Epoch: 22 [16000/37476                 (53%)]\tLoss: 0.001652\n",
      "Training stage for Flod 0 Epoch: 22 [19200/37476                 (64%)]\tLoss: 0.012079\n",
      "Training stage for Flod 0 Epoch: 22 [22400/37476                 (75%)]\tLoss: 0.092114\n",
      "Training stage for Flod 0 Epoch: 22 [25600/37476                 (85%)]\tLoss: 0.002909\n",
      "Training stage for Flod 0 Epoch: 22 [28800/37476                 (96%)]\tLoss: 0.037548\n",
      "Test set for fold0: Average Loss:           1.1942, Accuracy: 14698/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 23 [0/37476                 (0%)]\tLoss: 0.061076\n",
      "Training stage for Flod 0 Epoch: 23 [3200/37476                 (11%)]\tLoss: 0.000881\n",
      "Training stage for Flod 0 Epoch: 23 [6400/37476                 (21%)]\tLoss: 0.021012\n",
      "Training stage for Flod 0 Epoch: 23 [9600/37476                 (32%)]\tLoss: 0.042876\n",
      "Training stage for Flod 0 Epoch: 23 [12800/37476                 (43%)]\tLoss: 0.003202\n",
      "Training stage for Flod 0 Epoch: 23 [16000/37476                 (53%)]\tLoss: 0.021093\n",
      "Training stage for Flod 0 Epoch: 23 [19200/37476                 (64%)]\tLoss: 0.022116\n",
      "Training stage for Flod 0 Epoch: 23 [22400/37476                 (75%)]\tLoss: 0.197542\n",
      "Training stage for Flod 0 Epoch: 23 [25600/37476                 (85%)]\tLoss: 0.011688\n",
      "Training stage for Flod 0 Epoch: 23 [28800/37476                 (96%)]\tLoss: 0.088502\n",
      "Test set for fold0: Average Loss:           1.2036, Accuracy: 14736/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 24 [0/37476                 (0%)]\tLoss: 0.020978\n",
      "Training stage for Flod 0 Epoch: 24 [3200/37476                 (11%)]\tLoss: 0.286178\n",
      "Training stage for Flod 0 Epoch: 24 [6400/37476                 (21%)]\tLoss: 0.030589\n",
      "Training stage for Flod 0 Epoch: 24 [9600/37476                 (32%)]\tLoss: 0.037432\n",
      "Training stage for Flod 0 Epoch: 24 [12800/37476                 (43%)]\tLoss: 0.035823\n",
      "Training stage for Flod 0 Epoch: 24 [16000/37476                 (53%)]\tLoss: 0.018406\n",
      "Training stage for Flod 0 Epoch: 24 [19200/37476                 (64%)]\tLoss: 0.039741\n",
      "Training stage for Flod 0 Epoch: 24 [22400/37476                 (75%)]\tLoss: 0.176789\n",
      "Training stage for Flod 0 Epoch: 24 [25600/37476                 (85%)]\tLoss: 0.016387\n",
      "Training stage for Flod 0 Epoch: 24 [28800/37476                 (96%)]\tLoss: 0.069432\n",
      "Test set for fold0: Average Loss:           1.2142, Accuracy: 14746/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 25 [0/37476                 (0%)]\tLoss: 0.004584\n",
      "Training stage for Flod 0 Epoch: 25 [3200/37476                 (11%)]\tLoss: 0.024300\n",
      "Training stage for Flod 0 Epoch: 25 [6400/37476                 (21%)]\tLoss: 0.054464\n",
      "Training stage for Flod 0 Epoch: 25 [9600/37476                 (32%)]\tLoss: 0.000150\n",
      "Training stage for Flod 0 Epoch: 25 [12800/37476                 (43%)]\tLoss: 0.003263\n",
      "Training stage for Flod 0 Epoch: 25 [16000/37476                 (53%)]\tLoss: 0.000238\n",
      "Training stage for Flod 0 Epoch: 25 [19200/37476                 (64%)]\tLoss: 0.066412\n",
      "Training stage for Flod 0 Epoch: 25 [22400/37476                 (75%)]\tLoss: 0.003227\n",
      "Training stage for Flod 0 Epoch: 25 [25600/37476                 (85%)]\tLoss: 0.000342\n",
      "Training stage for Flod 0 Epoch: 25 [28800/37476                 (96%)]\tLoss: 0.004155\n",
      "Test set for fold0: Average Loss:           1.0920, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 26 [0/37476                 (0%)]\tLoss: 0.027660\n",
      "Training stage for Flod 0 Epoch: 26 [3200/37476                 (11%)]\tLoss: 0.004246\n",
      "Training stage for Flod 0 Epoch: 26 [6400/37476                 (21%)]\tLoss: 0.060527\n",
      "Training stage for Flod 0 Epoch: 26 [9600/37476                 (32%)]\tLoss: 0.000846\n",
      "Training stage for Flod 0 Epoch: 26 [12800/37476                 (43%)]\tLoss: 0.050722\n",
      "Training stage for Flod 0 Epoch: 26 [16000/37476                 (53%)]\tLoss: 0.078424\n",
      "Training stage for Flod 0 Epoch: 26 [19200/37476                 (64%)]\tLoss: 0.133331\n",
      "Training stage for Flod 0 Epoch: 26 [22400/37476                 (75%)]\tLoss: 0.000358\n",
      "Training stage for Flod 0 Epoch: 26 [25600/37476                 (85%)]\tLoss: 0.042158\n",
      "Training stage for Flod 0 Epoch: 26 [28800/37476                 (96%)]\tLoss: 0.000409\n",
      "Test set for fold0: Average Loss:           1.3157, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 27 [0/37476                 (0%)]\tLoss: 0.008083\n",
      "Training stage for Flod 0 Epoch: 27 [3200/37476                 (11%)]\tLoss: 0.062018\n",
      "Training stage for Flod 0 Epoch: 27 [6400/37476                 (21%)]\tLoss: 0.008568\n",
      "Training stage for Flod 0 Epoch: 27 [9600/37476                 (32%)]\tLoss: 0.000037\n",
      "Training stage for Flod 0 Epoch: 27 [12800/37476                 (43%)]\tLoss: 0.013134\n",
      "Training stage for Flod 0 Epoch: 27 [16000/37476                 (53%)]\tLoss: 0.097623\n",
      "Training stage for Flod 0 Epoch: 27 [19200/37476                 (64%)]\tLoss: 0.022397\n",
      "Training stage for Flod 0 Epoch: 27 [22400/37476                 (75%)]\tLoss: 0.041144\n",
      "Training stage for Flod 0 Epoch: 27 [25600/37476                 (85%)]\tLoss: 0.001313\n",
      "Training stage for Flod 0 Epoch: 27 [28800/37476                 (96%)]\tLoss: 0.000524\n",
      "Test set for fold0: Average Loss:           1.2680, Accuracy: 14727/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 28 [0/37476                 (0%)]\tLoss: 0.003396\n",
      "Training stage for Flod 0 Epoch: 28 [3200/37476                 (11%)]\tLoss: 0.045744\n",
      "Training stage for Flod 0 Epoch: 28 [6400/37476                 (21%)]\tLoss: 0.004019\n",
      "Training stage for Flod 0 Epoch: 28 [9600/37476                 (32%)]\tLoss: 0.031392\n",
      "Training stage for Flod 0 Epoch: 28 [12800/37476                 (43%)]\tLoss: 0.011474\n",
      "Training stage for Flod 0 Epoch: 28 [16000/37476                 (53%)]\tLoss: 0.051931\n",
      "Training stage for Flod 0 Epoch: 28 [19200/37476                 (64%)]\tLoss: 0.002001\n",
      "Training stage for Flod 0 Epoch: 28 [22400/37476                 (75%)]\tLoss: 0.078091\n",
      "Training stage for Flod 0 Epoch: 28 [25600/37476                 (85%)]\tLoss: 0.060407\n",
      "Training stage for Flod 0 Epoch: 28 [28800/37476                 (96%)]\tLoss: 0.048765\n",
      "Test set for fold0: Average Loss:           1.0343, Accuracy: 14722/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 29 [0/37476                 (0%)]\tLoss: 0.180369\n",
      "Training stage for Flod 0 Epoch: 29 [3200/37476                 (11%)]\tLoss: 0.023544\n",
      "Training stage for Flod 0 Epoch: 29 [6400/37476                 (21%)]\tLoss: 0.010690\n",
      "Training stage for Flod 0 Epoch: 29 [9600/37476                 (32%)]\tLoss: 0.004832\n",
      "Training stage for Flod 0 Epoch: 29 [12800/37476                 (43%)]\tLoss: 0.050945\n",
      "Training stage for Flod 0 Epoch: 29 [16000/37476                 (53%)]\tLoss: 0.008980\n",
      "Training stage for Flod 0 Epoch: 29 [19200/37476                 (64%)]\tLoss: 0.009510\n",
      "Training stage for Flod 0 Epoch: 29 [22400/37476                 (75%)]\tLoss: 0.135219\n",
      "Training stage for Flod 0 Epoch: 29 [25600/37476                 (85%)]\tLoss: 0.051546\n",
      "Training stage for Flod 0 Epoch: 29 [28800/37476                 (96%)]\tLoss: 0.241888\n",
      "Test set for fold0: Average Loss:           1.1649, Accuracy: 14707/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 30 [0/37476                 (0%)]\tLoss: 0.013178\n",
      "Training stage for Flod 0 Epoch: 30 [3200/37476                 (11%)]\tLoss: 0.001611\n",
      "Training stage for Flod 0 Epoch: 30 [6400/37476                 (21%)]\tLoss: 0.044237\n",
      "Training stage for Flod 0 Epoch: 30 [9600/37476                 (32%)]\tLoss: 0.051272\n",
      "Training stage for Flod 0 Epoch: 30 [12800/37476                 (43%)]\tLoss: 0.003150\n",
      "Training stage for Flod 0 Epoch: 30 [16000/37476                 (53%)]\tLoss: 0.000314\n",
      "Training stage for Flod 0 Epoch: 30 [19200/37476                 (64%)]\tLoss: 0.006376\n",
      "Training stage for Flod 0 Epoch: 30 [22400/37476                 (75%)]\tLoss: 0.002100\n",
      "Training stage for Flod 0 Epoch: 30 [25600/37476                 (85%)]\tLoss: 0.194351\n",
      "Training stage for Flod 0 Epoch: 30 [28800/37476                 (96%)]\tLoss: 0.008788\n",
      "Test set for fold0: Average Loss:           1.2716, Accuracy: 14723/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 31 [0/37476                 (0%)]\tLoss: 0.064509\n",
      "Training stage for Flod 0 Epoch: 31 [3200/37476                 (11%)]\tLoss: 0.014099\n",
      "Training stage for Flod 0 Epoch: 31 [6400/37476                 (21%)]\tLoss: 0.019941\n",
      "Training stage for Flod 0 Epoch: 31 [9600/37476                 (32%)]\tLoss: 0.167378\n",
      "Training stage for Flod 0 Epoch: 31 [12800/37476                 (43%)]\tLoss: 0.011593\n",
      "Training stage for Flod 0 Epoch: 31 [16000/37476                 (53%)]\tLoss: 0.432315\n",
      "Training stage for Flod 0 Epoch: 31 [19200/37476                 (64%)]\tLoss: 0.036494\n",
      "Training stage for Flod 0 Epoch: 31 [22400/37476                 (75%)]\tLoss: 0.059863\n",
      "Training stage for Flod 0 Epoch: 31 [25600/37476                 (85%)]\tLoss: 0.041420\n",
      "Training stage for Flod 0 Epoch: 31 [28800/37476                 (96%)]\tLoss: 0.006541\n",
      "Test set for fold0: Average Loss:           1.3367, Accuracy: 14738/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 32 [0/37476                 (0%)]\tLoss: 0.132194\n",
      "Training stage for Flod 0 Epoch: 32 [3200/37476                 (11%)]\tLoss: 0.085371\n",
      "Training stage for Flod 0 Epoch: 32 [6400/37476                 (21%)]\tLoss: 0.021488\n",
      "Training stage for Flod 0 Epoch: 32 [9600/37476                 (32%)]\tLoss: 0.064046\n",
      "Training stage for Flod 0 Epoch: 32 [12800/37476                 (43%)]\tLoss: 0.126483\n",
      "Training stage for Flod 0 Epoch: 32 [16000/37476                 (53%)]\tLoss: 0.082779\n",
      "Training stage for Flod 0 Epoch: 32 [19200/37476                 (64%)]\tLoss: 0.067149\n",
      "Training stage for Flod 0 Epoch: 32 [22400/37476                 (75%)]\tLoss: 0.016638\n",
      "Training stage for Flod 0 Epoch: 32 [25600/37476                 (85%)]\tLoss: 0.030443\n",
      "Training stage for Flod 0 Epoch: 32 [28800/37476                 (96%)]\tLoss: 0.029344\n",
      "Test set for fold0: Average Loss:           1.3418, Accuracy: 14671/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 33 [0/37476                 (0%)]\tLoss: 0.000667\n",
      "Training stage for Flod 0 Epoch: 33 [3200/37476                 (11%)]\tLoss: 0.065810\n",
      "Training stage for Flod 0 Epoch: 33 [6400/37476                 (21%)]\tLoss: 0.143841\n",
      "Training stage for Flod 0 Epoch: 33 [9600/37476                 (32%)]\tLoss: 0.005737\n",
      "Training stage for Flod 0 Epoch: 33 [12800/37476                 (43%)]\tLoss: 0.029395\n",
      "Training stage for Flod 0 Epoch: 33 [16000/37476                 (53%)]\tLoss: 0.000235\n",
      "Training stage for Flod 0 Epoch: 33 [19200/37476                 (64%)]\tLoss: 0.023256\n",
      "Training stage for Flod 0 Epoch: 33 [22400/37476                 (75%)]\tLoss: 0.073532\n",
      "Training stage for Flod 0 Epoch: 33 [25600/37476                 (85%)]\tLoss: 0.002264\n",
      "Training stage for Flod 0 Epoch: 33 [28800/37476                 (96%)]\tLoss: 0.006693\n",
      "Test set for fold0: Average Loss:           1.5644, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 34 [0/37476                 (0%)]\tLoss: 0.006734\n",
      "Training stage for Flod 0 Epoch: 34 [3200/37476                 (11%)]\tLoss: 0.012948\n",
      "Training stage for Flod 0 Epoch: 34 [6400/37476                 (21%)]\tLoss: 0.207017\n",
      "Training stage for Flod 0 Epoch: 34 [9600/37476                 (32%)]\tLoss: 0.002803\n",
      "Training stage for Flod 0 Epoch: 34 [12800/37476                 (43%)]\tLoss: 0.010397\n",
      "Training stage for Flod 0 Epoch: 34 [16000/37476                 (53%)]\tLoss: 0.014367\n",
      "Training stage for Flod 0 Epoch: 34 [19200/37476                 (64%)]\tLoss: 0.028440\n",
      "Training stage for Flod 0 Epoch: 34 [22400/37476                 (75%)]\tLoss: 0.023614\n",
      "Training stage for Flod 0 Epoch: 34 [25600/37476                 (85%)]\tLoss: 0.050884\n",
      "Training stage for Flod 0 Epoch: 34 [28800/37476                 (96%)]\tLoss: 0.205799\n",
      "Test set for fold0: Average Loss:           1.3770, Accuracy: 14699/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 35 [0/37476                 (0%)]\tLoss: 0.027124\n",
      "Training stage for Flod 0 Epoch: 35 [3200/37476                 (11%)]\tLoss: 0.010111\n",
      "Training stage for Flod 0 Epoch: 35 [6400/37476                 (21%)]\tLoss: 0.004669\n",
      "Training stage for Flod 0 Epoch: 35 [9600/37476                 (32%)]\tLoss: 0.018924\n",
      "Training stage for Flod 0 Epoch: 35 [12800/37476                 (43%)]\tLoss: 0.012703\n",
      "Training stage for Flod 0 Epoch: 35 [16000/37476                 (53%)]\tLoss: 0.070360\n",
      "Training stage for Flod 0 Epoch: 35 [19200/37476                 (64%)]\tLoss: 0.012210\n",
      "Training stage for Flod 0 Epoch: 35 [22400/37476                 (75%)]\tLoss: 0.088438\n",
      "Training stage for Flod 0 Epoch: 35 [25600/37476                 (85%)]\tLoss: 0.041006\n",
      "Training stage for Flod 0 Epoch: 35 [28800/37476                 (96%)]\tLoss: 0.038926\n",
      "Test set for fold0: Average Loss:           1.1831, Accuracy: 14765/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 36 [0/37476                 (0%)]\tLoss: 0.066070\n",
      "Training stage for Flod 0 Epoch: 36 [3200/37476                 (11%)]\tLoss: 0.028991\n",
      "Training stage for Flod 0 Epoch: 36 [6400/37476                 (21%)]\tLoss: 0.001787\n",
      "Training stage for Flod 0 Epoch: 36 [9600/37476                 (32%)]\tLoss: 0.003439\n",
      "Training stage for Flod 0 Epoch: 36 [12800/37476                 (43%)]\tLoss: 0.093463\n",
      "Training stage for Flod 0 Epoch: 36 [16000/37476                 (53%)]\tLoss: 0.018283\n",
      "Training stage for Flod 0 Epoch: 36 [19200/37476                 (64%)]\tLoss: 0.065601\n",
      "Training stage for Flod 0 Epoch: 36 [22400/37476                 (75%)]\tLoss: 0.023957\n",
      "Training stage for Flod 0 Epoch: 36 [25600/37476                 (85%)]\tLoss: 0.199816\n",
      "Training stage for Flod 0 Epoch: 36 [28800/37476                 (96%)]\tLoss: 0.002391\n",
      "Test set for fold0: Average Loss:           1.1185, Accuracy: 14742/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 37 [0/37476                 (0%)]\tLoss: 0.020719\n",
      "Training stage for Flod 0 Epoch: 37 [3200/37476                 (11%)]\tLoss: 0.009626\n",
      "Training stage for Flod 0 Epoch: 37 [6400/37476                 (21%)]\tLoss: 0.005954\n",
      "Training stage for Flod 0 Epoch: 37 [9600/37476                 (32%)]\tLoss: 0.024917\n",
      "Training stage for Flod 0 Epoch: 37 [12800/37476                 (43%)]\tLoss: 0.027050\n",
      "Training stage for Flod 0 Epoch: 37 [16000/37476                 (53%)]\tLoss: 0.001117\n",
      "Training stage for Flod 0 Epoch: 37 [19200/37476                 (64%)]\tLoss: 0.078470\n",
      "Training stage for Flod 0 Epoch: 37 [22400/37476                 (75%)]\tLoss: 0.000130\n",
      "Training stage for Flod 0 Epoch: 37 [25600/37476                 (85%)]\tLoss: 0.306297\n",
      "Training stage for Flod 0 Epoch: 37 [28800/37476                 (96%)]\tLoss: 0.002753\n",
      "Test set for fold0: Average Loss:           1.3180, Accuracy: 14732/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 38 [0/37476                 (0%)]\tLoss: 0.014303\n",
      "Training stage for Flod 0 Epoch: 38 [3200/37476                 (11%)]\tLoss: 0.000228\n",
      "Training stage for Flod 0 Epoch: 38 [6400/37476                 (21%)]\tLoss: 0.010942\n",
      "Training stage for Flod 0 Epoch: 38 [9600/37476                 (32%)]\tLoss: 0.077771\n",
      "Training stage for Flod 0 Epoch: 38 [12800/37476                 (43%)]\tLoss: 0.004663\n",
      "Training stage for Flod 0 Epoch: 38 [16000/37476                 (53%)]\tLoss: 0.012829\n",
      "Training stage for Flod 0 Epoch: 38 [19200/37476                 (64%)]\tLoss: 0.006490\n",
      "Training stage for Flod 0 Epoch: 38 [22400/37476                 (75%)]\tLoss: 0.091691\n",
      "Training stage for Flod 0 Epoch: 38 [25600/37476                 (85%)]\tLoss: 0.134400\n",
      "Training stage for Flod 0 Epoch: 38 [28800/37476                 (96%)]\tLoss: 0.004591\n",
      "Test set for fold0: Average Loss:           1.4403, Accuracy: 14734/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 39 [0/37476                 (0%)]\tLoss: 0.000153\n",
      "Training stage for Flod 0 Epoch: 39 [3200/37476                 (11%)]\tLoss: 0.051488\n",
      "Training stage for Flod 0 Epoch: 39 [6400/37476                 (21%)]\tLoss: 0.057376\n",
      "Training stage for Flod 0 Epoch: 39 [9600/37476                 (32%)]\tLoss: 0.013854\n",
      "Training stage for Flod 0 Epoch: 39 [12800/37476                 (43%)]\tLoss: 0.033975\n",
      "Training stage for Flod 0 Epoch: 39 [16000/37476                 (53%)]\tLoss: 0.001838\n",
      "Training stage for Flod 0 Epoch: 39 [19200/37476                 (64%)]\tLoss: 0.023687\n",
      "Training stage for Flod 0 Epoch: 39 [22400/37476                 (75%)]\tLoss: 0.011876\n",
      "Training stage for Flod 0 Epoch: 39 [25600/37476                 (85%)]\tLoss: 0.008353\n",
      "Training stage for Flod 0 Epoch: 39 [28800/37476                 (96%)]\tLoss: 0.015473\n",
      "Test set for fold0: Average Loss:           1.2373, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 40 [0/37476                 (0%)]\tLoss: 0.083253\n",
      "Training stage for Flod 0 Epoch: 40 [3200/37476                 (11%)]\tLoss: 0.048107\n",
      "Training stage for Flod 0 Epoch: 40 [6400/37476                 (21%)]\tLoss: 0.004079\n",
      "Training stage for Flod 0 Epoch: 40 [9600/37476                 (32%)]\tLoss: 0.076981\n",
      "Training stage for Flod 0 Epoch: 40 [12800/37476                 (43%)]\tLoss: 0.081498\n",
      "Training stage for Flod 0 Epoch: 40 [16000/37476                 (53%)]\tLoss: 0.057216\n",
      "Training stage for Flod 0 Epoch: 40 [19200/37476                 (64%)]\tLoss: 0.187162\n",
      "Training stage for Flod 0 Epoch: 40 [22400/37476                 (75%)]\tLoss: 0.008231\n",
      "Training stage for Flod 0 Epoch: 40 [25600/37476                 (85%)]\tLoss: 0.020131\n",
      "Training stage for Flod 0 Epoch: 40 [28800/37476                 (96%)]\tLoss: 0.014843\n",
      "Test set for fold0: Average Loss:           1.3056, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 41 [0/37476                 (0%)]\tLoss: 0.026379\n",
      "Training stage for Flod 0 Epoch: 41 [3200/37476                 (11%)]\tLoss: 0.008525\n",
      "Training stage for Flod 0 Epoch: 41 [6400/37476                 (21%)]\tLoss: 0.110196\n",
      "Training stage for Flod 0 Epoch: 41 [9600/37476                 (32%)]\tLoss: 0.097248\n",
      "Training stage for Flod 0 Epoch: 41 [12800/37476                 (43%)]\tLoss: 0.004435\n",
      "Training stage for Flod 0 Epoch: 41 [16000/37476                 (53%)]\tLoss: 0.021481\n",
      "Training stage for Flod 0 Epoch: 41 [19200/37476                 (64%)]\tLoss: 0.003633\n",
      "Training stage for Flod 0 Epoch: 41 [22400/37476                 (75%)]\tLoss: 0.009202\n",
      "Training stage for Flod 0 Epoch: 41 [25600/37476                 (85%)]\tLoss: 0.046450\n",
      "Training stage for Flod 0 Epoch: 41 [28800/37476                 (96%)]\tLoss: 0.015265\n",
      "Test set for fold0: Average Loss:           1.3022, Accuracy: 14734/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 42 [0/37476                 (0%)]\tLoss: 0.037337\n",
      "Training stage for Flod 0 Epoch: 42 [3200/37476                 (11%)]\tLoss: 0.061141\n",
      "Training stage for Flod 0 Epoch: 42 [6400/37476                 (21%)]\tLoss: 0.015471\n",
      "Training stage for Flod 0 Epoch: 42 [9600/37476                 (32%)]\tLoss: 0.000040\n",
      "Training stage for Flod 0 Epoch: 42 [12800/37476                 (43%)]\tLoss: 0.013707\n",
      "Training stage for Flod 0 Epoch: 42 [16000/37476                 (53%)]\tLoss: 0.059220\n",
      "Training stage for Flod 0 Epoch: 42 [19200/37476                 (64%)]\tLoss: 0.010807\n",
      "Training stage for Flod 0 Epoch: 42 [22400/37476                 (75%)]\tLoss: 0.020471\n",
      "Training stage for Flod 0 Epoch: 42 [25600/37476                 (85%)]\tLoss: 0.054969\n",
      "Training stage for Flod 0 Epoch: 42 [28800/37476                 (96%)]\tLoss: 0.015841\n",
      "Test set for fold0: Average Loss:           1.2700, Accuracy: 14777/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 43 [0/37476                 (0%)]\tLoss: 0.020184\n",
      "Training stage for Flod 0 Epoch: 43 [3200/37476                 (11%)]\tLoss: 0.005688\n",
      "Training stage for Flod 0 Epoch: 43 [6400/37476                 (21%)]\tLoss: 0.099935\n",
      "Training stage for Flod 0 Epoch: 43 [9600/37476                 (32%)]\tLoss: 0.030743\n",
      "Training stage for Flod 0 Epoch: 43 [12800/37476                 (43%)]\tLoss: 0.117313\n",
      "Training stage for Flod 0 Epoch: 43 [16000/37476                 (53%)]\tLoss: 0.008780\n",
      "Training stage for Flod 0 Epoch: 43 [19200/37476                 (64%)]\tLoss: 0.001126\n",
      "Training stage for Flod 0 Epoch: 43 [22400/37476                 (75%)]\tLoss: 0.003244\n",
      "Training stage for Flod 0 Epoch: 43 [25600/37476                 (85%)]\tLoss: 0.126438\n",
      "Training stage for Flod 0 Epoch: 43 [28800/37476                 (96%)]\tLoss: 0.222734\n",
      "Test set for fold0: Average Loss:           1.3643, Accuracy: 14717/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 44 [0/37476                 (0%)]\tLoss: 0.005167\n",
      "Training stage for Flod 0 Epoch: 44 [3200/37476                 (11%)]\tLoss: 0.023803\n",
      "Training stage for Flod 0 Epoch: 44 [6400/37476                 (21%)]\tLoss: 0.009243\n",
      "Training stage for Flod 0 Epoch: 44 [9600/37476                 (32%)]\tLoss: 0.282478\n",
      "Training stage for Flod 0 Epoch: 44 [12800/37476                 (43%)]\tLoss: 0.013410\n",
      "Training stage for Flod 0 Epoch: 44 [16000/37476                 (53%)]\tLoss: 0.008046\n",
      "Training stage for Flod 0 Epoch: 44 [19200/37476                 (64%)]\tLoss: 0.001255\n",
      "Training stage for Flod 0 Epoch: 44 [22400/37476                 (75%)]\tLoss: 0.000592\n",
      "Training stage for Flod 0 Epoch: 44 [25600/37476                 (85%)]\tLoss: 0.000663\n",
      "Training stage for Flod 0 Epoch: 44 [28800/37476                 (96%)]\tLoss: 0.038116\n",
      "Test set for fold0: Average Loss:           1.4217, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 45 [0/37476                 (0%)]\tLoss: 0.024792\n",
      "Training stage for Flod 0 Epoch: 45 [3200/37476                 (11%)]\tLoss: 0.026882\n",
      "Training stage for Flod 0 Epoch: 45 [6400/37476                 (21%)]\tLoss: 0.002631\n",
      "Training stage for Flod 0 Epoch: 45 [9600/37476                 (32%)]\tLoss: 0.034419\n",
      "Training stage for Flod 0 Epoch: 45 [12800/37476                 (43%)]\tLoss: 0.012151\n",
      "Training stage for Flod 0 Epoch: 45 [16000/37476                 (53%)]\tLoss: 0.076682\n",
      "Training stage for Flod 0 Epoch: 45 [19200/37476                 (64%)]\tLoss: 0.015552\n",
      "Training stage for Flod 0 Epoch: 45 [22400/37476                 (75%)]\tLoss: 0.019879\n",
      "Training stage for Flod 0 Epoch: 45 [25600/37476                 (85%)]\tLoss: 0.062224\n",
      "Training stage for Flod 0 Epoch: 45 [28800/37476                 (96%)]\tLoss: 0.013144\n",
      "Test set for fold0: Average Loss:           1.3631, Accuracy: 14722/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 46 [0/37476                 (0%)]\tLoss: 0.028088\n",
      "Training stage for Flod 0 Epoch: 46 [3200/37476                 (11%)]\tLoss: 0.002592\n",
      "Training stage for Flod 0 Epoch: 46 [6400/37476                 (21%)]\tLoss: 0.005624\n",
      "Training stage for Flod 0 Epoch: 46 [9600/37476                 (32%)]\tLoss: 0.103044\n",
      "Training stage for Flod 0 Epoch: 46 [12800/37476                 (43%)]\tLoss: 0.004870\n",
      "Training stage for Flod 0 Epoch: 46 [16000/37476                 (53%)]\tLoss: 0.027845\n",
      "Training stage for Flod 0 Epoch: 46 [19200/37476                 (64%)]\tLoss: 0.044416\n",
      "Training stage for Flod 0 Epoch: 46 [22400/37476                 (75%)]\tLoss: 0.000036\n",
      "Training stage for Flod 0 Epoch: 46 [25600/37476                 (85%)]\tLoss: 0.004117\n",
      "Training stage for Flod 0 Epoch: 46 [28800/37476                 (96%)]\tLoss: 0.001165\n",
      "Test set for fold0: Average Loss:           1.0987, Accuracy: 14725/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 47 [0/37476                 (0%)]\tLoss: 0.079736\n",
      "Training stage for Flod 0 Epoch: 47 [3200/37476                 (11%)]\tLoss: 0.005301\n",
      "Training stage for Flod 0 Epoch: 47 [6400/37476                 (21%)]\tLoss: 0.008002\n",
      "Training stage for Flod 0 Epoch: 47 [9600/37476                 (32%)]\tLoss: 0.000980\n",
      "Training stage for Flod 0 Epoch: 47 [12800/37476                 (43%)]\tLoss: 0.081203\n",
      "Training stage for Flod 0 Epoch: 47 [16000/37476                 (53%)]\tLoss: 0.008119\n",
      "Training stage for Flod 0 Epoch: 47 [19200/37476                 (64%)]\tLoss: 0.004208\n",
      "Training stage for Flod 0 Epoch: 47 [22400/37476                 (75%)]\tLoss: 0.003287\n",
      "Training stage for Flod 0 Epoch: 47 [25600/37476                 (85%)]\tLoss: 0.009058\n",
      "Training stage for Flod 0 Epoch: 47 [28800/37476                 (96%)]\tLoss: 0.014416\n",
      "Test set for fold0: Average Loss:           1.4968, Accuracy: 14757/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 48 [0/37476                 (0%)]\tLoss: 0.000255\n",
      "Training stage for Flod 0 Epoch: 48 [3200/37476                 (11%)]\tLoss: 0.003521\n",
      "Training stage for Flod 0 Epoch: 48 [6400/37476                 (21%)]\tLoss: 0.001626\n",
      "Training stage for Flod 0 Epoch: 48 [9600/37476                 (32%)]\tLoss: 0.009939\n",
      "Training stage for Flod 0 Epoch: 48 [12800/37476                 (43%)]\tLoss: 0.001769\n",
      "Training stage for Flod 0 Epoch: 48 [16000/37476                 (53%)]\tLoss: 0.020361\n",
      "Training stage for Flod 0 Epoch: 48 [19200/37476                 (64%)]\tLoss: 0.000997\n",
      "Training stage for Flod 0 Epoch: 48 [22400/37476                 (75%)]\tLoss: 0.009818\n",
      "Training stage for Flod 0 Epoch: 48 [25600/37476                 (85%)]\tLoss: 0.015594\n",
      "Training stage for Flod 0 Epoch: 48 [28800/37476                 (96%)]\tLoss: 0.104702\n",
      "Test set for fold0: Average Loss:           1.2918, Accuracy: 14796/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 49 [0/37476                 (0%)]\tLoss: 0.065359\n",
      "Training stage for Flod 0 Epoch: 49 [3200/37476                 (11%)]\tLoss: 0.024425\n",
      "Training stage for Flod 0 Epoch: 49 [6400/37476                 (21%)]\tLoss: 0.005849\n",
      "Training stage for Flod 0 Epoch: 49 [9600/37476                 (32%)]\tLoss: 0.052589\n",
      "Training stage for Flod 0 Epoch: 49 [12800/37476                 (43%)]\tLoss: 0.020726\n",
      "Training stage for Flod 0 Epoch: 49 [16000/37476                 (53%)]\tLoss: 0.000357\n",
      "Training stage for Flod 0 Epoch: 49 [19200/37476                 (64%)]\tLoss: 0.003521\n",
      "Training stage for Flod 0 Epoch: 49 [22400/37476                 (75%)]\tLoss: 0.000859\n",
      "Training stage for Flod 0 Epoch: 49 [25600/37476                 (85%)]\tLoss: 0.007028\n",
      "Training stage for Flod 0 Epoch: 49 [28800/37476                 (96%)]\tLoss: 0.025180\n",
      "Test set for fold0: Average Loss:           1.2447, Accuracy: 14753/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 50 [0/37476                 (0%)]\tLoss: 0.003989\n",
      "Training stage for Flod 0 Epoch: 50 [3200/37476                 (11%)]\tLoss: 0.033065\n",
      "Training stage for Flod 0 Epoch: 50 [6400/37476                 (21%)]\tLoss: 0.031332\n",
      "Training stage for Flod 0 Epoch: 50 [9600/37476                 (32%)]\tLoss: 0.027008\n",
      "Training stage for Flod 0 Epoch: 50 [12800/37476                 (43%)]\tLoss: 0.081301\n",
      "Training stage for Flod 0 Epoch: 50 [16000/37476                 (53%)]\tLoss: 0.126628\n",
      "Training stage for Flod 0 Epoch: 50 [19200/37476                 (64%)]\tLoss: 0.191146\n",
      "Training stage for Flod 0 Epoch: 50 [22400/37476                 (75%)]\tLoss: 0.007562\n",
      "Training stage for Flod 0 Epoch: 50 [25600/37476                 (85%)]\tLoss: 0.093922\n",
      "Training stage for Flod 0 Epoch: 50 [28800/37476                 (96%)]\tLoss: 0.000884\n",
      "Test set for fold0: Average Loss:           1.3624, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 51 [0/37476                 (0%)]\tLoss: 0.000567\n",
      "Training stage for Flod 0 Epoch: 51 [3200/37476                 (11%)]\tLoss: 0.066739\n",
      "Training stage for Flod 0 Epoch: 51 [6400/37476                 (21%)]\tLoss: 0.004971\n",
      "Training stage for Flod 0 Epoch: 51 [9600/37476                 (32%)]\tLoss: 0.105781\n",
      "Training stage for Flod 0 Epoch: 51 [12800/37476                 (43%)]\tLoss: 0.002621\n",
      "Training stage for Flod 0 Epoch: 51 [16000/37476                 (53%)]\tLoss: 0.044050\n",
      "Training stage for Flod 0 Epoch: 51 [19200/37476                 (64%)]\tLoss: 0.000846\n",
      "Training stage for Flod 0 Epoch: 51 [22400/37476                 (75%)]\tLoss: 0.018581\n",
      "Training stage for Flod 0 Epoch: 51 [25600/37476                 (85%)]\tLoss: 0.019553\n",
      "Training stage for Flod 0 Epoch: 51 [28800/37476                 (96%)]\tLoss: 0.006100\n",
      "Test set for fold0: Average Loss:           1.3835, Accuracy: 14777/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 52 [0/37476                 (0%)]\tLoss: 0.009522\n",
      "Training stage for Flod 0 Epoch: 52 [3200/37476                 (11%)]\tLoss: 0.039715\n",
      "Training stage for Flod 0 Epoch: 52 [6400/37476                 (21%)]\tLoss: 0.006373\n",
      "Training stage for Flod 0 Epoch: 52 [9600/37476                 (32%)]\tLoss: 0.019356\n",
      "Training stage for Flod 0 Epoch: 52 [12800/37476                 (43%)]\tLoss: 0.000335\n",
      "Training stage for Flod 0 Epoch: 52 [16000/37476                 (53%)]\tLoss: 0.006158\n",
      "Training stage for Flod 0 Epoch: 52 [19200/37476                 (64%)]\tLoss: 0.006978\n",
      "Training stage for Flod 0 Epoch: 52 [22400/37476                 (75%)]\tLoss: 0.000336\n",
      "Training stage for Flod 0 Epoch: 52 [25600/37476                 (85%)]\tLoss: 0.025401\n",
      "Training stage for Flod 0 Epoch: 52 [28800/37476                 (96%)]\tLoss: 0.416603\n",
      "Test set for fold0: Average Loss:           1.1494, Accuracy: 14770/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 53 [0/37476                 (0%)]\tLoss: 0.054068\n",
      "Training stage for Flod 0 Epoch: 53 [3200/37476                 (11%)]\tLoss: 0.000054\n",
      "Training stage for Flod 0 Epoch: 53 [6400/37476                 (21%)]\tLoss: 0.009536\n",
      "Training stage for Flod 0 Epoch: 53 [9600/37476                 (32%)]\tLoss: 0.006844\n",
      "Training stage for Flod 0 Epoch: 53 [12800/37476                 (43%)]\tLoss: 0.000921\n",
      "Training stage for Flod 0 Epoch: 53 [16000/37476                 (53%)]\tLoss: 0.012864\n",
      "Training stage for Flod 0 Epoch: 53 [19200/37476                 (64%)]\tLoss: 0.031365\n",
      "Training stage for Flod 0 Epoch: 53 [22400/37476                 (75%)]\tLoss: 0.021058\n",
      "Training stage for Flod 0 Epoch: 53 [25600/37476                 (85%)]\tLoss: 0.122266\n",
      "Training stage for Flod 0 Epoch: 53 [28800/37476                 (96%)]\tLoss: 0.086589\n",
      "Test set for fold0: Average Loss:           1.3899, Accuracy: 14793/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 54 [0/37476                 (0%)]\tLoss: 0.010859\n",
      "Training stage for Flod 0 Epoch: 54 [3200/37476                 (11%)]\tLoss: 0.021255\n",
      "Training stage for Flod 0 Epoch: 54 [6400/37476                 (21%)]\tLoss: 0.011047\n",
      "Training stage for Flod 0 Epoch: 54 [9600/37476                 (32%)]\tLoss: 0.000126\n",
      "Training stage for Flod 0 Epoch: 54 [12800/37476                 (43%)]\tLoss: 0.002242\n",
      "Training stage for Flod 0 Epoch: 54 [16000/37476                 (53%)]\tLoss: 0.055082\n",
      "Training stage for Flod 0 Epoch: 54 [19200/37476                 (64%)]\tLoss: 0.001061\n",
      "Training stage for Flod 0 Epoch: 54 [22400/37476                 (75%)]\tLoss: 0.063903\n",
      "Training stage for Flod 0 Epoch: 54 [25600/37476                 (85%)]\tLoss: 0.088539\n",
      "Training stage for Flod 0 Epoch: 54 [28800/37476                 (96%)]\tLoss: 0.007665\n",
      "Test set for fold0: Average Loss:           1.4525, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 55 [0/37476                 (0%)]\tLoss: 0.015022\n",
      "Training stage for Flod 0 Epoch: 55 [3200/37476                 (11%)]\tLoss: 0.023043\n",
      "Training stage for Flod 0 Epoch: 55 [6400/37476                 (21%)]\tLoss: 0.014242\n",
      "Training stage for Flod 0 Epoch: 55 [9600/37476                 (32%)]\tLoss: 0.004602\n",
      "Training stage for Flod 0 Epoch: 55 [12800/37476                 (43%)]\tLoss: 0.001986\n",
      "Training stage for Flod 0 Epoch: 55 [16000/37476                 (53%)]\tLoss: 0.030342\n",
      "Training stage for Flod 0 Epoch: 55 [19200/37476                 (64%)]\tLoss: 0.100922\n",
      "Training stage for Flod 0 Epoch: 55 [22400/37476                 (75%)]\tLoss: 0.042744\n",
      "Training stage for Flod 0 Epoch: 55 [25600/37476                 (85%)]\tLoss: 0.003356\n",
      "Training stage for Flod 0 Epoch: 55 [28800/37476                 (96%)]\tLoss: 0.062761\n",
      "Test set for fold0: Average Loss:           1.4286, Accuracy: 14775/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 56 [0/37476                 (0%)]\tLoss: 0.069814\n",
      "Training stage for Flod 0 Epoch: 56 [3200/37476                 (11%)]\tLoss: 0.009731\n",
      "Training stage for Flod 0 Epoch: 56 [6400/37476                 (21%)]\tLoss: 0.064527\n",
      "Training stage for Flod 0 Epoch: 56 [9600/37476                 (32%)]\tLoss: 0.004015\n",
      "Training stage for Flod 0 Epoch: 56 [12800/37476                 (43%)]\tLoss: 0.014959\n",
      "Training stage for Flod 0 Epoch: 56 [16000/37476                 (53%)]\tLoss: 0.000990\n",
      "Training stage for Flod 0 Epoch: 56 [19200/37476                 (64%)]\tLoss: 0.007953\n",
      "Training stage for Flod 0 Epoch: 56 [22400/37476                 (75%)]\tLoss: 0.087543\n",
      "Training stage for Flod 0 Epoch: 56 [25600/37476                 (85%)]\tLoss: 0.115279\n",
      "Training stage for Flod 0 Epoch: 56 [28800/37476                 (96%)]\tLoss: 0.038661\n",
      "Test set for fold0: Average Loss:           1.5496, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 57 [0/37476                 (0%)]\tLoss: 0.003005\n",
      "Training stage for Flod 0 Epoch: 57 [3200/37476                 (11%)]\tLoss: 0.000985\n",
      "Training stage for Flod 0 Epoch: 57 [6400/37476                 (21%)]\tLoss: 0.050251\n",
      "Training stage for Flod 0 Epoch: 57 [9600/37476                 (32%)]\tLoss: 0.123738\n",
      "Training stage for Flod 0 Epoch: 57 [12800/37476                 (43%)]\tLoss: 0.005801\n",
      "Training stage for Flod 0 Epoch: 57 [16000/37476                 (53%)]\tLoss: 0.001938\n",
      "Training stage for Flod 0 Epoch: 57 [19200/37476                 (64%)]\tLoss: 0.014410\n",
      "Training stage for Flod 0 Epoch: 57 [22400/37476                 (75%)]\tLoss: 0.000888\n",
      "Training stage for Flod 0 Epoch: 57 [25600/37476                 (85%)]\tLoss: 0.014733\n",
      "Training stage for Flod 0 Epoch: 57 [28800/37476                 (96%)]\tLoss: 0.099983\n",
      "Test set for fold0: Average Loss:           1.2008, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 58 [0/37476                 (0%)]\tLoss: 0.053606\n",
      "Training stage for Flod 0 Epoch: 58 [3200/37476                 (11%)]\tLoss: 0.177552\n",
      "Training stage for Flod 0 Epoch: 58 [6400/37476                 (21%)]\tLoss: 0.005690\n",
      "Training stage for Flod 0 Epoch: 58 [9600/37476                 (32%)]\tLoss: 0.005448\n",
      "Training stage for Flod 0 Epoch: 58 [12800/37476                 (43%)]\tLoss: 0.002961\n",
      "Training stage for Flod 0 Epoch: 58 [16000/37476                 (53%)]\tLoss: 0.009903\n",
      "Training stage for Flod 0 Epoch: 58 [19200/37476                 (64%)]\tLoss: 0.050082\n",
      "Training stage for Flod 0 Epoch: 58 [22400/37476                 (75%)]\tLoss: 0.013861\n",
      "Training stage for Flod 0 Epoch: 58 [25600/37476                 (85%)]\tLoss: 0.000114\n",
      "Training stage for Flod 0 Epoch: 58 [28800/37476                 (96%)]\tLoss: 0.022710\n",
      "Test set for fold0: Average Loss:           1.1991, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 59 [0/37476                 (0%)]\tLoss: 0.008363\n",
      "Training stage for Flod 0 Epoch: 59 [3200/37476                 (11%)]\tLoss: 0.008574\n",
      "Training stage for Flod 0 Epoch: 59 [6400/37476                 (21%)]\tLoss: 0.001227\n",
      "Training stage for Flod 0 Epoch: 59 [9600/37476                 (32%)]\tLoss: 0.000585\n",
      "Training stage for Flod 0 Epoch: 59 [12800/37476                 (43%)]\tLoss: 0.041641\n",
      "Training stage for Flod 0 Epoch: 59 [16000/37476                 (53%)]\tLoss: 0.001089\n",
      "Training stage for Flod 0 Epoch: 59 [19200/37476                 (64%)]\tLoss: 0.004838\n",
      "Training stage for Flod 0 Epoch: 59 [22400/37476                 (75%)]\tLoss: 0.020941\n",
      "Training stage for Flod 0 Epoch: 59 [25600/37476                 (85%)]\tLoss: 0.024292\n",
      "Training stage for Flod 0 Epoch: 59 [28800/37476                 (96%)]\tLoss: 0.081849\n",
      "Test set for fold0: Average Loss:           1.1791, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 60 [0/37476                 (0%)]\tLoss: 0.004003\n",
      "Training stage for Flod 0 Epoch: 60 [3200/37476                 (11%)]\tLoss: 0.001117\n",
      "Training stage for Flod 0 Epoch: 60 [6400/37476                 (21%)]\tLoss: 0.002314\n",
      "Training stage for Flod 0 Epoch: 60 [9600/37476                 (32%)]\tLoss: 0.052862\n",
      "Training stage for Flod 0 Epoch: 60 [12800/37476                 (43%)]\tLoss: 0.034879\n",
      "Training stage for Flod 0 Epoch: 60 [16000/37476                 (53%)]\tLoss: 0.003588\n",
      "Training stage for Flod 0 Epoch: 60 [19200/37476                 (64%)]\tLoss: 0.007265\n",
      "Training stage for Flod 0 Epoch: 60 [22400/37476                 (75%)]\tLoss: 0.160842\n",
      "Training stage for Flod 0 Epoch: 60 [25600/37476                 (85%)]\tLoss: 0.005530\n",
      "Training stage for Flod 0 Epoch: 60 [28800/37476                 (96%)]\tLoss: 0.000994\n",
      "Test set for fold0: Average Loss:           1.3440, Accuracy: 14727/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 61 [0/37476                 (0%)]\tLoss: 0.004611\n",
      "Training stage for Flod 0 Epoch: 61 [3200/37476                 (11%)]\tLoss: 0.022205\n",
      "Training stage for Flod 0 Epoch: 61 [6400/37476                 (21%)]\tLoss: 0.001923\n",
      "Training stage for Flod 0 Epoch: 61 [9600/37476                 (32%)]\tLoss: 0.012330\n",
      "Training stage for Flod 0 Epoch: 61 [12800/37476                 (43%)]\tLoss: 0.013753\n",
      "Training stage for Flod 0 Epoch: 61 [16000/37476                 (53%)]\tLoss: 0.159972\n",
      "Training stage for Flod 0 Epoch: 61 [19200/37476                 (64%)]\tLoss: 0.052695\n",
      "Training stage for Flod 0 Epoch: 61 [22400/37476                 (75%)]\tLoss: 0.054252\n",
      "Training stage for Flod 0 Epoch: 61 [25600/37476                 (85%)]\tLoss: 0.033711\n",
      "Training stage for Flod 0 Epoch: 61 [28800/37476                 (96%)]\tLoss: 0.000664\n",
      "Test set for fold0: Average Loss:           1.3177, Accuracy: 14792/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 62 [0/37476                 (0%)]\tLoss: 0.014621\n",
      "Training stage for Flod 0 Epoch: 62 [3200/37476                 (11%)]\tLoss: 0.163625\n",
      "Training stage for Flod 0 Epoch: 62 [6400/37476                 (21%)]\tLoss: 0.367516\n",
      "Training stage for Flod 0 Epoch: 62 [9600/37476                 (32%)]\tLoss: 0.052056\n",
      "Training stage for Flod 0 Epoch: 62 [12800/37476                 (43%)]\tLoss: 0.036443\n",
      "Training stage for Flod 0 Epoch: 62 [16000/37476                 (53%)]\tLoss: 0.010950\n",
      "Training stage for Flod 0 Epoch: 62 [19200/37476                 (64%)]\tLoss: 0.021001\n",
      "Training stage for Flod 0 Epoch: 62 [22400/37476                 (75%)]\tLoss: 0.085439\n",
      "Training stage for Flod 0 Epoch: 62 [25600/37476                 (85%)]\tLoss: 0.003189\n",
      "Training stage for Flod 0 Epoch: 62 [28800/37476                 (96%)]\tLoss: 0.035825\n",
      "Test set for fold0: Average Loss:           1.4074, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 63 [0/37476                 (0%)]\tLoss: 0.033637\n",
      "Training stage for Flod 0 Epoch: 63 [3200/37476                 (11%)]\tLoss: 0.310396\n",
      "Training stage for Flod 0 Epoch: 63 [6400/37476                 (21%)]\tLoss: 0.063808\n",
      "Training stage for Flod 0 Epoch: 63 [9600/37476                 (32%)]\tLoss: 0.002456\n",
      "Training stage for Flod 0 Epoch: 63 [12800/37476                 (43%)]\tLoss: 0.004582\n",
      "Training stage for Flod 0 Epoch: 63 [16000/37476                 (53%)]\tLoss: 0.020912\n",
      "Training stage for Flod 0 Epoch: 63 [19200/37476                 (64%)]\tLoss: 0.128364\n",
      "Training stage for Flod 0 Epoch: 63 [22400/37476                 (75%)]\tLoss: 0.013933\n",
      "Training stage for Flod 0 Epoch: 63 [25600/37476                 (85%)]\tLoss: 0.041107\n",
      "Training stage for Flod 0 Epoch: 63 [28800/37476                 (96%)]\tLoss: 0.026903\n",
      "Test set for fold0: Average Loss:           1.1749, Accuracy: 14780/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 64 [0/37476                 (0%)]\tLoss: 0.007273\n",
      "Training stage for Flod 0 Epoch: 64 [3200/37476                 (11%)]\tLoss: 0.005239\n",
      "Training stage for Flod 0 Epoch: 64 [6400/37476                 (21%)]\tLoss: 0.014499\n",
      "Training stage for Flod 0 Epoch: 64 [9600/37476                 (32%)]\tLoss: 0.032355\n",
      "Training stage for Flod 0 Epoch: 64 [12800/37476                 (43%)]\tLoss: 0.001584\n",
      "Training stage for Flod 0 Epoch: 64 [16000/37476                 (53%)]\tLoss: 0.058407\n",
      "Training stage for Flod 0 Epoch: 64 [19200/37476                 (64%)]\tLoss: 0.013896\n",
      "Training stage for Flod 0 Epoch: 64 [22400/37476                 (75%)]\tLoss: 0.003327\n",
      "Training stage for Flod 0 Epoch: 64 [25600/37476                 (85%)]\tLoss: 0.000728\n",
      "Training stage for Flod 0 Epoch: 64 [28800/37476                 (96%)]\tLoss: 0.094347\n",
      "Test set for fold0: Average Loss:           1.3753, Accuracy: 14787/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 65 [0/37476                 (0%)]\tLoss: 0.019266\n",
      "Training stage for Flod 0 Epoch: 65 [3200/37476                 (11%)]\tLoss: 0.020868\n",
      "Training stage for Flod 0 Epoch: 65 [6400/37476                 (21%)]\tLoss: 0.162966\n",
      "Training stage for Flod 0 Epoch: 65 [9600/37476                 (32%)]\tLoss: 0.000561\n",
      "Training stage for Flod 0 Epoch: 65 [12800/37476                 (43%)]\tLoss: 0.043523\n",
      "Training stage for Flod 0 Epoch: 65 [16000/37476                 (53%)]\tLoss: 0.004882\n",
      "Training stage for Flod 0 Epoch: 65 [19200/37476                 (64%)]\tLoss: 0.011665\n",
      "Training stage for Flod 0 Epoch: 65 [22400/37476                 (75%)]\tLoss: 0.006202\n",
      "Training stage for Flod 0 Epoch: 65 [25600/37476                 (85%)]\tLoss: 0.006067\n",
      "Training stage for Flod 0 Epoch: 65 [28800/37476                 (96%)]\tLoss: 0.001728\n",
      "Test set for fold0: Average Loss:           1.7224, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 66 [0/37476                 (0%)]\tLoss: 0.000037\n",
      "Training stage for Flod 0 Epoch: 66 [3200/37476                 (11%)]\tLoss: 0.026826\n",
      "Training stage for Flod 0 Epoch: 66 [6400/37476                 (21%)]\tLoss: 0.052616\n",
      "Training stage for Flod 0 Epoch: 66 [9600/37476                 (32%)]\tLoss: 0.129128\n",
      "Training stage for Flod 0 Epoch: 66 [12800/37476                 (43%)]\tLoss: 0.032387\n",
      "Training stage for Flod 0 Epoch: 66 [16000/37476                 (53%)]\tLoss: 0.006150\n",
      "Training stage for Flod 0 Epoch: 66 [19200/37476                 (64%)]\tLoss: 0.021676\n",
      "Training stage for Flod 0 Epoch: 66 [22400/37476                 (75%)]\tLoss: 0.034568\n",
      "Training stage for Flod 0 Epoch: 66 [25600/37476                 (85%)]\tLoss: 0.017743\n",
      "Training stage for Flod 0 Epoch: 66 [28800/37476                 (96%)]\tLoss: 0.004708\n",
      "Test set for fold0: Average Loss:           1.5160, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 67 [0/37476                 (0%)]\tLoss: 0.001227\n",
      "Training stage for Flod 0 Epoch: 67 [3200/37476                 (11%)]\tLoss: 0.030824\n",
      "Training stage for Flod 0 Epoch: 67 [6400/37476                 (21%)]\tLoss: 0.007785\n",
      "Training stage for Flod 0 Epoch: 67 [9600/37476                 (32%)]\tLoss: 0.007024\n",
      "Training stage for Flod 0 Epoch: 67 [12800/37476                 (43%)]\tLoss: 0.148030\n",
      "Training stage for Flod 0 Epoch: 67 [16000/37476                 (53%)]\tLoss: 0.000571\n",
      "Training stage for Flod 0 Epoch: 67 [19200/37476                 (64%)]\tLoss: 0.004613\n",
      "Training stage for Flod 0 Epoch: 67 [22400/37476                 (75%)]\tLoss: 0.011242\n",
      "Training stage for Flod 0 Epoch: 67 [25600/37476                 (85%)]\tLoss: 0.013424\n",
      "Training stage for Flod 0 Epoch: 67 [28800/37476                 (96%)]\tLoss: 0.029001\n",
      "Test set for fold0: Average Loss:           1.4034, Accuracy: 14704/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 68 [0/37476                 (0%)]\tLoss: 0.005972\n",
      "Training stage for Flod 0 Epoch: 68 [3200/37476                 (11%)]\tLoss: 0.016456\n",
      "Training stage for Flod 0 Epoch: 68 [6400/37476                 (21%)]\tLoss: 0.005670\n",
      "Training stage for Flod 0 Epoch: 68 [9600/37476                 (32%)]\tLoss: 0.008772\n",
      "Training stage for Flod 0 Epoch: 68 [12800/37476                 (43%)]\tLoss: 0.214955\n",
      "Training stage for Flod 0 Epoch: 68 [16000/37476                 (53%)]\tLoss: 0.022181\n",
      "Training stage for Flod 0 Epoch: 68 [19200/37476                 (64%)]\tLoss: 0.022392\n",
      "Training stage for Flod 0 Epoch: 68 [22400/37476                 (75%)]\tLoss: 0.053499\n",
      "Training stage for Flod 0 Epoch: 68 [25600/37476                 (85%)]\tLoss: 0.015058\n",
      "Training stage for Flod 0 Epoch: 68 [28800/37476                 (96%)]\tLoss: 0.002113\n",
      "Test set for fold0: Average Loss:           1.8553, Accuracy: 14689/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 69 [0/37476                 (0%)]\tLoss: 0.226901\n",
      "Training stage for Flod 0 Epoch: 69 [3200/37476                 (11%)]\tLoss: 0.006822\n",
      "Training stage for Flod 0 Epoch: 69 [6400/37476                 (21%)]\tLoss: 0.001646\n",
      "Training stage for Flod 0 Epoch: 69 [9600/37476                 (32%)]\tLoss: 0.008158\n",
      "Training stage for Flod 0 Epoch: 69 [12800/37476                 (43%)]\tLoss: 0.008560\n",
      "Training stage for Flod 0 Epoch: 69 [16000/37476                 (53%)]\tLoss: 0.000195\n",
      "Training stage for Flod 0 Epoch: 69 [19200/37476                 (64%)]\tLoss: 0.064149\n",
      "Training stage for Flod 0 Epoch: 69 [22400/37476                 (75%)]\tLoss: 0.013352\n",
      "Training stage for Flod 0 Epoch: 69 [25600/37476                 (85%)]\tLoss: 0.043694\n",
      "Training stage for Flod 0 Epoch: 69 [28800/37476                 (96%)]\tLoss: 0.042272\n",
      "Test set for fold0: Average Loss:           1.5271, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 70 [0/37476                 (0%)]\tLoss: 0.014128\n",
      "Training stage for Flod 0 Epoch: 70 [3200/37476                 (11%)]\tLoss: 0.013011\n",
      "Training stage for Flod 0 Epoch: 70 [6400/37476                 (21%)]\tLoss: 0.037024\n",
      "Training stage for Flod 0 Epoch: 70 [9600/37476                 (32%)]\tLoss: 0.016360\n",
      "Training stage for Flod 0 Epoch: 70 [12800/37476                 (43%)]\tLoss: 0.002334\n",
      "Training stage for Flod 0 Epoch: 70 [16000/37476                 (53%)]\tLoss: 0.006950\n",
      "Training stage for Flod 0 Epoch: 70 [19200/37476                 (64%)]\tLoss: 0.013108\n",
      "Training stage for Flod 0 Epoch: 70 [22400/37476                 (75%)]\tLoss: 0.024857\n",
      "Training stage for Flod 0 Epoch: 70 [25600/37476                 (85%)]\tLoss: 0.001808\n",
      "Training stage for Flod 0 Epoch: 70 [28800/37476                 (96%)]\tLoss: 0.001574\n",
      "Test set for fold0: Average Loss:           1.4005, Accuracy: 14789/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 71 [0/37476                 (0%)]\tLoss: 0.052171\n",
      "Training stage for Flod 0 Epoch: 71 [3200/37476                 (11%)]\tLoss: 0.032931\n",
      "Training stage for Flod 0 Epoch: 71 [6400/37476                 (21%)]\tLoss: 0.010157\n",
      "Training stage for Flod 0 Epoch: 71 [9600/37476                 (32%)]\tLoss: 0.003230\n",
      "Training stage for Flod 0 Epoch: 71 [12800/37476                 (43%)]\tLoss: 0.006491\n",
      "Training stage for Flod 0 Epoch: 71 [16000/37476                 (53%)]\tLoss: 0.062611\n",
      "Training stage for Flod 0 Epoch: 71 [19200/37476                 (64%)]\tLoss: 0.000089\n",
      "Training stage for Flod 0 Epoch: 71 [22400/37476                 (75%)]\tLoss: 0.001046\n",
      "Training stage for Flod 0 Epoch: 71 [25600/37476                 (85%)]\tLoss: 0.055754\n",
      "Training stage for Flod 0 Epoch: 71 [28800/37476                 (96%)]\tLoss: 0.010634\n",
      "Test set for fold0: Average Loss:           1.2133, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 72 [0/37476                 (0%)]\tLoss: 0.026047\n",
      "Training stage for Flod 0 Epoch: 72 [3200/37476                 (11%)]\tLoss: 0.005301\n",
      "Training stage for Flod 0 Epoch: 72 [6400/37476                 (21%)]\tLoss: 0.074018\n",
      "Training stage for Flod 0 Epoch: 72 [9600/37476                 (32%)]\tLoss: 0.003838\n",
      "Training stage for Flod 0 Epoch: 72 [12800/37476                 (43%)]\tLoss: 0.013816\n",
      "Training stage for Flod 0 Epoch: 72 [16000/37476                 (53%)]\tLoss: 0.038517\n",
      "Training stage for Flod 0 Epoch: 72 [19200/37476                 (64%)]\tLoss: 0.001097\n",
      "Training stage for Flod 0 Epoch: 72 [22400/37476                 (75%)]\tLoss: 0.015706\n",
      "Training stage for Flod 0 Epoch: 72 [25600/37476                 (85%)]\tLoss: 0.033483\n",
      "Training stage for Flod 0 Epoch: 72 [28800/37476                 (96%)]\tLoss: 0.011594\n",
      "Test set for fold0: Average Loss:           1.4338, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 73 [0/37476                 (0%)]\tLoss: 0.049915\n",
      "Training stage for Flod 0 Epoch: 73 [3200/37476                 (11%)]\tLoss: 0.017365\n",
      "Training stage for Flod 0 Epoch: 73 [6400/37476                 (21%)]\tLoss: 0.003700\n",
      "Training stage for Flod 0 Epoch: 73 [9600/37476                 (32%)]\tLoss: 0.077398\n",
      "Training stage for Flod 0 Epoch: 73 [12800/37476                 (43%)]\tLoss: 0.175940\n",
      "Training stage for Flod 0 Epoch: 73 [16000/37476                 (53%)]\tLoss: 0.078057\n",
      "Training stage for Flod 0 Epoch: 73 [19200/37476                 (64%)]\tLoss: 0.097035\n",
      "Training stage for Flod 0 Epoch: 73 [22400/37476                 (75%)]\tLoss: 0.010093\n",
      "Training stage for Flod 0 Epoch: 73 [25600/37476                 (85%)]\tLoss: 0.006814\n",
      "Training stage for Flod 0 Epoch: 73 [28800/37476                 (96%)]\tLoss: 0.016772\n",
      "Test set for fold0: Average Loss:           1.2896, Accuracy: 14673/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 74 [0/37476                 (0%)]\tLoss: 0.068628\n",
      "Training stage for Flod 0 Epoch: 74 [3200/37476                 (11%)]\tLoss: 0.000334\n",
      "Training stage for Flod 0 Epoch: 74 [6400/37476                 (21%)]\tLoss: 0.035088\n",
      "Training stage for Flod 0 Epoch: 74 [9600/37476                 (32%)]\tLoss: 0.035447\n",
      "Training stage for Flod 0 Epoch: 74 [12800/37476                 (43%)]\tLoss: 0.002392\n",
      "Training stage for Flod 0 Epoch: 74 [16000/37476                 (53%)]\tLoss: 0.013774\n",
      "Training stage for Flod 0 Epoch: 74 [19200/37476                 (64%)]\tLoss: 0.018210\n",
      "Training stage for Flod 0 Epoch: 74 [22400/37476                 (75%)]\tLoss: 0.032009\n",
      "Training stage for Flod 0 Epoch: 74 [25600/37476                 (85%)]\tLoss: 0.078774\n",
      "Training stage for Flod 0 Epoch: 74 [28800/37476                 (96%)]\tLoss: 0.037215\n",
      "Test set for fold0: Average Loss:           1.2176, Accuracy: 14780/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 75 [0/37476                 (0%)]\tLoss: 0.020622\n",
      "Training stage for Flod 0 Epoch: 75 [3200/37476                 (11%)]\tLoss: 0.118127\n",
      "Training stage for Flod 0 Epoch: 75 [6400/37476                 (21%)]\tLoss: 0.014498\n",
      "Training stage for Flod 0 Epoch: 75 [9600/37476                 (32%)]\tLoss: 0.008761\n",
      "Training stage for Flod 0 Epoch: 75 [12800/37476                 (43%)]\tLoss: 0.001133\n",
      "Training stage for Flod 0 Epoch: 75 [16000/37476                 (53%)]\tLoss: 0.003021\n",
      "Training stage for Flod 0 Epoch: 75 [19200/37476                 (64%)]\tLoss: 0.030034\n",
      "Training stage for Flod 0 Epoch: 75 [22400/37476                 (75%)]\tLoss: 0.034142\n",
      "Training stage for Flod 0 Epoch: 75 [25600/37476                 (85%)]\tLoss: 0.000647\n",
      "Training stage for Flod 0 Epoch: 75 [28800/37476                 (96%)]\tLoss: 0.007203\n",
      "Test set for fold0: Average Loss:           1.4963, Accuracy: 14699/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 76 [0/37476                 (0%)]\tLoss: 0.010043\n",
      "Training stage for Flod 0 Epoch: 76 [3200/37476                 (11%)]\tLoss: 0.014472\n",
      "Training stage for Flod 0 Epoch: 76 [6400/37476                 (21%)]\tLoss: 0.183688\n",
      "Training stage for Flod 0 Epoch: 76 [9600/37476                 (32%)]\tLoss: 0.180514\n",
      "Training stage for Flod 0 Epoch: 76 [12800/37476                 (43%)]\tLoss: 0.048735\n",
      "Training stage for Flod 0 Epoch: 76 [16000/37476                 (53%)]\tLoss: 0.008845\n",
      "Training stage for Flod 0 Epoch: 76 [19200/37476                 (64%)]\tLoss: 0.000294\n",
      "Training stage for Flod 0 Epoch: 76 [22400/37476                 (75%)]\tLoss: 0.051005\n",
      "Training stage for Flod 0 Epoch: 76 [25600/37476                 (85%)]\tLoss: 0.004197\n",
      "Training stage for Flod 0 Epoch: 76 [28800/37476                 (96%)]\tLoss: 0.000728\n",
      "Test set for fold0: Average Loss:           1.7800, Accuracy: 14765/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 77 [0/37476                 (0%)]\tLoss: 0.011946\n",
      "Training stage for Flod 0 Epoch: 77 [3200/37476                 (11%)]\tLoss: 0.030190\n",
      "Training stage for Flod 0 Epoch: 77 [6400/37476                 (21%)]\tLoss: 0.014920\n",
      "Training stage for Flod 0 Epoch: 77 [9600/37476                 (32%)]\tLoss: 0.010349\n",
      "Training stage for Flod 0 Epoch: 77 [12800/37476                 (43%)]\tLoss: 0.002190\n",
      "Training stage for Flod 0 Epoch: 77 [16000/37476                 (53%)]\tLoss: 0.025050\n",
      "Training stage for Flod 0 Epoch: 77 [19200/37476                 (64%)]\tLoss: 0.013520\n",
      "Training stage for Flod 0 Epoch: 77 [22400/37476                 (75%)]\tLoss: 0.009028\n",
      "Training stage for Flod 0 Epoch: 77 [25600/37476                 (85%)]\tLoss: 0.006147\n",
      "Training stage for Flod 0 Epoch: 77 [28800/37476                 (96%)]\tLoss: 0.005067\n",
      "Test set for fold0: Average Loss:           1.4390, Accuracy: 14742/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 78 [0/37476                 (0%)]\tLoss: 0.048411\n",
      "Training stage for Flod 0 Epoch: 78 [3200/37476                 (11%)]\tLoss: 0.032428\n",
      "Training stage for Flod 0 Epoch: 78 [6400/37476                 (21%)]\tLoss: 0.029050\n",
      "Training stage for Flod 0 Epoch: 78 [9600/37476                 (32%)]\tLoss: 0.065415\n",
      "Training stage for Flod 0 Epoch: 78 [12800/37476                 (43%)]\tLoss: 0.080420\n",
      "Training stage for Flod 0 Epoch: 78 [16000/37476                 (53%)]\tLoss: 0.001731\n",
      "Training stage for Flod 0 Epoch: 78 [19200/37476                 (64%)]\tLoss: 0.000335\n",
      "Training stage for Flod 0 Epoch: 78 [22400/37476                 (75%)]\tLoss: 0.000925\n",
      "Training stage for Flod 0 Epoch: 78 [25600/37476                 (85%)]\tLoss: 0.016348\n",
      "Training stage for Flod 0 Epoch: 78 [28800/37476                 (96%)]\tLoss: 0.001146\n",
      "Test set for fold0: Average Loss:           1.2132, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 79 [0/37476                 (0%)]\tLoss: 0.002243\n",
      "Training stage for Flod 0 Epoch: 79 [3200/37476                 (11%)]\tLoss: 0.013171\n",
      "Training stage for Flod 0 Epoch: 79 [6400/37476                 (21%)]\tLoss: 0.001515\n",
      "Training stage for Flod 0 Epoch: 79 [9600/37476                 (32%)]\tLoss: 0.010087\n",
      "Training stage for Flod 0 Epoch: 79 [12800/37476                 (43%)]\tLoss: 0.029968\n",
      "Training stage for Flod 0 Epoch: 79 [16000/37476                 (53%)]\tLoss: 0.061603\n",
      "Training stage for Flod 0 Epoch: 79 [19200/37476                 (64%)]\tLoss: 0.015931\n",
      "Training stage for Flod 0 Epoch: 79 [22400/37476                 (75%)]\tLoss: 0.015074\n",
      "Training stage for Flod 0 Epoch: 79 [25600/37476                 (85%)]\tLoss: 0.005516\n",
      "Training stage for Flod 0 Epoch: 79 [28800/37476                 (96%)]\tLoss: 0.005000\n",
      "Test set for fold0: Average Loss:           1.3788, Accuracy: 14763/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 80 [0/37476                 (0%)]\tLoss: 0.028764\n",
      "Training stage for Flod 0 Epoch: 80 [3200/37476                 (11%)]\tLoss: 0.005417\n",
      "Training stage for Flod 0 Epoch: 80 [6400/37476                 (21%)]\tLoss: 0.015442\n",
      "Training stage for Flod 0 Epoch: 80 [9600/37476                 (32%)]\tLoss: 0.233677\n",
      "Training stage for Flod 0 Epoch: 80 [12800/37476                 (43%)]\tLoss: 0.000082\n",
      "Training stage for Flod 0 Epoch: 80 [16000/37476                 (53%)]\tLoss: 0.036040\n",
      "Training stage for Flod 0 Epoch: 80 [19200/37476                 (64%)]\tLoss: 0.003033\n",
      "Training stage for Flod 0 Epoch: 80 [22400/37476                 (75%)]\tLoss: 0.007692\n",
      "Training stage for Flod 0 Epoch: 80 [25600/37476                 (85%)]\tLoss: 0.000743\n",
      "Training stage for Flod 0 Epoch: 80 [28800/37476                 (96%)]\tLoss: 0.030389\n",
      "Test set for fold0: Average Loss:           1.4232, Accuracy: 14719/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 81 [0/37476                 (0%)]\tLoss: 0.081851\n",
      "Training stage for Flod 0 Epoch: 81 [3200/37476                 (11%)]\tLoss: 0.005667\n",
      "Training stage for Flod 0 Epoch: 81 [6400/37476                 (21%)]\tLoss: 0.010122\n",
      "Training stage for Flod 0 Epoch: 81 [9600/37476                 (32%)]\tLoss: 0.072758\n",
      "Training stage for Flod 0 Epoch: 81 [12800/37476                 (43%)]\tLoss: 0.005039\n",
      "Training stage for Flod 0 Epoch: 81 [16000/37476                 (53%)]\tLoss: 0.021614\n",
      "Training stage for Flod 0 Epoch: 81 [19200/37476                 (64%)]\tLoss: 0.001265\n",
      "Training stage for Flod 0 Epoch: 81 [22400/37476                 (75%)]\tLoss: 0.000109\n",
      "Training stage for Flod 0 Epoch: 81 [25600/37476                 (85%)]\tLoss: 0.007056\n",
      "Training stage for Flod 0 Epoch: 81 [28800/37476                 (96%)]\tLoss: 0.000625\n",
      "Test set for fold0: Average Loss:           1.4226, Accuracy: 14792/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 82 [0/37476                 (0%)]\tLoss: 0.042144\n",
      "Training stage for Flod 0 Epoch: 82 [3200/37476                 (11%)]\tLoss: 0.001360\n",
      "Training stage for Flod 0 Epoch: 82 [6400/37476                 (21%)]\tLoss: 0.071947\n",
      "Training stage for Flod 0 Epoch: 82 [9600/37476                 (32%)]\tLoss: 0.088350\n",
      "Training stage for Flod 0 Epoch: 82 [12800/37476                 (43%)]\tLoss: 0.043110\n",
      "Training stage for Flod 0 Epoch: 82 [16000/37476                 (53%)]\tLoss: 0.089717\n",
      "Training stage for Flod 0 Epoch: 82 [19200/37476                 (64%)]\tLoss: 0.012649\n",
      "Training stage for Flod 0 Epoch: 82 [22400/37476                 (75%)]\tLoss: 0.005217\n",
      "Training stage for Flod 0 Epoch: 82 [25600/37476                 (85%)]\tLoss: 0.012898\n",
      "Training stage for Flod 0 Epoch: 82 [28800/37476                 (96%)]\tLoss: 0.015227\n",
      "Test set for fold0: Average Loss:           1.5729, Accuracy: 14802/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 83 [0/37476                 (0%)]\tLoss: 0.048079\n",
      "Training stage for Flod 0 Epoch: 83 [3200/37476                 (11%)]\tLoss: 0.004527\n",
      "Training stage for Flod 0 Epoch: 83 [6400/37476                 (21%)]\tLoss: 0.009676\n",
      "Training stage for Flod 0 Epoch: 83 [9600/37476                 (32%)]\tLoss: 0.012865\n",
      "Training stage for Flod 0 Epoch: 83 [12800/37476                 (43%)]\tLoss: 0.004762\n",
      "Training stage for Flod 0 Epoch: 83 [16000/37476                 (53%)]\tLoss: 0.009588\n",
      "Training stage for Flod 0 Epoch: 83 [19200/37476                 (64%)]\tLoss: 0.012206\n",
      "Training stage for Flod 0 Epoch: 83 [22400/37476                 (75%)]\tLoss: 0.022084\n",
      "Training stage for Flod 0 Epoch: 83 [25600/37476                 (85%)]\tLoss: 0.007369\n",
      "Training stage for Flod 0 Epoch: 83 [28800/37476                 (96%)]\tLoss: 0.027910\n",
      "Test set for fold0: Average Loss:           1.7242, Accuracy: 14781/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 84 [0/37476                 (0%)]\tLoss: 0.040357\n",
      "Training stage for Flod 0 Epoch: 84 [3200/37476                 (11%)]\tLoss: 0.056129\n",
      "Training stage for Flod 0 Epoch: 84 [6400/37476                 (21%)]\tLoss: 0.000160\n",
      "Training stage for Flod 0 Epoch: 84 [9600/37476                 (32%)]\tLoss: 0.005075\n",
      "Training stage for Flod 0 Epoch: 84 [12800/37476                 (43%)]\tLoss: 0.011021\n",
      "Training stage for Flod 0 Epoch: 84 [16000/37476                 (53%)]\tLoss: 0.035982\n",
      "Training stage for Flod 0 Epoch: 84 [19200/37476                 (64%)]\tLoss: 0.124678\n",
      "Training stage for Flod 0 Epoch: 84 [22400/37476                 (75%)]\tLoss: 0.031441\n",
      "Training stage for Flod 0 Epoch: 84 [25600/37476                 (85%)]\tLoss: 0.010251\n",
      "Training stage for Flod 0 Epoch: 84 [28800/37476                 (96%)]\tLoss: 0.002543\n",
      "Test set for fold0: Average Loss:           1.6121, Accuracy: 14802/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 85 [0/37476                 (0%)]\tLoss: 0.009952\n",
      "Training stage for Flod 0 Epoch: 85 [3200/37476                 (11%)]\tLoss: 0.002786\n",
      "Training stage for Flod 0 Epoch: 85 [6400/37476                 (21%)]\tLoss: 0.003503\n",
      "Training stage for Flod 0 Epoch: 85 [9600/37476                 (32%)]\tLoss: 0.003502\n",
      "Training stage for Flod 0 Epoch: 85 [12800/37476                 (43%)]\tLoss: 0.000308\n",
      "Training stage for Flod 0 Epoch: 85 [16000/37476                 (53%)]\tLoss: 0.000049\n",
      "Training stage for Flod 0 Epoch: 85 [19200/37476                 (64%)]\tLoss: 0.098468\n",
      "Training stage for Flod 0 Epoch: 85 [22400/37476                 (75%)]\tLoss: 0.090455\n",
      "Training stage for Flod 0 Epoch: 85 [25600/37476                 (85%)]\tLoss: 0.023383\n",
      "Training stage for Flod 0 Epoch: 85 [28800/37476                 (96%)]\tLoss: 0.003839\n",
      "Test set for fold0: Average Loss:           1.8643, Accuracy: 14792/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 86 [0/37476                 (0%)]\tLoss: 0.014798\n",
      "Training stage for Flod 0 Epoch: 86 [3200/37476                 (11%)]\tLoss: 0.001057\n",
      "Training stage for Flod 0 Epoch: 86 [6400/37476                 (21%)]\tLoss: 0.018581\n",
      "Training stage for Flod 0 Epoch: 86 [9600/37476                 (32%)]\tLoss: 0.060359\n",
      "Training stage for Flod 0 Epoch: 86 [12800/37476                 (43%)]\tLoss: 0.001265\n",
      "Training stage for Flod 0 Epoch: 86 [16000/37476                 (53%)]\tLoss: 0.012674\n",
      "Training stage for Flod 0 Epoch: 86 [19200/37476                 (64%)]\tLoss: 0.006972\n",
      "Training stage for Flod 0 Epoch: 86 [22400/37476                 (75%)]\tLoss: 0.002195\n",
      "Training stage for Flod 0 Epoch: 86 [25600/37476                 (85%)]\tLoss: 0.004584\n",
      "Training stage for Flod 0 Epoch: 86 [28800/37476                 (96%)]\tLoss: 0.021234\n",
      "Test set for fold0: Average Loss:           1.8621, Accuracy: 14782/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 87 [0/37476                 (0%)]\tLoss: 0.049299\n",
      "Training stage for Flod 0 Epoch: 87 [3200/37476                 (11%)]\tLoss: 0.012539\n",
      "Training stage for Flod 0 Epoch: 87 [6400/37476                 (21%)]\tLoss: 0.021660\n",
      "Training stage for Flod 0 Epoch: 87 [9600/37476                 (32%)]\tLoss: 0.000095\n",
      "Training stage for Flod 0 Epoch: 87 [12800/37476                 (43%)]\tLoss: 0.000591\n",
      "Training stage for Flod 0 Epoch: 87 [16000/37476                 (53%)]\tLoss: 0.020437\n",
      "Training stage for Flod 0 Epoch: 87 [19200/37476                 (64%)]\tLoss: 0.005946\n",
      "Training stage for Flod 0 Epoch: 87 [22400/37476                 (75%)]\tLoss: 0.009657\n",
      "Training stage for Flod 0 Epoch: 87 [25600/37476                 (85%)]\tLoss: 0.122665\n",
      "Training stage for Flod 0 Epoch: 87 [28800/37476                 (96%)]\tLoss: 0.002456\n",
      "Test set for fold0: Average Loss:           1.6307, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 88 [0/37476                 (0%)]\tLoss: 0.014496\n",
      "Training stage for Flod 0 Epoch: 88 [3200/37476                 (11%)]\tLoss: 0.059531\n",
      "Training stage for Flod 0 Epoch: 88 [6400/37476                 (21%)]\tLoss: 0.009586\n",
      "Training stage for Flod 0 Epoch: 88 [9600/37476                 (32%)]\tLoss: 0.026586\n",
      "Training stage for Flod 0 Epoch: 88 [12800/37476                 (43%)]\tLoss: 0.001543\n",
      "Training stage for Flod 0 Epoch: 88 [16000/37476                 (53%)]\tLoss: 0.010045\n",
      "Training stage for Flod 0 Epoch: 88 [19200/37476                 (64%)]\tLoss: 0.008994\n",
      "Training stage for Flod 0 Epoch: 88 [22400/37476                 (75%)]\tLoss: 0.052683\n",
      "Training stage for Flod 0 Epoch: 88 [25600/37476                 (85%)]\tLoss: 0.012484\n",
      "Training stage for Flod 0 Epoch: 88 [28800/37476                 (96%)]\tLoss: 0.006080\n",
      "Test set for fold0: Average Loss:           1.2645, Accuracy: 14793/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 89 [0/37476                 (0%)]\tLoss: 0.004179\n",
      "Training stage for Flod 0 Epoch: 89 [3200/37476                 (11%)]\tLoss: 0.009449\n",
      "Training stage for Flod 0 Epoch: 89 [6400/37476                 (21%)]\tLoss: 0.002131\n",
      "Training stage for Flod 0 Epoch: 89 [9600/37476                 (32%)]\tLoss: 0.001034\n",
      "Training stage for Flod 0 Epoch: 89 [12800/37476                 (43%)]\tLoss: 0.000348\n",
      "Training stage for Flod 0 Epoch: 89 [16000/37476                 (53%)]\tLoss: 0.020297\n",
      "Training stage for Flod 0 Epoch: 89 [19200/37476                 (64%)]\tLoss: 0.000675\n",
      "Training stage for Flod 0 Epoch: 89 [22400/37476                 (75%)]\tLoss: 0.036337\n",
      "Training stage for Flod 0 Epoch: 89 [25600/37476                 (85%)]\tLoss: 0.030510\n",
      "Training stage for Flod 0 Epoch: 89 [28800/37476                 (96%)]\tLoss: 0.123772\n",
      "Test set for fold0: Average Loss:           1.6705, Accuracy: 14805/37476           (40%)\n",
      "Training stage for Flod 0 Epoch: 90 [0/37476                 (0%)]\tLoss: 0.000360\n",
      "Training stage for Flod 0 Epoch: 90 [3200/37476                 (11%)]\tLoss: 0.003835\n",
      "Training stage for Flod 0 Epoch: 90 [6400/37476                 (21%)]\tLoss: 0.021023\n",
      "Training stage for Flod 0 Epoch: 90 [9600/37476                 (32%)]\tLoss: 0.002733\n",
      "Training stage for Flod 0 Epoch: 90 [12800/37476                 (43%)]\tLoss: 0.005023\n",
      "Training stage for Flod 0 Epoch: 90 [16000/37476                 (53%)]\tLoss: 0.222964\n",
      "Training stage for Flod 0 Epoch: 90 [19200/37476                 (64%)]\tLoss: 0.023140\n",
      "Training stage for Flod 0 Epoch: 90 [22400/37476                 (75%)]\tLoss: 0.016816\n",
      "Training stage for Flod 0 Epoch: 90 [25600/37476                 (85%)]\tLoss: 0.003634\n",
      "Training stage for Flod 0 Epoch: 90 [28800/37476                 (96%)]\tLoss: 0.017663\n",
      "Test set for fold0: Average Loss:           1.7759, Accuracy: 14745/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 91 [0/37476                 (0%)]\tLoss: 0.000121\n",
      "Training stage for Flod 0 Epoch: 91 [3200/37476                 (11%)]\tLoss: 0.007372\n",
      "Training stage for Flod 0 Epoch: 91 [6400/37476                 (21%)]\tLoss: 0.003294\n",
      "Training stage for Flod 0 Epoch: 91 [9600/37476                 (32%)]\tLoss: 0.004473\n",
      "Training stage for Flod 0 Epoch: 91 [12800/37476                 (43%)]\tLoss: 0.004925\n",
      "Training stage for Flod 0 Epoch: 91 [16000/37476                 (53%)]\tLoss: 0.003363\n",
      "Training stage for Flod 0 Epoch: 91 [19200/37476                 (64%)]\tLoss: 0.001863\n",
      "Training stage for Flod 0 Epoch: 91 [22400/37476                 (75%)]\tLoss: 0.018566\n",
      "Training stage for Flod 0 Epoch: 91 [25600/37476                 (85%)]\tLoss: 0.111668\n",
      "Training stage for Flod 0 Epoch: 91 [28800/37476                 (96%)]\tLoss: 0.000157\n",
      "Test set for fold0: Average Loss:           1.4960, Accuracy: 14802/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 92 [0/37476                 (0%)]\tLoss: 0.109342\n",
      "Training stage for Flod 0 Epoch: 92 [3200/37476                 (11%)]\tLoss: 0.028383\n",
      "Training stage for Flod 0 Epoch: 92 [6400/37476                 (21%)]\tLoss: 0.001755\n",
      "Training stage for Flod 0 Epoch: 92 [9600/37476                 (32%)]\tLoss: 0.026854\n",
      "Training stage for Flod 0 Epoch: 92 [12800/37476                 (43%)]\tLoss: 0.047351\n",
      "Training stage for Flod 0 Epoch: 92 [16000/37476                 (53%)]\tLoss: 0.001412\n",
      "Training stage for Flod 0 Epoch: 92 [19200/37476                 (64%)]\tLoss: 0.005663\n",
      "Training stage for Flod 0 Epoch: 92 [22400/37476                 (75%)]\tLoss: 0.013418\n",
      "Training stage for Flod 0 Epoch: 92 [25600/37476                 (85%)]\tLoss: 0.024698\n",
      "Training stage for Flod 0 Epoch: 92 [28800/37476                 (96%)]\tLoss: 0.080615\n",
      "Test set for fold0: Average Loss:           1.8057, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 93 [0/37476                 (0%)]\tLoss: 0.001370\n",
      "Training stage for Flod 0 Epoch: 93 [3200/37476                 (11%)]\tLoss: 0.000993\n",
      "Training stage for Flod 0 Epoch: 93 [6400/37476                 (21%)]\tLoss: 0.000068\n",
      "Training stage for Flod 0 Epoch: 93 [9600/37476                 (32%)]\tLoss: 0.040061\n",
      "Training stage for Flod 0 Epoch: 93 [12800/37476                 (43%)]\tLoss: 0.002678\n",
      "Training stage for Flod 0 Epoch: 93 [16000/37476                 (53%)]\tLoss: 0.003220\n",
      "Training stage for Flod 0 Epoch: 93 [19200/37476                 (64%)]\tLoss: 0.002754\n",
      "Training stage for Flod 0 Epoch: 93 [22400/37476                 (75%)]\tLoss: 0.011887\n",
      "Training stage for Flod 0 Epoch: 93 [25600/37476                 (85%)]\tLoss: 0.074242\n",
      "Training stage for Flod 0 Epoch: 93 [28800/37476                 (96%)]\tLoss: 0.021497\n",
      "Test set for fold0: Average Loss:           1.6905, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 94 [0/37476                 (0%)]\tLoss: 0.010884\n",
      "Training stage for Flod 0 Epoch: 94 [3200/37476                 (11%)]\tLoss: 0.016311\n",
      "Training stage for Flod 0 Epoch: 94 [6400/37476                 (21%)]\tLoss: 0.072233\n",
      "Training stage for Flod 0 Epoch: 94 [9600/37476                 (32%)]\tLoss: 0.096444\n",
      "Training stage for Flod 0 Epoch: 94 [12800/37476                 (43%)]\tLoss: 0.027610\n",
      "Training stage for Flod 0 Epoch: 94 [16000/37476                 (53%)]\tLoss: 0.006840\n",
      "Training stage for Flod 0 Epoch: 94 [19200/37476                 (64%)]\tLoss: 0.019296\n",
      "Training stage for Flod 0 Epoch: 94 [22400/37476                 (75%)]\tLoss: 0.005459\n",
      "Training stage for Flod 0 Epoch: 94 [25600/37476                 (85%)]\tLoss: 0.001191\n",
      "Training stage for Flod 0 Epoch: 94 [28800/37476                 (96%)]\tLoss: 0.027362\n",
      "Test set for fold0: Average Loss:           1.7209, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 95 [0/37476                 (0%)]\tLoss: 0.005571\n",
      "Training stage for Flod 0 Epoch: 95 [3200/37476                 (11%)]\tLoss: 0.000929\n",
      "Training stage for Flod 0 Epoch: 95 [6400/37476                 (21%)]\tLoss: 0.001992\n",
      "Training stage for Flod 0 Epoch: 95 [9600/37476                 (32%)]\tLoss: 0.001535\n",
      "Training stage for Flod 0 Epoch: 95 [12800/37476                 (43%)]\tLoss: 0.011484\n",
      "Training stage for Flod 0 Epoch: 95 [16000/37476                 (53%)]\tLoss: 0.008071\n",
      "Training stage for Flod 0 Epoch: 95 [19200/37476                 (64%)]\tLoss: 0.033467\n",
      "Training stage for Flod 0 Epoch: 95 [22400/37476                 (75%)]\tLoss: 0.005593\n",
      "Training stage for Flod 0 Epoch: 95 [25600/37476                 (85%)]\tLoss: 0.054663\n",
      "Training stage for Flod 0 Epoch: 95 [28800/37476                 (96%)]\tLoss: 0.069782\n",
      "Test set for fold0: Average Loss:           1.9078, Accuracy: 14731/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 96 [0/37476                 (0%)]\tLoss: 0.013723\n",
      "Training stage for Flod 0 Epoch: 96 [3200/37476                 (11%)]\tLoss: 0.035769\n",
      "Training stage for Flod 0 Epoch: 96 [6400/37476                 (21%)]\tLoss: 0.048508\n",
      "Training stage for Flod 0 Epoch: 96 [9600/37476                 (32%)]\tLoss: 0.009652\n",
      "Training stage for Flod 0 Epoch: 96 [12800/37476                 (43%)]\tLoss: 0.015071\n",
      "Training stage for Flod 0 Epoch: 96 [16000/37476                 (53%)]\tLoss: 0.034501\n",
      "Training stage for Flod 0 Epoch: 96 [19200/37476                 (64%)]\tLoss: 0.040700\n",
      "Training stage for Flod 0 Epoch: 96 [22400/37476                 (75%)]\tLoss: 0.025124\n",
      "Training stage for Flod 0 Epoch: 96 [25600/37476                 (85%)]\tLoss: 0.014076\n",
      "Training stage for Flod 0 Epoch: 96 [28800/37476                 (96%)]\tLoss: 0.021686\n",
      "Test set for fold0: Average Loss:           1.7022, Accuracy: 14806/37476           (40%)\n",
      "Training stage for Flod 0 Epoch: 97 [0/37476                 (0%)]\tLoss: 0.012643\n",
      "Training stage for Flod 0 Epoch: 97 [3200/37476                 (11%)]\tLoss: 0.020618\n",
      "Training stage for Flod 0 Epoch: 97 [6400/37476                 (21%)]\tLoss: 0.005556\n",
      "Training stage for Flod 0 Epoch: 97 [9600/37476                 (32%)]\tLoss: 0.007383\n",
      "Training stage for Flod 0 Epoch: 97 [12800/37476                 (43%)]\tLoss: 0.002019\n",
      "Training stage for Flod 0 Epoch: 97 [16000/37476                 (53%)]\tLoss: 0.010813\n",
      "Training stage for Flod 0 Epoch: 97 [19200/37476                 (64%)]\tLoss: 0.097004\n",
      "Training stage for Flod 0 Epoch: 97 [22400/37476                 (75%)]\tLoss: 0.021737\n",
      "Training stage for Flod 0 Epoch: 97 [25600/37476                 (85%)]\tLoss: 0.010996\n",
      "Training stage for Flod 0 Epoch: 97 [28800/37476                 (96%)]\tLoss: 0.112820\n",
      "Test set for fold0: Average Loss:           1.6067, Accuracy: 14777/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 98 [0/37476                 (0%)]\tLoss: 0.000572\n",
      "Training stage for Flod 0 Epoch: 98 [3200/37476                 (11%)]\tLoss: 0.047459\n",
      "Training stage for Flod 0 Epoch: 98 [6400/37476                 (21%)]\tLoss: 0.045419\n",
      "Training stage for Flod 0 Epoch: 98 [9600/37476                 (32%)]\tLoss: 0.032745\n",
      "Training stage for Flod 0 Epoch: 98 [12800/37476                 (43%)]\tLoss: 0.018696\n",
      "Training stage for Flod 0 Epoch: 98 [16000/37476                 (53%)]\tLoss: 0.010166\n",
      "Training stage for Flod 0 Epoch: 98 [19200/37476                 (64%)]\tLoss: 0.000853\n",
      "Training stage for Flod 0 Epoch: 98 [22400/37476                 (75%)]\tLoss: 0.041388\n",
      "Training stage for Flod 0 Epoch: 98 [25600/37476                 (85%)]\tLoss: 0.004535\n",
      "Training stage for Flod 0 Epoch: 98 [28800/37476                 (96%)]\tLoss: 0.006484\n",
      "Test set for fold0: Average Loss:           1.8854, Accuracy: 14792/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 99 [0/37476                 (0%)]\tLoss: 0.001968\n",
      "Training stage for Flod 0 Epoch: 99 [3200/37476                 (11%)]\tLoss: 0.003199\n",
      "Training stage for Flod 0 Epoch: 99 [6400/37476                 (21%)]\tLoss: 0.025806\n",
      "Training stage for Flod 0 Epoch: 99 [9600/37476                 (32%)]\tLoss: 0.046054\n",
      "Training stage for Flod 0 Epoch: 99 [12800/37476                 (43%)]\tLoss: 0.011710\n",
      "Training stage for Flod 0 Epoch: 99 [16000/37476                 (53%)]\tLoss: 0.020308\n",
      "Training stage for Flod 0 Epoch: 99 [19200/37476                 (64%)]\tLoss: 0.031946\n",
      "Training stage for Flod 0 Epoch: 99 [22400/37476                 (75%)]\tLoss: 0.070275\n",
      "Training stage for Flod 0 Epoch: 99 [25600/37476                 (85%)]\tLoss: 0.004907\n",
      "Training stage for Flod 0 Epoch: 99 [28800/37476                 (96%)]\tLoss: 0.015333\n",
      "Test set for fold0: Average Loss:           1.7769, Accuracy: 14792/37476           (39%)\n",
      "Training stage for Flod 0 Epoch: 100 [0/37476                 (0%)]\tLoss: 0.003251\n",
      "Training stage for Flod 0 Epoch: 100 [3200/37476                 (11%)]\tLoss: 0.010349\n",
      "Training stage for Flod 0 Epoch: 100 [6400/37476                 (21%)]\tLoss: 0.000226\n",
      "Training stage for Flod 0 Epoch: 100 [9600/37476                 (32%)]\tLoss: 0.011282\n",
      "Training stage for Flod 0 Epoch: 100 [12800/37476                 (43%)]\tLoss: 0.007195\n",
      "Training stage for Flod 0 Epoch: 100 [16000/37476                 (53%)]\tLoss: 0.026803\n",
      "Training stage for Flod 0 Epoch: 100 [19200/37476                 (64%)]\tLoss: 0.015811\n",
      "Training stage for Flod 0 Epoch: 100 [22400/37476                 (75%)]\tLoss: 0.003133\n",
      "Training stage for Flod 0 Epoch: 100 [25600/37476                 (85%)]\tLoss: 0.038159\n",
      "Training stage for Flod 0 Epoch: 100 [28800/37476                 (96%)]\tLoss: 0.003299\n",
      "Test set for fold0: Average Loss:           1.5715, Accuracy: 14785/37476           (39%)\n",
      "-------------------Fold 1-------------------\n",
      "Training stage for Flod 1 Epoch: 1 [0/37476                 (0%)]\tLoss: 0.548238\n",
      "Training stage for Flod 1 Epoch: 1 [3200/37476                 (11%)]\tLoss: 0.119609\n",
      "Training stage for Flod 1 Epoch: 1 [6400/37476                 (21%)]\tLoss: 0.021346\n",
      "Training stage for Flod 1 Epoch: 1 [9600/37476                 (32%)]\tLoss: 0.101239\n",
      "Training stage for Flod 1 Epoch: 1 [12800/37476                 (43%)]\tLoss: 0.014840\n",
      "Training stage for Flod 1 Epoch: 1 [16000/37476                 (53%)]\tLoss: 0.157256\n",
      "Training stage for Flod 1 Epoch: 1 [19200/37476                 (64%)]\tLoss: 0.375461\n",
      "Training stage for Flod 1 Epoch: 1 [22400/37476                 (75%)]\tLoss: 0.061094\n",
      "Training stage for Flod 1 Epoch: 1 [25600/37476                 (85%)]\tLoss: 0.011735\n",
      "Training stage for Flod 1 Epoch: 1 [28800/37476                 (96%)]\tLoss: 0.146204\n",
      "Test set for fold1: Average Loss:           0.7969, Accuracy: 14588/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 2 [0/37476                 (0%)]\tLoss: 0.072155\n",
      "Training stage for Flod 1 Epoch: 2 [3200/37476                 (11%)]\tLoss: 0.009925\n",
      "Training stage for Flod 1 Epoch: 2 [6400/37476                 (21%)]\tLoss: 0.007527\n",
      "Training stage for Flod 1 Epoch: 2 [9600/37476                 (32%)]\tLoss: 0.120858\n",
      "Training stage for Flod 1 Epoch: 2 [12800/37476                 (43%)]\tLoss: 0.144727\n",
      "Training stage for Flod 1 Epoch: 2 [16000/37476                 (53%)]\tLoss: 0.040275\n",
      "Training stage for Flod 1 Epoch: 2 [19200/37476                 (64%)]\tLoss: 0.199544\n",
      "Training stage for Flod 1 Epoch: 2 [22400/37476                 (75%)]\tLoss: 0.030161\n",
      "Training stage for Flod 1 Epoch: 2 [25600/37476                 (85%)]\tLoss: 0.256491\n",
      "Training stage for Flod 1 Epoch: 2 [28800/37476                 (96%)]\tLoss: 0.032789\n",
      "Test set for fold1: Average Loss:           0.8698, Accuracy: 14590/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 3 [0/37476                 (0%)]\tLoss: 0.062932\n",
      "Training stage for Flod 1 Epoch: 3 [3200/37476                 (11%)]\tLoss: 0.038113\n",
      "Training stage for Flod 1 Epoch: 3 [6400/37476                 (21%)]\tLoss: 0.052788\n",
      "Training stage for Flod 1 Epoch: 3 [9600/37476                 (32%)]\tLoss: 0.199989\n",
      "Training stage for Flod 1 Epoch: 3 [12800/37476                 (43%)]\tLoss: 0.074984\n",
      "Training stage for Flod 1 Epoch: 3 [16000/37476                 (53%)]\tLoss: 0.009498\n",
      "Training stage for Flod 1 Epoch: 3 [19200/37476                 (64%)]\tLoss: 0.031142\n",
      "Training stage for Flod 1 Epoch: 3 [22400/37476                 (75%)]\tLoss: 0.010218\n",
      "Training stage for Flod 1 Epoch: 3 [25600/37476                 (85%)]\tLoss: 0.001166\n",
      "Training stage for Flod 1 Epoch: 3 [28800/37476                 (96%)]\tLoss: 0.199620\n",
      "Test set for fold1: Average Loss:           0.8003, Accuracy: 14606/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 4 [0/37476                 (0%)]\tLoss: 0.005566\n",
      "Training stage for Flod 1 Epoch: 4 [3200/37476                 (11%)]\tLoss: 0.055336\n",
      "Training stage for Flod 1 Epoch: 4 [6400/37476                 (21%)]\tLoss: 0.041995\n",
      "Training stage for Flod 1 Epoch: 4 [9600/37476                 (32%)]\tLoss: 0.113949\n",
      "Training stage for Flod 1 Epoch: 4 [12800/37476                 (43%)]\tLoss: 0.001908\n",
      "Training stage for Flod 1 Epoch: 4 [16000/37476                 (53%)]\tLoss: 0.061631\n",
      "Training stage for Flod 1 Epoch: 4 [19200/37476                 (64%)]\tLoss: 0.079789\n",
      "Training stage for Flod 1 Epoch: 4 [22400/37476                 (75%)]\tLoss: 0.014285\n",
      "Training stage for Flod 1 Epoch: 4 [25600/37476                 (85%)]\tLoss: 0.276708\n",
      "Training stage for Flod 1 Epoch: 4 [28800/37476                 (96%)]\tLoss: 0.044102\n",
      "Test set for fold1: Average Loss:           0.9765, Accuracy: 14679/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 5 [0/37476                 (0%)]\tLoss: 0.011339\n",
      "Training stage for Flod 1 Epoch: 5 [3200/37476                 (11%)]\tLoss: 0.083869\n",
      "Training stage for Flod 1 Epoch: 5 [6400/37476                 (21%)]\tLoss: 0.053113\n",
      "Training stage for Flod 1 Epoch: 5 [9600/37476                 (32%)]\tLoss: 0.050727\n",
      "Training stage for Flod 1 Epoch: 5 [12800/37476                 (43%)]\tLoss: 0.027749\n",
      "Training stage for Flod 1 Epoch: 5 [16000/37476                 (53%)]\tLoss: 0.069974\n",
      "Training stage for Flod 1 Epoch: 5 [19200/37476                 (64%)]\tLoss: 0.066750\n",
      "Training stage for Flod 1 Epoch: 5 [22400/37476                 (75%)]\tLoss: 0.016424\n",
      "Training stage for Flod 1 Epoch: 5 [25600/37476                 (85%)]\tLoss: 0.081259\n",
      "Training stage for Flod 1 Epoch: 5 [28800/37476                 (96%)]\tLoss: 0.007916\n",
      "Test set for fold1: Average Loss:           0.9251, Accuracy: 14686/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 6 [0/37476                 (0%)]\tLoss: 0.040418\n",
      "Training stage for Flod 1 Epoch: 6 [3200/37476                 (11%)]\tLoss: 0.003765\n",
      "Training stage for Flod 1 Epoch: 6 [6400/37476                 (21%)]\tLoss: 0.053063\n",
      "Training stage for Flod 1 Epoch: 6 [9600/37476                 (32%)]\tLoss: 0.002514\n",
      "Training stage for Flod 1 Epoch: 6 [12800/37476                 (43%)]\tLoss: 0.039731\n",
      "Training stage for Flod 1 Epoch: 6 [16000/37476                 (53%)]\tLoss: 0.035783\n",
      "Training stage for Flod 1 Epoch: 6 [19200/37476                 (64%)]\tLoss: 0.043155\n",
      "Training stage for Flod 1 Epoch: 6 [22400/37476                 (75%)]\tLoss: 0.003028\n",
      "Training stage for Flod 1 Epoch: 6 [25600/37476                 (85%)]\tLoss: 0.111219\n",
      "Training stage for Flod 1 Epoch: 6 [28800/37476                 (96%)]\tLoss: 0.113001\n",
      "Test set for fold1: Average Loss:           0.9955, Accuracy: 14517/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 7 [0/37476                 (0%)]\tLoss: 0.135825\n",
      "Training stage for Flod 1 Epoch: 7 [3200/37476                 (11%)]\tLoss: 0.004684\n",
      "Training stage for Flod 1 Epoch: 7 [6400/37476                 (21%)]\tLoss: 0.010681\n",
      "Training stage for Flod 1 Epoch: 7 [9600/37476                 (32%)]\tLoss: 0.000973\n",
      "Training stage for Flod 1 Epoch: 7 [12800/37476                 (43%)]\tLoss: 0.115901\n",
      "Training stage for Flod 1 Epoch: 7 [16000/37476                 (53%)]\tLoss: 0.025980\n",
      "Training stage for Flod 1 Epoch: 7 [19200/37476                 (64%)]\tLoss: 0.113688\n",
      "Training stage for Flod 1 Epoch: 7 [22400/37476                 (75%)]\tLoss: 0.000219\n",
      "Training stage for Flod 1 Epoch: 7 [25600/37476                 (85%)]\tLoss: 0.008990\n",
      "Training stage for Flod 1 Epoch: 7 [28800/37476                 (96%)]\tLoss: 0.029047\n",
      "Test set for fold1: Average Loss:           0.9826, Accuracy: 14613/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 8 [0/37476                 (0%)]\tLoss: 0.381566\n",
      "Training stage for Flod 1 Epoch: 8 [3200/37476                 (11%)]\tLoss: 0.131475\n",
      "Training stage for Flod 1 Epoch: 8 [6400/37476                 (21%)]\tLoss: 0.013806\n",
      "Training stage for Flod 1 Epoch: 8 [9600/37476                 (32%)]\tLoss: 0.000345\n",
      "Training stage for Flod 1 Epoch: 8 [12800/37476                 (43%)]\tLoss: 0.016734\n",
      "Training stage for Flod 1 Epoch: 8 [16000/37476                 (53%)]\tLoss: 0.000839\n",
      "Training stage for Flod 1 Epoch: 8 [19200/37476                 (64%)]\tLoss: 0.050647\n",
      "Training stage for Flod 1 Epoch: 8 [22400/37476                 (75%)]\tLoss: 0.032535\n",
      "Training stage for Flod 1 Epoch: 8 [25600/37476                 (85%)]\tLoss: 0.062634\n",
      "Training stage for Flod 1 Epoch: 8 [28800/37476                 (96%)]\tLoss: 0.049778\n",
      "Test set for fold1: Average Loss:           0.9578, Accuracy: 14717/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 9 [0/37476                 (0%)]\tLoss: 0.054179\n",
      "Training stage for Flod 1 Epoch: 9 [3200/37476                 (11%)]\tLoss: 0.101090\n",
      "Training stage for Flod 1 Epoch: 9 [6400/37476                 (21%)]\tLoss: 0.035830\n",
      "Training stage for Flod 1 Epoch: 9 [9600/37476                 (32%)]\tLoss: 0.021168\n",
      "Training stage for Flod 1 Epoch: 9 [12800/37476                 (43%)]\tLoss: 0.002329\n",
      "Training stage for Flod 1 Epoch: 9 [16000/37476                 (53%)]\tLoss: 0.093953\n",
      "Training stage for Flod 1 Epoch: 9 [19200/37476                 (64%)]\tLoss: 0.019491\n",
      "Training stage for Flod 1 Epoch: 9 [22400/37476                 (75%)]\tLoss: 0.048335\n",
      "Training stage for Flod 1 Epoch: 9 [25600/37476                 (85%)]\tLoss: 0.131511\n",
      "Training stage for Flod 1 Epoch: 9 [28800/37476                 (96%)]\tLoss: 0.012291\n",
      "Test set for fold1: Average Loss:           1.0788, Accuracy: 14709/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 10 [0/37476                 (0%)]\tLoss: 0.004302\n",
      "Training stage for Flod 1 Epoch: 10 [3200/37476                 (11%)]\tLoss: 0.007178\n",
      "Training stage for Flod 1 Epoch: 10 [6400/37476                 (21%)]\tLoss: 0.014767\n",
      "Training stage for Flod 1 Epoch: 10 [9600/37476                 (32%)]\tLoss: 0.017894\n",
      "Training stage for Flod 1 Epoch: 10 [12800/37476                 (43%)]\tLoss: 0.099333\n",
      "Training stage for Flod 1 Epoch: 10 [16000/37476                 (53%)]\tLoss: 0.002437\n",
      "Training stage for Flod 1 Epoch: 10 [19200/37476                 (64%)]\tLoss: 0.260056\n",
      "Training stage for Flod 1 Epoch: 10 [22400/37476                 (75%)]\tLoss: 0.041054\n",
      "Training stage for Flod 1 Epoch: 10 [25600/37476                 (85%)]\tLoss: 0.083402\n",
      "Training stage for Flod 1 Epoch: 10 [28800/37476                 (96%)]\tLoss: 0.027559\n",
      "Test set for fold1: Average Loss:           1.0236, Accuracy: 14662/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 11 [0/37476                 (0%)]\tLoss: 0.179687\n",
      "Training stage for Flod 1 Epoch: 11 [3200/37476                 (11%)]\tLoss: 0.068720\n",
      "Training stage for Flod 1 Epoch: 11 [6400/37476                 (21%)]\tLoss: 0.027175\n",
      "Training stage for Flod 1 Epoch: 11 [9600/37476                 (32%)]\tLoss: 0.011279\n",
      "Training stage for Flod 1 Epoch: 11 [12800/37476                 (43%)]\tLoss: 0.045510\n",
      "Training stage for Flod 1 Epoch: 11 [16000/37476                 (53%)]\tLoss: 0.066050\n",
      "Training stage for Flod 1 Epoch: 11 [19200/37476                 (64%)]\tLoss: 0.000907\n",
      "Training stage for Flod 1 Epoch: 11 [22400/37476                 (75%)]\tLoss: 0.060110\n",
      "Training stage for Flod 1 Epoch: 11 [25600/37476                 (85%)]\tLoss: 0.000244\n",
      "Training stage for Flod 1 Epoch: 11 [28800/37476                 (96%)]\tLoss: 0.020418\n",
      "Test set for fold1: Average Loss:           1.1235, Accuracy: 14687/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 12 [0/37476                 (0%)]\tLoss: 0.175609\n",
      "Training stage for Flod 1 Epoch: 12 [3200/37476                 (11%)]\tLoss: 0.012928\n",
      "Training stage for Flod 1 Epoch: 12 [6400/37476                 (21%)]\tLoss: 0.106815\n",
      "Training stage for Flod 1 Epoch: 12 [9600/37476                 (32%)]\tLoss: 0.053455\n",
      "Training stage for Flod 1 Epoch: 12 [12800/37476                 (43%)]\tLoss: 0.020093\n",
      "Training stage for Flod 1 Epoch: 12 [16000/37476                 (53%)]\tLoss: 0.007211\n",
      "Training stage for Flod 1 Epoch: 12 [19200/37476                 (64%)]\tLoss: 0.030860\n",
      "Training stage for Flod 1 Epoch: 12 [22400/37476                 (75%)]\tLoss: 0.009244\n",
      "Training stage for Flod 1 Epoch: 12 [25600/37476                 (85%)]\tLoss: 0.067236\n",
      "Training stage for Flod 1 Epoch: 12 [28800/37476                 (96%)]\tLoss: 0.020209\n",
      "Test set for fold1: Average Loss:           1.0828, Accuracy: 14640/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 13 [0/37476                 (0%)]\tLoss: 0.075871\n",
      "Training stage for Flod 1 Epoch: 13 [3200/37476                 (11%)]\tLoss: 0.007402\n",
      "Training stage for Flod 1 Epoch: 13 [6400/37476                 (21%)]\tLoss: 0.157787\n",
      "Training stage for Flod 1 Epoch: 13 [9600/37476                 (32%)]\tLoss: 0.073801\n",
      "Training stage for Flod 1 Epoch: 13 [12800/37476                 (43%)]\tLoss: 0.080277\n",
      "Training stage for Flod 1 Epoch: 13 [16000/37476                 (53%)]\tLoss: 0.003231\n",
      "Training stage for Flod 1 Epoch: 13 [19200/37476                 (64%)]\tLoss: 0.052767\n",
      "Training stage for Flod 1 Epoch: 13 [22400/37476                 (75%)]\tLoss: 0.014616\n",
      "Training stage for Flod 1 Epoch: 13 [25600/37476                 (85%)]\tLoss: 0.067617\n",
      "Training stage for Flod 1 Epoch: 13 [28800/37476                 (96%)]\tLoss: 0.044293\n",
      "Test set for fold1: Average Loss:           1.1057, Accuracy: 14683/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 14 [0/37476                 (0%)]\tLoss: 0.168081\n",
      "Training stage for Flod 1 Epoch: 14 [3200/37476                 (11%)]\tLoss: 0.155318\n",
      "Training stage for Flod 1 Epoch: 14 [6400/37476                 (21%)]\tLoss: 0.008138\n",
      "Training stage for Flod 1 Epoch: 14 [9600/37476                 (32%)]\tLoss: 0.000640\n",
      "Training stage for Flod 1 Epoch: 14 [12800/37476                 (43%)]\tLoss: 0.027848\n",
      "Training stage for Flod 1 Epoch: 14 [16000/37476                 (53%)]\tLoss: 0.025084\n",
      "Training stage for Flod 1 Epoch: 14 [19200/37476                 (64%)]\tLoss: 0.006067\n",
      "Training stage for Flod 1 Epoch: 14 [22400/37476                 (75%)]\tLoss: 0.105798\n",
      "Training stage for Flod 1 Epoch: 14 [25600/37476                 (85%)]\tLoss: 0.161524\n",
      "Training stage for Flod 1 Epoch: 14 [28800/37476                 (96%)]\tLoss: 0.021002\n",
      "Test set for fold1: Average Loss:           1.1324, Accuracy: 14675/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 15 [0/37476                 (0%)]\tLoss: 0.001613\n",
      "Training stage for Flod 1 Epoch: 15 [3200/37476                 (11%)]\tLoss: 0.003976\n",
      "Training stage for Flod 1 Epoch: 15 [6400/37476                 (21%)]\tLoss: 0.010712\n",
      "Training stage for Flod 1 Epoch: 15 [9600/37476                 (32%)]\tLoss: 0.039188\n",
      "Training stage for Flod 1 Epoch: 15 [12800/37476                 (43%)]\tLoss: 0.046799\n",
      "Training stage for Flod 1 Epoch: 15 [16000/37476                 (53%)]\tLoss: 0.055356\n",
      "Training stage for Flod 1 Epoch: 15 [19200/37476                 (64%)]\tLoss: 0.001784\n",
      "Training stage for Flod 1 Epoch: 15 [22400/37476                 (75%)]\tLoss: 0.028373\n",
      "Training stage for Flod 1 Epoch: 15 [25600/37476                 (85%)]\tLoss: 0.014440\n",
      "Training stage for Flod 1 Epoch: 15 [28800/37476                 (96%)]\tLoss: 0.004885\n",
      "Test set for fold1: Average Loss:           1.1791, Accuracy: 14698/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 16 [0/37476                 (0%)]\tLoss: 0.005933\n",
      "Training stage for Flod 1 Epoch: 16 [3200/37476                 (11%)]\tLoss: 0.033934\n",
      "Training stage for Flod 1 Epoch: 16 [6400/37476                 (21%)]\tLoss: 0.134203\n",
      "Training stage for Flod 1 Epoch: 16 [9600/37476                 (32%)]\tLoss: 0.030290\n",
      "Training stage for Flod 1 Epoch: 16 [12800/37476                 (43%)]\tLoss: 0.020644\n",
      "Training stage for Flod 1 Epoch: 16 [16000/37476                 (53%)]\tLoss: 0.013982\n",
      "Training stage for Flod 1 Epoch: 16 [19200/37476                 (64%)]\tLoss: 0.032757\n",
      "Training stage for Flod 1 Epoch: 16 [22400/37476                 (75%)]\tLoss: 0.048448\n",
      "Training stage for Flod 1 Epoch: 16 [25600/37476                 (85%)]\tLoss: 0.007280\n",
      "Training stage for Flod 1 Epoch: 16 [28800/37476                 (96%)]\tLoss: 0.081033\n",
      "Test set for fold1: Average Loss:           1.0724, Accuracy: 14737/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 17 [0/37476                 (0%)]\tLoss: 0.028757\n",
      "Training stage for Flod 1 Epoch: 17 [3200/37476                 (11%)]\tLoss: 0.069488\n",
      "Training stage for Flod 1 Epoch: 17 [6400/37476                 (21%)]\tLoss: 0.042492\n",
      "Training stage for Flod 1 Epoch: 17 [9600/37476                 (32%)]\tLoss: 0.019750\n",
      "Training stage for Flod 1 Epoch: 17 [12800/37476                 (43%)]\tLoss: 0.028141\n",
      "Training stage for Flod 1 Epoch: 17 [16000/37476                 (53%)]\tLoss: 0.071770\n",
      "Training stage for Flod 1 Epoch: 17 [19200/37476                 (64%)]\tLoss: 0.001149\n",
      "Training stage for Flod 1 Epoch: 17 [22400/37476                 (75%)]\tLoss: 0.068278\n",
      "Training stage for Flod 1 Epoch: 17 [25600/37476                 (85%)]\tLoss: 0.002379\n",
      "Training stage for Flod 1 Epoch: 17 [28800/37476                 (96%)]\tLoss: 0.031581\n",
      "Test set for fold1: Average Loss:           1.1386, Accuracy: 14735/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 18 [0/37476                 (0%)]\tLoss: 0.067657\n",
      "Training stage for Flod 1 Epoch: 18 [3200/37476                 (11%)]\tLoss: 0.388089\n",
      "Training stage for Flod 1 Epoch: 18 [6400/37476                 (21%)]\tLoss: 0.006683\n",
      "Training stage for Flod 1 Epoch: 18 [9600/37476                 (32%)]\tLoss: 0.078796\n",
      "Training stage for Flod 1 Epoch: 18 [12800/37476                 (43%)]\tLoss: 0.067081\n",
      "Training stage for Flod 1 Epoch: 18 [16000/37476                 (53%)]\tLoss: 0.035070\n",
      "Training stage for Flod 1 Epoch: 18 [19200/37476                 (64%)]\tLoss: 0.025482\n",
      "Training stage for Flod 1 Epoch: 18 [22400/37476                 (75%)]\tLoss: 0.014825\n",
      "Training stage for Flod 1 Epoch: 18 [25600/37476                 (85%)]\tLoss: 0.032770\n",
      "Training stage for Flod 1 Epoch: 18 [28800/37476                 (96%)]\tLoss: 0.008665\n",
      "Test set for fold1: Average Loss:           1.1250, Accuracy: 14717/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 19 [0/37476                 (0%)]\tLoss: 0.002432\n",
      "Training stage for Flod 1 Epoch: 19 [3200/37476                 (11%)]\tLoss: 0.026046\n",
      "Training stage for Flod 1 Epoch: 19 [6400/37476                 (21%)]\tLoss: 0.003036\n",
      "Training stage for Flod 1 Epoch: 19 [9600/37476                 (32%)]\tLoss: 0.069623\n",
      "Training stage for Flod 1 Epoch: 19 [12800/37476                 (43%)]\tLoss: 0.165481\n",
      "Training stage for Flod 1 Epoch: 19 [16000/37476                 (53%)]\tLoss: 0.000646\n",
      "Training stage for Flod 1 Epoch: 19 [19200/37476                 (64%)]\tLoss: 0.021672\n",
      "Training stage for Flod 1 Epoch: 19 [22400/37476                 (75%)]\tLoss: 0.033661\n",
      "Training stage for Flod 1 Epoch: 19 [25600/37476                 (85%)]\tLoss: 0.045846\n",
      "Training stage for Flod 1 Epoch: 19 [28800/37476                 (96%)]\tLoss: 0.172111\n",
      "Test set for fold1: Average Loss:           1.1772, Accuracy: 14724/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 20 [0/37476                 (0%)]\tLoss: 0.008140\n",
      "Training stage for Flod 1 Epoch: 20 [3200/37476                 (11%)]\tLoss: 0.070596\n",
      "Training stage for Flod 1 Epoch: 20 [6400/37476                 (21%)]\tLoss: 0.037089\n",
      "Training stage for Flod 1 Epoch: 20 [9600/37476                 (32%)]\tLoss: 0.018340\n",
      "Training stage for Flod 1 Epoch: 20 [12800/37476                 (43%)]\tLoss: 0.063506\n",
      "Training stage for Flod 1 Epoch: 20 [16000/37476                 (53%)]\tLoss: 0.045760\n",
      "Training stage for Flod 1 Epoch: 20 [19200/37476                 (64%)]\tLoss: 0.221006\n",
      "Training stage for Flod 1 Epoch: 20 [22400/37476                 (75%)]\tLoss: 0.015068\n",
      "Training stage for Flod 1 Epoch: 20 [25600/37476                 (85%)]\tLoss: 0.120586\n",
      "Training stage for Flod 1 Epoch: 20 [28800/37476                 (96%)]\tLoss: 0.008401\n",
      "Test set for fold1: Average Loss:           1.0780, Accuracy: 14740/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 21 [0/37476                 (0%)]\tLoss: 0.021098\n",
      "Training stage for Flod 1 Epoch: 21 [3200/37476                 (11%)]\tLoss: 0.108513\n",
      "Training stage for Flod 1 Epoch: 21 [6400/37476                 (21%)]\tLoss: 0.007700\n",
      "Training stage for Flod 1 Epoch: 21 [9600/37476                 (32%)]\tLoss: 0.000005\n",
      "Training stage for Flod 1 Epoch: 21 [12800/37476                 (43%)]\tLoss: 0.003916\n",
      "Training stage for Flod 1 Epoch: 21 [16000/37476                 (53%)]\tLoss: 0.086161\n",
      "Training stage for Flod 1 Epoch: 21 [19200/37476                 (64%)]\tLoss: 0.021875\n",
      "Training stage for Flod 1 Epoch: 21 [22400/37476                 (75%)]\tLoss: 0.069415\n",
      "Training stage for Flod 1 Epoch: 21 [25600/37476                 (85%)]\tLoss: 0.022587\n",
      "Training stage for Flod 1 Epoch: 21 [28800/37476                 (96%)]\tLoss: 0.151934\n",
      "Test set for fold1: Average Loss:           1.1238, Accuracy: 14733/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 22 [0/37476                 (0%)]\tLoss: 0.012199\n",
      "Training stage for Flod 1 Epoch: 22 [3200/37476                 (11%)]\tLoss: 0.009626\n",
      "Training stage for Flod 1 Epoch: 22 [6400/37476                 (21%)]\tLoss: 0.026663\n",
      "Training stage for Flod 1 Epoch: 22 [9600/37476                 (32%)]\tLoss: 0.003409\n",
      "Training stage for Flod 1 Epoch: 22 [12800/37476                 (43%)]\tLoss: 0.049920\n",
      "Training stage for Flod 1 Epoch: 22 [16000/37476                 (53%)]\tLoss: 0.006774\n",
      "Training stage for Flod 1 Epoch: 22 [19200/37476                 (64%)]\tLoss: 0.009069\n",
      "Training stage for Flod 1 Epoch: 22 [22400/37476                 (75%)]\tLoss: 0.011473\n",
      "Training stage for Flod 1 Epoch: 22 [25600/37476                 (85%)]\tLoss: 0.003888\n",
      "Training stage for Flod 1 Epoch: 22 [28800/37476                 (96%)]\tLoss: 0.005197\n",
      "Test set for fold1: Average Loss:           1.0703, Accuracy: 14761/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 23 [0/37476                 (0%)]\tLoss: 0.012736\n",
      "Training stage for Flod 1 Epoch: 23 [3200/37476                 (11%)]\tLoss: 0.069162\n",
      "Training stage for Flod 1 Epoch: 23 [6400/37476                 (21%)]\tLoss: 0.007599\n",
      "Training stage for Flod 1 Epoch: 23 [9600/37476                 (32%)]\tLoss: 0.010879\n",
      "Training stage for Flod 1 Epoch: 23 [12800/37476                 (43%)]\tLoss: 0.107571\n",
      "Training stage for Flod 1 Epoch: 23 [16000/37476                 (53%)]\tLoss: 0.011024\n",
      "Training stage for Flod 1 Epoch: 23 [19200/37476                 (64%)]\tLoss: 0.015556\n",
      "Training stage for Flod 1 Epoch: 23 [22400/37476                 (75%)]\tLoss: 0.007757\n",
      "Training stage for Flod 1 Epoch: 23 [25600/37476                 (85%)]\tLoss: 0.079227\n",
      "Training stage for Flod 1 Epoch: 23 [28800/37476                 (96%)]\tLoss: 0.004298\n",
      "Test set for fold1: Average Loss:           0.9989, Accuracy: 14727/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 24 [0/37476                 (0%)]\tLoss: 0.070503\n",
      "Training stage for Flod 1 Epoch: 24 [3200/37476                 (11%)]\tLoss: 0.201976\n",
      "Training stage for Flod 1 Epoch: 24 [6400/37476                 (21%)]\tLoss: 0.046574\n",
      "Training stage for Flod 1 Epoch: 24 [9600/37476                 (32%)]\tLoss: 0.001092\n",
      "Training stage for Flod 1 Epoch: 24 [12800/37476                 (43%)]\tLoss: 0.002723\n",
      "Training stage for Flod 1 Epoch: 24 [16000/37476                 (53%)]\tLoss: 0.062694\n",
      "Training stage for Flod 1 Epoch: 24 [19200/37476                 (64%)]\tLoss: 0.003236\n",
      "Training stage for Flod 1 Epoch: 24 [22400/37476                 (75%)]\tLoss: 0.018929\n",
      "Training stage for Flod 1 Epoch: 24 [25600/37476                 (85%)]\tLoss: 0.008409\n",
      "Training stage for Flod 1 Epoch: 24 [28800/37476                 (96%)]\tLoss: 0.014390\n",
      "Test set for fold1: Average Loss:           1.0737, Accuracy: 14717/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 25 [0/37476                 (0%)]\tLoss: 0.016583\n",
      "Training stage for Flod 1 Epoch: 25 [3200/37476                 (11%)]\tLoss: 0.132931\n",
      "Training stage for Flod 1 Epoch: 25 [6400/37476                 (21%)]\tLoss: 0.010938\n",
      "Training stage for Flod 1 Epoch: 25 [9600/37476                 (32%)]\tLoss: 0.022914\n",
      "Training stage for Flod 1 Epoch: 25 [12800/37476                 (43%)]\tLoss: 0.081371\n",
      "Training stage for Flod 1 Epoch: 25 [16000/37476                 (53%)]\tLoss: 0.116181\n",
      "Training stage for Flod 1 Epoch: 25 [19200/37476                 (64%)]\tLoss: 0.001162\n",
      "Training stage for Flod 1 Epoch: 25 [22400/37476                 (75%)]\tLoss: 0.128792\n",
      "Training stage for Flod 1 Epoch: 25 [25600/37476                 (85%)]\tLoss: 0.026391\n",
      "Training stage for Flod 1 Epoch: 25 [28800/37476                 (96%)]\tLoss: 0.032158\n",
      "Test set for fold1: Average Loss:           1.2636, Accuracy: 14733/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 26 [0/37476                 (0%)]\tLoss: 0.107286\n",
      "Training stage for Flod 1 Epoch: 26 [3200/37476                 (11%)]\tLoss: 0.001165\n",
      "Training stage for Flod 1 Epoch: 26 [6400/37476                 (21%)]\tLoss: 0.000333\n",
      "Training stage for Flod 1 Epoch: 26 [9600/37476                 (32%)]\tLoss: 0.124703\n",
      "Training stage for Flod 1 Epoch: 26 [12800/37476                 (43%)]\tLoss: 0.004216\n",
      "Training stage for Flod 1 Epoch: 26 [16000/37476                 (53%)]\tLoss: 0.018298\n",
      "Training stage for Flod 1 Epoch: 26 [19200/37476                 (64%)]\tLoss: 0.011565\n",
      "Training stage for Flod 1 Epoch: 26 [22400/37476                 (75%)]\tLoss: 0.019560\n",
      "Training stage for Flod 1 Epoch: 26 [25600/37476                 (85%)]\tLoss: 0.005712\n",
      "Training stage for Flod 1 Epoch: 26 [28800/37476                 (96%)]\tLoss: 0.038831\n",
      "Test set for fold1: Average Loss:           1.1878, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 27 [0/37476                 (0%)]\tLoss: 0.031095\n",
      "Training stage for Flod 1 Epoch: 27 [3200/37476                 (11%)]\tLoss: 0.000487\n",
      "Training stage for Flod 1 Epoch: 27 [6400/37476                 (21%)]\tLoss: 0.000774\n",
      "Training stage for Flod 1 Epoch: 27 [9600/37476                 (32%)]\tLoss: 0.097684\n",
      "Training stage for Flod 1 Epoch: 27 [12800/37476                 (43%)]\tLoss: 0.065882\n",
      "Training stage for Flod 1 Epoch: 27 [16000/37476                 (53%)]\tLoss: 0.001959\n",
      "Training stage for Flod 1 Epoch: 27 [19200/37476                 (64%)]\tLoss: 0.014463\n",
      "Training stage for Flod 1 Epoch: 27 [22400/37476                 (75%)]\tLoss: 0.025141\n",
      "Training stage for Flod 1 Epoch: 27 [25600/37476                 (85%)]\tLoss: 0.253877\n",
      "Training stage for Flod 1 Epoch: 27 [28800/37476                 (96%)]\tLoss: 0.141771\n",
      "Test set for fold1: Average Loss:           1.2849, Accuracy: 14729/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 28 [0/37476                 (0%)]\tLoss: 0.001246\n",
      "Training stage for Flod 1 Epoch: 28 [3200/37476                 (11%)]\tLoss: 0.058344\n",
      "Training stage for Flod 1 Epoch: 28 [6400/37476                 (21%)]\tLoss: 0.014234\n",
      "Training stage for Flod 1 Epoch: 28 [9600/37476                 (32%)]\tLoss: 0.016680\n",
      "Training stage for Flod 1 Epoch: 28 [12800/37476                 (43%)]\tLoss: 0.054202\n",
      "Training stage for Flod 1 Epoch: 28 [16000/37476                 (53%)]\tLoss: 0.005921\n",
      "Training stage for Flod 1 Epoch: 28 [19200/37476                 (64%)]\tLoss: 0.025761\n",
      "Training stage for Flod 1 Epoch: 28 [22400/37476                 (75%)]\tLoss: 0.005537\n",
      "Training stage for Flod 1 Epoch: 28 [25600/37476                 (85%)]\tLoss: 0.002422\n",
      "Training stage for Flod 1 Epoch: 28 [28800/37476                 (96%)]\tLoss: 0.048356\n",
      "Test set for fold1: Average Loss:           1.2033, Accuracy: 14716/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 29 [0/37476                 (0%)]\tLoss: 0.042418\n",
      "Training stage for Flod 1 Epoch: 29 [3200/37476                 (11%)]\tLoss: 0.048363\n",
      "Training stage for Flod 1 Epoch: 29 [6400/37476                 (21%)]\tLoss: 0.005933\n",
      "Training stage for Flod 1 Epoch: 29 [9600/37476                 (32%)]\tLoss: 0.051839\n",
      "Training stage for Flod 1 Epoch: 29 [12800/37476                 (43%)]\tLoss: 0.001211\n",
      "Training stage for Flod 1 Epoch: 29 [16000/37476                 (53%)]\tLoss: 0.178992\n",
      "Training stage for Flod 1 Epoch: 29 [19200/37476                 (64%)]\tLoss: 0.001937\n",
      "Training stage for Flod 1 Epoch: 29 [22400/37476                 (75%)]\tLoss: 0.042202\n",
      "Training stage for Flod 1 Epoch: 29 [25600/37476                 (85%)]\tLoss: 0.004834\n",
      "Training stage for Flod 1 Epoch: 29 [28800/37476                 (96%)]\tLoss: 0.055667\n",
      "Test set for fold1: Average Loss:           1.1140, Accuracy: 14723/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 30 [0/37476                 (0%)]\tLoss: 0.021219\n",
      "Training stage for Flod 1 Epoch: 30 [3200/37476                 (11%)]\tLoss: 0.003943\n",
      "Training stage for Flod 1 Epoch: 30 [6400/37476                 (21%)]\tLoss: 0.299131\n",
      "Training stage for Flod 1 Epoch: 30 [9600/37476                 (32%)]\tLoss: 0.013274\n",
      "Training stage for Flod 1 Epoch: 30 [12800/37476                 (43%)]\tLoss: 0.005639\n",
      "Training stage for Flod 1 Epoch: 30 [16000/37476                 (53%)]\tLoss: 0.002508\n",
      "Training stage for Flod 1 Epoch: 30 [19200/37476                 (64%)]\tLoss: 0.134426\n",
      "Training stage for Flod 1 Epoch: 30 [22400/37476                 (75%)]\tLoss: 0.048603\n",
      "Training stage for Flod 1 Epoch: 30 [25600/37476                 (85%)]\tLoss: 0.003397\n",
      "Training stage for Flod 1 Epoch: 30 [28800/37476                 (96%)]\tLoss: 0.022141\n",
      "Test set for fold1: Average Loss:           1.2737, Accuracy: 14740/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 31 [0/37476                 (0%)]\tLoss: 0.007625\n",
      "Training stage for Flod 1 Epoch: 31 [3200/37476                 (11%)]\tLoss: 0.001958\n",
      "Training stage for Flod 1 Epoch: 31 [6400/37476                 (21%)]\tLoss: 0.007545\n",
      "Training stage for Flod 1 Epoch: 31 [9600/37476                 (32%)]\tLoss: 0.003655\n",
      "Training stage for Flod 1 Epoch: 31 [12800/37476                 (43%)]\tLoss: 0.000831\n",
      "Training stage for Flod 1 Epoch: 31 [16000/37476                 (53%)]\tLoss: 0.018081\n",
      "Training stage for Flod 1 Epoch: 31 [19200/37476                 (64%)]\tLoss: 0.008083\n",
      "Training stage for Flod 1 Epoch: 31 [22400/37476                 (75%)]\tLoss: 0.010135\n",
      "Training stage for Flod 1 Epoch: 31 [25600/37476                 (85%)]\tLoss: 0.007820\n",
      "Training stage for Flod 1 Epoch: 31 [28800/37476                 (96%)]\tLoss: 0.126711\n",
      "Test set for fold1: Average Loss:           1.1819, Accuracy: 14737/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 32 [0/37476                 (0%)]\tLoss: 0.026009\n",
      "Training stage for Flod 1 Epoch: 32 [3200/37476                 (11%)]\tLoss: 0.018814\n",
      "Training stage for Flod 1 Epoch: 32 [6400/37476                 (21%)]\tLoss: 0.014928\n",
      "Training stage for Flod 1 Epoch: 32 [9600/37476                 (32%)]\tLoss: 0.084264\n",
      "Training stage for Flod 1 Epoch: 32 [12800/37476                 (43%)]\tLoss: 0.049682\n",
      "Training stage for Flod 1 Epoch: 32 [16000/37476                 (53%)]\tLoss: 0.010138\n",
      "Training stage for Flod 1 Epoch: 32 [19200/37476                 (64%)]\tLoss: 0.002190\n",
      "Training stage for Flod 1 Epoch: 32 [22400/37476                 (75%)]\tLoss: 0.001007\n",
      "Training stage for Flod 1 Epoch: 32 [25600/37476                 (85%)]\tLoss: 0.002003\n",
      "Training stage for Flod 1 Epoch: 32 [28800/37476                 (96%)]\tLoss: 0.009914\n",
      "Test set for fold1: Average Loss:           1.2781, Accuracy: 14707/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 33 [0/37476                 (0%)]\tLoss: 0.004622\n",
      "Training stage for Flod 1 Epoch: 33 [3200/37476                 (11%)]\tLoss: 0.028584\n",
      "Training stage for Flod 1 Epoch: 33 [6400/37476                 (21%)]\tLoss: 0.023904\n",
      "Training stage for Flod 1 Epoch: 33 [9600/37476                 (32%)]\tLoss: 0.034166\n",
      "Training stage for Flod 1 Epoch: 33 [12800/37476                 (43%)]\tLoss: 0.000873\n",
      "Training stage for Flod 1 Epoch: 33 [16000/37476                 (53%)]\tLoss: 0.136223\n",
      "Training stage for Flod 1 Epoch: 33 [19200/37476                 (64%)]\tLoss: 0.019452\n",
      "Training stage for Flod 1 Epoch: 33 [22400/37476                 (75%)]\tLoss: 0.010465\n",
      "Training stage for Flod 1 Epoch: 33 [25600/37476                 (85%)]\tLoss: 0.335540\n",
      "Training stage for Flod 1 Epoch: 33 [28800/37476                 (96%)]\tLoss: 0.010196\n",
      "Test set for fold1: Average Loss:           1.3458, Accuracy: 14716/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 34 [0/37476                 (0%)]\tLoss: 0.151980\n",
      "Training stage for Flod 1 Epoch: 34 [3200/37476                 (11%)]\tLoss: 0.027570\n",
      "Training stage for Flod 1 Epoch: 34 [6400/37476                 (21%)]\tLoss: 0.176988\n",
      "Training stage for Flod 1 Epoch: 34 [9600/37476                 (32%)]\tLoss: 0.005670\n",
      "Training stage for Flod 1 Epoch: 34 [12800/37476                 (43%)]\tLoss: 0.106811\n",
      "Training stage for Flod 1 Epoch: 34 [16000/37476                 (53%)]\tLoss: 0.004051\n",
      "Training stage for Flod 1 Epoch: 34 [19200/37476                 (64%)]\tLoss: 0.042875\n",
      "Training stage for Flod 1 Epoch: 34 [22400/37476                 (75%)]\tLoss: 0.040244\n",
      "Training stage for Flod 1 Epoch: 34 [25600/37476                 (85%)]\tLoss: 0.002032\n",
      "Training stage for Flod 1 Epoch: 34 [28800/37476                 (96%)]\tLoss: 0.000736\n",
      "Test set for fold1: Average Loss:           1.2300, Accuracy: 14777/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 35 [0/37476                 (0%)]\tLoss: 0.024198\n",
      "Training stage for Flod 1 Epoch: 35 [3200/37476                 (11%)]\tLoss: 0.005290\n",
      "Training stage for Flod 1 Epoch: 35 [6400/37476                 (21%)]\tLoss: 0.020395\n",
      "Training stage for Flod 1 Epoch: 35 [9600/37476                 (32%)]\tLoss: 0.002792\n",
      "Training stage for Flod 1 Epoch: 35 [12800/37476                 (43%)]\tLoss: 0.021177\n",
      "Training stage for Flod 1 Epoch: 35 [16000/37476                 (53%)]\tLoss: 0.016436\n",
      "Training stage for Flod 1 Epoch: 35 [19200/37476                 (64%)]\tLoss: 0.011547\n",
      "Training stage for Flod 1 Epoch: 35 [22400/37476                 (75%)]\tLoss: 0.023204\n",
      "Training stage for Flod 1 Epoch: 35 [25600/37476                 (85%)]\tLoss: 0.013734\n",
      "Training stage for Flod 1 Epoch: 35 [28800/37476                 (96%)]\tLoss: 0.031720\n",
      "Test set for fold1: Average Loss:           1.0645, Accuracy: 14752/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 36 [0/37476                 (0%)]\tLoss: 0.025401\n",
      "Training stage for Flod 1 Epoch: 36 [3200/37476                 (11%)]\tLoss: 0.045731\n",
      "Training stage for Flod 1 Epoch: 36 [6400/37476                 (21%)]\tLoss: 0.011899\n",
      "Training stage for Flod 1 Epoch: 36 [9600/37476                 (32%)]\tLoss: 0.045258\n",
      "Training stage for Flod 1 Epoch: 36 [12800/37476                 (43%)]\tLoss: 0.024249\n",
      "Training stage for Flod 1 Epoch: 36 [16000/37476                 (53%)]\tLoss: 0.039041\n",
      "Training stage for Flod 1 Epoch: 36 [19200/37476                 (64%)]\tLoss: 0.037151\n",
      "Training stage for Flod 1 Epoch: 36 [22400/37476                 (75%)]\tLoss: 0.005940\n",
      "Training stage for Flod 1 Epoch: 36 [25600/37476                 (85%)]\tLoss: 0.000813\n",
      "Training stage for Flod 1 Epoch: 36 [28800/37476                 (96%)]\tLoss: 0.045325\n",
      "Test set for fold1: Average Loss:           1.0832, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 37 [0/37476                 (0%)]\tLoss: 0.065861\n",
      "Training stage for Flod 1 Epoch: 37 [3200/37476                 (11%)]\tLoss: 0.018484\n",
      "Training stage for Flod 1 Epoch: 37 [6400/37476                 (21%)]\tLoss: 0.019814\n",
      "Training stage for Flod 1 Epoch: 37 [9600/37476                 (32%)]\tLoss: 0.005217\n",
      "Training stage for Flod 1 Epoch: 37 [12800/37476                 (43%)]\tLoss: 0.019677\n",
      "Training stage for Flod 1 Epoch: 37 [16000/37476                 (53%)]\tLoss: 0.000531\n",
      "Training stage for Flod 1 Epoch: 37 [19200/37476                 (64%)]\tLoss: 0.015375\n",
      "Training stage for Flod 1 Epoch: 37 [22400/37476                 (75%)]\tLoss: 0.086848\n",
      "Training stage for Flod 1 Epoch: 37 [25600/37476                 (85%)]\tLoss: 0.153661\n",
      "Training stage for Flod 1 Epoch: 37 [28800/37476                 (96%)]\tLoss: 0.016418\n",
      "Test set for fold1: Average Loss:           1.1863, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 38 [0/37476                 (0%)]\tLoss: 0.008290\n",
      "Training stage for Flod 1 Epoch: 38 [3200/37476                 (11%)]\tLoss: 0.053680\n",
      "Training stage for Flod 1 Epoch: 38 [6400/37476                 (21%)]\tLoss: 0.006612\n",
      "Training stage for Flod 1 Epoch: 38 [9600/37476                 (32%)]\tLoss: 0.000702\n",
      "Training stage for Flod 1 Epoch: 38 [12800/37476                 (43%)]\tLoss: 0.027526\n",
      "Training stage for Flod 1 Epoch: 38 [16000/37476                 (53%)]\tLoss: 0.000900\n",
      "Training stage for Flod 1 Epoch: 38 [19200/37476                 (64%)]\tLoss: 0.069091\n",
      "Training stage for Flod 1 Epoch: 38 [22400/37476                 (75%)]\tLoss: 0.013470\n",
      "Training stage for Flod 1 Epoch: 38 [25600/37476                 (85%)]\tLoss: 0.006077\n",
      "Training stage for Flod 1 Epoch: 38 [28800/37476                 (96%)]\tLoss: 0.082026\n",
      "Test set for fold1: Average Loss:           1.0862, Accuracy: 14745/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 39 [0/37476                 (0%)]\tLoss: 0.080867\n",
      "Training stage for Flod 1 Epoch: 39 [3200/37476                 (11%)]\tLoss: 0.132450\n",
      "Training stage for Flod 1 Epoch: 39 [6400/37476                 (21%)]\tLoss: 0.002856\n",
      "Training stage for Flod 1 Epoch: 39 [9600/37476                 (32%)]\tLoss: 0.001973\n",
      "Training stage for Flod 1 Epoch: 39 [12800/37476                 (43%)]\tLoss: 0.034116\n",
      "Training stage for Flod 1 Epoch: 39 [16000/37476                 (53%)]\tLoss: 0.039385\n",
      "Training stage for Flod 1 Epoch: 39 [19200/37476                 (64%)]\tLoss: 0.032238\n",
      "Training stage for Flod 1 Epoch: 39 [22400/37476                 (75%)]\tLoss: 0.078812\n",
      "Training stage for Flod 1 Epoch: 39 [25600/37476                 (85%)]\tLoss: 0.074994\n",
      "Training stage for Flod 1 Epoch: 39 [28800/37476                 (96%)]\tLoss: 0.029319\n",
      "Test set for fold1: Average Loss:           1.2210, Accuracy: 14738/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 40 [0/37476                 (0%)]\tLoss: 0.008834\n",
      "Training stage for Flod 1 Epoch: 40 [3200/37476                 (11%)]\tLoss: 0.102874\n",
      "Training stage for Flod 1 Epoch: 40 [6400/37476                 (21%)]\tLoss: 0.028524\n",
      "Training stage for Flod 1 Epoch: 40 [9600/37476                 (32%)]\tLoss: 0.171737\n",
      "Training stage for Flod 1 Epoch: 40 [12800/37476                 (43%)]\tLoss: 0.010404\n",
      "Training stage for Flod 1 Epoch: 40 [16000/37476                 (53%)]\tLoss: 0.021455\n",
      "Training stage for Flod 1 Epoch: 40 [19200/37476                 (64%)]\tLoss: 0.025570\n",
      "Training stage for Flod 1 Epoch: 40 [22400/37476                 (75%)]\tLoss: 0.068135\n",
      "Training stage for Flod 1 Epoch: 40 [25600/37476                 (85%)]\tLoss: 0.022895\n",
      "Training stage for Flod 1 Epoch: 40 [28800/37476                 (96%)]\tLoss: 0.017780\n",
      "Test set for fold1: Average Loss:           1.3465, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 41 [0/37476                 (0%)]\tLoss: 0.008384\n",
      "Training stage for Flod 1 Epoch: 41 [3200/37476                 (11%)]\tLoss: 0.001229\n",
      "Training stage for Flod 1 Epoch: 41 [6400/37476                 (21%)]\tLoss: 0.082853\n",
      "Training stage for Flod 1 Epoch: 41 [9600/37476                 (32%)]\tLoss: 0.024360\n",
      "Training stage for Flod 1 Epoch: 41 [12800/37476                 (43%)]\tLoss: 0.045552\n",
      "Training stage for Flod 1 Epoch: 41 [16000/37476                 (53%)]\tLoss: 0.181514\n",
      "Training stage for Flod 1 Epoch: 41 [19200/37476                 (64%)]\tLoss: 0.004513\n",
      "Training stage for Flod 1 Epoch: 41 [22400/37476                 (75%)]\tLoss: 0.011549\n",
      "Training stage for Flod 1 Epoch: 41 [25600/37476                 (85%)]\tLoss: 0.014496\n",
      "Training stage for Flod 1 Epoch: 41 [28800/37476                 (96%)]\tLoss: 0.033263\n",
      "Test set for fold1: Average Loss:           1.2066, Accuracy: 14758/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 42 [0/37476                 (0%)]\tLoss: 0.012489\n",
      "Training stage for Flod 1 Epoch: 42 [3200/37476                 (11%)]\tLoss: 0.002731\n",
      "Training stage for Flod 1 Epoch: 42 [6400/37476                 (21%)]\tLoss: 0.008895\n",
      "Training stage for Flod 1 Epoch: 42 [9600/37476                 (32%)]\tLoss: 0.042386\n",
      "Training stage for Flod 1 Epoch: 42 [12800/37476                 (43%)]\tLoss: 0.026844\n",
      "Training stage for Flod 1 Epoch: 42 [16000/37476                 (53%)]\tLoss: 0.033358\n",
      "Training stage for Flod 1 Epoch: 42 [19200/37476                 (64%)]\tLoss: 0.022152\n",
      "Training stage for Flod 1 Epoch: 42 [22400/37476                 (75%)]\tLoss: 0.078628\n",
      "Training stage for Flod 1 Epoch: 42 [25600/37476                 (85%)]\tLoss: 0.019910\n",
      "Training stage for Flod 1 Epoch: 42 [28800/37476                 (96%)]\tLoss: 0.001365\n",
      "Test set for fold1: Average Loss:           1.1613, Accuracy: 14766/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 43 [0/37476                 (0%)]\tLoss: 0.059395\n",
      "Training stage for Flod 1 Epoch: 43 [3200/37476                 (11%)]\tLoss: 0.025362\n",
      "Training stage for Flod 1 Epoch: 43 [6400/37476                 (21%)]\tLoss: 0.018108\n",
      "Training stage for Flod 1 Epoch: 43 [9600/37476                 (32%)]\tLoss: 0.205965\n",
      "Training stage for Flod 1 Epoch: 43 [12800/37476                 (43%)]\tLoss: 0.005493\n",
      "Training stage for Flod 1 Epoch: 43 [16000/37476                 (53%)]\tLoss: 0.017563\n",
      "Training stage for Flod 1 Epoch: 43 [19200/37476                 (64%)]\tLoss: 0.005187\n",
      "Training stage for Flod 1 Epoch: 43 [22400/37476                 (75%)]\tLoss: 0.031675\n",
      "Training stage for Flod 1 Epoch: 43 [25600/37476                 (85%)]\tLoss: 0.000047\n",
      "Training stage for Flod 1 Epoch: 43 [28800/37476                 (96%)]\tLoss: 0.010480\n",
      "Test set for fold1: Average Loss:           1.2590, Accuracy: 14652/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 44 [0/37476                 (0%)]\tLoss: 0.063219\n",
      "Training stage for Flod 1 Epoch: 44 [3200/37476                 (11%)]\tLoss: 0.001363\n",
      "Training stage for Flod 1 Epoch: 44 [6400/37476                 (21%)]\tLoss: 0.038226\n",
      "Training stage for Flod 1 Epoch: 44 [9600/37476                 (32%)]\tLoss: 0.002973\n",
      "Training stage for Flod 1 Epoch: 44 [12800/37476                 (43%)]\tLoss: 0.019477\n",
      "Training stage for Flod 1 Epoch: 44 [16000/37476                 (53%)]\tLoss: 0.000752\n",
      "Training stage for Flod 1 Epoch: 44 [19200/37476                 (64%)]\tLoss: 0.023965\n",
      "Training stage for Flod 1 Epoch: 44 [22400/37476                 (75%)]\tLoss: 0.069814\n",
      "Training stage for Flod 1 Epoch: 44 [25600/37476                 (85%)]\tLoss: 0.019291\n",
      "Training stage for Flod 1 Epoch: 44 [28800/37476                 (96%)]\tLoss: 0.064839\n",
      "Test set for fold1: Average Loss:           1.4893, Accuracy: 14713/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 45 [0/37476                 (0%)]\tLoss: 0.056853\n",
      "Training stage for Flod 1 Epoch: 45 [3200/37476                 (11%)]\tLoss: 0.020143\n",
      "Training stage for Flod 1 Epoch: 45 [6400/37476                 (21%)]\tLoss: 0.025149\n",
      "Training stage for Flod 1 Epoch: 45 [9600/37476                 (32%)]\tLoss: 0.017066\n",
      "Training stage for Flod 1 Epoch: 45 [12800/37476                 (43%)]\tLoss: 0.105375\n",
      "Training stage for Flod 1 Epoch: 45 [16000/37476                 (53%)]\tLoss: 0.018013\n",
      "Training stage for Flod 1 Epoch: 45 [19200/37476                 (64%)]\tLoss: 0.004055\n",
      "Training stage for Flod 1 Epoch: 45 [22400/37476                 (75%)]\tLoss: 0.003238\n",
      "Training stage for Flod 1 Epoch: 45 [25600/37476                 (85%)]\tLoss: 0.056550\n",
      "Training stage for Flod 1 Epoch: 45 [28800/37476                 (96%)]\tLoss: 0.028568\n",
      "Test set for fold1: Average Loss:           1.4316, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 46 [0/37476                 (0%)]\tLoss: 0.000623\n",
      "Training stage for Flod 1 Epoch: 46 [3200/37476                 (11%)]\tLoss: 0.035073\n",
      "Training stage for Flod 1 Epoch: 46 [6400/37476                 (21%)]\tLoss: 0.009710\n",
      "Training stage for Flod 1 Epoch: 46 [9600/37476                 (32%)]\tLoss: 0.024100\n",
      "Training stage for Flod 1 Epoch: 46 [12800/37476                 (43%)]\tLoss: 0.006988\n",
      "Training stage for Flod 1 Epoch: 46 [16000/37476                 (53%)]\tLoss: 0.019107\n",
      "Training stage for Flod 1 Epoch: 46 [19200/37476                 (64%)]\tLoss: 0.021246\n",
      "Training stage for Flod 1 Epoch: 46 [22400/37476                 (75%)]\tLoss: 0.163966\n",
      "Training stage for Flod 1 Epoch: 46 [25600/37476                 (85%)]\tLoss: 0.006219\n",
      "Training stage for Flod 1 Epoch: 46 [28800/37476                 (96%)]\tLoss: 0.016507\n",
      "Test set for fold1: Average Loss:           1.4080, Accuracy: 14741/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 47 [0/37476                 (0%)]\tLoss: 0.093441\n",
      "Training stage for Flod 1 Epoch: 47 [3200/37476                 (11%)]\tLoss: 0.023176\n",
      "Training stage for Flod 1 Epoch: 47 [6400/37476                 (21%)]\tLoss: 0.000400\n",
      "Training stage for Flod 1 Epoch: 47 [9600/37476                 (32%)]\tLoss: 0.006088\n",
      "Training stage for Flod 1 Epoch: 47 [12800/37476                 (43%)]\tLoss: 0.006279\n",
      "Training stage for Flod 1 Epoch: 47 [16000/37476                 (53%)]\tLoss: 0.122286\n",
      "Training stage for Flod 1 Epoch: 47 [19200/37476                 (64%)]\tLoss: 0.000511\n",
      "Training stage for Flod 1 Epoch: 47 [22400/37476                 (75%)]\tLoss: 0.012035\n",
      "Training stage for Flod 1 Epoch: 47 [25600/37476                 (85%)]\tLoss: 0.056592\n",
      "Training stage for Flod 1 Epoch: 47 [28800/37476                 (96%)]\tLoss: 0.007269\n",
      "Test set for fold1: Average Loss:           1.1459, Accuracy: 14769/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 48 [0/37476                 (0%)]\tLoss: 0.012113\n",
      "Training stage for Flod 1 Epoch: 48 [3200/37476                 (11%)]\tLoss: 0.014233\n",
      "Training stage for Flod 1 Epoch: 48 [6400/37476                 (21%)]\tLoss: 0.001996\n",
      "Training stage for Flod 1 Epoch: 48 [9600/37476                 (32%)]\tLoss: 0.003725\n",
      "Training stage for Flod 1 Epoch: 48 [12800/37476                 (43%)]\tLoss: 0.014515\n",
      "Training stage for Flod 1 Epoch: 48 [16000/37476                 (53%)]\tLoss: 0.136369\n",
      "Training stage for Flod 1 Epoch: 48 [19200/37476                 (64%)]\tLoss: 0.002609\n",
      "Training stage for Flod 1 Epoch: 48 [22400/37476                 (75%)]\tLoss: 0.000395\n",
      "Training stage for Flod 1 Epoch: 48 [25600/37476                 (85%)]\tLoss: 0.003457\n",
      "Training stage for Flod 1 Epoch: 48 [28800/37476                 (96%)]\tLoss: 0.013733\n",
      "Test set for fold1: Average Loss:           1.1636, Accuracy: 14775/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 49 [0/37476                 (0%)]\tLoss: 0.012867\n",
      "Training stage for Flod 1 Epoch: 49 [3200/37476                 (11%)]\tLoss: 0.024011\n",
      "Training stage for Flod 1 Epoch: 49 [6400/37476                 (21%)]\tLoss: 0.039055\n",
      "Training stage for Flod 1 Epoch: 49 [9600/37476                 (32%)]\tLoss: 0.002281\n",
      "Training stage for Flod 1 Epoch: 49 [12800/37476                 (43%)]\tLoss: 0.041429\n",
      "Training stage for Flod 1 Epoch: 49 [16000/37476                 (53%)]\tLoss: 0.006114\n",
      "Training stage for Flod 1 Epoch: 49 [19200/37476                 (64%)]\tLoss: 0.013313\n",
      "Training stage for Flod 1 Epoch: 49 [22400/37476                 (75%)]\tLoss: 0.116450\n",
      "Training stage for Flod 1 Epoch: 49 [25600/37476                 (85%)]\tLoss: 0.005764\n",
      "Training stage for Flod 1 Epoch: 49 [28800/37476                 (96%)]\tLoss: 0.033672\n",
      "Test set for fold1: Average Loss:           1.3507, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 50 [0/37476                 (0%)]\tLoss: 0.236415\n",
      "Training stage for Flod 1 Epoch: 50 [3200/37476                 (11%)]\tLoss: 0.001027\n",
      "Training stage for Flod 1 Epoch: 50 [6400/37476                 (21%)]\tLoss: 0.015082\n",
      "Training stage for Flod 1 Epoch: 50 [9600/37476                 (32%)]\tLoss: 0.012002\n",
      "Training stage for Flod 1 Epoch: 50 [12800/37476                 (43%)]\tLoss: 0.002363\n",
      "Training stage for Flod 1 Epoch: 50 [16000/37476                 (53%)]\tLoss: 0.025524\n",
      "Training stage for Flod 1 Epoch: 50 [19200/37476                 (64%)]\tLoss: 0.000628\n",
      "Training stage for Flod 1 Epoch: 50 [22400/37476                 (75%)]\tLoss: 0.078295\n",
      "Training stage for Flod 1 Epoch: 50 [25600/37476                 (85%)]\tLoss: 0.000086\n",
      "Training stage for Flod 1 Epoch: 50 [28800/37476                 (96%)]\tLoss: 0.039514\n",
      "Test set for fold1: Average Loss:           1.1672, Accuracy: 14630/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 51 [0/37476                 (0%)]\tLoss: 0.016715\n",
      "Training stage for Flod 1 Epoch: 51 [3200/37476                 (11%)]\tLoss: 0.004849\n",
      "Training stage for Flod 1 Epoch: 51 [6400/37476                 (21%)]\tLoss: 0.080828\n",
      "Training stage for Flod 1 Epoch: 51 [9600/37476                 (32%)]\tLoss: 0.023374\n",
      "Training stage for Flod 1 Epoch: 51 [12800/37476                 (43%)]\tLoss: 0.004975\n",
      "Training stage for Flod 1 Epoch: 51 [16000/37476                 (53%)]\tLoss: 0.004849\n",
      "Training stage for Flod 1 Epoch: 51 [19200/37476                 (64%)]\tLoss: 0.041157\n",
      "Training stage for Flod 1 Epoch: 51 [22400/37476                 (75%)]\tLoss: 0.016071\n",
      "Training stage for Flod 1 Epoch: 51 [25600/37476                 (85%)]\tLoss: 0.010791\n",
      "Training stage for Flod 1 Epoch: 51 [28800/37476                 (96%)]\tLoss: 0.061344\n",
      "Test set for fold1: Average Loss:           1.3057, Accuracy: 14776/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 52 [0/37476                 (0%)]\tLoss: 0.134164\n",
      "Training stage for Flod 1 Epoch: 52 [3200/37476                 (11%)]\tLoss: 0.004055\n",
      "Training stage for Flod 1 Epoch: 52 [6400/37476                 (21%)]\tLoss: 0.000925\n",
      "Training stage for Flod 1 Epoch: 52 [9600/37476                 (32%)]\tLoss: 0.009574\n",
      "Training stage for Flod 1 Epoch: 52 [12800/37476                 (43%)]\tLoss: 0.001309\n",
      "Training stage for Flod 1 Epoch: 52 [16000/37476                 (53%)]\tLoss: 0.015352\n",
      "Training stage for Flod 1 Epoch: 52 [19200/37476                 (64%)]\tLoss: 0.011097\n",
      "Training stage for Flod 1 Epoch: 52 [22400/37476                 (75%)]\tLoss: 0.128324\n",
      "Training stage for Flod 1 Epoch: 52 [25600/37476                 (85%)]\tLoss: 0.006538\n",
      "Training stage for Flod 1 Epoch: 52 [28800/37476                 (96%)]\tLoss: 0.027619\n",
      "Test set for fold1: Average Loss:           1.2971, Accuracy: 14765/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 53 [0/37476                 (0%)]\tLoss: 0.008757\n",
      "Training stage for Flod 1 Epoch: 53 [3200/37476                 (11%)]\tLoss: 0.148157\n",
      "Training stage for Flod 1 Epoch: 53 [6400/37476                 (21%)]\tLoss: 0.016892\n",
      "Training stage for Flod 1 Epoch: 53 [9600/37476                 (32%)]\tLoss: 0.000872\n",
      "Training stage for Flod 1 Epoch: 53 [12800/37476                 (43%)]\tLoss: 0.022962\n",
      "Training stage for Flod 1 Epoch: 53 [16000/37476                 (53%)]\tLoss: 0.088566\n",
      "Training stage for Flod 1 Epoch: 53 [19200/37476                 (64%)]\tLoss: 0.004572\n",
      "Training stage for Flod 1 Epoch: 53 [22400/37476                 (75%)]\tLoss: 0.005469\n",
      "Training stage for Flod 1 Epoch: 53 [25600/37476                 (85%)]\tLoss: 0.138582\n",
      "Training stage for Flod 1 Epoch: 53 [28800/37476                 (96%)]\tLoss: 0.000553\n",
      "Test set for fold1: Average Loss:           1.3392, Accuracy: 14746/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 54 [0/37476                 (0%)]\tLoss: 0.024024\n",
      "Training stage for Flod 1 Epoch: 54 [3200/37476                 (11%)]\tLoss: 0.036539\n",
      "Training stage for Flod 1 Epoch: 54 [6400/37476                 (21%)]\tLoss: 0.008082\n",
      "Training stage for Flod 1 Epoch: 54 [9600/37476                 (32%)]\tLoss: 0.017596\n",
      "Training stage for Flod 1 Epoch: 54 [12800/37476                 (43%)]\tLoss: 0.007277\n",
      "Training stage for Flod 1 Epoch: 54 [16000/37476                 (53%)]\tLoss: 0.000211\n",
      "Training stage for Flod 1 Epoch: 54 [19200/37476                 (64%)]\tLoss: 0.023677\n",
      "Training stage for Flod 1 Epoch: 54 [22400/37476                 (75%)]\tLoss: 0.006971\n",
      "Training stage for Flod 1 Epoch: 54 [25600/37476                 (85%)]\tLoss: 0.002271\n",
      "Training stage for Flod 1 Epoch: 54 [28800/37476                 (96%)]\tLoss: 0.006037\n",
      "Test set for fold1: Average Loss:           1.4903, Accuracy: 14707/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 55 [0/37476                 (0%)]\tLoss: 0.036116\n",
      "Training stage for Flod 1 Epoch: 55 [3200/37476                 (11%)]\tLoss: 0.021521\n",
      "Training stage for Flod 1 Epoch: 55 [6400/37476                 (21%)]\tLoss: 0.041794\n",
      "Training stage for Flod 1 Epoch: 55 [9600/37476                 (32%)]\tLoss: 0.000874\n",
      "Training stage for Flod 1 Epoch: 55 [12800/37476                 (43%)]\tLoss: 0.044121\n",
      "Training stage for Flod 1 Epoch: 55 [16000/37476                 (53%)]\tLoss: 0.035517\n",
      "Training stage for Flod 1 Epoch: 55 [19200/37476                 (64%)]\tLoss: 0.004934\n",
      "Training stage for Flod 1 Epoch: 55 [22400/37476                 (75%)]\tLoss: 0.072465\n",
      "Training stage for Flod 1 Epoch: 55 [25600/37476                 (85%)]\tLoss: 0.012665\n",
      "Training stage for Flod 1 Epoch: 55 [28800/37476                 (96%)]\tLoss: 0.001208\n",
      "Test set for fold1: Average Loss:           1.3535, Accuracy: 14726/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 56 [0/37476                 (0%)]\tLoss: 0.005246\n",
      "Training stage for Flod 1 Epoch: 56 [3200/37476                 (11%)]\tLoss: 0.013173\n",
      "Training stage for Flod 1 Epoch: 56 [6400/37476                 (21%)]\tLoss: 0.073774\n",
      "Training stage for Flod 1 Epoch: 56 [9600/37476                 (32%)]\tLoss: 0.014363\n",
      "Training stage for Flod 1 Epoch: 56 [12800/37476                 (43%)]\tLoss: 0.001713\n",
      "Training stage for Flod 1 Epoch: 56 [16000/37476                 (53%)]\tLoss: 0.003139\n",
      "Training stage for Flod 1 Epoch: 56 [19200/37476                 (64%)]\tLoss: 0.003828\n",
      "Training stage for Flod 1 Epoch: 56 [22400/37476                 (75%)]\tLoss: 0.069646\n",
      "Training stage for Flod 1 Epoch: 56 [25600/37476                 (85%)]\tLoss: 0.058039\n",
      "Training stage for Flod 1 Epoch: 56 [28800/37476                 (96%)]\tLoss: 0.000246\n",
      "Test set for fold1: Average Loss:           1.2216, Accuracy: 14731/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 57 [0/37476                 (0%)]\tLoss: 0.008669\n",
      "Training stage for Flod 1 Epoch: 57 [3200/37476                 (11%)]\tLoss: 0.121035\n",
      "Training stage for Flod 1 Epoch: 57 [6400/37476                 (21%)]\tLoss: 0.102568\n",
      "Training stage for Flod 1 Epoch: 57 [9600/37476                 (32%)]\tLoss: 0.004955\n",
      "Training stage for Flod 1 Epoch: 57 [12800/37476                 (43%)]\tLoss: 0.032341\n",
      "Training stage for Flod 1 Epoch: 57 [16000/37476                 (53%)]\tLoss: 0.001045\n",
      "Training stage for Flod 1 Epoch: 57 [19200/37476                 (64%)]\tLoss: 0.003762\n",
      "Training stage for Flod 1 Epoch: 57 [22400/37476                 (75%)]\tLoss: 0.013114\n",
      "Training stage for Flod 1 Epoch: 57 [25600/37476                 (85%)]\tLoss: 0.014478\n",
      "Training stage for Flod 1 Epoch: 57 [28800/37476                 (96%)]\tLoss: 0.005141\n",
      "Test set for fold1: Average Loss:           1.4816, Accuracy: 14757/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 58 [0/37476                 (0%)]\tLoss: 0.014027\n",
      "Training stage for Flod 1 Epoch: 58 [3200/37476                 (11%)]\tLoss: 0.007688\n",
      "Training stage for Flod 1 Epoch: 58 [6400/37476                 (21%)]\tLoss: 0.028418\n",
      "Training stage for Flod 1 Epoch: 58 [9600/37476                 (32%)]\tLoss: 0.046412\n",
      "Training stage for Flod 1 Epoch: 58 [12800/37476                 (43%)]\tLoss: 0.023379\n",
      "Training stage for Flod 1 Epoch: 58 [16000/37476                 (53%)]\tLoss: 0.072717\n",
      "Training stage for Flod 1 Epoch: 58 [19200/37476                 (64%)]\tLoss: 0.042437\n",
      "Training stage for Flod 1 Epoch: 58 [22400/37476                 (75%)]\tLoss: 0.010547\n",
      "Training stage for Flod 1 Epoch: 58 [25600/37476                 (85%)]\tLoss: 0.050023\n",
      "Training stage for Flod 1 Epoch: 58 [28800/37476                 (96%)]\tLoss: 0.057453\n",
      "Test set for fold1: Average Loss:           1.3795, Accuracy: 14694/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 59 [0/37476                 (0%)]\tLoss: 0.111721\n",
      "Training stage for Flod 1 Epoch: 59 [3200/37476                 (11%)]\tLoss: 0.014263\n",
      "Training stage for Flod 1 Epoch: 59 [6400/37476                 (21%)]\tLoss: 0.004211\n",
      "Training stage for Flod 1 Epoch: 59 [9600/37476                 (32%)]\tLoss: 0.057005\n",
      "Training stage for Flod 1 Epoch: 59 [12800/37476                 (43%)]\tLoss: 0.062077\n",
      "Training stage for Flod 1 Epoch: 59 [16000/37476                 (53%)]\tLoss: 0.005446\n",
      "Training stage for Flod 1 Epoch: 59 [19200/37476                 (64%)]\tLoss: 0.037328\n",
      "Training stage for Flod 1 Epoch: 59 [22400/37476                 (75%)]\tLoss: 0.016562\n",
      "Training stage for Flod 1 Epoch: 59 [25600/37476                 (85%)]\tLoss: 0.024544\n",
      "Training stage for Flod 1 Epoch: 59 [28800/37476                 (96%)]\tLoss: 0.039069\n",
      "Test set for fold1: Average Loss:           1.2688, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 60 [0/37476                 (0%)]\tLoss: 0.021406\n",
      "Training stage for Flod 1 Epoch: 60 [3200/37476                 (11%)]\tLoss: 0.053984\n",
      "Training stage for Flod 1 Epoch: 60 [6400/37476                 (21%)]\tLoss: 0.067578\n",
      "Training stage for Flod 1 Epoch: 60 [9600/37476                 (32%)]\tLoss: 0.160170\n",
      "Training stage for Flod 1 Epoch: 60 [12800/37476                 (43%)]\tLoss: 0.001911\n",
      "Training stage for Flod 1 Epoch: 60 [16000/37476                 (53%)]\tLoss: 0.001739\n",
      "Training stage for Flod 1 Epoch: 60 [19200/37476                 (64%)]\tLoss: 0.006741\n",
      "Training stage for Flod 1 Epoch: 60 [22400/37476                 (75%)]\tLoss: 0.005138\n",
      "Training stage for Flod 1 Epoch: 60 [25600/37476                 (85%)]\tLoss: 0.081579\n",
      "Training stage for Flod 1 Epoch: 60 [28800/37476                 (96%)]\tLoss: 0.120781\n",
      "Test set for fold1: Average Loss:           1.3606, Accuracy: 14777/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 61 [0/37476                 (0%)]\tLoss: 0.002163\n",
      "Training stage for Flod 1 Epoch: 61 [3200/37476                 (11%)]\tLoss: 0.006050\n",
      "Training stage for Flod 1 Epoch: 61 [6400/37476                 (21%)]\tLoss: 0.002733\n",
      "Training stage for Flod 1 Epoch: 61 [9600/37476                 (32%)]\tLoss: 0.001859\n",
      "Training stage for Flod 1 Epoch: 61 [12800/37476                 (43%)]\tLoss: 0.029544\n",
      "Training stage for Flod 1 Epoch: 61 [16000/37476                 (53%)]\tLoss: 0.041072\n",
      "Training stage for Flod 1 Epoch: 61 [19200/37476                 (64%)]\tLoss: 0.107011\n",
      "Training stage for Flod 1 Epoch: 61 [22400/37476                 (75%)]\tLoss: 0.018632\n",
      "Training stage for Flod 1 Epoch: 61 [25600/37476                 (85%)]\tLoss: 0.002786\n",
      "Training stage for Flod 1 Epoch: 61 [28800/37476                 (96%)]\tLoss: 0.001297\n",
      "Test set for fold1: Average Loss:           1.3451, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 62 [0/37476                 (0%)]\tLoss: 0.003582\n",
      "Training stage for Flod 1 Epoch: 62 [3200/37476                 (11%)]\tLoss: 0.076973\n",
      "Training stage for Flod 1 Epoch: 62 [6400/37476                 (21%)]\tLoss: 0.017657\n",
      "Training stage for Flod 1 Epoch: 62 [9600/37476                 (32%)]\tLoss: 0.054567\n",
      "Training stage for Flod 1 Epoch: 62 [12800/37476                 (43%)]\tLoss: 0.004498\n",
      "Training stage for Flod 1 Epoch: 62 [16000/37476                 (53%)]\tLoss: 0.006280\n",
      "Training stage for Flod 1 Epoch: 62 [19200/37476                 (64%)]\tLoss: 0.002659\n",
      "Training stage for Flod 1 Epoch: 62 [22400/37476                 (75%)]\tLoss: 0.025342\n",
      "Training stage for Flod 1 Epoch: 62 [25600/37476                 (85%)]\tLoss: 0.084350\n",
      "Training stage for Flod 1 Epoch: 62 [28800/37476                 (96%)]\tLoss: 0.000214\n",
      "Test set for fold1: Average Loss:           1.5487, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 63 [0/37476                 (0%)]\tLoss: 0.000370\n",
      "Training stage for Flod 1 Epoch: 63 [3200/37476                 (11%)]\tLoss: 0.001487\n",
      "Training stage for Flod 1 Epoch: 63 [6400/37476                 (21%)]\tLoss: 0.028483\n",
      "Training stage for Flod 1 Epoch: 63 [9600/37476                 (32%)]\tLoss: 0.035390\n",
      "Training stage for Flod 1 Epoch: 63 [12800/37476                 (43%)]\tLoss: 0.006296\n",
      "Training stage for Flod 1 Epoch: 63 [16000/37476                 (53%)]\tLoss: 0.034232\n",
      "Training stage for Flod 1 Epoch: 63 [19200/37476                 (64%)]\tLoss: 0.048707\n",
      "Training stage for Flod 1 Epoch: 63 [22400/37476                 (75%)]\tLoss: 0.016368\n",
      "Training stage for Flod 1 Epoch: 63 [25600/37476                 (85%)]\tLoss: 0.030259\n",
      "Training stage for Flod 1 Epoch: 63 [28800/37476                 (96%)]\tLoss: 0.007060\n",
      "Test set for fold1: Average Loss:           1.4128, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 64 [0/37476                 (0%)]\tLoss: 0.033853\n",
      "Training stage for Flod 1 Epoch: 64 [3200/37476                 (11%)]\tLoss: 0.001693\n",
      "Training stage for Flod 1 Epoch: 64 [6400/37476                 (21%)]\tLoss: 0.054361\n",
      "Training stage for Flod 1 Epoch: 64 [9600/37476                 (32%)]\tLoss: 0.001273\n",
      "Training stage for Flod 1 Epoch: 64 [12800/37476                 (43%)]\tLoss: 0.006390\n",
      "Training stage for Flod 1 Epoch: 64 [16000/37476                 (53%)]\tLoss: 0.016330\n",
      "Training stage for Flod 1 Epoch: 64 [19200/37476                 (64%)]\tLoss: 0.005403\n",
      "Training stage for Flod 1 Epoch: 64 [22400/37476                 (75%)]\tLoss: 0.000450\n",
      "Training stage for Flod 1 Epoch: 64 [25600/37476                 (85%)]\tLoss: 0.005018\n",
      "Training stage for Flod 1 Epoch: 64 [28800/37476                 (96%)]\tLoss: 0.001217\n",
      "Test set for fold1: Average Loss:           1.5896, Accuracy: 14750/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 65 [0/37476                 (0%)]\tLoss: 0.001369\n",
      "Training stage for Flod 1 Epoch: 65 [3200/37476                 (11%)]\tLoss: 0.140930\n",
      "Training stage for Flod 1 Epoch: 65 [6400/37476                 (21%)]\tLoss: 0.009517\n",
      "Training stage for Flod 1 Epoch: 65 [9600/37476                 (32%)]\tLoss: 0.003639\n",
      "Training stage for Flod 1 Epoch: 65 [12800/37476                 (43%)]\tLoss: 0.009882\n",
      "Training stage for Flod 1 Epoch: 65 [16000/37476                 (53%)]\tLoss: 0.001118\n",
      "Training stage for Flod 1 Epoch: 65 [19200/37476                 (64%)]\tLoss: 0.018536\n",
      "Training stage for Flod 1 Epoch: 65 [22400/37476                 (75%)]\tLoss: 0.028915\n",
      "Training stage for Flod 1 Epoch: 65 [25600/37476                 (85%)]\tLoss: 0.010808\n",
      "Training stage for Flod 1 Epoch: 65 [28800/37476                 (96%)]\tLoss: 0.003069\n",
      "Test set for fold1: Average Loss:           1.3961, Accuracy: 14748/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 66 [0/37476                 (0%)]\tLoss: 0.039697\n",
      "Training stage for Flod 1 Epoch: 66 [3200/37476                 (11%)]\tLoss: 0.075762\n",
      "Training stage for Flod 1 Epoch: 66 [6400/37476                 (21%)]\tLoss: 0.043566\n",
      "Training stage for Flod 1 Epoch: 66 [9600/37476                 (32%)]\tLoss: 0.005578\n",
      "Training stage for Flod 1 Epoch: 66 [12800/37476                 (43%)]\tLoss: 0.005827\n",
      "Training stage for Flod 1 Epoch: 66 [16000/37476                 (53%)]\tLoss: 0.251313\n",
      "Training stage for Flod 1 Epoch: 66 [19200/37476                 (64%)]\tLoss: 0.003577\n",
      "Training stage for Flod 1 Epoch: 66 [22400/37476                 (75%)]\tLoss: 0.002105\n",
      "Training stage for Flod 1 Epoch: 66 [25600/37476                 (85%)]\tLoss: 0.054599\n",
      "Training stage for Flod 1 Epoch: 66 [28800/37476                 (96%)]\tLoss: 0.053513\n",
      "Test set for fold1: Average Loss:           1.5115, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 67 [0/37476                 (0%)]\tLoss: 0.002781\n",
      "Training stage for Flod 1 Epoch: 67 [3200/37476                 (11%)]\tLoss: 0.022790\n",
      "Training stage for Flod 1 Epoch: 67 [6400/37476                 (21%)]\tLoss: 0.000107\n",
      "Training stage for Flod 1 Epoch: 67 [9600/37476                 (32%)]\tLoss: 0.006806\n",
      "Training stage for Flod 1 Epoch: 67 [12800/37476                 (43%)]\tLoss: 0.070125\n",
      "Training stage for Flod 1 Epoch: 67 [16000/37476                 (53%)]\tLoss: 0.010261\n",
      "Training stage for Flod 1 Epoch: 67 [19200/37476                 (64%)]\tLoss: 0.014380\n",
      "Training stage for Flod 1 Epoch: 67 [22400/37476                 (75%)]\tLoss: 0.010238\n",
      "Training stage for Flod 1 Epoch: 67 [25600/37476                 (85%)]\tLoss: 0.017234\n",
      "Training stage for Flod 1 Epoch: 67 [28800/37476                 (96%)]\tLoss: 0.016035\n",
      "Test set for fold1: Average Loss:           1.2226, Accuracy: 14738/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 68 [0/37476                 (0%)]\tLoss: 0.067027\n",
      "Training stage for Flod 1 Epoch: 68 [3200/37476                 (11%)]\tLoss: 0.000133\n",
      "Training stage for Flod 1 Epoch: 68 [6400/37476                 (21%)]\tLoss: 0.023194\n",
      "Training stage for Flod 1 Epoch: 68 [9600/37476                 (32%)]\tLoss: 0.000664\n",
      "Training stage for Flod 1 Epoch: 68 [12800/37476                 (43%)]\tLoss: 0.000742\n",
      "Training stage for Flod 1 Epoch: 68 [16000/37476                 (53%)]\tLoss: 0.000742\n",
      "Training stage for Flod 1 Epoch: 68 [19200/37476                 (64%)]\tLoss: 0.008067\n",
      "Training stage for Flod 1 Epoch: 68 [22400/37476                 (75%)]\tLoss: 0.018028\n",
      "Training stage for Flod 1 Epoch: 68 [25600/37476                 (85%)]\tLoss: 0.013362\n",
      "Training stage for Flod 1 Epoch: 68 [28800/37476                 (96%)]\tLoss: 0.003452\n",
      "Test set for fold1: Average Loss:           1.3430, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 69 [0/37476                 (0%)]\tLoss: 0.005237\n",
      "Training stage for Flod 1 Epoch: 69 [3200/37476                 (11%)]\tLoss: 0.220286\n",
      "Training stage for Flod 1 Epoch: 69 [6400/37476                 (21%)]\tLoss: 0.001693\n",
      "Training stage for Flod 1 Epoch: 69 [9600/37476                 (32%)]\tLoss: 0.010970\n",
      "Training stage for Flod 1 Epoch: 69 [12800/37476                 (43%)]\tLoss: 0.012840\n",
      "Training stage for Flod 1 Epoch: 69 [16000/37476                 (53%)]\tLoss: 0.003889\n",
      "Training stage for Flod 1 Epoch: 69 [19200/37476                 (64%)]\tLoss: 0.116588\n",
      "Training stage for Flod 1 Epoch: 69 [22400/37476                 (75%)]\tLoss: 0.065924\n",
      "Training stage for Flod 1 Epoch: 69 [25600/37476                 (85%)]\tLoss: 0.185964\n",
      "Training stage for Flod 1 Epoch: 69 [28800/37476                 (96%)]\tLoss: 0.040310\n",
      "Test set for fold1: Average Loss:           1.2446, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 70 [0/37476                 (0%)]\tLoss: 0.017424\n",
      "Training stage for Flod 1 Epoch: 70 [3200/37476                 (11%)]\tLoss: 0.000138\n",
      "Training stage for Flod 1 Epoch: 70 [6400/37476                 (21%)]\tLoss: 0.045900\n",
      "Training stage for Flod 1 Epoch: 70 [9600/37476                 (32%)]\tLoss: 0.167545\n",
      "Training stage for Flod 1 Epoch: 70 [12800/37476                 (43%)]\tLoss: 0.006923\n",
      "Training stage for Flod 1 Epoch: 70 [16000/37476                 (53%)]\tLoss: 0.000796\n",
      "Training stage for Flod 1 Epoch: 70 [19200/37476                 (64%)]\tLoss: 0.022688\n",
      "Training stage for Flod 1 Epoch: 70 [22400/37476                 (75%)]\tLoss: 0.000463\n",
      "Training stage for Flod 1 Epoch: 70 [25600/37476                 (85%)]\tLoss: 0.000214\n",
      "Training stage for Flod 1 Epoch: 70 [28800/37476                 (96%)]\tLoss: 0.013977\n",
      "Test set for fold1: Average Loss:           1.5516, Accuracy: 14744/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 71 [0/37476                 (0%)]\tLoss: 0.001226\n",
      "Training stage for Flod 1 Epoch: 71 [3200/37476                 (11%)]\tLoss: 0.002578\n",
      "Training stage for Flod 1 Epoch: 71 [6400/37476                 (21%)]\tLoss: 0.170391\n",
      "Training stage for Flod 1 Epoch: 71 [9600/37476                 (32%)]\tLoss: 0.057760\n",
      "Training stage for Flod 1 Epoch: 71 [12800/37476                 (43%)]\tLoss: 0.091527\n",
      "Training stage for Flod 1 Epoch: 71 [16000/37476                 (53%)]\tLoss: 0.013469\n",
      "Training stage for Flod 1 Epoch: 71 [19200/37476                 (64%)]\tLoss: 0.043732\n",
      "Training stage for Flod 1 Epoch: 71 [22400/37476                 (75%)]\tLoss: 0.035244\n",
      "Training stage for Flod 1 Epoch: 71 [25600/37476                 (85%)]\tLoss: 0.002013\n",
      "Training stage for Flod 1 Epoch: 71 [28800/37476                 (96%)]\tLoss: 0.019835\n",
      "Test set for fold1: Average Loss:           1.5567, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 72 [0/37476                 (0%)]\tLoss: 0.004172\n",
      "Training stage for Flod 1 Epoch: 72 [3200/37476                 (11%)]\tLoss: 0.003518\n",
      "Training stage for Flod 1 Epoch: 72 [6400/37476                 (21%)]\tLoss: 0.002140\n",
      "Training stage for Flod 1 Epoch: 72 [9600/37476                 (32%)]\tLoss: 0.030077\n",
      "Training stage for Flod 1 Epoch: 72 [12800/37476                 (43%)]\tLoss: 0.030394\n",
      "Training stage for Flod 1 Epoch: 72 [16000/37476                 (53%)]\tLoss: 0.003977\n",
      "Training stage for Flod 1 Epoch: 72 [19200/37476                 (64%)]\tLoss: 0.019407\n",
      "Training stage for Flod 1 Epoch: 72 [22400/37476                 (75%)]\tLoss: 0.039515\n",
      "Training stage for Flod 1 Epoch: 72 [25600/37476                 (85%)]\tLoss: 0.003711\n",
      "Training stage for Flod 1 Epoch: 72 [28800/37476                 (96%)]\tLoss: 0.000258\n",
      "Test set for fold1: Average Loss:           1.4193, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 73 [0/37476                 (0%)]\tLoss: 0.008704\n",
      "Training stage for Flod 1 Epoch: 73 [3200/37476                 (11%)]\tLoss: 0.001509\n",
      "Training stage for Flod 1 Epoch: 73 [6400/37476                 (21%)]\tLoss: 0.019846\n",
      "Training stage for Flod 1 Epoch: 73 [9600/37476                 (32%)]\tLoss: 0.004363\n",
      "Training stage for Flod 1 Epoch: 73 [12800/37476                 (43%)]\tLoss: 0.002848\n",
      "Training stage for Flod 1 Epoch: 73 [16000/37476                 (53%)]\tLoss: 0.002498\n",
      "Training stage for Flod 1 Epoch: 73 [19200/37476                 (64%)]\tLoss: 0.002581\n",
      "Training stage for Flod 1 Epoch: 73 [22400/37476                 (75%)]\tLoss: 0.003147\n",
      "Training stage for Flod 1 Epoch: 73 [25600/37476                 (85%)]\tLoss: 0.024614\n",
      "Training stage for Flod 1 Epoch: 73 [28800/37476                 (96%)]\tLoss: 0.005059\n",
      "Test set for fold1: Average Loss:           1.4849, Accuracy: 14757/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 74 [0/37476                 (0%)]\tLoss: 0.082194\n",
      "Training stage for Flod 1 Epoch: 74 [3200/37476                 (11%)]\tLoss: 0.024201\n",
      "Training stage for Flod 1 Epoch: 74 [6400/37476                 (21%)]\tLoss: 0.005373\n",
      "Training stage for Flod 1 Epoch: 74 [9600/37476                 (32%)]\tLoss: 0.017643\n",
      "Training stage for Flod 1 Epoch: 74 [12800/37476                 (43%)]\tLoss: 0.007235\n",
      "Training stage for Flod 1 Epoch: 74 [16000/37476                 (53%)]\tLoss: 0.019777\n",
      "Training stage for Flod 1 Epoch: 74 [19200/37476                 (64%)]\tLoss: 0.008185\n",
      "Training stage for Flod 1 Epoch: 74 [22400/37476                 (75%)]\tLoss: 0.000163\n",
      "Training stage for Flod 1 Epoch: 74 [25600/37476                 (85%)]\tLoss: 0.006135\n",
      "Training stage for Flod 1 Epoch: 74 [28800/37476                 (96%)]\tLoss: 0.039021\n",
      "Test set for fold1: Average Loss:           1.4892, Accuracy: 14768/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 75 [0/37476                 (0%)]\tLoss: 0.000636\n",
      "Training stage for Flod 1 Epoch: 75 [3200/37476                 (11%)]\tLoss: 0.002684\n",
      "Training stage for Flod 1 Epoch: 75 [6400/37476                 (21%)]\tLoss: 0.000135\n",
      "Training stage for Flod 1 Epoch: 75 [9600/37476                 (32%)]\tLoss: 0.000273\n",
      "Training stage for Flod 1 Epoch: 75 [12800/37476                 (43%)]\tLoss: 0.018876\n",
      "Training stage for Flod 1 Epoch: 75 [16000/37476                 (53%)]\tLoss: 0.006985\n",
      "Training stage for Flod 1 Epoch: 75 [19200/37476                 (64%)]\tLoss: 0.000380\n",
      "Training stage for Flod 1 Epoch: 75 [22400/37476                 (75%)]\tLoss: 0.000534\n",
      "Training stage for Flod 1 Epoch: 75 [25600/37476                 (85%)]\tLoss: 0.023109\n",
      "Training stage for Flod 1 Epoch: 75 [28800/37476                 (96%)]\tLoss: 0.006306\n",
      "Test set for fold1: Average Loss:           1.8071, Accuracy: 14773/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 76 [0/37476                 (0%)]\tLoss: 0.001305\n",
      "Training stage for Flod 1 Epoch: 76 [3200/37476                 (11%)]\tLoss: 0.003036\n",
      "Training stage for Flod 1 Epoch: 76 [6400/37476                 (21%)]\tLoss: 0.017428\n",
      "Training stage for Flod 1 Epoch: 76 [9600/37476                 (32%)]\tLoss: 0.037354\n",
      "Training stage for Flod 1 Epoch: 76 [12800/37476                 (43%)]\tLoss: 0.005151\n",
      "Training stage for Flod 1 Epoch: 76 [16000/37476                 (53%)]\tLoss: 0.000054\n",
      "Training stage for Flod 1 Epoch: 76 [19200/37476                 (64%)]\tLoss: 0.198189\n",
      "Training stage for Flod 1 Epoch: 76 [22400/37476                 (75%)]\tLoss: 0.011871\n",
      "Training stage for Flod 1 Epoch: 76 [25600/37476                 (85%)]\tLoss: 0.039070\n",
      "Training stage for Flod 1 Epoch: 76 [28800/37476                 (96%)]\tLoss: 0.000345\n",
      "Test set for fold1: Average Loss:           1.4807, Accuracy: 14750/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 77 [0/37476                 (0%)]\tLoss: 0.000315\n",
      "Training stage for Flod 1 Epoch: 77 [3200/37476                 (11%)]\tLoss: 0.008624\n",
      "Training stage for Flod 1 Epoch: 77 [6400/37476                 (21%)]\tLoss: 0.092527\n",
      "Training stage for Flod 1 Epoch: 77 [9600/37476                 (32%)]\tLoss: 0.012999\n",
      "Training stage for Flod 1 Epoch: 77 [12800/37476                 (43%)]\tLoss: 0.027437\n",
      "Training stage for Flod 1 Epoch: 77 [16000/37476                 (53%)]\tLoss: 0.011605\n",
      "Training stage for Flod 1 Epoch: 77 [19200/37476                 (64%)]\tLoss: 0.000220\n",
      "Training stage for Flod 1 Epoch: 77 [22400/37476                 (75%)]\tLoss: 0.029977\n",
      "Training stage for Flod 1 Epoch: 77 [25600/37476                 (85%)]\tLoss: 0.001558\n",
      "Training stage for Flod 1 Epoch: 77 [28800/37476                 (96%)]\tLoss: 0.001688\n",
      "Test set for fold1: Average Loss:           1.4645, Accuracy: 14775/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 78 [0/37476                 (0%)]\tLoss: 0.037147\n",
      "Training stage for Flod 1 Epoch: 78 [3200/37476                 (11%)]\tLoss: 0.001330\n",
      "Training stage for Flod 1 Epoch: 78 [6400/37476                 (21%)]\tLoss: 0.043049\n",
      "Training stage for Flod 1 Epoch: 78 [9600/37476                 (32%)]\tLoss: 0.000172\n",
      "Training stage for Flod 1 Epoch: 78 [12800/37476                 (43%)]\tLoss: 0.005901\n",
      "Training stage for Flod 1 Epoch: 78 [16000/37476                 (53%)]\tLoss: 0.030729\n",
      "Training stage for Flod 1 Epoch: 78 [19200/37476                 (64%)]\tLoss: 0.000271\n",
      "Training stage for Flod 1 Epoch: 78 [22400/37476                 (75%)]\tLoss: 0.016531\n",
      "Training stage for Flod 1 Epoch: 78 [25600/37476                 (85%)]\tLoss: 0.019358\n",
      "Training stage for Flod 1 Epoch: 78 [28800/37476                 (96%)]\tLoss: 0.034066\n",
      "Test set for fold1: Average Loss:           1.5278, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 79 [0/37476                 (0%)]\tLoss: 0.017201\n",
      "Training stage for Flod 1 Epoch: 79 [3200/37476                 (11%)]\tLoss: 0.001077\n",
      "Training stage for Flod 1 Epoch: 79 [6400/37476                 (21%)]\tLoss: 0.068849\n",
      "Training stage for Flod 1 Epoch: 79 [9600/37476                 (32%)]\tLoss: 0.022529\n",
      "Training stage for Flod 1 Epoch: 79 [12800/37476                 (43%)]\tLoss: 0.010330\n",
      "Training stage for Flod 1 Epoch: 79 [16000/37476                 (53%)]\tLoss: 0.000076\n",
      "Training stage for Flod 1 Epoch: 79 [19200/37476                 (64%)]\tLoss: 0.006426\n",
      "Training stage for Flod 1 Epoch: 79 [22400/37476                 (75%)]\tLoss: 0.017312\n",
      "Training stage for Flod 1 Epoch: 79 [25600/37476                 (85%)]\tLoss: 0.063528\n",
      "Training stage for Flod 1 Epoch: 79 [28800/37476                 (96%)]\tLoss: 0.000706\n",
      "Test set for fold1: Average Loss:           1.4628, Accuracy: 14660/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 80 [0/37476                 (0%)]\tLoss: 0.012774\n",
      "Training stage for Flod 1 Epoch: 80 [3200/37476                 (11%)]\tLoss: 0.039273\n",
      "Training stage for Flod 1 Epoch: 80 [6400/37476                 (21%)]\tLoss: 0.027922\n",
      "Training stage for Flod 1 Epoch: 80 [9600/37476                 (32%)]\tLoss: 0.006127\n",
      "Training stage for Flod 1 Epoch: 80 [12800/37476                 (43%)]\tLoss: 0.005999\n",
      "Training stage for Flod 1 Epoch: 80 [16000/37476                 (53%)]\tLoss: 0.085425\n",
      "Training stage for Flod 1 Epoch: 80 [19200/37476                 (64%)]\tLoss: 0.000880\n",
      "Training stage for Flod 1 Epoch: 80 [22400/37476                 (75%)]\tLoss: 0.028191\n",
      "Training stage for Flod 1 Epoch: 80 [25600/37476                 (85%)]\tLoss: 0.012252\n",
      "Training stage for Flod 1 Epoch: 80 [28800/37476                 (96%)]\tLoss: 0.005327\n",
      "Test set for fold1: Average Loss:           1.4712, Accuracy: 14747/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 81 [0/37476                 (0%)]\tLoss: 0.000713\n",
      "Training stage for Flod 1 Epoch: 81 [3200/37476                 (11%)]\tLoss: 0.006443\n",
      "Training stage for Flod 1 Epoch: 81 [6400/37476                 (21%)]\tLoss: 0.003464\n",
      "Training stage for Flod 1 Epoch: 81 [9600/37476                 (32%)]\tLoss: 0.018137\n",
      "Training stage for Flod 1 Epoch: 81 [12800/37476                 (43%)]\tLoss: 0.008362\n",
      "Training stage for Flod 1 Epoch: 81 [16000/37476                 (53%)]\tLoss: 0.001769\n",
      "Training stage for Flod 1 Epoch: 81 [19200/37476                 (64%)]\tLoss: 0.008277\n",
      "Training stage for Flod 1 Epoch: 81 [22400/37476                 (75%)]\tLoss: 0.076653\n",
      "Training stage for Flod 1 Epoch: 81 [25600/37476                 (85%)]\tLoss: 0.002029\n",
      "Training stage for Flod 1 Epoch: 81 [28800/37476                 (96%)]\tLoss: 0.004559\n",
      "Test set for fold1: Average Loss:           1.3752, Accuracy: 14744/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 82 [0/37476                 (0%)]\tLoss: 0.027394\n",
      "Training stage for Flod 1 Epoch: 82 [3200/37476                 (11%)]\tLoss: 0.017102\n",
      "Training stage for Flod 1 Epoch: 82 [6400/37476                 (21%)]\tLoss: 0.012932\n",
      "Training stage for Flod 1 Epoch: 82 [9600/37476                 (32%)]\tLoss: 0.007076\n",
      "Training stage for Flod 1 Epoch: 82 [12800/37476                 (43%)]\tLoss: 0.004226\n",
      "Training stage for Flod 1 Epoch: 82 [16000/37476                 (53%)]\tLoss: 0.007517\n",
      "Training stage for Flod 1 Epoch: 82 [19200/37476                 (64%)]\tLoss: 0.000111\n",
      "Training stage for Flod 1 Epoch: 82 [22400/37476                 (75%)]\tLoss: 0.008195\n",
      "Training stage for Flod 1 Epoch: 82 [25600/37476                 (85%)]\tLoss: 0.090464\n",
      "Training stage for Flod 1 Epoch: 82 [28800/37476                 (96%)]\tLoss: 0.001523\n",
      "Test set for fold1: Average Loss:           1.4677, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 83 [0/37476                 (0%)]\tLoss: 0.041777\n",
      "Training stage for Flod 1 Epoch: 83 [3200/37476                 (11%)]\tLoss: 0.237587\n",
      "Training stage for Flod 1 Epoch: 83 [6400/37476                 (21%)]\tLoss: 0.000138\n",
      "Training stage for Flod 1 Epoch: 83 [9600/37476                 (32%)]\tLoss: 0.003342\n",
      "Training stage for Flod 1 Epoch: 83 [12800/37476                 (43%)]\tLoss: 0.003208\n",
      "Training stage for Flod 1 Epoch: 83 [16000/37476                 (53%)]\tLoss: 0.010198\n",
      "Training stage for Flod 1 Epoch: 83 [19200/37476                 (64%)]\tLoss: 0.000147\n",
      "Training stage for Flod 1 Epoch: 83 [22400/37476                 (75%)]\tLoss: 0.004641\n",
      "Training stage for Flod 1 Epoch: 83 [25600/37476                 (85%)]\tLoss: 0.006369\n",
      "Training stage for Flod 1 Epoch: 83 [28800/37476                 (96%)]\tLoss: 0.002281\n",
      "Test set for fold1: Average Loss:           1.4589, Accuracy: 14752/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 84 [0/37476                 (0%)]\tLoss: 0.008703\n",
      "Training stage for Flod 1 Epoch: 84 [3200/37476                 (11%)]\tLoss: 0.003465\n",
      "Training stage for Flod 1 Epoch: 84 [6400/37476                 (21%)]\tLoss: 0.000936\n",
      "Training stage for Flod 1 Epoch: 84 [9600/37476                 (32%)]\tLoss: 0.020876\n",
      "Training stage for Flod 1 Epoch: 84 [12800/37476                 (43%)]\tLoss: 0.030256\n",
      "Training stage for Flod 1 Epoch: 84 [16000/37476                 (53%)]\tLoss: 0.028902\n",
      "Training stage for Flod 1 Epoch: 84 [19200/37476                 (64%)]\tLoss: 0.006042\n",
      "Training stage for Flod 1 Epoch: 84 [22400/37476                 (75%)]\tLoss: 0.000051\n",
      "Training stage for Flod 1 Epoch: 84 [25600/37476                 (85%)]\tLoss: 0.010345\n",
      "Training stage for Flod 1 Epoch: 84 [28800/37476                 (96%)]\tLoss: 0.008623\n",
      "Test set for fold1: Average Loss:           1.7256, Accuracy: 14742/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 85 [0/37476                 (0%)]\tLoss: 0.002013\n",
      "Training stage for Flod 1 Epoch: 85 [3200/37476                 (11%)]\tLoss: 0.022522\n",
      "Training stage for Flod 1 Epoch: 85 [6400/37476                 (21%)]\tLoss: 0.044614\n",
      "Training stage for Flod 1 Epoch: 85 [9600/37476                 (32%)]\tLoss: 0.016441\n",
      "Training stage for Flod 1 Epoch: 85 [12800/37476                 (43%)]\tLoss: 0.008621\n",
      "Training stage for Flod 1 Epoch: 85 [16000/37476                 (53%)]\tLoss: 0.001584\n",
      "Training stage for Flod 1 Epoch: 85 [19200/37476                 (64%)]\tLoss: 0.002918\n",
      "Training stage for Flod 1 Epoch: 85 [22400/37476                 (75%)]\tLoss: 0.005868\n",
      "Training stage for Flod 1 Epoch: 85 [25600/37476                 (85%)]\tLoss: 0.004586\n",
      "Training stage for Flod 1 Epoch: 85 [28800/37476                 (96%)]\tLoss: 0.008373\n",
      "Test set for fold1: Average Loss:           1.9424, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 86 [0/37476                 (0%)]\tLoss: 0.000135\n",
      "Training stage for Flod 1 Epoch: 86 [3200/37476                 (11%)]\tLoss: 0.059276\n",
      "Training stage for Flod 1 Epoch: 86 [6400/37476                 (21%)]\tLoss: 0.026331\n",
      "Training stage for Flod 1 Epoch: 86 [9600/37476                 (32%)]\tLoss: 0.000041\n",
      "Training stage for Flod 1 Epoch: 86 [12800/37476                 (43%)]\tLoss: 0.005753\n",
      "Training stage for Flod 1 Epoch: 86 [16000/37476                 (53%)]\tLoss: 0.002774\n",
      "Training stage for Flod 1 Epoch: 86 [19200/37476                 (64%)]\tLoss: 0.018125\n",
      "Training stage for Flod 1 Epoch: 86 [22400/37476                 (75%)]\tLoss: 0.002482\n",
      "Training stage for Flod 1 Epoch: 86 [25600/37476                 (85%)]\tLoss: 0.021889\n",
      "Training stage for Flod 1 Epoch: 86 [28800/37476                 (96%)]\tLoss: 0.010967\n",
      "Test set for fold1: Average Loss:           1.7134, Accuracy: 14782/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 87 [0/37476                 (0%)]\tLoss: 0.000887\n",
      "Training stage for Flod 1 Epoch: 87 [3200/37476                 (11%)]\tLoss: 0.021600\n",
      "Training stage for Flod 1 Epoch: 87 [6400/37476                 (21%)]\tLoss: 0.000168\n",
      "Training stage for Flod 1 Epoch: 87 [9600/37476                 (32%)]\tLoss: 0.000695\n",
      "Training stage for Flod 1 Epoch: 87 [12800/37476                 (43%)]\tLoss: 0.006451\n",
      "Training stage for Flod 1 Epoch: 87 [16000/37476                 (53%)]\tLoss: 0.012411\n",
      "Training stage for Flod 1 Epoch: 87 [19200/37476                 (64%)]\tLoss: 0.011646\n",
      "Training stage for Flod 1 Epoch: 87 [22400/37476                 (75%)]\tLoss: 0.016264\n",
      "Training stage for Flod 1 Epoch: 87 [25600/37476                 (85%)]\tLoss: 0.031421\n",
      "Training stage for Flod 1 Epoch: 87 [28800/37476                 (96%)]\tLoss: 0.086502\n",
      "Test set for fold1: Average Loss:           1.5273, Accuracy: 14787/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 88 [0/37476                 (0%)]\tLoss: 0.006813\n",
      "Training stage for Flod 1 Epoch: 88 [3200/37476                 (11%)]\tLoss: 0.002563\n",
      "Training stage for Flod 1 Epoch: 88 [6400/37476                 (21%)]\tLoss: 0.000851\n",
      "Training stage for Flod 1 Epoch: 88 [9600/37476                 (32%)]\tLoss: 0.219674\n",
      "Training stage for Flod 1 Epoch: 88 [12800/37476                 (43%)]\tLoss: 0.043399\n",
      "Training stage for Flod 1 Epoch: 88 [16000/37476                 (53%)]\tLoss: 0.000017\n",
      "Training stage for Flod 1 Epoch: 88 [19200/37476                 (64%)]\tLoss: 0.000361\n",
      "Training stage for Flod 1 Epoch: 88 [22400/37476                 (75%)]\tLoss: 0.007514\n",
      "Training stage for Flod 1 Epoch: 88 [25600/37476                 (85%)]\tLoss: 0.059274\n",
      "Training stage for Flod 1 Epoch: 88 [28800/37476                 (96%)]\tLoss: 0.007248\n",
      "Test set for fold1: Average Loss:           2.0621, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 89 [0/37476                 (0%)]\tLoss: 0.000118\n",
      "Training stage for Flod 1 Epoch: 89 [3200/37476                 (11%)]\tLoss: 0.014937\n",
      "Training stage for Flod 1 Epoch: 89 [6400/37476                 (21%)]\tLoss: 0.012463\n",
      "Training stage for Flod 1 Epoch: 89 [9600/37476                 (32%)]\tLoss: 0.092411\n",
      "Training stage for Flod 1 Epoch: 89 [12800/37476                 (43%)]\tLoss: 0.033736\n",
      "Training stage for Flod 1 Epoch: 89 [16000/37476                 (53%)]\tLoss: 0.046796\n",
      "Training stage for Flod 1 Epoch: 89 [19200/37476                 (64%)]\tLoss: 0.086564\n",
      "Training stage for Flod 1 Epoch: 89 [22400/37476                 (75%)]\tLoss: 0.000778\n",
      "Training stage for Flod 1 Epoch: 89 [25600/37476                 (85%)]\tLoss: 0.000038\n",
      "Training stage for Flod 1 Epoch: 89 [28800/37476                 (96%)]\tLoss: 0.105729\n",
      "Test set for fold1: Average Loss:           1.8328, Accuracy: 14776/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 90 [0/37476                 (0%)]\tLoss: 0.013090\n",
      "Training stage for Flod 1 Epoch: 90 [3200/37476                 (11%)]\tLoss: 0.000430\n",
      "Training stage for Flod 1 Epoch: 90 [6400/37476                 (21%)]\tLoss: 0.001880\n",
      "Training stage for Flod 1 Epoch: 90 [9600/37476                 (32%)]\tLoss: 0.037818\n",
      "Training stage for Flod 1 Epoch: 90 [12800/37476                 (43%)]\tLoss: 0.024213\n",
      "Training stage for Flod 1 Epoch: 90 [16000/37476                 (53%)]\tLoss: 0.050606\n",
      "Training stage for Flod 1 Epoch: 90 [19200/37476                 (64%)]\tLoss: 0.002935\n",
      "Training stage for Flod 1 Epoch: 90 [22400/37476                 (75%)]\tLoss: 0.006890\n",
      "Training stage for Flod 1 Epoch: 90 [25600/37476                 (85%)]\tLoss: 0.058048\n",
      "Training stage for Flod 1 Epoch: 90 [28800/37476                 (96%)]\tLoss: 0.055920\n",
      "Test set for fold1: Average Loss:           1.6703, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 91 [0/37476                 (0%)]\tLoss: 0.004820\n",
      "Training stage for Flod 1 Epoch: 91 [3200/37476                 (11%)]\tLoss: 0.015316\n",
      "Training stage for Flod 1 Epoch: 91 [6400/37476                 (21%)]\tLoss: 0.005905\n",
      "Training stage for Flod 1 Epoch: 91 [9600/37476                 (32%)]\tLoss: 0.000673\n",
      "Training stage for Flod 1 Epoch: 91 [12800/37476                 (43%)]\tLoss: 0.003097\n",
      "Training stage for Flod 1 Epoch: 91 [16000/37476                 (53%)]\tLoss: 0.053727\n",
      "Training stage for Flod 1 Epoch: 91 [19200/37476                 (64%)]\tLoss: 0.000044\n",
      "Training stage for Flod 1 Epoch: 91 [22400/37476                 (75%)]\tLoss: 0.005985\n",
      "Training stage for Flod 1 Epoch: 91 [25600/37476                 (85%)]\tLoss: 0.001083\n",
      "Training stage for Flod 1 Epoch: 91 [28800/37476                 (96%)]\tLoss: 0.036934\n",
      "Test set for fold1: Average Loss:           1.5958, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 92 [0/37476                 (0%)]\tLoss: 0.046260\n",
      "Training stage for Flod 1 Epoch: 92 [3200/37476                 (11%)]\tLoss: 0.101954\n",
      "Training stage for Flod 1 Epoch: 92 [6400/37476                 (21%)]\tLoss: 0.039686\n",
      "Training stage for Flod 1 Epoch: 92 [9600/37476                 (32%)]\tLoss: 0.015164\n",
      "Training stage for Flod 1 Epoch: 92 [12800/37476                 (43%)]\tLoss: 0.057673\n",
      "Training stage for Flod 1 Epoch: 92 [16000/37476                 (53%)]\tLoss: 0.001501\n",
      "Training stage for Flod 1 Epoch: 92 [19200/37476                 (64%)]\tLoss: 0.003549\n",
      "Training stage for Flod 1 Epoch: 92 [22400/37476                 (75%)]\tLoss: 0.011867\n",
      "Training stage for Flod 1 Epoch: 92 [25600/37476                 (85%)]\tLoss: 0.140126\n",
      "Training stage for Flod 1 Epoch: 92 [28800/37476                 (96%)]\tLoss: 0.020608\n",
      "Test set for fold1: Average Loss:           1.8735, Accuracy: 14770/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 93 [0/37476                 (0%)]\tLoss: 0.023295\n",
      "Training stage for Flod 1 Epoch: 93 [3200/37476                 (11%)]\tLoss: 0.142631\n",
      "Training stage for Flod 1 Epoch: 93 [6400/37476                 (21%)]\tLoss: 0.008970\n",
      "Training stage for Flod 1 Epoch: 93 [9600/37476                 (32%)]\tLoss: 0.014410\n",
      "Training stage for Flod 1 Epoch: 93 [12800/37476                 (43%)]\tLoss: 0.061425\n",
      "Training stage for Flod 1 Epoch: 93 [16000/37476                 (53%)]\tLoss: 0.424848\n",
      "Training stage for Flod 1 Epoch: 93 [19200/37476                 (64%)]\tLoss: 0.001863\n",
      "Training stage for Flod 1 Epoch: 93 [22400/37476                 (75%)]\tLoss: 0.005887\n",
      "Training stage for Flod 1 Epoch: 93 [25600/37476                 (85%)]\tLoss: 0.046966\n",
      "Training stage for Flod 1 Epoch: 93 [28800/37476                 (96%)]\tLoss: 0.001017\n",
      "Test set for fold1: Average Loss:           1.6382, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 94 [0/37476                 (0%)]\tLoss: 0.055553\n",
      "Training stage for Flod 1 Epoch: 94 [3200/37476                 (11%)]\tLoss: 0.024523\n",
      "Training stage for Flod 1 Epoch: 94 [6400/37476                 (21%)]\tLoss: 0.001611\n",
      "Training stage for Flod 1 Epoch: 94 [9600/37476                 (32%)]\tLoss: 0.007276\n",
      "Training stage for Flod 1 Epoch: 94 [12800/37476                 (43%)]\tLoss: 0.000277\n",
      "Training stage for Flod 1 Epoch: 94 [16000/37476                 (53%)]\tLoss: 0.146451\n",
      "Training stage for Flod 1 Epoch: 94 [19200/37476                 (64%)]\tLoss: 0.009084\n",
      "Training stage for Flod 1 Epoch: 94 [22400/37476                 (75%)]\tLoss: 0.009401\n",
      "Training stage for Flod 1 Epoch: 94 [25600/37476                 (85%)]\tLoss: 0.000404\n",
      "Training stage for Flod 1 Epoch: 94 [28800/37476                 (96%)]\tLoss: 0.011571\n",
      "Test set for fold1: Average Loss:           1.8340, Accuracy: 14735/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 95 [0/37476                 (0%)]\tLoss: 0.005956\n",
      "Training stage for Flod 1 Epoch: 95 [3200/37476                 (11%)]\tLoss: 0.100361\n",
      "Training stage for Flod 1 Epoch: 95 [6400/37476                 (21%)]\tLoss: 0.000382\n",
      "Training stage for Flod 1 Epoch: 95 [9600/37476                 (32%)]\tLoss: 0.002047\n",
      "Training stage for Flod 1 Epoch: 95 [12800/37476                 (43%)]\tLoss: 0.039378\n",
      "Training stage for Flod 1 Epoch: 95 [16000/37476                 (53%)]\tLoss: 0.014008\n",
      "Training stage for Flod 1 Epoch: 95 [19200/37476                 (64%)]\tLoss: 0.010807\n",
      "Training stage for Flod 1 Epoch: 95 [22400/37476                 (75%)]\tLoss: 0.010620\n",
      "Training stage for Flod 1 Epoch: 95 [25600/37476                 (85%)]\tLoss: 0.000384\n",
      "Training stage for Flod 1 Epoch: 95 [28800/37476                 (96%)]\tLoss: 0.010599\n",
      "Test set for fold1: Average Loss:           1.5249, Accuracy: 14752/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 96 [0/37476                 (0%)]\tLoss: 0.012123\n",
      "Training stage for Flod 1 Epoch: 96 [3200/37476                 (11%)]\tLoss: 0.003109\n",
      "Training stage for Flod 1 Epoch: 96 [6400/37476                 (21%)]\tLoss: 0.088011\n",
      "Training stage for Flod 1 Epoch: 96 [9600/37476                 (32%)]\tLoss: 0.059754\n",
      "Training stage for Flod 1 Epoch: 96 [12800/37476                 (43%)]\tLoss: 0.002777\n",
      "Training stage for Flod 1 Epoch: 96 [16000/37476                 (53%)]\tLoss: 0.001803\n",
      "Training stage for Flod 1 Epoch: 96 [19200/37476                 (64%)]\tLoss: 0.052867\n",
      "Training stage for Flod 1 Epoch: 96 [22400/37476                 (75%)]\tLoss: 0.115523\n",
      "Training stage for Flod 1 Epoch: 96 [25600/37476                 (85%)]\tLoss: 0.001403\n",
      "Training stage for Flod 1 Epoch: 96 [28800/37476                 (96%)]\tLoss: 0.007695\n",
      "Test set for fold1: Average Loss:           1.7151, Accuracy: 14740/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 97 [0/37476                 (0%)]\tLoss: 0.017923\n",
      "Training stage for Flod 1 Epoch: 97 [3200/37476                 (11%)]\tLoss: 0.014806\n",
      "Training stage for Flod 1 Epoch: 97 [6400/37476                 (21%)]\tLoss: 0.041219\n",
      "Training stage for Flod 1 Epoch: 97 [9600/37476                 (32%)]\tLoss: 0.024228\n",
      "Training stage for Flod 1 Epoch: 97 [12800/37476                 (43%)]\tLoss: 0.032125\n",
      "Training stage for Flod 1 Epoch: 97 [16000/37476                 (53%)]\tLoss: 0.009007\n",
      "Training stage for Flod 1 Epoch: 97 [19200/37476                 (64%)]\tLoss: 0.008694\n",
      "Training stage for Flod 1 Epoch: 97 [22400/37476                 (75%)]\tLoss: 0.007636\n",
      "Training stage for Flod 1 Epoch: 97 [25600/37476                 (85%)]\tLoss: 0.027457\n",
      "Training stage for Flod 1 Epoch: 97 [28800/37476                 (96%)]\tLoss: 0.003236\n",
      "Test set for fold1: Average Loss:           1.7378, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 98 [0/37476                 (0%)]\tLoss: 0.000786\n",
      "Training stage for Flod 1 Epoch: 98 [3200/37476                 (11%)]\tLoss: 0.002000\n",
      "Training stage for Flod 1 Epoch: 98 [6400/37476                 (21%)]\tLoss: 0.014226\n",
      "Training stage for Flod 1 Epoch: 98 [9600/37476                 (32%)]\tLoss: 0.024987\n",
      "Training stage for Flod 1 Epoch: 98 [12800/37476                 (43%)]\tLoss: 0.001369\n",
      "Training stage for Flod 1 Epoch: 98 [16000/37476                 (53%)]\tLoss: 0.025462\n",
      "Training stage for Flod 1 Epoch: 98 [19200/37476                 (64%)]\tLoss: 0.019595\n",
      "Training stage for Flod 1 Epoch: 98 [22400/37476                 (75%)]\tLoss: 0.041553\n",
      "Training stage for Flod 1 Epoch: 98 [25600/37476                 (85%)]\tLoss: 0.003765\n",
      "Training stage for Flod 1 Epoch: 98 [28800/37476                 (96%)]\tLoss: 0.012255\n",
      "Test set for fold1: Average Loss:           1.9100, Accuracy: 14724/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 99 [0/37476                 (0%)]\tLoss: 0.004954\n",
      "Training stage for Flod 1 Epoch: 99 [3200/37476                 (11%)]\tLoss: 0.020763\n",
      "Training stage for Flod 1 Epoch: 99 [6400/37476                 (21%)]\tLoss: 0.034713\n",
      "Training stage for Flod 1 Epoch: 99 [9600/37476                 (32%)]\tLoss: 0.000067\n",
      "Training stage for Flod 1 Epoch: 99 [12800/37476                 (43%)]\tLoss: 0.000504\n",
      "Training stage for Flod 1 Epoch: 99 [16000/37476                 (53%)]\tLoss: 0.001464\n",
      "Training stage for Flod 1 Epoch: 99 [19200/37476                 (64%)]\tLoss: 0.040320\n",
      "Training stage for Flod 1 Epoch: 99 [22400/37476                 (75%)]\tLoss: 0.104946\n",
      "Training stage for Flod 1 Epoch: 99 [25600/37476                 (85%)]\tLoss: 0.015426\n",
      "Training stage for Flod 1 Epoch: 99 [28800/37476                 (96%)]\tLoss: 0.117372\n",
      "Test set for fold1: Average Loss:           1.6593, Accuracy: 14792/37476           (39%)\n",
      "Training stage for Flod 1 Epoch: 100 [0/37476                 (0%)]\tLoss: 0.002843\n",
      "Training stage for Flod 1 Epoch: 100 [3200/37476                 (11%)]\tLoss: 0.001569\n",
      "Training stage for Flod 1 Epoch: 100 [6400/37476                 (21%)]\tLoss: 0.013463\n",
      "Training stage for Flod 1 Epoch: 100 [9600/37476                 (32%)]\tLoss: 0.012313\n",
      "Training stage for Flod 1 Epoch: 100 [12800/37476                 (43%)]\tLoss: 0.002044\n",
      "Training stage for Flod 1 Epoch: 100 [16000/37476                 (53%)]\tLoss: 0.004944\n",
      "Training stage for Flod 1 Epoch: 100 [19200/37476                 (64%)]\tLoss: 0.022170\n",
      "Training stage for Flod 1 Epoch: 100 [22400/37476                 (75%)]\tLoss: 0.018066\n",
      "Training stage for Flod 1 Epoch: 100 [25600/37476                 (85%)]\tLoss: 0.073631\n",
      "Training stage for Flod 1 Epoch: 100 [28800/37476                 (96%)]\tLoss: 0.018453\n",
      "Test set for fold1: Average Loss:           1.7448, Accuracy: 14777/37476           (39%)\n",
      "-------------------Fold 2-------------------\n",
      "Training stage for Flod 2 Epoch: 1 [0/37476                 (0%)]\tLoss: 0.968827\n",
      "Training stage for Flod 2 Epoch: 1 [3200/37476                 (11%)]\tLoss: 0.008571\n",
      "Training stage for Flod 2 Epoch: 1 [6400/37476                 (21%)]\tLoss: 0.129519\n",
      "Training stage for Flod 2 Epoch: 1 [9600/37476                 (32%)]\tLoss: 0.007888\n",
      "Training stage for Flod 2 Epoch: 1 [12800/37476                 (43%)]\tLoss: 0.020861\n",
      "Training stage for Flod 2 Epoch: 1 [16000/37476                 (53%)]\tLoss: 0.265058\n",
      "Training stage for Flod 2 Epoch: 1 [19200/37476                 (64%)]\tLoss: 0.012504\n",
      "Training stage for Flod 2 Epoch: 1 [22400/37476                 (75%)]\tLoss: 0.010832\n",
      "Training stage for Flod 2 Epoch: 1 [25600/37476                 (85%)]\tLoss: 0.016019\n",
      "Training stage for Flod 2 Epoch: 1 [28800/37476                 (96%)]\tLoss: 0.156668\n",
      "Test set for fold2: Average Loss:           0.7691, Accuracy: 14588/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 2 [0/37476                 (0%)]\tLoss: 0.088320\n",
      "Training stage for Flod 2 Epoch: 2 [3200/37476                 (11%)]\tLoss: 0.028308\n",
      "Training stage for Flod 2 Epoch: 2 [6400/37476                 (21%)]\tLoss: 0.067223\n",
      "Training stage for Flod 2 Epoch: 2 [9600/37476                 (32%)]\tLoss: 0.081683\n",
      "Training stage for Flod 2 Epoch: 2 [12800/37476                 (43%)]\tLoss: 0.082179\n",
      "Training stage for Flod 2 Epoch: 2 [16000/37476                 (53%)]\tLoss: 0.184993\n",
      "Training stage for Flod 2 Epoch: 2 [19200/37476                 (64%)]\tLoss: 0.025814\n",
      "Training stage for Flod 2 Epoch: 2 [22400/37476                 (75%)]\tLoss: 0.049552\n",
      "Training stage for Flod 2 Epoch: 2 [25600/37476                 (85%)]\tLoss: 0.076630\n",
      "Training stage for Flod 2 Epoch: 2 [28800/37476                 (96%)]\tLoss: 0.002494\n",
      "Test set for fold2: Average Loss:           0.8109, Accuracy: 14606/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 3 [0/37476                 (0%)]\tLoss: 0.005884\n",
      "Training stage for Flod 2 Epoch: 3 [3200/37476                 (11%)]\tLoss: 0.108441\n",
      "Training stage for Flod 2 Epoch: 3 [6400/37476                 (21%)]\tLoss: 0.004581\n",
      "Training stage for Flod 2 Epoch: 3 [9600/37476                 (32%)]\tLoss: 0.123369\n",
      "Training stage for Flod 2 Epoch: 3 [12800/37476                 (43%)]\tLoss: 0.027012\n",
      "Training stage for Flod 2 Epoch: 3 [16000/37476                 (53%)]\tLoss: 0.051377\n",
      "Training stage for Flod 2 Epoch: 3 [19200/37476                 (64%)]\tLoss: 0.241687\n",
      "Training stage for Flod 2 Epoch: 3 [22400/37476                 (75%)]\tLoss: 0.001209\n",
      "Training stage for Flod 2 Epoch: 3 [25600/37476                 (85%)]\tLoss: 0.036213\n",
      "Training stage for Flod 2 Epoch: 3 [28800/37476                 (96%)]\tLoss: 0.001768\n",
      "Test set for fold2: Average Loss:           0.8032, Accuracy: 14622/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 4 [0/37476                 (0%)]\tLoss: 0.004188\n",
      "Training stage for Flod 2 Epoch: 4 [3200/37476                 (11%)]\tLoss: 0.017661\n",
      "Training stage for Flod 2 Epoch: 4 [6400/37476                 (21%)]\tLoss: 0.047715\n",
      "Training stage for Flod 2 Epoch: 4 [9600/37476                 (32%)]\tLoss: 0.104775\n",
      "Training stage for Flod 2 Epoch: 4 [12800/37476                 (43%)]\tLoss: 0.015370\n",
      "Training stage for Flod 2 Epoch: 4 [16000/37476                 (53%)]\tLoss: 0.046029\n",
      "Training stage for Flod 2 Epoch: 4 [19200/37476                 (64%)]\tLoss: 0.089659\n",
      "Training stage for Flod 2 Epoch: 4 [22400/37476                 (75%)]\tLoss: 0.043215\n",
      "Training stage for Flod 2 Epoch: 4 [25600/37476                 (85%)]\tLoss: 0.145482\n",
      "Training stage for Flod 2 Epoch: 4 [28800/37476                 (96%)]\tLoss: 0.037172\n",
      "Test set for fold2: Average Loss:           0.8104, Accuracy: 14601/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 5 [0/37476                 (0%)]\tLoss: 0.003559\n",
      "Training stage for Flod 2 Epoch: 5 [3200/37476                 (11%)]\tLoss: 0.362056\n",
      "Training stage for Flod 2 Epoch: 5 [6400/37476                 (21%)]\tLoss: 0.103769\n",
      "Training stage for Flod 2 Epoch: 5 [9600/37476                 (32%)]\tLoss: 0.220136\n",
      "Training stage for Flod 2 Epoch: 5 [12800/37476                 (43%)]\tLoss: 0.032643\n",
      "Training stage for Flod 2 Epoch: 5 [16000/37476                 (53%)]\tLoss: 0.011539\n",
      "Training stage for Flod 2 Epoch: 5 [19200/37476                 (64%)]\tLoss: 0.004012\n",
      "Training stage for Flod 2 Epoch: 5 [22400/37476                 (75%)]\tLoss: 0.011639\n",
      "Training stage for Flod 2 Epoch: 5 [25600/37476                 (85%)]\tLoss: 0.002239\n",
      "Training stage for Flod 2 Epoch: 5 [28800/37476                 (96%)]\tLoss: 0.014022\n",
      "Test set for fold2: Average Loss:           0.8409, Accuracy: 14712/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 6 [0/37476                 (0%)]\tLoss: 0.022988\n",
      "Training stage for Flod 2 Epoch: 6 [3200/37476                 (11%)]\tLoss: 0.000461\n",
      "Training stage for Flod 2 Epoch: 6 [6400/37476                 (21%)]\tLoss: 0.002118\n",
      "Training stage for Flod 2 Epoch: 6 [9600/37476                 (32%)]\tLoss: 0.015594\n",
      "Training stage for Flod 2 Epoch: 6 [12800/37476                 (43%)]\tLoss: 0.053964\n",
      "Training stage for Flod 2 Epoch: 6 [16000/37476                 (53%)]\tLoss: 0.008031\n",
      "Training stage for Flod 2 Epoch: 6 [19200/37476                 (64%)]\tLoss: 0.018909\n",
      "Training stage for Flod 2 Epoch: 6 [22400/37476                 (75%)]\tLoss: 0.212301\n",
      "Training stage for Flod 2 Epoch: 6 [25600/37476                 (85%)]\tLoss: 0.024457\n",
      "Training stage for Flod 2 Epoch: 6 [28800/37476                 (96%)]\tLoss: 0.001517\n",
      "Test set for fold2: Average Loss:           0.8970, Accuracy: 14662/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 7 [0/37476                 (0%)]\tLoss: 0.045490\n",
      "Training stage for Flod 2 Epoch: 7 [3200/37476                 (11%)]\tLoss: 0.080867\n",
      "Training stage for Flod 2 Epoch: 7 [6400/37476                 (21%)]\tLoss: 0.023628\n",
      "Training stage for Flod 2 Epoch: 7 [9600/37476                 (32%)]\tLoss: 0.035316\n",
      "Training stage for Flod 2 Epoch: 7 [12800/37476                 (43%)]\tLoss: 0.026391\n",
      "Training stage for Flod 2 Epoch: 7 [16000/37476                 (53%)]\tLoss: 0.166566\n",
      "Training stage for Flod 2 Epoch: 7 [19200/37476                 (64%)]\tLoss: 0.071828\n",
      "Training stage for Flod 2 Epoch: 7 [22400/37476                 (75%)]\tLoss: 0.061204\n",
      "Training stage for Flod 2 Epoch: 7 [25600/37476                 (85%)]\tLoss: 0.061145\n",
      "Training stage for Flod 2 Epoch: 7 [28800/37476                 (96%)]\tLoss: 0.029558\n",
      "Test set for fold2: Average Loss:           0.9441, Accuracy: 14628/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 8 [0/37476                 (0%)]\tLoss: 0.030085\n",
      "Training stage for Flod 2 Epoch: 8 [3200/37476                 (11%)]\tLoss: 0.044478\n",
      "Training stage for Flod 2 Epoch: 8 [6400/37476                 (21%)]\tLoss: 0.143191\n",
      "Training stage for Flod 2 Epoch: 8 [9600/37476                 (32%)]\tLoss: 0.110669\n",
      "Training stage for Flod 2 Epoch: 8 [12800/37476                 (43%)]\tLoss: 0.064275\n",
      "Training stage for Flod 2 Epoch: 8 [16000/37476                 (53%)]\tLoss: 0.067855\n",
      "Training stage for Flod 2 Epoch: 8 [19200/37476                 (64%)]\tLoss: 0.000425\n",
      "Training stage for Flod 2 Epoch: 8 [22400/37476                 (75%)]\tLoss: 0.063154\n",
      "Training stage for Flod 2 Epoch: 8 [25600/37476                 (85%)]\tLoss: 0.034825\n",
      "Training stage for Flod 2 Epoch: 8 [28800/37476                 (96%)]\tLoss: 0.121023\n",
      "Test set for fold2: Average Loss:           0.9023, Accuracy: 14653/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 9 [0/37476                 (0%)]\tLoss: 0.001800\n",
      "Training stage for Flod 2 Epoch: 9 [3200/37476                 (11%)]\tLoss: 0.041478\n",
      "Training stage for Flod 2 Epoch: 9 [6400/37476                 (21%)]\tLoss: 0.178922\n",
      "Training stage for Flod 2 Epoch: 9 [9600/37476                 (32%)]\tLoss: 0.007064\n",
      "Training stage for Flod 2 Epoch: 9 [12800/37476                 (43%)]\tLoss: 0.020270\n",
      "Training stage for Flod 2 Epoch: 9 [16000/37476                 (53%)]\tLoss: 0.000459\n",
      "Training stage for Flod 2 Epoch: 9 [19200/37476                 (64%)]\tLoss: 0.018398\n",
      "Training stage for Flod 2 Epoch: 9 [22400/37476                 (75%)]\tLoss: 0.009862\n",
      "Training stage for Flod 2 Epoch: 9 [25600/37476                 (85%)]\tLoss: 0.077660\n",
      "Training stage for Flod 2 Epoch: 9 [28800/37476                 (96%)]\tLoss: 0.069280\n",
      "Test set for fold2: Average Loss:           0.8887, Accuracy: 14491/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 10 [0/37476                 (0%)]\tLoss: 0.104615\n",
      "Training stage for Flod 2 Epoch: 10 [3200/37476                 (11%)]\tLoss: 0.056545\n",
      "Training stage for Flod 2 Epoch: 10 [6400/37476                 (21%)]\tLoss: 0.002844\n",
      "Training stage for Flod 2 Epoch: 10 [9600/37476                 (32%)]\tLoss: 0.163174\n",
      "Training stage for Flod 2 Epoch: 10 [12800/37476                 (43%)]\tLoss: 0.095911\n",
      "Training stage for Flod 2 Epoch: 10 [16000/37476                 (53%)]\tLoss: 0.076389\n",
      "Training stage for Flod 2 Epoch: 10 [19200/37476                 (64%)]\tLoss: 0.013383\n",
      "Training stage for Flod 2 Epoch: 10 [22400/37476                 (75%)]\tLoss: 0.023707\n",
      "Training stage for Flod 2 Epoch: 10 [25600/37476                 (85%)]\tLoss: 0.071185\n",
      "Training stage for Flod 2 Epoch: 10 [28800/37476                 (96%)]\tLoss: 0.004515\n",
      "Test set for fold2: Average Loss:           0.8608, Accuracy: 14721/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 11 [0/37476                 (0%)]\tLoss: 0.108988\n",
      "Training stage for Flod 2 Epoch: 11 [3200/37476                 (11%)]\tLoss: 0.071976\n",
      "Training stage for Flod 2 Epoch: 11 [6400/37476                 (21%)]\tLoss: 0.015798\n",
      "Training stage for Flod 2 Epoch: 11 [9600/37476                 (32%)]\tLoss: 0.040010\n",
      "Training stage for Flod 2 Epoch: 11 [12800/37476                 (43%)]\tLoss: 0.075979\n",
      "Training stage for Flod 2 Epoch: 11 [16000/37476                 (53%)]\tLoss: 0.002568\n",
      "Training stage for Flod 2 Epoch: 11 [19200/37476                 (64%)]\tLoss: 0.210365\n",
      "Training stage for Flod 2 Epoch: 11 [22400/37476                 (75%)]\tLoss: 0.001056\n",
      "Training stage for Flod 2 Epoch: 11 [25600/37476                 (85%)]\tLoss: 0.075632\n",
      "Training stage for Flod 2 Epoch: 11 [28800/37476                 (96%)]\tLoss: 0.109619\n",
      "Test set for fold2: Average Loss:           0.9089, Accuracy: 14658/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 12 [0/37476                 (0%)]\tLoss: 0.001042\n",
      "Training stage for Flod 2 Epoch: 12 [3200/37476                 (11%)]\tLoss: 0.038862\n",
      "Training stage for Flod 2 Epoch: 12 [6400/37476                 (21%)]\tLoss: 0.065199\n",
      "Training stage for Flod 2 Epoch: 12 [9600/37476                 (32%)]\tLoss: 0.174680\n",
      "Training stage for Flod 2 Epoch: 12 [12800/37476                 (43%)]\tLoss: 0.023553\n",
      "Training stage for Flod 2 Epoch: 12 [16000/37476                 (53%)]\tLoss: 0.001930\n",
      "Training stage for Flod 2 Epoch: 12 [19200/37476                 (64%)]\tLoss: 0.000620\n",
      "Training stage for Flod 2 Epoch: 12 [22400/37476                 (75%)]\tLoss: 0.355010\n",
      "Training stage for Flod 2 Epoch: 12 [25600/37476                 (85%)]\tLoss: 0.009715\n",
      "Training stage for Flod 2 Epoch: 12 [28800/37476                 (96%)]\tLoss: 0.066638\n",
      "Test set for fold2: Average Loss:           0.9293, Accuracy: 14713/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 13 [0/37476                 (0%)]\tLoss: 0.003965\n",
      "Training stage for Flod 2 Epoch: 13 [3200/37476                 (11%)]\tLoss: 0.025122\n",
      "Training stage for Flod 2 Epoch: 13 [6400/37476                 (21%)]\tLoss: 0.015195\n",
      "Training stage for Flod 2 Epoch: 13 [9600/37476                 (32%)]\tLoss: 0.000117\n",
      "Training stage for Flod 2 Epoch: 13 [12800/37476                 (43%)]\tLoss: 0.034858\n",
      "Training stage for Flod 2 Epoch: 13 [16000/37476                 (53%)]\tLoss: 0.012311\n",
      "Training stage for Flod 2 Epoch: 13 [19200/37476                 (64%)]\tLoss: 0.073545\n",
      "Training stage for Flod 2 Epoch: 13 [22400/37476                 (75%)]\tLoss: 0.009219\n",
      "Training stage for Flod 2 Epoch: 13 [25600/37476                 (85%)]\tLoss: 0.077541\n",
      "Training stage for Flod 2 Epoch: 13 [28800/37476                 (96%)]\tLoss: 0.032573\n",
      "Test set for fold2: Average Loss:           0.9292, Accuracy: 14713/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 14 [0/37476                 (0%)]\tLoss: 0.009523\n",
      "Training stage for Flod 2 Epoch: 14 [3200/37476                 (11%)]\tLoss: 0.019341\n",
      "Training stage for Flod 2 Epoch: 14 [6400/37476                 (21%)]\tLoss: 0.020278\n",
      "Training stage for Flod 2 Epoch: 14 [9600/37476                 (32%)]\tLoss: 0.043371\n",
      "Training stage for Flod 2 Epoch: 14 [12800/37476                 (43%)]\tLoss: 0.018761\n",
      "Training stage for Flod 2 Epoch: 14 [16000/37476                 (53%)]\tLoss: 0.039749\n",
      "Training stage for Flod 2 Epoch: 14 [19200/37476                 (64%)]\tLoss: 0.016321\n",
      "Training stage for Flod 2 Epoch: 14 [22400/37476                 (75%)]\tLoss: 0.089625\n",
      "Training stage for Flod 2 Epoch: 14 [25600/37476                 (85%)]\tLoss: 0.017711\n",
      "Training stage for Flod 2 Epoch: 14 [28800/37476                 (96%)]\tLoss: 0.027621\n",
      "Test set for fold2: Average Loss:           0.9354, Accuracy: 14739/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 15 [0/37476                 (0%)]\tLoss: 0.114073\n",
      "Training stage for Flod 2 Epoch: 15 [3200/37476                 (11%)]\tLoss: 0.105663\n",
      "Training stage for Flod 2 Epoch: 15 [6400/37476                 (21%)]\tLoss: 0.094310\n",
      "Training stage for Flod 2 Epoch: 15 [9600/37476                 (32%)]\tLoss: 0.190397\n",
      "Training stage for Flod 2 Epoch: 15 [12800/37476                 (43%)]\tLoss: 0.018620\n",
      "Training stage for Flod 2 Epoch: 15 [16000/37476                 (53%)]\tLoss: 0.001795\n",
      "Training stage for Flod 2 Epoch: 15 [19200/37476                 (64%)]\tLoss: 0.053356\n",
      "Training stage for Flod 2 Epoch: 15 [22400/37476                 (75%)]\tLoss: 0.077916\n",
      "Training stage for Flod 2 Epoch: 15 [25600/37476                 (85%)]\tLoss: 0.010827\n",
      "Training stage for Flod 2 Epoch: 15 [28800/37476                 (96%)]\tLoss: 0.042182\n",
      "Test set for fold2: Average Loss:           1.0037, Accuracy: 14701/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 16 [0/37476                 (0%)]\tLoss: 0.001615\n",
      "Training stage for Flod 2 Epoch: 16 [3200/37476                 (11%)]\tLoss: 0.003086\n",
      "Training stage for Flod 2 Epoch: 16 [6400/37476                 (21%)]\tLoss: 0.026266\n",
      "Training stage for Flod 2 Epoch: 16 [9600/37476                 (32%)]\tLoss: 0.067854\n",
      "Training stage for Flod 2 Epoch: 16 [12800/37476                 (43%)]\tLoss: 0.014105\n",
      "Training stage for Flod 2 Epoch: 16 [16000/37476                 (53%)]\tLoss: 0.095541\n",
      "Training stage for Flod 2 Epoch: 16 [19200/37476                 (64%)]\tLoss: 0.005775\n",
      "Training stage for Flod 2 Epoch: 16 [22400/37476                 (75%)]\tLoss: 0.081280\n",
      "Training stage for Flod 2 Epoch: 16 [25600/37476                 (85%)]\tLoss: 0.062626\n",
      "Training stage for Flod 2 Epoch: 16 [28800/37476                 (96%)]\tLoss: 0.005103\n",
      "Test set for fold2: Average Loss:           1.0140, Accuracy: 14650/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 17 [0/37476                 (0%)]\tLoss: 0.072489\n",
      "Training stage for Flod 2 Epoch: 17 [3200/37476                 (11%)]\tLoss: 0.016169\n",
      "Training stage for Flod 2 Epoch: 17 [6400/37476                 (21%)]\tLoss: 0.000063\n",
      "Training stage for Flod 2 Epoch: 17 [9600/37476                 (32%)]\tLoss: 0.005798\n",
      "Training stage for Flod 2 Epoch: 17 [12800/37476                 (43%)]\tLoss: 0.002919\n",
      "Training stage for Flod 2 Epoch: 17 [16000/37476                 (53%)]\tLoss: 0.088204\n",
      "Training stage for Flod 2 Epoch: 17 [19200/37476                 (64%)]\tLoss: 0.032178\n",
      "Training stage for Flod 2 Epoch: 17 [22400/37476                 (75%)]\tLoss: 0.033334\n",
      "Training stage for Flod 2 Epoch: 17 [25600/37476                 (85%)]\tLoss: 0.000396\n",
      "Training stage for Flod 2 Epoch: 17 [28800/37476                 (96%)]\tLoss: 0.063344\n",
      "Test set for fold2: Average Loss:           0.9205, Accuracy: 14740/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 18 [0/37476                 (0%)]\tLoss: 0.042437\n",
      "Training stage for Flod 2 Epoch: 18 [3200/37476                 (11%)]\tLoss: 0.065651\n",
      "Training stage for Flod 2 Epoch: 18 [6400/37476                 (21%)]\tLoss: 0.045871\n",
      "Training stage for Flod 2 Epoch: 18 [9600/37476                 (32%)]\tLoss: 0.196056\n",
      "Training stage for Flod 2 Epoch: 18 [12800/37476                 (43%)]\tLoss: 0.000650\n",
      "Training stage for Flod 2 Epoch: 18 [16000/37476                 (53%)]\tLoss: 0.258252\n",
      "Training stage for Flod 2 Epoch: 18 [19200/37476                 (64%)]\tLoss: 0.010198\n",
      "Training stage for Flod 2 Epoch: 18 [22400/37476                 (75%)]\tLoss: 0.033496\n",
      "Training stage for Flod 2 Epoch: 18 [25600/37476                 (85%)]\tLoss: 0.034595\n",
      "Training stage for Flod 2 Epoch: 18 [28800/37476                 (96%)]\tLoss: 0.038227\n",
      "Test set for fold2: Average Loss:           0.9927, Accuracy: 14735/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 19 [0/37476                 (0%)]\tLoss: 0.017295\n",
      "Training stage for Flod 2 Epoch: 19 [3200/37476                 (11%)]\tLoss: 0.001493\n",
      "Training stage for Flod 2 Epoch: 19 [6400/37476                 (21%)]\tLoss: 0.022268\n",
      "Training stage for Flod 2 Epoch: 19 [9600/37476                 (32%)]\tLoss: 0.011641\n",
      "Training stage for Flod 2 Epoch: 19 [12800/37476                 (43%)]\tLoss: 0.046632\n",
      "Training stage for Flod 2 Epoch: 19 [16000/37476                 (53%)]\tLoss: 0.001781\n",
      "Training stage for Flod 2 Epoch: 19 [19200/37476                 (64%)]\tLoss: 0.027595\n",
      "Training stage for Flod 2 Epoch: 19 [22400/37476                 (75%)]\tLoss: 0.038260\n",
      "Training stage for Flod 2 Epoch: 19 [25600/37476                 (85%)]\tLoss: 0.087542\n",
      "Training stage for Flod 2 Epoch: 19 [28800/37476                 (96%)]\tLoss: 0.010572\n",
      "Test set for fold2: Average Loss:           1.1122, Accuracy: 14714/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 20 [0/37476                 (0%)]\tLoss: 0.079384\n",
      "Training stage for Flod 2 Epoch: 20 [3200/37476                 (11%)]\tLoss: 0.033874\n",
      "Training stage for Flod 2 Epoch: 20 [6400/37476                 (21%)]\tLoss: 0.076740\n",
      "Training stage for Flod 2 Epoch: 20 [9600/37476                 (32%)]\tLoss: 0.147933\n",
      "Training stage for Flod 2 Epoch: 20 [12800/37476                 (43%)]\tLoss: 0.013014\n",
      "Training stage for Flod 2 Epoch: 20 [16000/37476                 (53%)]\tLoss: 0.017203\n",
      "Training stage for Flod 2 Epoch: 20 [19200/37476                 (64%)]\tLoss: 0.000707\n",
      "Training stage for Flod 2 Epoch: 20 [22400/37476                 (75%)]\tLoss: 0.030519\n",
      "Training stage for Flod 2 Epoch: 20 [25600/37476                 (85%)]\tLoss: 0.036752\n",
      "Training stage for Flod 2 Epoch: 20 [28800/37476                 (96%)]\tLoss: 0.004990\n",
      "Test set for fold2: Average Loss:           1.0346, Accuracy: 14729/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 21 [0/37476                 (0%)]\tLoss: 0.021571\n",
      "Training stage for Flod 2 Epoch: 21 [3200/37476                 (11%)]\tLoss: 0.040027\n",
      "Training stage for Flod 2 Epoch: 21 [6400/37476                 (21%)]\tLoss: 0.019435\n",
      "Training stage for Flod 2 Epoch: 21 [9600/37476                 (32%)]\tLoss: 0.106579\n",
      "Training stage for Flod 2 Epoch: 21 [12800/37476                 (43%)]\tLoss: 0.011216\n",
      "Training stage for Flod 2 Epoch: 21 [16000/37476                 (53%)]\tLoss: 0.042700\n",
      "Training stage for Flod 2 Epoch: 21 [19200/37476                 (64%)]\tLoss: 0.208973\n",
      "Training stage for Flod 2 Epoch: 21 [22400/37476                 (75%)]\tLoss: 0.019471\n",
      "Training stage for Flod 2 Epoch: 21 [25600/37476                 (85%)]\tLoss: 0.001234\n",
      "Training stage for Flod 2 Epoch: 21 [28800/37476                 (96%)]\tLoss: 0.053292\n",
      "Test set for fold2: Average Loss:           0.9762, Accuracy: 14683/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 22 [0/37476                 (0%)]\tLoss: 0.021003\n",
      "Training stage for Flod 2 Epoch: 22 [3200/37476                 (11%)]\tLoss: 0.048023\n",
      "Training stage for Flod 2 Epoch: 22 [6400/37476                 (21%)]\tLoss: 0.017658\n",
      "Training stage for Flod 2 Epoch: 22 [9600/37476                 (32%)]\tLoss: 0.030560\n",
      "Training stage for Flod 2 Epoch: 22 [12800/37476                 (43%)]\tLoss: 0.043987\n",
      "Training stage for Flod 2 Epoch: 22 [16000/37476                 (53%)]\tLoss: 0.012574\n",
      "Training stage for Flod 2 Epoch: 22 [19200/37476                 (64%)]\tLoss: 0.087076\n",
      "Training stage for Flod 2 Epoch: 22 [22400/37476                 (75%)]\tLoss: 0.008941\n",
      "Training stage for Flod 2 Epoch: 22 [25600/37476                 (85%)]\tLoss: 0.045990\n",
      "Training stage for Flod 2 Epoch: 22 [28800/37476                 (96%)]\tLoss: 0.007201\n",
      "Test set for fold2: Average Loss:           0.9068, Accuracy: 14715/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 23 [0/37476                 (0%)]\tLoss: 0.072324\n",
      "Training stage for Flod 2 Epoch: 23 [3200/37476                 (11%)]\tLoss: 0.020779\n",
      "Training stage for Flod 2 Epoch: 23 [6400/37476                 (21%)]\tLoss: 0.199196\n",
      "Training stage for Flod 2 Epoch: 23 [9600/37476                 (32%)]\tLoss: 0.037557\n",
      "Training stage for Flod 2 Epoch: 23 [12800/37476                 (43%)]\tLoss: 0.128193\n",
      "Training stage for Flod 2 Epoch: 23 [16000/37476                 (53%)]\tLoss: 0.020779\n",
      "Training stage for Flod 2 Epoch: 23 [19200/37476                 (64%)]\tLoss: 0.351462\n",
      "Training stage for Flod 2 Epoch: 23 [22400/37476                 (75%)]\tLoss: 0.075125\n",
      "Training stage for Flod 2 Epoch: 23 [25600/37476                 (85%)]\tLoss: 0.064952\n",
      "Training stage for Flod 2 Epoch: 23 [28800/37476                 (96%)]\tLoss: 0.005612\n",
      "Test set for fold2: Average Loss:           1.0468, Accuracy: 14733/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 24 [0/37476                 (0%)]\tLoss: 0.000285\n",
      "Training stage for Flod 2 Epoch: 24 [3200/37476                 (11%)]\tLoss: 0.006158\n",
      "Training stage for Flod 2 Epoch: 24 [6400/37476                 (21%)]\tLoss: 0.059256\n",
      "Training stage for Flod 2 Epoch: 24 [9600/37476                 (32%)]\tLoss: 0.014987\n",
      "Training stage for Flod 2 Epoch: 24 [12800/37476                 (43%)]\tLoss: 0.003488\n",
      "Training stage for Flod 2 Epoch: 24 [16000/37476                 (53%)]\tLoss: 0.056187\n",
      "Training stage for Flod 2 Epoch: 24 [19200/37476                 (64%)]\tLoss: 0.003252\n",
      "Training stage for Flod 2 Epoch: 24 [22400/37476                 (75%)]\tLoss: 0.006522\n",
      "Training stage for Flod 2 Epoch: 24 [25600/37476                 (85%)]\tLoss: 0.032081\n",
      "Training stage for Flod 2 Epoch: 24 [28800/37476                 (96%)]\tLoss: 0.009462\n",
      "Test set for fold2: Average Loss:           0.9905, Accuracy: 14724/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 25 [0/37476                 (0%)]\tLoss: 0.010644\n",
      "Training stage for Flod 2 Epoch: 25 [3200/37476                 (11%)]\tLoss: 0.152389\n",
      "Training stage for Flod 2 Epoch: 25 [6400/37476                 (21%)]\tLoss: 0.035091\n",
      "Training stage for Flod 2 Epoch: 25 [9600/37476                 (32%)]\tLoss: 0.014629\n",
      "Training stage for Flod 2 Epoch: 25 [12800/37476                 (43%)]\tLoss: 0.013707\n",
      "Training stage for Flod 2 Epoch: 25 [16000/37476                 (53%)]\tLoss: 0.000947\n",
      "Training stage for Flod 2 Epoch: 25 [19200/37476                 (64%)]\tLoss: 0.001595\n",
      "Training stage for Flod 2 Epoch: 25 [22400/37476                 (75%)]\tLoss: 0.000295\n",
      "Training stage for Flod 2 Epoch: 25 [25600/37476                 (85%)]\tLoss: 0.094293\n",
      "Training stage for Flod 2 Epoch: 25 [28800/37476                 (96%)]\tLoss: 0.009634\n",
      "Test set for fold2: Average Loss:           1.0911, Accuracy: 14752/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 26 [0/37476                 (0%)]\tLoss: 0.009834\n",
      "Training stage for Flod 2 Epoch: 26 [3200/37476                 (11%)]\tLoss: 0.011543\n",
      "Training stage for Flod 2 Epoch: 26 [6400/37476                 (21%)]\tLoss: 0.033451\n",
      "Training stage for Flod 2 Epoch: 26 [9600/37476                 (32%)]\tLoss: 0.019254\n",
      "Training stage for Flod 2 Epoch: 26 [12800/37476                 (43%)]\tLoss: 0.007715\n",
      "Training stage for Flod 2 Epoch: 26 [16000/37476                 (53%)]\tLoss: 0.069614\n",
      "Training stage for Flod 2 Epoch: 26 [19200/37476                 (64%)]\tLoss: 0.026411\n",
      "Training stage for Flod 2 Epoch: 26 [22400/37476                 (75%)]\tLoss: 0.006097\n",
      "Training stage for Flod 2 Epoch: 26 [25600/37476                 (85%)]\tLoss: 0.708568\n",
      "Training stage for Flod 2 Epoch: 26 [28800/37476                 (96%)]\tLoss: 0.032550\n",
      "Test set for fold2: Average Loss:           1.0564, Accuracy: 14732/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 27 [0/37476                 (0%)]\tLoss: 0.047374\n",
      "Training stage for Flod 2 Epoch: 27 [3200/37476                 (11%)]\tLoss: 0.127720\n",
      "Training stage for Flod 2 Epoch: 27 [6400/37476                 (21%)]\tLoss: 0.020906\n",
      "Training stage for Flod 2 Epoch: 27 [9600/37476                 (32%)]\tLoss: 0.047695\n",
      "Training stage for Flod 2 Epoch: 27 [12800/37476                 (43%)]\tLoss: 0.016352\n",
      "Training stage for Flod 2 Epoch: 27 [16000/37476                 (53%)]\tLoss: 0.106109\n",
      "Training stage for Flod 2 Epoch: 27 [19200/37476                 (64%)]\tLoss: 0.023508\n",
      "Training stage for Flod 2 Epoch: 27 [22400/37476                 (75%)]\tLoss: 0.032900\n",
      "Training stage for Flod 2 Epoch: 27 [25600/37476                 (85%)]\tLoss: 0.004026\n",
      "Training stage for Flod 2 Epoch: 27 [28800/37476                 (96%)]\tLoss: 0.004652\n",
      "Test set for fold2: Average Loss:           1.0953, Accuracy: 14588/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 28 [0/37476                 (0%)]\tLoss: 0.047145\n",
      "Training stage for Flod 2 Epoch: 28 [3200/37476                 (11%)]\tLoss: 0.011726\n",
      "Training stage for Flod 2 Epoch: 28 [6400/37476                 (21%)]\tLoss: 0.188622\n",
      "Training stage for Flod 2 Epoch: 28 [9600/37476                 (32%)]\tLoss: 0.004841\n",
      "Training stage for Flod 2 Epoch: 28 [12800/37476                 (43%)]\tLoss: 0.000261\n",
      "Training stage for Flod 2 Epoch: 28 [16000/37476                 (53%)]\tLoss: 0.043883\n",
      "Training stage for Flod 2 Epoch: 28 [19200/37476                 (64%)]\tLoss: 0.023840\n",
      "Training stage for Flod 2 Epoch: 28 [22400/37476                 (75%)]\tLoss: 0.252719\n",
      "Training stage for Flod 2 Epoch: 28 [25600/37476                 (85%)]\tLoss: 0.003463\n",
      "Training stage for Flod 2 Epoch: 28 [28800/37476                 (96%)]\tLoss: 0.008440\n",
      "Test set for fold2: Average Loss:           1.0292, Accuracy: 14720/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 29 [0/37476                 (0%)]\tLoss: 0.001252\n",
      "Training stage for Flod 2 Epoch: 29 [3200/37476                 (11%)]\tLoss: 0.034326\n",
      "Training stage for Flod 2 Epoch: 29 [6400/37476                 (21%)]\tLoss: 0.008006\n",
      "Training stage for Flod 2 Epoch: 29 [9600/37476                 (32%)]\tLoss: 0.099110\n",
      "Training stage for Flod 2 Epoch: 29 [12800/37476                 (43%)]\tLoss: 0.019877\n",
      "Training stage for Flod 2 Epoch: 29 [16000/37476                 (53%)]\tLoss: 0.015580\n",
      "Training stage for Flod 2 Epoch: 29 [19200/37476                 (64%)]\tLoss: 0.064874\n",
      "Training stage for Flod 2 Epoch: 29 [22400/37476                 (75%)]\tLoss: 0.004663\n",
      "Training stage for Flod 2 Epoch: 29 [25600/37476                 (85%)]\tLoss: 0.041935\n",
      "Training stage for Flod 2 Epoch: 29 [28800/37476                 (96%)]\tLoss: 0.000357\n",
      "Test set for fold2: Average Loss:           1.1013, Accuracy: 14752/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 30 [0/37476                 (0%)]\tLoss: 0.006210\n",
      "Training stage for Flod 2 Epoch: 30 [3200/37476                 (11%)]\tLoss: 0.053129\n",
      "Training stage for Flod 2 Epoch: 30 [6400/37476                 (21%)]\tLoss: 0.002671\n",
      "Training stage for Flod 2 Epoch: 30 [9600/37476                 (32%)]\tLoss: 0.102908\n",
      "Training stage for Flod 2 Epoch: 30 [12800/37476                 (43%)]\tLoss: 0.052906\n",
      "Training stage for Flod 2 Epoch: 30 [16000/37476                 (53%)]\tLoss: 0.002031\n",
      "Training stage for Flod 2 Epoch: 30 [19200/37476                 (64%)]\tLoss: 0.030485\n",
      "Training stage for Flod 2 Epoch: 30 [22400/37476                 (75%)]\tLoss: 0.017864\n",
      "Training stage for Flod 2 Epoch: 30 [25600/37476                 (85%)]\tLoss: 0.036979\n",
      "Training stage for Flod 2 Epoch: 30 [28800/37476                 (96%)]\tLoss: 0.021996\n",
      "Test set for fold2: Average Loss:           1.0189, Accuracy: 14758/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 31 [0/37476                 (0%)]\tLoss: 0.023004\n",
      "Training stage for Flod 2 Epoch: 31 [3200/37476                 (11%)]\tLoss: 0.010368\n",
      "Training stage for Flod 2 Epoch: 31 [6400/37476                 (21%)]\tLoss: 0.008211\n",
      "Training stage for Flod 2 Epoch: 31 [9600/37476                 (32%)]\tLoss: 0.014486\n",
      "Training stage for Flod 2 Epoch: 31 [12800/37476                 (43%)]\tLoss: 0.005873\n",
      "Training stage for Flod 2 Epoch: 31 [16000/37476                 (53%)]\tLoss: 0.001641\n",
      "Training stage for Flod 2 Epoch: 31 [19200/37476                 (64%)]\tLoss: 0.057412\n",
      "Training stage for Flod 2 Epoch: 31 [22400/37476                 (75%)]\tLoss: 0.002962\n",
      "Training stage for Flod 2 Epoch: 31 [25600/37476                 (85%)]\tLoss: 0.040052\n",
      "Training stage for Flod 2 Epoch: 31 [28800/37476                 (96%)]\tLoss: 0.085832\n",
      "Test set for fold2: Average Loss:           1.0007, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 32 [0/37476                 (0%)]\tLoss: 0.010504\n",
      "Training stage for Flod 2 Epoch: 32 [3200/37476                 (11%)]\tLoss: 0.008531\n",
      "Training stage for Flod 2 Epoch: 32 [6400/37476                 (21%)]\tLoss: 0.001683\n",
      "Training stage for Flod 2 Epoch: 32 [9600/37476                 (32%)]\tLoss: 0.094298\n",
      "Training stage for Flod 2 Epoch: 32 [12800/37476                 (43%)]\tLoss: 0.027066\n",
      "Training stage for Flod 2 Epoch: 32 [16000/37476                 (53%)]\tLoss: 0.038244\n",
      "Training stage for Flod 2 Epoch: 32 [19200/37476                 (64%)]\tLoss: 0.000565\n",
      "Training stage for Flod 2 Epoch: 32 [22400/37476                 (75%)]\tLoss: 0.017482\n",
      "Training stage for Flod 2 Epoch: 32 [25600/37476                 (85%)]\tLoss: 0.010289\n",
      "Training stage for Flod 2 Epoch: 32 [28800/37476                 (96%)]\tLoss: 0.076388\n",
      "Test set for fold2: Average Loss:           0.9332, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 33 [0/37476                 (0%)]\tLoss: 0.012003\n",
      "Training stage for Flod 2 Epoch: 33 [3200/37476                 (11%)]\tLoss: 0.014815\n",
      "Training stage for Flod 2 Epoch: 33 [6400/37476                 (21%)]\tLoss: 0.065529\n",
      "Training stage for Flod 2 Epoch: 33 [9600/37476                 (32%)]\tLoss: 0.003001\n",
      "Training stage for Flod 2 Epoch: 33 [12800/37476                 (43%)]\tLoss: 0.131327\n",
      "Training stage for Flod 2 Epoch: 33 [16000/37476                 (53%)]\tLoss: 0.030650\n",
      "Training stage for Flod 2 Epoch: 33 [19200/37476                 (64%)]\tLoss: 0.003901\n",
      "Training stage for Flod 2 Epoch: 33 [22400/37476                 (75%)]\tLoss: 0.012766\n",
      "Training stage for Flod 2 Epoch: 33 [25600/37476                 (85%)]\tLoss: 0.000616\n",
      "Training stage for Flod 2 Epoch: 33 [28800/37476                 (96%)]\tLoss: 0.012584\n",
      "Test set for fold2: Average Loss:           1.0094, Accuracy: 14743/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 34 [0/37476                 (0%)]\tLoss: 0.007624\n",
      "Training stage for Flod 2 Epoch: 34 [3200/37476                 (11%)]\tLoss: 0.007370\n",
      "Training stage for Flod 2 Epoch: 34 [6400/37476                 (21%)]\tLoss: 0.119793\n",
      "Training stage for Flod 2 Epoch: 34 [9600/37476                 (32%)]\tLoss: 0.018154\n",
      "Training stage for Flod 2 Epoch: 34 [12800/37476                 (43%)]\tLoss: 0.054352\n",
      "Training stage for Flod 2 Epoch: 34 [16000/37476                 (53%)]\tLoss: 0.000827\n",
      "Training stage for Flod 2 Epoch: 34 [19200/37476                 (64%)]\tLoss: 0.125900\n",
      "Training stage for Flod 2 Epoch: 34 [22400/37476                 (75%)]\tLoss: 0.021916\n",
      "Training stage for Flod 2 Epoch: 34 [25600/37476                 (85%)]\tLoss: 0.009403\n",
      "Training stage for Flod 2 Epoch: 34 [28800/37476                 (96%)]\tLoss: 0.021179\n",
      "Test set for fold2: Average Loss:           1.0044, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 35 [0/37476                 (0%)]\tLoss: 0.009190\n",
      "Training stage for Flod 2 Epoch: 35 [3200/37476                 (11%)]\tLoss: 0.054457\n",
      "Training stage for Flod 2 Epoch: 35 [6400/37476                 (21%)]\tLoss: 0.052764\n",
      "Training stage for Flod 2 Epoch: 35 [9600/37476                 (32%)]\tLoss: 0.010761\n",
      "Training stage for Flod 2 Epoch: 35 [12800/37476                 (43%)]\tLoss: 0.000629\n",
      "Training stage for Flod 2 Epoch: 35 [16000/37476                 (53%)]\tLoss: 0.052265\n",
      "Training stage for Flod 2 Epoch: 35 [19200/37476                 (64%)]\tLoss: 0.003964\n",
      "Training stage for Flod 2 Epoch: 35 [22400/37476                 (75%)]\tLoss: 0.017771\n",
      "Training stage for Flod 2 Epoch: 35 [25600/37476                 (85%)]\tLoss: 0.011460\n",
      "Training stage for Flod 2 Epoch: 35 [28800/37476                 (96%)]\tLoss: 0.103702\n",
      "Test set for fold2: Average Loss:           1.0254, Accuracy: 14681/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 36 [0/37476                 (0%)]\tLoss: 0.058561\n",
      "Training stage for Flod 2 Epoch: 36 [3200/37476                 (11%)]\tLoss: 0.084557\n",
      "Training stage for Flod 2 Epoch: 36 [6400/37476                 (21%)]\tLoss: 0.003680\n",
      "Training stage for Flod 2 Epoch: 36 [9600/37476                 (32%)]\tLoss: 0.041193\n",
      "Training stage for Flod 2 Epoch: 36 [12800/37476                 (43%)]\tLoss: 0.412842\n",
      "Training stage for Flod 2 Epoch: 36 [16000/37476                 (53%)]\tLoss: 0.001309\n",
      "Training stage for Flod 2 Epoch: 36 [19200/37476                 (64%)]\tLoss: 0.003632\n",
      "Training stage for Flod 2 Epoch: 36 [22400/37476                 (75%)]\tLoss: 0.005076\n",
      "Training stage for Flod 2 Epoch: 36 [25600/37476                 (85%)]\tLoss: 0.000526\n",
      "Training stage for Flod 2 Epoch: 36 [28800/37476                 (96%)]\tLoss: 0.002575\n",
      "Test set for fold2: Average Loss:           1.2456, Accuracy: 14769/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 37 [0/37476                 (0%)]\tLoss: 0.012976\n",
      "Training stage for Flod 2 Epoch: 37 [3200/37476                 (11%)]\tLoss: 0.058316\n",
      "Training stage for Flod 2 Epoch: 37 [6400/37476                 (21%)]\tLoss: 0.128178\n",
      "Training stage for Flod 2 Epoch: 37 [9600/37476                 (32%)]\tLoss: 0.044660\n",
      "Training stage for Flod 2 Epoch: 37 [12800/37476                 (43%)]\tLoss: 0.027559\n",
      "Training stage for Flod 2 Epoch: 37 [16000/37476                 (53%)]\tLoss: 0.022100\n",
      "Training stage for Flod 2 Epoch: 37 [19200/37476                 (64%)]\tLoss: 0.129910\n",
      "Training stage for Flod 2 Epoch: 37 [22400/37476                 (75%)]\tLoss: 0.063321\n",
      "Training stage for Flod 2 Epoch: 37 [25600/37476                 (85%)]\tLoss: 0.014504\n",
      "Training stage for Flod 2 Epoch: 37 [28800/37476                 (96%)]\tLoss: 0.075625\n",
      "Test set for fold2: Average Loss:           1.0747, Accuracy: 14745/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 38 [0/37476                 (0%)]\tLoss: 0.028316\n",
      "Training stage for Flod 2 Epoch: 38 [3200/37476                 (11%)]\tLoss: 0.003934\n",
      "Training stage for Flod 2 Epoch: 38 [6400/37476                 (21%)]\tLoss: 0.011706\n",
      "Training stage for Flod 2 Epoch: 38 [9600/37476                 (32%)]\tLoss: 0.075292\n",
      "Training stage for Flod 2 Epoch: 38 [12800/37476                 (43%)]\tLoss: 0.030586\n",
      "Training stage for Flod 2 Epoch: 38 [16000/37476                 (53%)]\tLoss: 0.010480\n",
      "Training stage for Flod 2 Epoch: 38 [19200/37476                 (64%)]\tLoss: 0.044163\n",
      "Training stage for Flod 2 Epoch: 38 [22400/37476                 (75%)]\tLoss: 0.010841\n",
      "Training stage for Flod 2 Epoch: 38 [25600/37476                 (85%)]\tLoss: 0.043000\n",
      "Training stage for Flod 2 Epoch: 38 [28800/37476                 (96%)]\tLoss: 0.013150\n",
      "Test set for fold2: Average Loss:           1.1086, Accuracy: 14738/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 39 [0/37476                 (0%)]\tLoss: 0.113912\n",
      "Training stage for Flod 2 Epoch: 39 [3200/37476                 (11%)]\tLoss: 0.063709\n",
      "Training stage for Flod 2 Epoch: 39 [6400/37476                 (21%)]\tLoss: 0.007342\n",
      "Training stage for Flod 2 Epoch: 39 [9600/37476                 (32%)]\tLoss: 0.000364\n",
      "Training stage for Flod 2 Epoch: 39 [12800/37476                 (43%)]\tLoss: 0.004250\n",
      "Training stage for Flod 2 Epoch: 39 [16000/37476                 (53%)]\tLoss: 0.069322\n",
      "Training stage for Flod 2 Epoch: 39 [19200/37476                 (64%)]\tLoss: 0.038443\n",
      "Training stage for Flod 2 Epoch: 39 [22400/37476                 (75%)]\tLoss: 0.111068\n",
      "Training stage for Flod 2 Epoch: 39 [25600/37476                 (85%)]\tLoss: 0.010141\n",
      "Training stage for Flod 2 Epoch: 39 [28800/37476                 (96%)]\tLoss: 0.033821\n",
      "Test set for fold2: Average Loss:           1.1037, Accuracy: 14741/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 40 [0/37476                 (0%)]\tLoss: 0.035342\n",
      "Training stage for Flod 2 Epoch: 40 [3200/37476                 (11%)]\tLoss: 0.110015\n",
      "Training stage for Flod 2 Epoch: 40 [6400/37476                 (21%)]\tLoss: 0.032951\n",
      "Training stage for Flod 2 Epoch: 40 [9600/37476                 (32%)]\tLoss: 0.007742\n",
      "Training stage for Flod 2 Epoch: 40 [12800/37476                 (43%)]\tLoss: 0.000478\n",
      "Training stage for Flod 2 Epoch: 40 [16000/37476                 (53%)]\tLoss: 0.000060\n",
      "Training stage for Flod 2 Epoch: 40 [19200/37476                 (64%)]\tLoss: 0.162670\n",
      "Training stage for Flod 2 Epoch: 40 [22400/37476                 (75%)]\tLoss: 0.072220\n",
      "Training stage for Flod 2 Epoch: 40 [25600/37476                 (85%)]\tLoss: 0.000632\n",
      "Training stage for Flod 2 Epoch: 40 [28800/37476                 (96%)]\tLoss: 0.029300\n",
      "Test set for fold2: Average Loss:           1.0675, Accuracy: 14770/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 41 [0/37476                 (0%)]\tLoss: 0.004962\n",
      "Training stage for Flod 2 Epoch: 41 [3200/37476                 (11%)]\tLoss: 0.009028\n",
      "Training stage for Flod 2 Epoch: 41 [6400/37476                 (21%)]\tLoss: 0.108889\n",
      "Training stage for Flod 2 Epoch: 41 [9600/37476                 (32%)]\tLoss: 0.037646\n",
      "Training stage for Flod 2 Epoch: 41 [12800/37476                 (43%)]\tLoss: 0.097034\n",
      "Training stage for Flod 2 Epoch: 41 [16000/37476                 (53%)]\tLoss: 0.003782\n",
      "Training stage for Flod 2 Epoch: 41 [19200/37476                 (64%)]\tLoss: 0.007972\n",
      "Training stage for Flod 2 Epoch: 41 [22400/37476                 (75%)]\tLoss: 0.074101\n",
      "Training stage for Flod 2 Epoch: 41 [25600/37476                 (85%)]\tLoss: 0.148342\n",
      "Training stage for Flod 2 Epoch: 41 [28800/37476                 (96%)]\tLoss: 0.119407\n",
      "Test set for fold2: Average Loss:           1.0931, Accuracy: 14709/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 42 [0/37476                 (0%)]\tLoss: 0.089404\n",
      "Training stage for Flod 2 Epoch: 42 [3200/37476                 (11%)]\tLoss: 0.108274\n",
      "Training stage for Flod 2 Epoch: 42 [6400/37476                 (21%)]\tLoss: 0.136432\n",
      "Training stage for Flod 2 Epoch: 42 [9600/37476                 (32%)]\tLoss: 0.019709\n",
      "Training stage for Flod 2 Epoch: 42 [12800/37476                 (43%)]\tLoss: 0.053755\n",
      "Training stage for Flod 2 Epoch: 42 [16000/37476                 (53%)]\tLoss: 0.100094\n",
      "Training stage for Flod 2 Epoch: 42 [19200/37476                 (64%)]\tLoss: 0.010178\n",
      "Training stage for Flod 2 Epoch: 42 [22400/37476                 (75%)]\tLoss: 0.010610\n",
      "Training stage for Flod 2 Epoch: 42 [25600/37476                 (85%)]\tLoss: 0.025241\n",
      "Training stage for Flod 2 Epoch: 42 [28800/37476                 (96%)]\tLoss: 0.003679\n",
      "Test set for fold2: Average Loss:           1.1041, Accuracy: 14769/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 43 [0/37476                 (0%)]\tLoss: 0.050310\n",
      "Training stage for Flod 2 Epoch: 43 [3200/37476                 (11%)]\tLoss: 0.012206\n",
      "Training stage for Flod 2 Epoch: 43 [6400/37476                 (21%)]\tLoss: 0.175765\n",
      "Training stage for Flod 2 Epoch: 43 [9600/37476                 (32%)]\tLoss: 0.076678\n",
      "Training stage for Flod 2 Epoch: 43 [12800/37476                 (43%)]\tLoss: 0.007298\n",
      "Training stage for Flod 2 Epoch: 43 [16000/37476                 (53%)]\tLoss: 0.039953\n",
      "Training stage for Flod 2 Epoch: 43 [19200/37476                 (64%)]\tLoss: 0.069945\n",
      "Training stage for Flod 2 Epoch: 43 [22400/37476                 (75%)]\tLoss: 0.002848\n",
      "Training stage for Flod 2 Epoch: 43 [25600/37476                 (85%)]\tLoss: 0.003811\n",
      "Training stage for Flod 2 Epoch: 43 [28800/37476                 (96%)]\tLoss: 0.004274\n",
      "Test set for fold2: Average Loss:           1.2090, Accuracy: 14702/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 44 [0/37476                 (0%)]\tLoss: 0.005572\n",
      "Training stage for Flod 2 Epoch: 44 [3200/37476                 (11%)]\tLoss: 0.007134\n",
      "Training stage for Flod 2 Epoch: 44 [6400/37476                 (21%)]\tLoss: 0.099172\n",
      "Training stage for Flod 2 Epoch: 44 [9600/37476                 (32%)]\tLoss: 0.003588\n",
      "Training stage for Flod 2 Epoch: 44 [12800/37476                 (43%)]\tLoss: 0.000647\n",
      "Training stage for Flod 2 Epoch: 44 [16000/37476                 (53%)]\tLoss: 0.003747\n",
      "Training stage for Flod 2 Epoch: 44 [19200/37476                 (64%)]\tLoss: 0.030338\n",
      "Training stage for Flod 2 Epoch: 44 [22400/37476                 (75%)]\tLoss: 0.015164\n",
      "Training stage for Flod 2 Epoch: 44 [25600/37476                 (85%)]\tLoss: 0.007975\n",
      "Training stage for Flod 2 Epoch: 44 [28800/37476                 (96%)]\tLoss: 0.066249\n",
      "Test set for fold2: Average Loss:           1.0498, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 45 [0/37476                 (0%)]\tLoss: 0.013966\n",
      "Training stage for Flod 2 Epoch: 45 [3200/37476                 (11%)]\tLoss: 0.001886\n",
      "Training stage for Flod 2 Epoch: 45 [6400/37476                 (21%)]\tLoss: 0.004380\n",
      "Training stage for Flod 2 Epoch: 45 [9600/37476                 (32%)]\tLoss: 0.007317\n",
      "Training stage for Flod 2 Epoch: 45 [12800/37476                 (43%)]\tLoss: 0.168182\n",
      "Training stage for Flod 2 Epoch: 45 [16000/37476                 (53%)]\tLoss: 0.014066\n",
      "Training stage for Flod 2 Epoch: 45 [19200/37476                 (64%)]\tLoss: 0.027779\n",
      "Training stage for Flod 2 Epoch: 45 [22400/37476                 (75%)]\tLoss: 0.032730\n",
      "Training stage for Flod 2 Epoch: 45 [25600/37476                 (85%)]\tLoss: 0.003300\n",
      "Training stage for Flod 2 Epoch: 45 [28800/37476                 (96%)]\tLoss: 0.017353\n",
      "Test set for fold2: Average Loss:           1.2137, Accuracy: 14780/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 46 [0/37476                 (0%)]\tLoss: 0.012821\n",
      "Training stage for Flod 2 Epoch: 46 [3200/37476                 (11%)]\tLoss: 0.086611\n",
      "Training stage for Flod 2 Epoch: 46 [6400/37476                 (21%)]\tLoss: 0.011319\n",
      "Training stage for Flod 2 Epoch: 46 [9600/37476                 (32%)]\tLoss: 0.001624\n",
      "Training stage for Flod 2 Epoch: 46 [12800/37476                 (43%)]\tLoss: 0.013966\n",
      "Training stage for Flod 2 Epoch: 46 [16000/37476                 (53%)]\tLoss: 0.079847\n",
      "Training stage for Flod 2 Epoch: 46 [19200/37476                 (64%)]\tLoss: 0.070475\n",
      "Training stage for Flod 2 Epoch: 46 [22400/37476                 (75%)]\tLoss: 0.097693\n",
      "Training stage for Flod 2 Epoch: 46 [25600/37476                 (85%)]\tLoss: 0.056900\n",
      "Training stage for Flod 2 Epoch: 46 [28800/37476                 (96%)]\tLoss: 0.073996\n",
      "Test set for fold2: Average Loss:           1.2694, Accuracy: 14727/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 47 [0/37476                 (0%)]\tLoss: 0.000252\n",
      "Training stage for Flod 2 Epoch: 47 [3200/37476                 (11%)]\tLoss: 0.068175\n",
      "Training stage for Flod 2 Epoch: 47 [6400/37476                 (21%)]\tLoss: 0.001967\n",
      "Training stage for Flod 2 Epoch: 47 [9600/37476                 (32%)]\tLoss: 0.002838\n",
      "Training stage for Flod 2 Epoch: 47 [12800/37476                 (43%)]\tLoss: 0.023658\n",
      "Training stage for Flod 2 Epoch: 47 [16000/37476                 (53%)]\tLoss: 0.017744\n",
      "Training stage for Flod 2 Epoch: 47 [19200/37476                 (64%)]\tLoss: 0.001798\n",
      "Training stage for Flod 2 Epoch: 47 [22400/37476                 (75%)]\tLoss: 0.022133\n",
      "Training stage for Flod 2 Epoch: 47 [25600/37476                 (85%)]\tLoss: 0.021622\n",
      "Training stage for Flod 2 Epoch: 47 [28800/37476                 (96%)]\tLoss: 0.004715\n",
      "Test set for fold2: Average Loss:           1.1404, Accuracy: 14758/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 48 [0/37476                 (0%)]\tLoss: 0.042923\n",
      "Training stage for Flod 2 Epoch: 48 [3200/37476                 (11%)]\tLoss: 0.000419\n",
      "Training stage for Flod 2 Epoch: 48 [6400/37476                 (21%)]\tLoss: 0.006778\n",
      "Training stage for Flod 2 Epoch: 48 [9600/37476                 (32%)]\tLoss: 0.038951\n",
      "Training stage for Flod 2 Epoch: 48 [12800/37476                 (43%)]\tLoss: 0.003843\n",
      "Training stage for Flod 2 Epoch: 48 [16000/37476                 (53%)]\tLoss: 0.043918\n",
      "Training stage for Flod 2 Epoch: 48 [19200/37476                 (64%)]\tLoss: 0.001524\n",
      "Training stage for Flod 2 Epoch: 48 [22400/37476                 (75%)]\tLoss: 0.003886\n",
      "Training stage for Flod 2 Epoch: 48 [25600/37476                 (85%)]\tLoss: 0.001222\n",
      "Training stage for Flod 2 Epoch: 48 [28800/37476                 (96%)]\tLoss: 0.001143\n",
      "Test set for fold2: Average Loss:           1.0685, Accuracy: 14717/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 49 [0/37476                 (0%)]\tLoss: 0.006471\n",
      "Training stage for Flod 2 Epoch: 49 [3200/37476                 (11%)]\tLoss: 0.053728\n",
      "Training stage for Flod 2 Epoch: 49 [6400/37476                 (21%)]\tLoss: 0.091800\n",
      "Training stage for Flod 2 Epoch: 49 [9600/37476                 (32%)]\tLoss: 0.000553\n",
      "Training stage for Flod 2 Epoch: 49 [12800/37476                 (43%)]\tLoss: 0.004091\n",
      "Training stage for Flod 2 Epoch: 49 [16000/37476                 (53%)]\tLoss: 0.034378\n",
      "Training stage for Flod 2 Epoch: 49 [19200/37476                 (64%)]\tLoss: 0.001588\n",
      "Training stage for Flod 2 Epoch: 49 [22400/37476                 (75%)]\tLoss: 0.017171\n",
      "Training stage for Flod 2 Epoch: 49 [25600/37476                 (85%)]\tLoss: 0.008502\n",
      "Training stage for Flod 2 Epoch: 49 [28800/37476                 (96%)]\tLoss: 0.085231\n",
      "Test set for fold2: Average Loss:           1.1215, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 50 [0/37476                 (0%)]\tLoss: 0.037291\n",
      "Training stage for Flod 2 Epoch: 50 [3200/37476                 (11%)]\tLoss: 0.011753\n",
      "Training stage for Flod 2 Epoch: 50 [6400/37476                 (21%)]\tLoss: 0.080514\n",
      "Training stage for Flod 2 Epoch: 50 [9600/37476                 (32%)]\tLoss: 0.000118\n",
      "Training stage for Flod 2 Epoch: 50 [12800/37476                 (43%)]\tLoss: 0.069641\n",
      "Training stage for Flod 2 Epoch: 50 [16000/37476                 (53%)]\tLoss: 0.002917\n",
      "Training stage for Flod 2 Epoch: 50 [19200/37476                 (64%)]\tLoss: 0.014445\n",
      "Training stage for Flod 2 Epoch: 50 [22400/37476                 (75%)]\tLoss: 0.034228\n",
      "Training stage for Flod 2 Epoch: 50 [25600/37476                 (85%)]\tLoss: 0.048553\n",
      "Training stage for Flod 2 Epoch: 50 [28800/37476                 (96%)]\tLoss: 0.065357\n",
      "Test set for fold2: Average Loss:           1.0785, Accuracy: 14721/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 51 [0/37476                 (0%)]\tLoss: 0.025004\n",
      "Training stage for Flod 2 Epoch: 51 [3200/37476                 (11%)]\tLoss: 0.018994\n",
      "Training stage for Flod 2 Epoch: 51 [6400/37476                 (21%)]\tLoss: 0.004958\n",
      "Training stage for Flod 2 Epoch: 51 [9600/37476                 (32%)]\tLoss: 0.001731\n",
      "Training stage for Flod 2 Epoch: 51 [12800/37476                 (43%)]\tLoss: 0.001084\n",
      "Training stage for Flod 2 Epoch: 51 [16000/37476                 (53%)]\tLoss: 0.017994\n",
      "Training stage for Flod 2 Epoch: 51 [19200/37476                 (64%)]\tLoss: 0.022817\n",
      "Training stage for Flod 2 Epoch: 51 [22400/37476                 (75%)]\tLoss: 0.021204\n",
      "Training stage for Flod 2 Epoch: 51 [25600/37476                 (85%)]\tLoss: 0.000167\n",
      "Training stage for Flod 2 Epoch: 51 [28800/37476                 (96%)]\tLoss: 0.000272\n",
      "Test set for fold2: Average Loss:           1.2300, Accuracy: 14700/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 52 [0/37476                 (0%)]\tLoss: 0.003306\n",
      "Training stage for Flod 2 Epoch: 52 [3200/37476                 (11%)]\tLoss: 0.025959\n",
      "Training stage for Flod 2 Epoch: 52 [6400/37476                 (21%)]\tLoss: 0.151521\n",
      "Training stage for Flod 2 Epoch: 52 [9600/37476                 (32%)]\tLoss: 0.052569\n",
      "Training stage for Flod 2 Epoch: 52 [12800/37476                 (43%)]\tLoss: 0.005388\n",
      "Training stage for Flod 2 Epoch: 52 [16000/37476                 (53%)]\tLoss: 0.011450\n",
      "Training stage for Flod 2 Epoch: 52 [19200/37476                 (64%)]\tLoss: 0.027704\n",
      "Training stage for Flod 2 Epoch: 52 [22400/37476                 (75%)]\tLoss: 0.013723\n",
      "Training stage for Flod 2 Epoch: 52 [25600/37476                 (85%)]\tLoss: 0.015919\n",
      "Training stage for Flod 2 Epoch: 52 [28800/37476                 (96%)]\tLoss: 0.030876\n",
      "Test set for fold2: Average Loss:           1.0687, Accuracy: 14800/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 53 [0/37476                 (0%)]\tLoss: 0.068960\n",
      "Training stage for Flod 2 Epoch: 53 [3200/37476                 (11%)]\tLoss: 0.021335\n",
      "Training stage for Flod 2 Epoch: 53 [6400/37476                 (21%)]\tLoss: 0.023258\n",
      "Training stage for Flod 2 Epoch: 53 [9600/37476                 (32%)]\tLoss: 0.198328\n",
      "Training stage for Flod 2 Epoch: 53 [12800/37476                 (43%)]\tLoss: 0.005606\n",
      "Training stage for Flod 2 Epoch: 53 [16000/37476                 (53%)]\tLoss: 0.047096\n",
      "Training stage for Flod 2 Epoch: 53 [19200/37476                 (64%)]\tLoss: 0.016522\n",
      "Training stage for Flod 2 Epoch: 53 [22400/37476                 (75%)]\tLoss: 0.047441\n",
      "Training stage for Flod 2 Epoch: 53 [25600/37476                 (85%)]\tLoss: 0.054836\n",
      "Training stage for Flod 2 Epoch: 53 [28800/37476                 (96%)]\tLoss: 0.013013\n",
      "Test set for fold2: Average Loss:           1.1532, Accuracy: 14775/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 54 [0/37476                 (0%)]\tLoss: 0.002545\n",
      "Training stage for Flod 2 Epoch: 54 [3200/37476                 (11%)]\tLoss: 0.029518\n",
      "Training stage for Flod 2 Epoch: 54 [6400/37476                 (21%)]\tLoss: 0.010417\n",
      "Training stage for Flod 2 Epoch: 54 [9600/37476                 (32%)]\tLoss: 0.006933\n",
      "Training stage for Flod 2 Epoch: 54 [12800/37476                 (43%)]\tLoss: 0.011456\n",
      "Training stage for Flod 2 Epoch: 54 [16000/37476                 (53%)]\tLoss: 0.028888\n",
      "Training stage for Flod 2 Epoch: 54 [19200/37476                 (64%)]\tLoss: 0.010939\n",
      "Training stage for Flod 2 Epoch: 54 [22400/37476                 (75%)]\tLoss: 0.007039\n",
      "Training stage for Flod 2 Epoch: 54 [25600/37476                 (85%)]\tLoss: 0.019874\n",
      "Training stage for Flod 2 Epoch: 54 [28800/37476                 (96%)]\tLoss: 0.078770\n",
      "Test set for fold2: Average Loss:           1.2308, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 55 [0/37476                 (0%)]\tLoss: 0.146429\n",
      "Training stage for Flod 2 Epoch: 55 [3200/37476                 (11%)]\tLoss: 0.000880\n",
      "Training stage for Flod 2 Epoch: 55 [6400/37476                 (21%)]\tLoss: 0.003975\n",
      "Training stage for Flod 2 Epoch: 55 [9600/37476                 (32%)]\tLoss: 0.098133\n",
      "Training stage for Flod 2 Epoch: 55 [12800/37476                 (43%)]\tLoss: 0.012276\n",
      "Training stage for Flod 2 Epoch: 55 [16000/37476                 (53%)]\tLoss: 0.001716\n",
      "Training stage for Flod 2 Epoch: 55 [19200/37476                 (64%)]\tLoss: 0.038305\n",
      "Training stage for Flod 2 Epoch: 55 [22400/37476                 (75%)]\tLoss: 0.006361\n",
      "Training stage for Flod 2 Epoch: 55 [25600/37476                 (85%)]\tLoss: 0.001813\n",
      "Training stage for Flod 2 Epoch: 55 [28800/37476                 (96%)]\tLoss: 0.012243\n",
      "Test set for fold2: Average Loss:           1.1772, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 56 [0/37476                 (0%)]\tLoss: 0.001419\n",
      "Training stage for Flod 2 Epoch: 56 [3200/37476                 (11%)]\tLoss: 0.210925\n",
      "Training stage for Flod 2 Epoch: 56 [6400/37476                 (21%)]\tLoss: 0.034482\n",
      "Training stage for Flod 2 Epoch: 56 [9600/37476                 (32%)]\tLoss: 0.033669\n",
      "Training stage for Flod 2 Epoch: 56 [12800/37476                 (43%)]\tLoss: 0.002992\n",
      "Training stage for Flod 2 Epoch: 56 [16000/37476                 (53%)]\tLoss: 0.003260\n",
      "Training stage for Flod 2 Epoch: 56 [19200/37476                 (64%)]\tLoss: 0.177796\n",
      "Training stage for Flod 2 Epoch: 56 [22400/37476                 (75%)]\tLoss: 0.057605\n",
      "Training stage for Flod 2 Epoch: 56 [25600/37476                 (85%)]\tLoss: 0.010246\n",
      "Training stage for Flod 2 Epoch: 56 [28800/37476                 (96%)]\tLoss: 0.085937\n",
      "Test set for fold2: Average Loss:           1.1246, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 57 [0/37476                 (0%)]\tLoss: 0.001485\n",
      "Training stage for Flod 2 Epoch: 57 [3200/37476                 (11%)]\tLoss: 0.014564\n",
      "Training stage for Flod 2 Epoch: 57 [6400/37476                 (21%)]\tLoss: 0.088864\n",
      "Training stage for Flod 2 Epoch: 57 [9600/37476                 (32%)]\tLoss: 0.052013\n",
      "Training stage for Flod 2 Epoch: 57 [12800/37476                 (43%)]\tLoss: 0.036893\n",
      "Training stage for Flod 2 Epoch: 57 [16000/37476                 (53%)]\tLoss: 0.031270\n",
      "Training stage for Flod 2 Epoch: 57 [19200/37476                 (64%)]\tLoss: 0.071265\n",
      "Training stage for Flod 2 Epoch: 57 [22400/37476                 (75%)]\tLoss: 0.002947\n",
      "Training stage for Flod 2 Epoch: 57 [25600/37476                 (85%)]\tLoss: 0.009202\n",
      "Training stage for Flod 2 Epoch: 57 [28800/37476                 (96%)]\tLoss: 0.000236\n",
      "Test set for fold2: Average Loss:           1.2011, Accuracy: 14772/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 58 [0/37476                 (0%)]\tLoss: 0.000873\n",
      "Training stage for Flod 2 Epoch: 58 [3200/37476                 (11%)]\tLoss: 0.010094\n",
      "Training stage for Flod 2 Epoch: 58 [6400/37476                 (21%)]\tLoss: 0.009961\n",
      "Training stage for Flod 2 Epoch: 58 [9600/37476                 (32%)]\tLoss: 0.053915\n",
      "Training stage for Flod 2 Epoch: 58 [12800/37476                 (43%)]\tLoss: 0.068617\n",
      "Training stage for Flod 2 Epoch: 58 [16000/37476                 (53%)]\tLoss: 0.017500\n",
      "Training stage for Flod 2 Epoch: 58 [19200/37476                 (64%)]\tLoss: 0.011267\n",
      "Training stage for Flod 2 Epoch: 58 [22400/37476                 (75%)]\tLoss: 0.046301\n",
      "Training stage for Flod 2 Epoch: 58 [25600/37476                 (85%)]\tLoss: 0.042821\n",
      "Training stage for Flod 2 Epoch: 58 [28800/37476                 (96%)]\tLoss: 0.000444\n",
      "Test set for fold2: Average Loss:           1.0348, Accuracy: 14741/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 59 [0/37476                 (0%)]\tLoss: 0.047047\n",
      "Training stage for Flod 2 Epoch: 59 [3200/37476                 (11%)]\tLoss: 0.021946\n",
      "Training stage for Flod 2 Epoch: 59 [6400/37476                 (21%)]\tLoss: 0.016967\n",
      "Training stage for Flod 2 Epoch: 59 [9600/37476                 (32%)]\tLoss: 0.092216\n",
      "Training stage for Flod 2 Epoch: 59 [12800/37476                 (43%)]\tLoss: 0.001420\n",
      "Training stage for Flod 2 Epoch: 59 [16000/37476                 (53%)]\tLoss: 0.032417\n",
      "Training stage for Flod 2 Epoch: 59 [19200/37476                 (64%)]\tLoss: 0.008694\n",
      "Training stage for Flod 2 Epoch: 59 [22400/37476                 (75%)]\tLoss: 0.099569\n",
      "Training stage for Flod 2 Epoch: 59 [25600/37476                 (85%)]\tLoss: 0.016317\n",
      "Training stage for Flod 2 Epoch: 59 [28800/37476                 (96%)]\tLoss: 0.002644\n",
      "Test set for fold2: Average Loss:           1.1955, Accuracy: 14776/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 60 [0/37476                 (0%)]\tLoss: 0.018529\n",
      "Training stage for Flod 2 Epoch: 60 [3200/37476                 (11%)]\tLoss: 0.008638\n",
      "Training stage for Flod 2 Epoch: 60 [6400/37476                 (21%)]\tLoss: 0.012802\n",
      "Training stage for Flod 2 Epoch: 60 [9600/37476                 (32%)]\tLoss: 0.015102\n",
      "Training stage for Flod 2 Epoch: 60 [12800/37476                 (43%)]\tLoss: 0.036869\n",
      "Training stage for Flod 2 Epoch: 60 [16000/37476                 (53%)]\tLoss: 0.138800\n",
      "Training stage for Flod 2 Epoch: 60 [19200/37476                 (64%)]\tLoss: 0.023000\n",
      "Training stage for Flod 2 Epoch: 60 [22400/37476                 (75%)]\tLoss: 0.010841\n",
      "Training stage for Flod 2 Epoch: 60 [25600/37476                 (85%)]\tLoss: 0.005309\n",
      "Training stage for Flod 2 Epoch: 60 [28800/37476                 (96%)]\tLoss: 0.026571\n",
      "Test set for fold2: Average Loss:           1.1051, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 61 [0/37476                 (0%)]\tLoss: 0.002907\n",
      "Training stage for Flod 2 Epoch: 61 [3200/37476                 (11%)]\tLoss: 0.019935\n",
      "Training stage for Flod 2 Epoch: 61 [6400/37476                 (21%)]\tLoss: 0.008527\n",
      "Training stage for Flod 2 Epoch: 61 [9600/37476                 (32%)]\tLoss: 0.032260\n",
      "Training stage for Flod 2 Epoch: 61 [12800/37476                 (43%)]\tLoss: 0.011295\n",
      "Training stage for Flod 2 Epoch: 61 [16000/37476                 (53%)]\tLoss: 0.012536\n",
      "Training stage for Flod 2 Epoch: 61 [19200/37476                 (64%)]\tLoss: 0.039699\n",
      "Training stage for Flod 2 Epoch: 61 [22400/37476                 (75%)]\tLoss: 0.000128\n",
      "Training stage for Flod 2 Epoch: 61 [25600/37476                 (85%)]\tLoss: 0.058299\n",
      "Training stage for Flod 2 Epoch: 61 [28800/37476                 (96%)]\tLoss: 0.021384\n",
      "Test set for fold2: Average Loss:           1.2142, Accuracy: 14763/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 62 [0/37476                 (0%)]\tLoss: 0.055592\n",
      "Training stage for Flod 2 Epoch: 62 [3200/37476                 (11%)]\tLoss: 0.004343\n",
      "Training stage for Flod 2 Epoch: 62 [6400/37476                 (21%)]\tLoss: 0.009870\n",
      "Training stage for Flod 2 Epoch: 62 [9600/37476                 (32%)]\tLoss: 0.017221\n",
      "Training stage for Flod 2 Epoch: 62 [12800/37476                 (43%)]\tLoss: 0.165978\n",
      "Training stage for Flod 2 Epoch: 62 [16000/37476                 (53%)]\tLoss: 0.256781\n",
      "Training stage for Flod 2 Epoch: 62 [19200/37476                 (64%)]\tLoss: 0.000024\n",
      "Training stage for Flod 2 Epoch: 62 [22400/37476                 (75%)]\tLoss: 0.216429\n",
      "Training stage for Flod 2 Epoch: 62 [25600/37476                 (85%)]\tLoss: 0.008740\n",
      "Training stage for Flod 2 Epoch: 62 [28800/37476                 (96%)]\tLoss: 0.018994\n",
      "Test set for fold2: Average Loss:           1.0864, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 63 [0/37476                 (0%)]\tLoss: 0.019199\n",
      "Training stage for Flod 2 Epoch: 63 [3200/37476                 (11%)]\tLoss: 0.002006\n",
      "Training stage for Flod 2 Epoch: 63 [6400/37476                 (21%)]\tLoss: 0.080173\n",
      "Training stage for Flod 2 Epoch: 63 [9600/37476                 (32%)]\tLoss: 0.007795\n",
      "Training stage for Flod 2 Epoch: 63 [12800/37476                 (43%)]\tLoss: 0.044992\n",
      "Training stage for Flod 2 Epoch: 63 [16000/37476                 (53%)]\tLoss: 0.040914\n",
      "Training stage for Flod 2 Epoch: 63 [19200/37476                 (64%)]\tLoss: 0.137095\n",
      "Training stage for Flod 2 Epoch: 63 [22400/37476                 (75%)]\tLoss: 0.002797\n",
      "Training stage for Flod 2 Epoch: 63 [25600/37476                 (85%)]\tLoss: 0.030917\n",
      "Training stage for Flod 2 Epoch: 63 [28800/37476                 (96%)]\tLoss: 0.004888\n",
      "Test set for fold2: Average Loss:           1.1764, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 64 [0/37476                 (0%)]\tLoss: 0.000458\n",
      "Training stage for Flod 2 Epoch: 64 [3200/37476                 (11%)]\tLoss: 0.036417\n",
      "Training stage for Flod 2 Epoch: 64 [6400/37476                 (21%)]\tLoss: 0.022964\n",
      "Training stage for Flod 2 Epoch: 64 [9600/37476                 (32%)]\tLoss: 0.113506\n",
      "Training stage for Flod 2 Epoch: 64 [12800/37476                 (43%)]\tLoss: 0.013732\n",
      "Training stage for Flod 2 Epoch: 64 [16000/37476                 (53%)]\tLoss: 0.032903\n",
      "Training stage for Flod 2 Epoch: 64 [19200/37476                 (64%)]\tLoss: 0.064113\n",
      "Training stage for Flod 2 Epoch: 64 [22400/37476                 (75%)]\tLoss: 0.001157\n",
      "Training stage for Flod 2 Epoch: 64 [25600/37476                 (85%)]\tLoss: 0.068381\n",
      "Training stage for Flod 2 Epoch: 64 [28800/37476                 (96%)]\tLoss: 0.056076\n",
      "Test set for fold2: Average Loss:           0.9986, Accuracy: 14755/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 65 [0/37476                 (0%)]\tLoss: 0.012465\n",
      "Training stage for Flod 2 Epoch: 65 [3200/37476                 (11%)]\tLoss: 0.104650\n",
      "Training stage for Flod 2 Epoch: 65 [6400/37476                 (21%)]\tLoss: 0.024578\n",
      "Training stage for Flod 2 Epoch: 65 [9600/37476                 (32%)]\tLoss: 0.000563\n",
      "Training stage for Flod 2 Epoch: 65 [12800/37476                 (43%)]\tLoss: 0.014287\n",
      "Training stage for Flod 2 Epoch: 65 [16000/37476                 (53%)]\tLoss: 0.016515\n",
      "Training stage for Flod 2 Epoch: 65 [19200/37476                 (64%)]\tLoss: 0.062559\n",
      "Training stage for Flod 2 Epoch: 65 [22400/37476                 (75%)]\tLoss: 0.001356\n",
      "Training stage for Flod 2 Epoch: 65 [25600/37476                 (85%)]\tLoss: 0.037869\n",
      "Training stage for Flod 2 Epoch: 65 [28800/37476                 (96%)]\tLoss: 0.004375\n",
      "Test set for fold2: Average Loss:           1.3414, Accuracy: 14793/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 66 [0/37476                 (0%)]\tLoss: 0.033898\n",
      "Training stage for Flod 2 Epoch: 66 [3200/37476                 (11%)]\tLoss: 0.074641\n",
      "Training stage for Flod 2 Epoch: 66 [6400/37476                 (21%)]\tLoss: 0.026585\n",
      "Training stage for Flod 2 Epoch: 66 [9600/37476                 (32%)]\tLoss: 0.021621\n",
      "Training stage for Flod 2 Epoch: 66 [12800/37476                 (43%)]\tLoss: 0.101542\n",
      "Training stage for Flod 2 Epoch: 66 [16000/37476                 (53%)]\tLoss: 0.001375\n",
      "Training stage for Flod 2 Epoch: 66 [19200/37476                 (64%)]\tLoss: 0.013171\n",
      "Training stage for Flod 2 Epoch: 66 [22400/37476                 (75%)]\tLoss: 0.008244\n",
      "Training stage for Flod 2 Epoch: 66 [25600/37476                 (85%)]\tLoss: 0.024602\n",
      "Training stage for Flod 2 Epoch: 66 [28800/37476                 (96%)]\tLoss: 0.000961\n",
      "Test set for fold2: Average Loss:           1.2513, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 67 [0/37476                 (0%)]\tLoss: 0.002603\n",
      "Training stage for Flod 2 Epoch: 67 [3200/37476                 (11%)]\tLoss: 0.007120\n",
      "Training stage for Flod 2 Epoch: 67 [6400/37476                 (21%)]\tLoss: 0.011074\n",
      "Training stage for Flod 2 Epoch: 67 [9600/37476                 (32%)]\tLoss: 0.027417\n",
      "Training stage for Flod 2 Epoch: 67 [12800/37476                 (43%)]\tLoss: 0.064644\n",
      "Training stage for Flod 2 Epoch: 67 [16000/37476                 (53%)]\tLoss: 0.256274\n",
      "Training stage for Flod 2 Epoch: 67 [19200/37476                 (64%)]\tLoss: 0.029078\n",
      "Training stage for Flod 2 Epoch: 67 [22400/37476                 (75%)]\tLoss: 0.047767\n",
      "Training stage for Flod 2 Epoch: 67 [25600/37476                 (85%)]\tLoss: 0.040097\n",
      "Training stage for Flod 2 Epoch: 67 [28800/37476                 (96%)]\tLoss: 0.000933\n",
      "Test set for fold2: Average Loss:           1.2239, Accuracy: 14680/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 68 [0/37476                 (0%)]\tLoss: 0.013649\n",
      "Training stage for Flod 2 Epoch: 68 [3200/37476                 (11%)]\tLoss: 0.041241\n",
      "Training stage for Flod 2 Epoch: 68 [6400/37476                 (21%)]\tLoss: 0.012910\n",
      "Training stage for Flod 2 Epoch: 68 [9600/37476                 (32%)]\tLoss: 0.000421\n",
      "Training stage for Flod 2 Epoch: 68 [12800/37476                 (43%)]\tLoss: 0.084974\n",
      "Training stage for Flod 2 Epoch: 68 [16000/37476                 (53%)]\tLoss: 0.000279\n",
      "Training stage for Flod 2 Epoch: 68 [19200/37476                 (64%)]\tLoss: 0.019435\n",
      "Training stage for Flod 2 Epoch: 68 [22400/37476                 (75%)]\tLoss: 0.007610\n",
      "Training stage for Flod 2 Epoch: 68 [25600/37476                 (85%)]\tLoss: 0.061238\n",
      "Training stage for Flod 2 Epoch: 68 [28800/37476                 (96%)]\tLoss: 0.042971\n",
      "Test set for fold2: Average Loss:           1.2779, Accuracy: 14772/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 69 [0/37476                 (0%)]\tLoss: 0.002728\n",
      "Training stage for Flod 2 Epoch: 69 [3200/37476                 (11%)]\tLoss: 0.000791\n",
      "Training stage for Flod 2 Epoch: 69 [6400/37476                 (21%)]\tLoss: 0.080566\n",
      "Training stage for Flod 2 Epoch: 69 [9600/37476                 (32%)]\tLoss: 0.049621\n",
      "Training stage for Flod 2 Epoch: 69 [12800/37476                 (43%)]\tLoss: 0.269336\n",
      "Training stage for Flod 2 Epoch: 69 [16000/37476                 (53%)]\tLoss: 0.199175\n",
      "Training stage for Flod 2 Epoch: 69 [19200/37476                 (64%)]\tLoss: 0.002343\n",
      "Training stage for Flod 2 Epoch: 69 [22400/37476                 (75%)]\tLoss: 0.000447\n",
      "Training stage for Flod 2 Epoch: 69 [25600/37476                 (85%)]\tLoss: 0.002337\n",
      "Training stage for Flod 2 Epoch: 69 [28800/37476                 (96%)]\tLoss: 0.004153\n",
      "Test set for fold2: Average Loss:           1.2877, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 70 [0/37476                 (0%)]\tLoss: 0.139869\n",
      "Training stage for Flod 2 Epoch: 70 [3200/37476                 (11%)]\tLoss: 0.001051\n",
      "Training stage for Flod 2 Epoch: 70 [6400/37476                 (21%)]\tLoss: 0.012319\n",
      "Training stage for Flod 2 Epoch: 70 [9600/37476                 (32%)]\tLoss: 0.031312\n",
      "Training stage for Flod 2 Epoch: 70 [12800/37476                 (43%)]\tLoss: 0.028134\n",
      "Training stage for Flod 2 Epoch: 70 [16000/37476                 (53%)]\tLoss: 0.013899\n",
      "Training stage for Flod 2 Epoch: 70 [19200/37476                 (64%)]\tLoss: 0.000192\n",
      "Training stage for Flod 2 Epoch: 70 [22400/37476                 (75%)]\tLoss: 0.045189\n",
      "Training stage for Flod 2 Epoch: 70 [25600/37476                 (85%)]\tLoss: 0.006866\n",
      "Training stage for Flod 2 Epoch: 70 [28800/37476                 (96%)]\tLoss: 0.016995\n",
      "Test set for fold2: Average Loss:           1.1072, Accuracy: 14793/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 71 [0/37476                 (0%)]\tLoss: 0.029442\n",
      "Training stage for Flod 2 Epoch: 71 [3200/37476                 (11%)]\tLoss: 0.084206\n",
      "Training stage for Flod 2 Epoch: 71 [6400/37476                 (21%)]\tLoss: 0.044279\n",
      "Training stage for Flod 2 Epoch: 71 [9600/37476                 (32%)]\tLoss: 0.006860\n",
      "Training stage for Flod 2 Epoch: 71 [12800/37476                 (43%)]\tLoss: 0.048505\n",
      "Training stage for Flod 2 Epoch: 71 [16000/37476                 (53%)]\tLoss: 0.101728\n",
      "Training stage for Flod 2 Epoch: 71 [19200/37476                 (64%)]\tLoss: 0.159784\n",
      "Training stage for Flod 2 Epoch: 71 [22400/37476                 (75%)]\tLoss: 0.032152\n",
      "Training stage for Flod 2 Epoch: 71 [25600/37476                 (85%)]\tLoss: 0.000879\n",
      "Training stage for Flod 2 Epoch: 71 [28800/37476                 (96%)]\tLoss: 0.008126\n",
      "Test set for fold2: Average Loss:           1.2437, Accuracy: 14787/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 72 [0/37476                 (0%)]\tLoss: 0.006941\n",
      "Training stage for Flod 2 Epoch: 72 [3200/37476                 (11%)]\tLoss: 0.012732\n",
      "Training stage for Flod 2 Epoch: 72 [6400/37476                 (21%)]\tLoss: 0.065922\n",
      "Training stage for Flod 2 Epoch: 72 [9600/37476                 (32%)]\tLoss: 0.002552\n",
      "Training stage for Flod 2 Epoch: 72 [12800/37476                 (43%)]\tLoss: 0.006897\n",
      "Training stage for Flod 2 Epoch: 72 [16000/37476                 (53%)]\tLoss: 0.004577\n",
      "Training stage for Flod 2 Epoch: 72 [19200/37476                 (64%)]\tLoss: 0.005847\n",
      "Training stage for Flod 2 Epoch: 72 [22400/37476                 (75%)]\tLoss: 0.000350\n",
      "Training stage for Flod 2 Epoch: 72 [25600/37476                 (85%)]\tLoss: 0.000663\n",
      "Training stage for Flod 2 Epoch: 72 [28800/37476                 (96%)]\tLoss: 0.023784\n",
      "Test set for fold2: Average Loss:           1.2110, Accuracy: 14802/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 73 [0/37476                 (0%)]\tLoss: 0.000867\n",
      "Training stage for Flod 2 Epoch: 73 [3200/37476                 (11%)]\tLoss: 0.011810\n",
      "Training stage for Flod 2 Epoch: 73 [6400/37476                 (21%)]\tLoss: 0.022629\n",
      "Training stage for Flod 2 Epoch: 73 [9600/37476                 (32%)]\tLoss: 0.000795\n",
      "Training stage for Flod 2 Epoch: 73 [12800/37476                 (43%)]\tLoss: 0.004542\n",
      "Training stage for Flod 2 Epoch: 73 [16000/37476                 (53%)]\tLoss: 0.035214\n",
      "Training stage for Flod 2 Epoch: 73 [19200/37476                 (64%)]\tLoss: 0.036094\n",
      "Training stage for Flod 2 Epoch: 73 [22400/37476                 (75%)]\tLoss: 0.015106\n",
      "Training stage for Flod 2 Epoch: 73 [25600/37476                 (85%)]\tLoss: 0.000986\n",
      "Training stage for Flod 2 Epoch: 73 [28800/37476                 (96%)]\tLoss: 0.000680\n",
      "Test set for fold2: Average Loss:           1.3018, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 74 [0/37476                 (0%)]\tLoss: 0.049975\n",
      "Training stage for Flod 2 Epoch: 74 [3200/37476                 (11%)]\tLoss: 0.004153\n",
      "Training stage for Flod 2 Epoch: 74 [6400/37476                 (21%)]\tLoss: 0.106672\n",
      "Training stage for Flod 2 Epoch: 74 [9600/37476                 (32%)]\tLoss: 0.173592\n",
      "Training stage for Flod 2 Epoch: 74 [12800/37476                 (43%)]\tLoss: 0.021872\n",
      "Training stage for Flod 2 Epoch: 74 [16000/37476                 (53%)]\tLoss: 0.007756\n",
      "Training stage for Flod 2 Epoch: 74 [19200/37476                 (64%)]\tLoss: 0.008820\n",
      "Training stage for Flod 2 Epoch: 74 [22400/37476                 (75%)]\tLoss: 0.017379\n",
      "Training stage for Flod 2 Epoch: 74 [25600/37476                 (85%)]\tLoss: 0.004966\n",
      "Training stage for Flod 2 Epoch: 74 [28800/37476                 (96%)]\tLoss: 0.050264\n",
      "Test set for fold2: Average Loss:           1.0744, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 75 [0/37476                 (0%)]\tLoss: 0.016578\n",
      "Training stage for Flod 2 Epoch: 75 [3200/37476                 (11%)]\tLoss: 0.091589\n",
      "Training stage for Flod 2 Epoch: 75 [6400/37476                 (21%)]\tLoss: 0.000859\n",
      "Training stage for Flod 2 Epoch: 75 [9600/37476                 (32%)]\tLoss: 0.059926\n",
      "Training stage for Flod 2 Epoch: 75 [12800/37476                 (43%)]\tLoss: 0.126616\n",
      "Training stage for Flod 2 Epoch: 75 [16000/37476                 (53%)]\tLoss: 0.005746\n",
      "Training stage for Flod 2 Epoch: 75 [19200/37476                 (64%)]\tLoss: 0.040471\n",
      "Training stage for Flod 2 Epoch: 75 [22400/37476                 (75%)]\tLoss: 0.000410\n",
      "Training stage for Flod 2 Epoch: 75 [25600/37476                 (85%)]\tLoss: 0.004426\n",
      "Training stage for Flod 2 Epoch: 75 [28800/37476                 (96%)]\tLoss: 0.009010\n",
      "Test set for fold2: Average Loss:           1.2582, Accuracy: 14786/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 76 [0/37476                 (0%)]\tLoss: 0.003644\n",
      "Training stage for Flod 2 Epoch: 76 [3200/37476                 (11%)]\tLoss: 0.014570\n",
      "Training stage for Flod 2 Epoch: 76 [6400/37476                 (21%)]\tLoss: 0.031674\n",
      "Training stage for Flod 2 Epoch: 76 [9600/37476                 (32%)]\tLoss: 0.073967\n",
      "Training stage for Flod 2 Epoch: 76 [12800/37476                 (43%)]\tLoss: 0.016450\n",
      "Training stage for Flod 2 Epoch: 76 [16000/37476                 (53%)]\tLoss: 0.002994\n",
      "Training stage for Flod 2 Epoch: 76 [19200/37476                 (64%)]\tLoss: 0.022294\n",
      "Training stage for Flod 2 Epoch: 76 [22400/37476                 (75%)]\tLoss: 0.016167\n",
      "Training stage for Flod 2 Epoch: 76 [25600/37476                 (85%)]\tLoss: 0.007365\n",
      "Training stage for Flod 2 Epoch: 76 [28800/37476                 (96%)]\tLoss: 0.014486\n",
      "Test set for fold2: Average Loss:           1.1616, Accuracy: 14775/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 77 [0/37476                 (0%)]\tLoss: 0.038981\n",
      "Training stage for Flod 2 Epoch: 77 [3200/37476                 (11%)]\tLoss: 0.025268\n",
      "Training stage for Flod 2 Epoch: 77 [6400/37476                 (21%)]\tLoss: 0.025823\n",
      "Training stage for Flod 2 Epoch: 77 [9600/37476                 (32%)]\tLoss: 0.002587\n",
      "Training stage for Flod 2 Epoch: 77 [12800/37476                 (43%)]\tLoss: 0.175289\n",
      "Training stage for Flod 2 Epoch: 77 [16000/37476                 (53%)]\tLoss: 0.021462\n",
      "Training stage for Flod 2 Epoch: 77 [19200/37476                 (64%)]\tLoss: 0.002344\n",
      "Training stage for Flod 2 Epoch: 77 [22400/37476                 (75%)]\tLoss: 0.008327\n",
      "Training stage for Flod 2 Epoch: 77 [25600/37476                 (85%)]\tLoss: 0.005860\n",
      "Training stage for Flod 2 Epoch: 77 [28800/37476                 (96%)]\tLoss: 0.138620\n",
      "Test set for fold2: Average Loss:           1.0687, Accuracy: 14765/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 78 [0/37476                 (0%)]\tLoss: 0.042764\n",
      "Training stage for Flod 2 Epoch: 78 [3200/37476                 (11%)]\tLoss: 0.181457\n",
      "Training stage for Flod 2 Epoch: 78 [6400/37476                 (21%)]\tLoss: 0.001830\n",
      "Training stage for Flod 2 Epoch: 78 [9600/37476                 (32%)]\tLoss: 0.021926\n",
      "Training stage for Flod 2 Epoch: 78 [12800/37476                 (43%)]\tLoss: 0.056089\n",
      "Training stage for Flod 2 Epoch: 78 [16000/37476                 (53%)]\tLoss: 0.011007\n",
      "Training stage for Flod 2 Epoch: 78 [19200/37476                 (64%)]\tLoss: 0.031385\n",
      "Training stage for Flod 2 Epoch: 78 [22400/37476                 (75%)]\tLoss: 0.007617\n",
      "Training stage for Flod 2 Epoch: 78 [25600/37476                 (85%)]\tLoss: 0.003942\n",
      "Training stage for Flod 2 Epoch: 78 [28800/37476                 (96%)]\tLoss: 0.001373\n",
      "Test set for fold2: Average Loss:           1.1645, Accuracy: 14631/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 79 [0/37476                 (0%)]\tLoss: 0.127157\n",
      "Training stage for Flod 2 Epoch: 79 [3200/37476                 (11%)]\tLoss: 0.010041\n",
      "Training stage for Flod 2 Epoch: 79 [6400/37476                 (21%)]\tLoss: 0.004615\n",
      "Training stage for Flod 2 Epoch: 79 [9600/37476                 (32%)]\tLoss: 0.031681\n",
      "Training stage for Flod 2 Epoch: 79 [12800/37476                 (43%)]\tLoss: 0.003344\n",
      "Training stage for Flod 2 Epoch: 79 [16000/37476                 (53%)]\tLoss: 0.038746\n",
      "Training stage for Flod 2 Epoch: 79 [19200/37476                 (64%)]\tLoss: 0.025395\n",
      "Training stage for Flod 2 Epoch: 79 [22400/37476                 (75%)]\tLoss: 0.113814\n",
      "Training stage for Flod 2 Epoch: 79 [25600/37476                 (85%)]\tLoss: 0.014937\n",
      "Training stage for Flod 2 Epoch: 79 [28800/37476                 (96%)]\tLoss: 0.008971\n",
      "Test set for fold2: Average Loss:           1.2842, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 80 [0/37476                 (0%)]\tLoss: 0.001645\n",
      "Training stage for Flod 2 Epoch: 80 [3200/37476                 (11%)]\tLoss: 0.123357\n",
      "Training stage for Flod 2 Epoch: 80 [6400/37476                 (21%)]\tLoss: 0.044520\n",
      "Training stage for Flod 2 Epoch: 80 [9600/37476                 (32%)]\tLoss: 0.017682\n",
      "Training stage for Flod 2 Epoch: 80 [12800/37476                 (43%)]\tLoss: 0.088060\n",
      "Training stage for Flod 2 Epoch: 80 [16000/37476                 (53%)]\tLoss: 0.004565\n",
      "Training stage for Flod 2 Epoch: 80 [19200/37476                 (64%)]\tLoss: 0.000036\n",
      "Training stage for Flod 2 Epoch: 80 [22400/37476                 (75%)]\tLoss: 0.098693\n",
      "Training stage for Flod 2 Epoch: 80 [25600/37476                 (85%)]\tLoss: 0.004655\n",
      "Training stage for Flod 2 Epoch: 80 [28800/37476                 (96%)]\tLoss: 0.093043\n",
      "Test set for fold2: Average Loss:           1.1676, Accuracy: 14796/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 81 [0/37476                 (0%)]\tLoss: 0.010509\n",
      "Training stage for Flod 2 Epoch: 81 [3200/37476                 (11%)]\tLoss: 0.006142\n",
      "Training stage for Flod 2 Epoch: 81 [6400/37476                 (21%)]\tLoss: 0.002826\n",
      "Training stage for Flod 2 Epoch: 81 [9600/37476                 (32%)]\tLoss: 0.070800\n",
      "Training stage for Flod 2 Epoch: 81 [12800/37476                 (43%)]\tLoss: 0.021126\n",
      "Training stage for Flod 2 Epoch: 81 [16000/37476                 (53%)]\tLoss: 0.022412\n",
      "Training stage for Flod 2 Epoch: 81 [19200/37476                 (64%)]\tLoss: 0.011385\n",
      "Training stage for Flod 2 Epoch: 81 [22400/37476                 (75%)]\tLoss: 0.026642\n",
      "Training stage for Flod 2 Epoch: 81 [25600/37476                 (85%)]\tLoss: 0.241816\n",
      "Training stage for Flod 2 Epoch: 81 [28800/37476                 (96%)]\tLoss: 0.004245\n",
      "Test set for fold2: Average Loss:           1.2997, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 82 [0/37476                 (0%)]\tLoss: 0.039847\n",
      "Training stage for Flod 2 Epoch: 82 [3200/37476                 (11%)]\tLoss: 0.033616\n",
      "Training stage for Flod 2 Epoch: 82 [6400/37476                 (21%)]\tLoss: 0.068835\n",
      "Training stage for Flod 2 Epoch: 82 [9600/37476                 (32%)]\tLoss: 0.012109\n",
      "Training stage for Flod 2 Epoch: 82 [12800/37476                 (43%)]\tLoss: 0.005101\n",
      "Training stage for Flod 2 Epoch: 82 [16000/37476                 (53%)]\tLoss: 0.001915\n",
      "Training stage for Flod 2 Epoch: 82 [19200/37476                 (64%)]\tLoss: 0.048612\n",
      "Training stage for Flod 2 Epoch: 82 [22400/37476                 (75%)]\tLoss: 0.051294\n",
      "Training stage for Flod 2 Epoch: 82 [25600/37476                 (85%)]\tLoss: 0.011396\n",
      "Training stage for Flod 2 Epoch: 82 [28800/37476                 (96%)]\tLoss: 0.006159\n",
      "Test set for fold2: Average Loss:           1.0878, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 83 [0/37476                 (0%)]\tLoss: 0.043673\n",
      "Training stage for Flod 2 Epoch: 83 [3200/37476                 (11%)]\tLoss: 0.002402\n",
      "Training stage for Flod 2 Epoch: 83 [6400/37476                 (21%)]\tLoss: 0.013718\n",
      "Training stage for Flod 2 Epoch: 83 [9600/37476                 (32%)]\tLoss: 0.000068\n",
      "Training stage for Flod 2 Epoch: 83 [12800/37476                 (43%)]\tLoss: 0.016200\n",
      "Training stage for Flod 2 Epoch: 83 [16000/37476                 (53%)]\tLoss: 0.041107\n",
      "Training stage for Flod 2 Epoch: 83 [19200/37476                 (64%)]\tLoss: 0.055463\n",
      "Training stage for Flod 2 Epoch: 83 [22400/37476                 (75%)]\tLoss: 0.017431\n",
      "Training stage for Flod 2 Epoch: 83 [25600/37476                 (85%)]\tLoss: 0.046663\n",
      "Training stage for Flod 2 Epoch: 83 [28800/37476                 (96%)]\tLoss: 0.000821\n",
      "Test set for fold2: Average Loss:           1.3465, Accuracy: 14782/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 84 [0/37476                 (0%)]\tLoss: 0.001044\n",
      "Training stage for Flod 2 Epoch: 84 [3200/37476                 (11%)]\tLoss: 0.003893\n",
      "Training stage for Flod 2 Epoch: 84 [6400/37476                 (21%)]\tLoss: 0.004131\n",
      "Training stage for Flod 2 Epoch: 84 [9600/37476                 (32%)]\tLoss: 0.040443\n",
      "Training stage for Flod 2 Epoch: 84 [12800/37476                 (43%)]\tLoss: 0.004915\n",
      "Training stage for Flod 2 Epoch: 84 [16000/37476                 (53%)]\tLoss: 0.033669\n",
      "Training stage for Flod 2 Epoch: 84 [19200/37476                 (64%)]\tLoss: 0.014578\n",
      "Training stage for Flod 2 Epoch: 84 [22400/37476                 (75%)]\tLoss: 0.000623\n",
      "Training stage for Flod 2 Epoch: 84 [25600/37476                 (85%)]\tLoss: 0.043155\n",
      "Training stage for Flod 2 Epoch: 84 [28800/37476                 (96%)]\tLoss: 0.006614\n",
      "Test set for fold2: Average Loss:           1.1846, Accuracy: 14787/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 85 [0/37476                 (0%)]\tLoss: 0.007196\n",
      "Training stage for Flod 2 Epoch: 85 [3200/37476                 (11%)]\tLoss: 0.005567\n",
      "Training stage for Flod 2 Epoch: 85 [6400/37476                 (21%)]\tLoss: 0.074824\n",
      "Training stage for Flod 2 Epoch: 85 [9600/37476                 (32%)]\tLoss: 0.016657\n",
      "Training stage for Flod 2 Epoch: 85 [12800/37476                 (43%)]\tLoss: 0.029419\n",
      "Training stage for Flod 2 Epoch: 85 [16000/37476                 (53%)]\tLoss: 0.015386\n",
      "Training stage for Flod 2 Epoch: 85 [19200/37476                 (64%)]\tLoss: 0.002409\n",
      "Training stage for Flod 2 Epoch: 85 [22400/37476                 (75%)]\tLoss: 0.019274\n",
      "Training stage for Flod 2 Epoch: 85 [25600/37476                 (85%)]\tLoss: 0.010211\n",
      "Training stage for Flod 2 Epoch: 85 [28800/37476                 (96%)]\tLoss: 0.003208\n",
      "Test set for fold2: Average Loss:           1.4478, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 86 [0/37476                 (0%)]\tLoss: 0.016039\n",
      "Training stage for Flod 2 Epoch: 86 [3200/37476                 (11%)]\tLoss: 0.003567\n",
      "Training stage for Flod 2 Epoch: 86 [6400/37476                 (21%)]\tLoss: 0.021338\n",
      "Training stage for Flod 2 Epoch: 86 [9600/37476                 (32%)]\tLoss: 0.004230\n",
      "Training stage for Flod 2 Epoch: 86 [12800/37476                 (43%)]\tLoss: 0.042167\n",
      "Training stage for Flod 2 Epoch: 86 [16000/37476                 (53%)]\tLoss: 0.001226\n",
      "Training stage for Flod 2 Epoch: 86 [19200/37476                 (64%)]\tLoss: 0.002873\n",
      "Training stage for Flod 2 Epoch: 86 [22400/37476                 (75%)]\tLoss: 0.011444\n",
      "Training stage for Flod 2 Epoch: 86 [25600/37476                 (85%)]\tLoss: 0.056713\n",
      "Training stage for Flod 2 Epoch: 86 [28800/37476                 (96%)]\tLoss: 0.001130\n",
      "Test set for fold2: Average Loss:           1.2289, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 87 [0/37476                 (0%)]\tLoss: 0.002118\n",
      "Training stage for Flod 2 Epoch: 87 [3200/37476                 (11%)]\tLoss: 0.006363\n",
      "Training stage for Flod 2 Epoch: 87 [6400/37476                 (21%)]\tLoss: 0.070153\n",
      "Training stage for Flod 2 Epoch: 87 [9600/37476                 (32%)]\tLoss: 0.000588\n",
      "Training stage for Flod 2 Epoch: 87 [12800/37476                 (43%)]\tLoss: 0.000783\n",
      "Training stage for Flod 2 Epoch: 87 [16000/37476                 (53%)]\tLoss: 0.134997\n",
      "Training stage for Flod 2 Epoch: 87 [19200/37476                 (64%)]\tLoss: 0.063896\n",
      "Training stage for Flod 2 Epoch: 87 [22400/37476                 (75%)]\tLoss: 0.003907\n",
      "Training stage for Flod 2 Epoch: 87 [25600/37476                 (85%)]\tLoss: 0.277129\n",
      "Training stage for Flod 2 Epoch: 87 [28800/37476                 (96%)]\tLoss: 0.139018\n",
      "Test set for fold2: Average Loss:           1.2016, Accuracy: 14789/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 88 [0/37476                 (0%)]\tLoss: 0.002971\n",
      "Training stage for Flod 2 Epoch: 88 [3200/37476                 (11%)]\tLoss: 0.000668\n",
      "Training stage for Flod 2 Epoch: 88 [6400/37476                 (21%)]\tLoss: 0.046602\n",
      "Training stage for Flod 2 Epoch: 88 [9600/37476                 (32%)]\tLoss: 0.013181\n",
      "Training stage for Flod 2 Epoch: 88 [12800/37476                 (43%)]\tLoss: 0.014775\n",
      "Training stage for Flod 2 Epoch: 88 [16000/37476                 (53%)]\tLoss: 0.121948\n",
      "Training stage for Flod 2 Epoch: 88 [19200/37476                 (64%)]\tLoss: 0.028712\n",
      "Training stage for Flod 2 Epoch: 88 [22400/37476                 (75%)]\tLoss: 0.003891\n",
      "Training stage for Flod 2 Epoch: 88 [25600/37476                 (85%)]\tLoss: 0.009498\n",
      "Training stage for Flod 2 Epoch: 88 [28800/37476                 (96%)]\tLoss: 0.013713\n",
      "Test set for fold2: Average Loss:           1.5061, Accuracy: 14722/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 89 [0/37476                 (0%)]\tLoss: 0.050908\n",
      "Training stage for Flod 2 Epoch: 89 [3200/37476                 (11%)]\tLoss: 0.000817\n",
      "Training stage for Flod 2 Epoch: 89 [6400/37476                 (21%)]\tLoss: 0.000304\n",
      "Training stage for Flod 2 Epoch: 89 [9600/37476                 (32%)]\tLoss: 0.000583\n",
      "Training stage for Flod 2 Epoch: 89 [12800/37476                 (43%)]\tLoss: 0.004848\n",
      "Training stage for Flod 2 Epoch: 89 [16000/37476                 (53%)]\tLoss: 0.057960\n",
      "Training stage for Flod 2 Epoch: 89 [19200/37476                 (64%)]\tLoss: 0.003275\n",
      "Training stage for Flod 2 Epoch: 89 [22400/37476                 (75%)]\tLoss: 0.009483\n",
      "Training stage for Flod 2 Epoch: 89 [25600/37476                 (85%)]\tLoss: 0.001036\n",
      "Training stage for Flod 2 Epoch: 89 [28800/37476                 (96%)]\tLoss: 0.019463\n",
      "Test set for fold2: Average Loss:           1.4138, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 90 [0/37476                 (0%)]\tLoss: 0.119971\n",
      "Training stage for Flod 2 Epoch: 90 [3200/37476                 (11%)]\tLoss: 0.090668\n",
      "Training stage for Flod 2 Epoch: 90 [6400/37476                 (21%)]\tLoss: 0.001246\n",
      "Training stage for Flod 2 Epoch: 90 [9600/37476                 (32%)]\tLoss: 0.023524\n",
      "Training stage for Flod 2 Epoch: 90 [12800/37476                 (43%)]\tLoss: 0.010281\n",
      "Training stage for Flod 2 Epoch: 90 [16000/37476                 (53%)]\tLoss: 0.018717\n",
      "Training stage for Flod 2 Epoch: 90 [19200/37476                 (64%)]\tLoss: 0.004533\n",
      "Training stage for Flod 2 Epoch: 90 [22400/37476                 (75%)]\tLoss: 0.004819\n",
      "Training stage for Flod 2 Epoch: 90 [25600/37476                 (85%)]\tLoss: 0.013784\n",
      "Training stage for Flod 2 Epoch: 90 [28800/37476                 (96%)]\tLoss: 0.004548\n",
      "Test set for fold2: Average Loss:           1.4054, Accuracy: 14782/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 91 [0/37476                 (0%)]\tLoss: 0.009164\n",
      "Training stage for Flod 2 Epoch: 91 [3200/37476                 (11%)]\tLoss: 0.111881\n",
      "Training stage for Flod 2 Epoch: 91 [6400/37476                 (21%)]\tLoss: 0.006633\n",
      "Training stage for Flod 2 Epoch: 91 [9600/37476                 (32%)]\tLoss: 0.017564\n",
      "Training stage for Flod 2 Epoch: 91 [12800/37476                 (43%)]\tLoss: 0.007831\n",
      "Training stage for Flod 2 Epoch: 91 [16000/37476                 (53%)]\tLoss: 0.007758\n",
      "Training stage for Flod 2 Epoch: 91 [19200/37476                 (64%)]\tLoss: 0.035918\n",
      "Training stage for Flod 2 Epoch: 91 [22400/37476                 (75%)]\tLoss: 0.039684\n",
      "Training stage for Flod 2 Epoch: 91 [25600/37476                 (85%)]\tLoss: 0.000814\n",
      "Training stage for Flod 2 Epoch: 91 [28800/37476                 (96%)]\tLoss: 0.008229\n",
      "Test set for fold2: Average Loss:           1.1610, Accuracy: 14761/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 92 [0/37476                 (0%)]\tLoss: 0.001727\n",
      "Training stage for Flod 2 Epoch: 92 [3200/37476                 (11%)]\tLoss: 0.002673\n",
      "Training stage for Flod 2 Epoch: 92 [6400/37476                 (21%)]\tLoss: 0.002544\n",
      "Training stage for Flod 2 Epoch: 92 [9600/37476                 (32%)]\tLoss: 0.089450\n",
      "Training stage for Flod 2 Epoch: 92 [12800/37476                 (43%)]\tLoss: 0.015586\n",
      "Training stage for Flod 2 Epoch: 92 [16000/37476                 (53%)]\tLoss: 0.006909\n",
      "Training stage for Flod 2 Epoch: 92 [19200/37476                 (64%)]\tLoss: 0.010126\n",
      "Training stage for Flod 2 Epoch: 92 [22400/37476                 (75%)]\tLoss: 0.004741\n",
      "Training stage for Flod 2 Epoch: 92 [25600/37476                 (85%)]\tLoss: 0.000011\n",
      "Training stage for Flod 2 Epoch: 92 [28800/37476                 (96%)]\tLoss: 0.006791\n",
      "Test set for fold2: Average Loss:           1.2197, Accuracy: 14782/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 93 [0/37476                 (0%)]\tLoss: 0.062862\n",
      "Training stage for Flod 2 Epoch: 93 [3200/37476                 (11%)]\tLoss: 0.113451\n",
      "Training stage for Flod 2 Epoch: 93 [6400/37476                 (21%)]\tLoss: 0.015670\n",
      "Training stage for Flod 2 Epoch: 93 [9600/37476                 (32%)]\tLoss: 0.008168\n",
      "Training stage for Flod 2 Epoch: 93 [12800/37476                 (43%)]\tLoss: 0.088740\n",
      "Training stage for Flod 2 Epoch: 93 [16000/37476                 (53%)]\tLoss: 0.003151\n",
      "Training stage for Flod 2 Epoch: 93 [19200/37476                 (64%)]\tLoss: 0.000008\n",
      "Training stage for Flod 2 Epoch: 93 [22400/37476                 (75%)]\tLoss: 0.056404\n",
      "Training stage for Flod 2 Epoch: 93 [25600/37476                 (85%)]\tLoss: 0.181403\n",
      "Training stage for Flod 2 Epoch: 93 [28800/37476                 (96%)]\tLoss: 0.003393\n",
      "Test set for fold2: Average Loss:           1.2817, Accuracy: 14810/37476           (40%)\n",
      "Training stage for Flod 2 Epoch: 94 [0/37476                 (0%)]\tLoss: 0.005717\n",
      "Training stage for Flod 2 Epoch: 94 [3200/37476                 (11%)]\tLoss: 0.068528\n",
      "Training stage for Flod 2 Epoch: 94 [6400/37476                 (21%)]\tLoss: 0.001803\n",
      "Training stage for Flod 2 Epoch: 94 [9600/37476                 (32%)]\tLoss: 0.098088\n",
      "Training stage for Flod 2 Epoch: 94 [12800/37476                 (43%)]\tLoss: 0.013512\n",
      "Training stage for Flod 2 Epoch: 94 [16000/37476                 (53%)]\tLoss: 0.008428\n",
      "Training stage for Flod 2 Epoch: 94 [19200/37476                 (64%)]\tLoss: 0.007886\n",
      "Training stage for Flod 2 Epoch: 94 [22400/37476                 (75%)]\tLoss: 0.001961\n",
      "Training stage for Flod 2 Epoch: 94 [25600/37476                 (85%)]\tLoss: 0.000103\n",
      "Training stage for Flod 2 Epoch: 94 [28800/37476                 (96%)]\tLoss: 0.000196\n",
      "Test set for fold2: Average Loss:           1.4345, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 95 [0/37476                 (0%)]\tLoss: 0.075204\n",
      "Training stage for Flod 2 Epoch: 95 [3200/37476                 (11%)]\tLoss: 0.009951\n",
      "Training stage for Flod 2 Epoch: 95 [6400/37476                 (21%)]\tLoss: 0.035799\n",
      "Training stage for Flod 2 Epoch: 95 [9600/37476                 (32%)]\tLoss: 0.048880\n",
      "Training stage for Flod 2 Epoch: 95 [12800/37476                 (43%)]\tLoss: 0.005721\n",
      "Training stage for Flod 2 Epoch: 95 [16000/37476                 (53%)]\tLoss: 0.000389\n",
      "Training stage for Flod 2 Epoch: 95 [19200/37476                 (64%)]\tLoss: 0.021351\n",
      "Training stage for Flod 2 Epoch: 95 [22400/37476                 (75%)]\tLoss: 0.006388\n",
      "Training stage for Flod 2 Epoch: 95 [25600/37476                 (85%)]\tLoss: 0.059990\n",
      "Training stage for Flod 2 Epoch: 95 [28800/37476                 (96%)]\tLoss: 0.042299\n",
      "Test set for fold2: Average Loss:           1.5358, Accuracy: 14772/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 96 [0/37476                 (0%)]\tLoss: 0.002996\n",
      "Training stage for Flod 2 Epoch: 96 [3200/37476                 (11%)]\tLoss: 0.027636\n",
      "Training stage for Flod 2 Epoch: 96 [6400/37476                 (21%)]\tLoss: 0.023673\n",
      "Training stage for Flod 2 Epoch: 96 [9600/37476                 (32%)]\tLoss: 0.007360\n",
      "Training stage for Flod 2 Epoch: 96 [12800/37476                 (43%)]\tLoss: 0.039226\n",
      "Training stage for Flod 2 Epoch: 96 [16000/37476                 (53%)]\tLoss: 0.007093\n",
      "Training stage for Flod 2 Epoch: 96 [19200/37476                 (64%)]\tLoss: 0.007870\n",
      "Training stage for Flod 2 Epoch: 96 [22400/37476                 (75%)]\tLoss: 0.000006\n",
      "Training stage for Flod 2 Epoch: 96 [25600/37476                 (85%)]\tLoss: 0.000989\n",
      "Training stage for Flod 2 Epoch: 96 [28800/37476                 (96%)]\tLoss: 0.001591\n",
      "Test set for fold2: Average Loss:           1.2601, Accuracy: 14782/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 97 [0/37476                 (0%)]\tLoss: 0.053420\n",
      "Training stage for Flod 2 Epoch: 97 [3200/37476                 (11%)]\tLoss: 0.021510\n",
      "Training stage for Flod 2 Epoch: 97 [6400/37476                 (21%)]\tLoss: 0.002905\n",
      "Training stage for Flod 2 Epoch: 97 [9600/37476                 (32%)]\tLoss: 0.019697\n",
      "Training stage for Flod 2 Epoch: 97 [12800/37476                 (43%)]\tLoss: 0.005626\n",
      "Training stage for Flod 2 Epoch: 97 [16000/37476                 (53%)]\tLoss: 0.020985\n",
      "Training stage for Flod 2 Epoch: 97 [19200/37476                 (64%)]\tLoss: 0.001567\n",
      "Training stage for Flod 2 Epoch: 97 [22400/37476                 (75%)]\tLoss: 0.044117\n",
      "Training stage for Flod 2 Epoch: 97 [25600/37476                 (85%)]\tLoss: 0.000110\n",
      "Training stage for Flod 2 Epoch: 97 [28800/37476                 (96%)]\tLoss: 0.086601\n",
      "Test set for fold2: Average Loss:           1.3039, Accuracy: 14793/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 98 [0/37476                 (0%)]\tLoss: 0.033660\n",
      "Training stage for Flod 2 Epoch: 98 [3200/37476                 (11%)]\tLoss: 0.043941\n",
      "Training stage for Flod 2 Epoch: 98 [6400/37476                 (21%)]\tLoss: 0.059423\n",
      "Training stage for Flod 2 Epoch: 98 [9600/37476                 (32%)]\tLoss: 0.013492\n",
      "Training stage for Flod 2 Epoch: 98 [12800/37476                 (43%)]\tLoss: 0.001223\n",
      "Training stage for Flod 2 Epoch: 98 [16000/37476                 (53%)]\tLoss: 0.019888\n",
      "Training stage for Flod 2 Epoch: 98 [19200/37476                 (64%)]\tLoss: 0.020392\n",
      "Training stage for Flod 2 Epoch: 98 [22400/37476                 (75%)]\tLoss: 0.000063\n",
      "Training stage for Flod 2 Epoch: 98 [25600/37476                 (85%)]\tLoss: 0.000788\n",
      "Training stage for Flod 2 Epoch: 98 [28800/37476                 (96%)]\tLoss: 0.024322\n",
      "Test set for fold2: Average Loss:           1.3035, Accuracy: 14805/37476           (40%)\n",
      "Training stage for Flod 2 Epoch: 99 [0/37476                 (0%)]\tLoss: 0.008921\n",
      "Training stage for Flod 2 Epoch: 99 [3200/37476                 (11%)]\tLoss: 0.000087\n",
      "Training stage for Flod 2 Epoch: 99 [6400/37476                 (21%)]\tLoss: 0.136179\n",
      "Training stage for Flod 2 Epoch: 99 [9600/37476                 (32%)]\tLoss: 0.118426\n",
      "Training stage for Flod 2 Epoch: 99 [12800/37476                 (43%)]\tLoss: 0.000305\n",
      "Training stage for Flod 2 Epoch: 99 [16000/37476                 (53%)]\tLoss: 0.008313\n",
      "Training stage for Flod 2 Epoch: 99 [19200/37476                 (64%)]\tLoss: 0.027554\n",
      "Training stage for Flod 2 Epoch: 99 [22400/37476                 (75%)]\tLoss: 0.019970\n",
      "Training stage for Flod 2 Epoch: 99 [25600/37476                 (85%)]\tLoss: 0.001505\n",
      "Training stage for Flod 2 Epoch: 99 [28800/37476                 (96%)]\tLoss: 0.011992\n",
      "Test set for fold2: Average Loss:           1.2585, Accuracy: 14799/37476           (39%)\n",
      "Training stage for Flod 2 Epoch: 100 [0/37476                 (0%)]\tLoss: 0.000173\n",
      "Training stage for Flod 2 Epoch: 100 [3200/37476                 (11%)]\tLoss: 0.023988\n",
      "Training stage for Flod 2 Epoch: 100 [6400/37476                 (21%)]\tLoss: 0.012058\n",
      "Training stage for Flod 2 Epoch: 100 [9600/37476                 (32%)]\tLoss: 0.004426\n",
      "Training stage for Flod 2 Epoch: 100 [12800/37476                 (43%)]\tLoss: 0.009406\n",
      "Training stage for Flod 2 Epoch: 100 [16000/37476                 (53%)]\tLoss: 0.012753\n",
      "Training stage for Flod 2 Epoch: 100 [19200/37476                 (64%)]\tLoss: 0.006029\n",
      "Training stage for Flod 2 Epoch: 100 [22400/37476                 (75%)]\tLoss: 0.004787\n",
      "Training stage for Flod 2 Epoch: 100 [25600/37476                 (85%)]\tLoss: 0.001351\n",
      "Training stage for Flod 2 Epoch: 100 [28800/37476                 (96%)]\tLoss: 0.048435\n",
      "Test set for fold2: Average Loss:           1.3060, Accuracy: 14785/37476           (39%)\n",
      "-------------------Fold 3-------------------\n",
      "Training stage for Flod 3 Epoch: 1 [0/37476                 (0%)]\tLoss: 0.714743\n",
      "Training stage for Flod 3 Epoch: 1 [3200/37476                 (11%)]\tLoss: 0.037145\n",
      "Training stage for Flod 3 Epoch: 1 [6400/37476                 (21%)]\tLoss: 0.021517\n",
      "Training stage for Flod 3 Epoch: 1 [9600/37476                 (32%)]\tLoss: 0.156327\n",
      "Training stage for Flod 3 Epoch: 1 [12800/37476                 (43%)]\tLoss: 0.127039\n",
      "Training stage for Flod 3 Epoch: 1 [16000/37476                 (53%)]\tLoss: 0.038139\n",
      "Training stage for Flod 3 Epoch: 1 [19200/37476                 (64%)]\tLoss: 0.042680\n",
      "Training stage for Flod 3 Epoch: 1 [22400/37476                 (75%)]\tLoss: 0.008212\n",
      "Training stage for Flod 3 Epoch: 1 [25600/37476                 (85%)]\tLoss: 0.049725\n",
      "Training stage for Flod 3 Epoch: 1 [28800/37476                 (96%)]\tLoss: 0.086508\n",
      "Test set for fold3: Average Loss:           0.7652, Accuracy: 14588/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 2 [0/37476                 (0%)]\tLoss: 0.003319\n",
      "Training stage for Flod 3 Epoch: 2 [3200/37476                 (11%)]\tLoss: 0.074528\n",
      "Training stage for Flod 3 Epoch: 2 [6400/37476                 (21%)]\tLoss: 0.158164\n",
      "Training stage for Flod 3 Epoch: 2 [9600/37476                 (32%)]\tLoss: 0.029552\n",
      "Training stage for Flod 3 Epoch: 2 [12800/37476                 (43%)]\tLoss: 0.120622\n",
      "Training stage for Flod 3 Epoch: 2 [16000/37476                 (53%)]\tLoss: 0.004307\n",
      "Training stage for Flod 3 Epoch: 2 [19200/37476                 (64%)]\tLoss: 0.002062\n",
      "Training stage for Flod 3 Epoch: 2 [22400/37476                 (75%)]\tLoss: 0.125970\n",
      "Training stage for Flod 3 Epoch: 2 [25600/37476                 (85%)]\tLoss: 0.171856\n",
      "Training stage for Flod 3 Epoch: 2 [28800/37476                 (96%)]\tLoss: 0.092136\n",
      "Test set for fold3: Average Loss:           0.7500, Accuracy: 14627/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 3 [0/37476                 (0%)]\tLoss: 0.216273\n",
      "Training stage for Flod 3 Epoch: 3 [3200/37476                 (11%)]\tLoss: 0.008535\n",
      "Training stage for Flod 3 Epoch: 3 [6400/37476                 (21%)]\tLoss: 0.166254\n",
      "Training stage for Flod 3 Epoch: 3 [9600/37476                 (32%)]\tLoss: 0.004125\n",
      "Training stage for Flod 3 Epoch: 3 [12800/37476                 (43%)]\tLoss: 0.005979\n",
      "Training stage for Flod 3 Epoch: 3 [16000/37476                 (53%)]\tLoss: 0.142786\n",
      "Training stage for Flod 3 Epoch: 3 [19200/37476                 (64%)]\tLoss: 0.004714\n",
      "Training stage for Flod 3 Epoch: 3 [22400/37476                 (75%)]\tLoss: 0.028077\n",
      "Training stage for Flod 3 Epoch: 3 [25600/37476                 (85%)]\tLoss: 0.005752\n",
      "Training stage for Flod 3 Epoch: 3 [28800/37476                 (96%)]\tLoss: 0.139023\n",
      "Test set for fold3: Average Loss:           0.7410, Accuracy: 14668/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 4 [0/37476                 (0%)]\tLoss: 0.043323\n",
      "Training stage for Flod 3 Epoch: 4 [3200/37476                 (11%)]\tLoss: 0.029044\n",
      "Training stage for Flod 3 Epoch: 4 [6400/37476                 (21%)]\tLoss: 0.005640\n",
      "Training stage for Flod 3 Epoch: 4 [9600/37476                 (32%)]\tLoss: 0.012811\n",
      "Training stage for Flod 3 Epoch: 4 [12800/37476                 (43%)]\tLoss: 0.013067\n",
      "Training stage for Flod 3 Epoch: 4 [16000/37476                 (53%)]\tLoss: 0.026101\n",
      "Training stage for Flod 3 Epoch: 4 [19200/37476                 (64%)]\tLoss: 0.005995\n",
      "Training stage for Flod 3 Epoch: 4 [22400/37476                 (75%)]\tLoss: 0.069010\n",
      "Training stage for Flod 3 Epoch: 4 [25600/37476                 (85%)]\tLoss: 0.031540\n",
      "Training stage for Flod 3 Epoch: 4 [28800/37476                 (96%)]\tLoss: 0.006444\n",
      "Test set for fold3: Average Loss:           0.7913, Accuracy: 14677/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 5 [0/37476                 (0%)]\tLoss: 0.178767\n",
      "Training stage for Flod 3 Epoch: 5 [3200/37476                 (11%)]\tLoss: 0.015120\n",
      "Training stage for Flod 3 Epoch: 5 [6400/37476                 (21%)]\tLoss: 0.021500\n",
      "Training stage for Flod 3 Epoch: 5 [9600/37476                 (32%)]\tLoss: 0.092110\n",
      "Training stage for Flod 3 Epoch: 5 [12800/37476                 (43%)]\tLoss: 0.065658\n",
      "Training stage for Flod 3 Epoch: 5 [16000/37476                 (53%)]\tLoss: 0.175809\n",
      "Training stage for Flod 3 Epoch: 5 [19200/37476                 (64%)]\tLoss: 0.061886\n",
      "Training stage for Flod 3 Epoch: 5 [22400/37476                 (75%)]\tLoss: 0.053040\n",
      "Training stage for Flod 3 Epoch: 5 [25600/37476                 (85%)]\tLoss: 0.081032\n",
      "Training stage for Flod 3 Epoch: 5 [28800/37476                 (96%)]\tLoss: 0.140693\n",
      "Test set for fold3: Average Loss:           0.8036, Accuracy: 14693/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 6 [0/37476                 (0%)]\tLoss: 0.014100\n",
      "Training stage for Flod 3 Epoch: 6 [3200/37476                 (11%)]\tLoss: 0.020431\n",
      "Training stage for Flod 3 Epoch: 6 [6400/37476                 (21%)]\tLoss: 0.164855\n",
      "Training stage for Flod 3 Epoch: 6 [9600/37476                 (32%)]\tLoss: 0.001113\n",
      "Training stage for Flod 3 Epoch: 6 [12800/37476                 (43%)]\tLoss: 0.113174\n",
      "Training stage for Flod 3 Epoch: 6 [16000/37476                 (53%)]\tLoss: 0.019033\n",
      "Training stage for Flod 3 Epoch: 6 [19200/37476                 (64%)]\tLoss: 0.071771\n",
      "Training stage for Flod 3 Epoch: 6 [22400/37476                 (75%)]\tLoss: 0.064782\n",
      "Training stage for Flod 3 Epoch: 6 [25600/37476                 (85%)]\tLoss: 0.016006\n",
      "Training stage for Flod 3 Epoch: 6 [28800/37476                 (96%)]\tLoss: 0.083879\n",
      "Test set for fold3: Average Loss:           0.8106, Accuracy: 14680/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 7 [0/37476                 (0%)]\tLoss: 0.187100\n",
      "Training stage for Flod 3 Epoch: 7 [3200/37476                 (11%)]\tLoss: 0.079591\n",
      "Training stage for Flod 3 Epoch: 7 [6400/37476                 (21%)]\tLoss: 0.011508\n",
      "Training stage for Flod 3 Epoch: 7 [9600/37476                 (32%)]\tLoss: 0.130580\n",
      "Training stage for Flod 3 Epoch: 7 [12800/37476                 (43%)]\tLoss: 0.032097\n",
      "Training stage for Flod 3 Epoch: 7 [16000/37476                 (53%)]\tLoss: 0.031030\n",
      "Training stage for Flod 3 Epoch: 7 [19200/37476                 (64%)]\tLoss: 0.122233\n",
      "Training stage for Flod 3 Epoch: 7 [22400/37476                 (75%)]\tLoss: 0.048053\n",
      "Training stage for Flod 3 Epoch: 7 [25600/37476                 (85%)]\tLoss: 0.049316\n",
      "Training stage for Flod 3 Epoch: 7 [28800/37476                 (96%)]\tLoss: 0.046421\n",
      "Test set for fold3: Average Loss:           0.7814, Accuracy: 14599/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 8 [0/37476                 (0%)]\tLoss: 0.004611\n",
      "Training stage for Flod 3 Epoch: 8 [3200/37476                 (11%)]\tLoss: 0.041383\n",
      "Training stage for Flod 3 Epoch: 8 [6400/37476                 (21%)]\tLoss: 0.011502\n",
      "Training stage for Flod 3 Epoch: 8 [9600/37476                 (32%)]\tLoss: 0.041807\n",
      "Training stage for Flod 3 Epoch: 8 [12800/37476                 (43%)]\tLoss: 0.030757\n",
      "Training stage for Flod 3 Epoch: 8 [16000/37476                 (53%)]\tLoss: 0.019183\n",
      "Training stage for Flod 3 Epoch: 8 [19200/37476                 (64%)]\tLoss: 0.016228\n",
      "Training stage for Flod 3 Epoch: 8 [22400/37476                 (75%)]\tLoss: 0.142622\n",
      "Training stage for Flod 3 Epoch: 8 [25600/37476                 (85%)]\tLoss: 0.061218\n",
      "Training stage for Flod 3 Epoch: 8 [28800/37476                 (96%)]\tLoss: 0.146901\n",
      "Test set for fold3: Average Loss:           0.7830, Accuracy: 14688/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 9 [0/37476                 (0%)]\tLoss: 0.086575\n",
      "Training stage for Flod 3 Epoch: 9 [3200/37476                 (11%)]\tLoss: 0.218454\n",
      "Training stage for Flod 3 Epoch: 9 [6400/37476                 (21%)]\tLoss: 0.076221\n",
      "Training stage for Flod 3 Epoch: 9 [9600/37476                 (32%)]\tLoss: 0.021158\n",
      "Training stage for Flod 3 Epoch: 9 [12800/37476                 (43%)]\tLoss: 0.057770\n",
      "Training stage for Flod 3 Epoch: 9 [16000/37476                 (53%)]\tLoss: 0.002599\n",
      "Training stage for Flod 3 Epoch: 9 [19200/37476                 (64%)]\tLoss: 0.010675\n",
      "Training stage for Flod 3 Epoch: 9 [22400/37476                 (75%)]\tLoss: 0.007530\n",
      "Training stage for Flod 3 Epoch: 9 [25600/37476                 (85%)]\tLoss: 0.007471\n",
      "Training stage for Flod 3 Epoch: 9 [28800/37476                 (96%)]\tLoss: 0.002607\n",
      "Test set for fold3: Average Loss:           0.8305, Accuracy: 14672/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 10 [0/37476                 (0%)]\tLoss: 0.000911\n",
      "Training stage for Flod 3 Epoch: 10 [3200/37476                 (11%)]\tLoss: 0.058216\n",
      "Training stage for Flod 3 Epoch: 10 [6400/37476                 (21%)]\tLoss: 0.075355\n",
      "Training stage for Flod 3 Epoch: 10 [9600/37476                 (32%)]\tLoss: 0.139869\n",
      "Training stage for Flod 3 Epoch: 10 [12800/37476                 (43%)]\tLoss: 0.049601\n",
      "Training stage for Flod 3 Epoch: 10 [16000/37476                 (53%)]\tLoss: 0.108231\n",
      "Training stage for Flod 3 Epoch: 10 [19200/37476                 (64%)]\tLoss: 0.017287\n",
      "Training stage for Flod 3 Epoch: 10 [22400/37476                 (75%)]\tLoss: 0.057400\n",
      "Training stage for Flod 3 Epoch: 10 [25600/37476                 (85%)]\tLoss: 0.154491\n",
      "Training stage for Flod 3 Epoch: 10 [28800/37476                 (96%)]\tLoss: 0.046207\n",
      "Test set for fold3: Average Loss:           0.8782, Accuracy: 14686/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 11 [0/37476                 (0%)]\tLoss: 0.003505\n",
      "Training stage for Flod 3 Epoch: 11 [3200/37476                 (11%)]\tLoss: 0.003301\n",
      "Training stage for Flod 3 Epoch: 11 [6400/37476                 (21%)]\tLoss: 0.022020\n",
      "Training stage for Flod 3 Epoch: 11 [9600/37476                 (32%)]\tLoss: 0.102131\n",
      "Training stage for Flod 3 Epoch: 11 [12800/37476                 (43%)]\tLoss: 0.027807\n",
      "Training stage for Flod 3 Epoch: 11 [16000/37476                 (53%)]\tLoss: 0.008938\n",
      "Training stage for Flod 3 Epoch: 11 [19200/37476                 (64%)]\tLoss: 0.012979\n",
      "Training stage for Flod 3 Epoch: 11 [22400/37476                 (75%)]\tLoss: 0.009408\n",
      "Training stage for Flod 3 Epoch: 11 [25600/37476                 (85%)]\tLoss: 0.065280\n",
      "Training stage for Flod 3 Epoch: 11 [28800/37476                 (96%)]\tLoss: 0.002081\n",
      "Test set for fold3: Average Loss:           0.8283, Accuracy: 14722/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 12 [0/37476                 (0%)]\tLoss: 0.037763\n",
      "Training stage for Flod 3 Epoch: 12 [3200/37476                 (11%)]\tLoss: 0.084720\n",
      "Training stage for Flod 3 Epoch: 12 [6400/37476                 (21%)]\tLoss: 0.006826\n",
      "Training stage for Flod 3 Epoch: 12 [9600/37476                 (32%)]\tLoss: 0.002437\n",
      "Training stage for Flod 3 Epoch: 12 [12800/37476                 (43%)]\tLoss: 0.011095\n",
      "Training stage for Flod 3 Epoch: 12 [16000/37476                 (53%)]\tLoss: 0.061209\n",
      "Training stage for Flod 3 Epoch: 12 [19200/37476                 (64%)]\tLoss: 0.108445\n",
      "Training stage for Flod 3 Epoch: 12 [22400/37476                 (75%)]\tLoss: 0.002061\n",
      "Training stage for Flod 3 Epoch: 12 [25600/37476                 (85%)]\tLoss: 0.053274\n",
      "Training stage for Flod 3 Epoch: 12 [28800/37476                 (96%)]\tLoss: 0.085898\n",
      "Test set for fold3: Average Loss:           0.8065, Accuracy: 14569/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 13 [0/37476                 (0%)]\tLoss: 0.076202\n",
      "Training stage for Flod 3 Epoch: 13 [3200/37476                 (11%)]\tLoss: 0.012079\n",
      "Training stage for Flod 3 Epoch: 13 [6400/37476                 (21%)]\tLoss: 0.084698\n",
      "Training stage for Flod 3 Epoch: 13 [9600/37476                 (32%)]\tLoss: 0.010028\n",
      "Training stage for Flod 3 Epoch: 13 [12800/37476                 (43%)]\tLoss: 0.141444\n",
      "Training stage for Flod 3 Epoch: 13 [16000/37476                 (53%)]\tLoss: 0.041312\n",
      "Training stage for Flod 3 Epoch: 13 [19200/37476                 (64%)]\tLoss: 0.310268\n",
      "Training stage for Flod 3 Epoch: 13 [22400/37476                 (75%)]\tLoss: 0.086775\n",
      "Training stage for Flod 3 Epoch: 13 [25600/37476                 (85%)]\tLoss: 0.132649\n",
      "Training stage for Flod 3 Epoch: 13 [28800/37476                 (96%)]\tLoss: 0.059648\n",
      "Test set for fold3: Average Loss:           0.8312, Accuracy: 14704/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 14 [0/37476                 (0%)]\tLoss: 0.011009\n",
      "Training stage for Flod 3 Epoch: 14 [3200/37476                 (11%)]\tLoss: 0.050389\n",
      "Training stage for Flod 3 Epoch: 14 [6400/37476                 (21%)]\tLoss: 0.006460\n",
      "Training stage for Flod 3 Epoch: 14 [9600/37476                 (32%)]\tLoss: 0.015761\n",
      "Training stage for Flod 3 Epoch: 14 [12800/37476                 (43%)]\tLoss: 0.011142\n",
      "Training stage for Flod 3 Epoch: 14 [16000/37476                 (53%)]\tLoss: 0.162793\n",
      "Training stage for Flod 3 Epoch: 14 [19200/37476                 (64%)]\tLoss: 0.089950\n",
      "Training stage for Flod 3 Epoch: 14 [22400/37476                 (75%)]\tLoss: 0.060202\n",
      "Training stage for Flod 3 Epoch: 14 [25600/37476                 (85%)]\tLoss: 0.008362\n",
      "Training stage for Flod 3 Epoch: 14 [28800/37476                 (96%)]\tLoss: 0.021958\n",
      "Test set for fold3: Average Loss:           0.8212, Accuracy: 14734/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 15 [0/37476                 (0%)]\tLoss: 0.021464\n",
      "Training stage for Flod 3 Epoch: 15 [3200/37476                 (11%)]\tLoss: 0.021465\n",
      "Training stage for Flod 3 Epoch: 15 [6400/37476                 (21%)]\tLoss: 0.038517\n",
      "Training stage for Flod 3 Epoch: 15 [9600/37476                 (32%)]\tLoss: 0.012753\n",
      "Training stage for Flod 3 Epoch: 15 [12800/37476                 (43%)]\tLoss: 0.154552\n",
      "Training stage for Flod 3 Epoch: 15 [16000/37476                 (53%)]\tLoss: 0.081457\n",
      "Training stage for Flod 3 Epoch: 15 [19200/37476                 (64%)]\tLoss: 0.004547\n",
      "Training stage for Flod 3 Epoch: 15 [22400/37476                 (75%)]\tLoss: 0.044022\n",
      "Training stage for Flod 3 Epoch: 15 [25600/37476                 (85%)]\tLoss: 0.288198\n",
      "Training stage for Flod 3 Epoch: 15 [28800/37476                 (96%)]\tLoss: 0.014295\n",
      "Test set for fold3: Average Loss:           0.8250, Accuracy: 14731/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 16 [0/37476                 (0%)]\tLoss: 0.002640\n",
      "Training stage for Flod 3 Epoch: 16 [3200/37476                 (11%)]\tLoss: 0.001362\n",
      "Training stage for Flod 3 Epoch: 16 [6400/37476                 (21%)]\tLoss: 0.011431\n",
      "Training stage for Flod 3 Epoch: 16 [9600/37476                 (32%)]\tLoss: 0.000269\n",
      "Training stage for Flod 3 Epoch: 16 [12800/37476                 (43%)]\tLoss: 0.047088\n",
      "Training stage for Flod 3 Epoch: 16 [16000/37476                 (53%)]\tLoss: 0.023011\n",
      "Training stage for Flod 3 Epoch: 16 [19200/37476                 (64%)]\tLoss: 0.171346\n",
      "Training stage for Flod 3 Epoch: 16 [22400/37476                 (75%)]\tLoss: 0.067773\n",
      "Training stage for Flod 3 Epoch: 16 [25600/37476                 (85%)]\tLoss: 0.025469\n",
      "Training stage for Flod 3 Epoch: 16 [28800/37476                 (96%)]\tLoss: 0.013832\n",
      "Test set for fold3: Average Loss:           0.8984, Accuracy: 14694/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 17 [0/37476                 (0%)]\tLoss: 0.059926\n",
      "Training stage for Flod 3 Epoch: 17 [3200/37476                 (11%)]\tLoss: 0.028376\n",
      "Training stage for Flod 3 Epoch: 17 [6400/37476                 (21%)]\tLoss: 0.010398\n",
      "Training stage for Flod 3 Epoch: 17 [9600/37476                 (32%)]\tLoss: 0.033211\n",
      "Training stage for Flod 3 Epoch: 17 [12800/37476                 (43%)]\tLoss: 0.008812\n",
      "Training stage for Flod 3 Epoch: 17 [16000/37476                 (53%)]\tLoss: 0.003366\n",
      "Training stage for Flod 3 Epoch: 17 [19200/37476                 (64%)]\tLoss: 0.185438\n",
      "Training stage for Flod 3 Epoch: 17 [22400/37476                 (75%)]\tLoss: 0.001288\n",
      "Training stage for Flod 3 Epoch: 17 [25600/37476                 (85%)]\tLoss: 0.034411\n",
      "Training stage for Flod 3 Epoch: 17 [28800/37476                 (96%)]\tLoss: 0.001844\n",
      "Test set for fold3: Average Loss:           0.8392, Accuracy: 14737/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 18 [0/37476                 (0%)]\tLoss: 0.032384\n",
      "Training stage for Flod 3 Epoch: 18 [3200/37476                 (11%)]\tLoss: 0.078386\n",
      "Training stage for Flod 3 Epoch: 18 [6400/37476                 (21%)]\tLoss: 0.129238\n",
      "Training stage for Flod 3 Epoch: 18 [9600/37476                 (32%)]\tLoss: 0.015901\n",
      "Training stage for Flod 3 Epoch: 18 [12800/37476                 (43%)]\tLoss: 0.001895\n",
      "Training stage for Flod 3 Epoch: 18 [16000/37476                 (53%)]\tLoss: 0.087982\n",
      "Training stage for Flod 3 Epoch: 18 [19200/37476                 (64%)]\tLoss: 0.010272\n",
      "Training stage for Flod 3 Epoch: 18 [22400/37476                 (75%)]\tLoss: 0.089863\n",
      "Training stage for Flod 3 Epoch: 18 [25600/37476                 (85%)]\tLoss: 0.027641\n",
      "Training stage for Flod 3 Epoch: 18 [28800/37476                 (96%)]\tLoss: 0.014850\n",
      "Test set for fold3: Average Loss:           0.8292, Accuracy: 14734/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 19 [0/37476                 (0%)]\tLoss: 0.018386\n",
      "Training stage for Flod 3 Epoch: 19 [3200/37476                 (11%)]\tLoss: 0.080054\n",
      "Training stage for Flod 3 Epoch: 19 [6400/37476                 (21%)]\tLoss: 0.043412\n",
      "Training stage for Flod 3 Epoch: 19 [9600/37476                 (32%)]\tLoss: 0.082633\n",
      "Training stage for Flod 3 Epoch: 19 [12800/37476                 (43%)]\tLoss: 0.023322\n",
      "Training stage for Flod 3 Epoch: 19 [16000/37476                 (53%)]\tLoss: 0.065894\n",
      "Training stage for Flod 3 Epoch: 19 [19200/37476                 (64%)]\tLoss: 0.007021\n",
      "Training stage for Flod 3 Epoch: 19 [22400/37476                 (75%)]\tLoss: 0.136254\n",
      "Training stage for Flod 3 Epoch: 19 [25600/37476                 (85%)]\tLoss: 0.049340\n",
      "Training stage for Flod 3 Epoch: 19 [28800/37476                 (96%)]\tLoss: 0.037290\n",
      "Test set for fold3: Average Loss:           0.8440, Accuracy: 14713/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 20 [0/37476                 (0%)]\tLoss: 0.059346\n",
      "Training stage for Flod 3 Epoch: 20 [3200/37476                 (11%)]\tLoss: 0.032065\n",
      "Training stage for Flod 3 Epoch: 20 [6400/37476                 (21%)]\tLoss: 0.031298\n",
      "Training stage for Flod 3 Epoch: 20 [9600/37476                 (32%)]\tLoss: 0.003967\n",
      "Training stage for Flod 3 Epoch: 20 [12800/37476                 (43%)]\tLoss: 0.090228\n",
      "Training stage for Flod 3 Epoch: 20 [16000/37476                 (53%)]\tLoss: 0.098089\n",
      "Training stage for Flod 3 Epoch: 20 [19200/37476                 (64%)]\tLoss: 0.007507\n",
      "Training stage for Flod 3 Epoch: 20 [22400/37476                 (75%)]\tLoss: 0.003168\n",
      "Training stage for Flod 3 Epoch: 20 [25600/37476                 (85%)]\tLoss: 0.103554\n",
      "Training stage for Flod 3 Epoch: 20 [28800/37476                 (96%)]\tLoss: 0.006739\n",
      "Test set for fold3: Average Loss:           0.8543, Accuracy: 14687/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 21 [0/37476                 (0%)]\tLoss: 0.053591\n",
      "Training stage for Flod 3 Epoch: 21 [3200/37476                 (11%)]\tLoss: 0.028708\n",
      "Training stage for Flod 3 Epoch: 21 [6400/37476                 (21%)]\tLoss: 0.032771\n",
      "Training stage for Flod 3 Epoch: 21 [9600/37476                 (32%)]\tLoss: 0.002142\n",
      "Training stage for Flod 3 Epoch: 21 [12800/37476                 (43%)]\tLoss: 0.002632\n",
      "Training stage for Flod 3 Epoch: 21 [16000/37476                 (53%)]\tLoss: 0.153290\n",
      "Training stage for Flod 3 Epoch: 21 [19200/37476                 (64%)]\tLoss: 0.000712\n",
      "Training stage for Flod 3 Epoch: 21 [22400/37476                 (75%)]\tLoss: 0.010354\n",
      "Training stage for Flod 3 Epoch: 21 [25600/37476                 (85%)]\tLoss: 0.075150\n",
      "Training stage for Flod 3 Epoch: 21 [28800/37476                 (96%)]\tLoss: 0.012851\n",
      "Test set for fold3: Average Loss:           0.8356, Accuracy: 14742/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 22 [0/37476                 (0%)]\tLoss: 0.013316\n",
      "Training stage for Flod 3 Epoch: 22 [3200/37476                 (11%)]\tLoss: 0.015976\n",
      "Training stage for Flod 3 Epoch: 22 [6400/37476                 (21%)]\tLoss: 0.008635\n",
      "Training stage for Flod 3 Epoch: 22 [9600/37476                 (32%)]\tLoss: 0.012338\n",
      "Training stage for Flod 3 Epoch: 22 [12800/37476                 (43%)]\tLoss: 0.026578\n",
      "Training stage for Flod 3 Epoch: 22 [16000/37476                 (53%)]\tLoss: 0.145905\n",
      "Training stage for Flod 3 Epoch: 22 [19200/37476                 (64%)]\tLoss: 0.029217\n",
      "Training stage for Flod 3 Epoch: 22 [22400/37476                 (75%)]\tLoss: 0.029115\n",
      "Training stage for Flod 3 Epoch: 22 [25600/37476                 (85%)]\tLoss: 0.132172\n",
      "Training stage for Flod 3 Epoch: 22 [28800/37476                 (96%)]\tLoss: 0.000654\n",
      "Test set for fold3: Average Loss:           0.9149, Accuracy: 14554/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 23 [0/37476                 (0%)]\tLoss: 0.097748\n",
      "Training stage for Flod 3 Epoch: 23 [3200/37476                 (11%)]\tLoss: 0.269890\n",
      "Training stage for Flod 3 Epoch: 23 [6400/37476                 (21%)]\tLoss: 0.088047\n",
      "Training stage for Flod 3 Epoch: 23 [9600/37476                 (32%)]\tLoss: 0.027169\n",
      "Training stage for Flod 3 Epoch: 23 [12800/37476                 (43%)]\tLoss: 0.135023\n",
      "Training stage for Flod 3 Epoch: 23 [16000/37476                 (53%)]\tLoss: 0.002316\n",
      "Training stage for Flod 3 Epoch: 23 [19200/37476                 (64%)]\tLoss: 0.021567\n",
      "Training stage for Flod 3 Epoch: 23 [22400/37476                 (75%)]\tLoss: 0.011216\n",
      "Training stage for Flod 3 Epoch: 23 [25600/37476                 (85%)]\tLoss: 0.006929\n",
      "Training stage for Flod 3 Epoch: 23 [28800/37476                 (96%)]\tLoss: 0.032412\n",
      "Test set for fold3: Average Loss:           0.8941, Accuracy: 14741/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 24 [0/37476                 (0%)]\tLoss: 0.008299\n",
      "Training stage for Flod 3 Epoch: 24 [3200/37476                 (11%)]\tLoss: 0.052246\n",
      "Training stage for Flod 3 Epoch: 24 [6400/37476                 (21%)]\tLoss: 0.005088\n",
      "Training stage for Flod 3 Epoch: 24 [9600/37476                 (32%)]\tLoss: 0.016474\n",
      "Training stage for Flod 3 Epoch: 24 [12800/37476                 (43%)]\tLoss: 0.022418\n",
      "Training stage for Flod 3 Epoch: 24 [16000/37476                 (53%)]\tLoss: 0.022934\n",
      "Training stage for Flod 3 Epoch: 24 [19200/37476                 (64%)]\tLoss: 0.064764\n",
      "Training stage for Flod 3 Epoch: 24 [22400/37476                 (75%)]\tLoss: 0.038593\n",
      "Training stage for Flod 3 Epoch: 24 [25600/37476                 (85%)]\tLoss: 0.007846\n",
      "Training stage for Flod 3 Epoch: 24 [28800/37476                 (96%)]\tLoss: 0.076961\n",
      "Test set for fold3: Average Loss:           0.9452, Accuracy: 14726/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 25 [0/37476                 (0%)]\tLoss: 0.145207\n",
      "Training stage for Flod 3 Epoch: 25 [3200/37476                 (11%)]\tLoss: 0.002142\n",
      "Training stage for Flod 3 Epoch: 25 [6400/37476                 (21%)]\tLoss: 0.040444\n",
      "Training stage for Flod 3 Epoch: 25 [9600/37476                 (32%)]\tLoss: 0.018278\n",
      "Training stage for Flod 3 Epoch: 25 [12800/37476                 (43%)]\tLoss: 0.007939\n",
      "Training stage for Flod 3 Epoch: 25 [16000/37476                 (53%)]\tLoss: 0.007701\n",
      "Training stage for Flod 3 Epoch: 25 [19200/37476                 (64%)]\tLoss: 0.024804\n",
      "Training stage for Flod 3 Epoch: 25 [22400/37476                 (75%)]\tLoss: 0.004256\n",
      "Training stage for Flod 3 Epoch: 25 [25600/37476                 (85%)]\tLoss: 0.004654\n",
      "Training stage for Flod 3 Epoch: 25 [28800/37476                 (96%)]\tLoss: 0.157291\n",
      "Test set for fold3: Average Loss:           0.9908, Accuracy: 14684/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 26 [0/37476                 (0%)]\tLoss: 0.094716\n",
      "Training stage for Flod 3 Epoch: 26 [3200/37476                 (11%)]\tLoss: 0.084477\n",
      "Training stage for Flod 3 Epoch: 26 [6400/37476                 (21%)]\tLoss: 0.112795\n",
      "Training stage for Flod 3 Epoch: 26 [9600/37476                 (32%)]\tLoss: 0.021102\n",
      "Training stage for Flod 3 Epoch: 26 [12800/37476                 (43%)]\tLoss: 0.006937\n",
      "Training stage for Flod 3 Epoch: 26 [16000/37476                 (53%)]\tLoss: 0.102093\n",
      "Training stage for Flod 3 Epoch: 26 [19200/37476                 (64%)]\tLoss: 0.055161\n",
      "Training stage for Flod 3 Epoch: 26 [22400/37476                 (75%)]\tLoss: 0.320494\n",
      "Training stage for Flod 3 Epoch: 26 [25600/37476                 (85%)]\tLoss: 0.002674\n",
      "Training stage for Flod 3 Epoch: 26 [28800/37476                 (96%)]\tLoss: 0.033381\n",
      "Test set for fold3: Average Loss:           0.9277, Accuracy: 14750/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 27 [0/37476                 (0%)]\tLoss: 0.075424\n",
      "Training stage for Flod 3 Epoch: 27 [3200/37476                 (11%)]\tLoss: 0.017950\n",
      "Training stage for Flod 3 Epoch: 27 [6400/37476                 (21%)]\tLoss: 0.057191\n",
      "Training stage for Flod 3 Epoch: 27 [9600/37476                 (32%)]\tLoss: 0.016650\n",
      "Training stage for Flod 3 Epoch: 27 [12800/37476                 (43%)]\tLoss: 0.073731\n",
      "Training stage for Flod 3 Epoch: 27 [16000/37476                 (53%)]\tLoss: 0.239529\n",
      "Training stage for Flod 3 Epoch: 27 [19200/37476                 (64%)]\tLoss: 0.111328\n",
      "Training stage for Flod 3 Epoch: 27 [22400/37476                 (75%)]\tLoss: 0.021539\n",
      "Training stage for Flod 3 Epoch: 27 [25600/37476                 (85%)]\tLoss: 0.046630\n",
      "Training stage for Flod 3 Epoch: 27 [28800/37476                 (96%)]\tLoss: 0.129200\n",
      "Test set for fold3: Average Loss:           0.8770, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 28 [0/37476                 (0%)]\tLoss: 0.022691\n",
      "Training stage for Flod 3 Epoch: 28 [3200/37476                 (11%)]\tLoss: 0.028593\n",
      "Training stage for Flod 3 Epoch: 28 [6400/37476                 (21%)]\tLoss: 0.054133\n",
      "Training stage for Flod 3 Epoch: 28 [9600/37476                 (32%)]\tLoss: 0.012804\n",
      "Training stage for Flod 3 Epoch: 28 [12800/37476                 (43%)]\tLoss: 0.095844\n",
      "Training stage for Flod 3 Epoch: 28 [16000/37476                 (53%)]\tLoss: 0.089400\n",
      "Training stage for Flod 3 Epoch: 28 [19200/37476                 (64%)]\tLoss: 0.031022\n",
      "Training stage for Flod 3 Epoch: 28 [22400/37476                 (75%)]\tLoss: 0.023659\n",
      "Training stage for Flod 3 Epoch: 28 [25600/37476                 (85%)]\tLoss: 0.029172\n",
      "Training stage for Flod 3 Epoch: 28 [28800/37476                 (96%)]\tLoss: 0.065555\n",
      "Test set for fold3: Average Loss:           0.8142, Accuracy: 14753/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 29 [0/37476                 (0%)]\tLoss: 0.020411\n",
      "Training stage for Flod 3 Epoch: 29 [3200/37476                 (11%)]\tLoss: 0.000932\n",
      "Training stage for Flod 3 Epoch: 29 [6400/37476                 (21%)]\tLoss: 0.011950\n",
      "Training stage for Flod 3 Epoch: 29 [9600/37476                 (32%)]\tLoss: 0.007997\n",
      "Training stage for Flod 3 Epoch: 29 [12800/37476                 (43%)]\tLoss: 0.008147\n",
      "Training stage for Flod 3 Epoch: 29 [16000/37476                 (53%)]\tLoss: 0.109597\n",
      "Training stage for Flod 3 Epoch: 29 [19200/37476                 (64%)]\tLoss: 0.006089\n",
      "Training stage for Flod 3 Epoch: 29 [22400/37476                 (75%)]\tLoss: 0.056859\n",
      "Training stage for Flod 3 Epoch: 29 [25600/37476                 (85%)]\tLoss: 0.010561\n",
      "Training stage for Flod 3 Epoch: 29 [28800/37476                 (96%)]\tLoss: 0.081455\n",
      "Test set for fold3: Average Loss:           0.9635, Accuracy: 14741/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 30 [0/37476                 (0%)]\tLoss: 0.048358\n",
      "Training stage for Flod 3 Epoch: 30 [3200/37476                 (11%)]\tLoss: 0.109775\n",
      "Training stage for Flod 3 Epoch: 30 [6400/37476                 (21%)]\tLoss: 0.002783\n",
      "Training stage for Flod 3 Epoch: 30 [9600/37476                 (32%)]\tLoss: 0.025780\n",
      "Training stage for Flod 3 Epoch: 30 [12800/37476                 (43%)]\tLoss: 0.005039\n",
      "Training stage for Flod 3 Epoch: 30 [16000/37476                 (53%)]\tLoss: 0.052945\n",
      "Training stage for Flod 3 Epoch: 30 [19200/37476                 (64%)]\tLoss: 0.061828\n",
      "Training stage for Flod 3 Epoch: 30 [22400/37476                 (75%)]\tLoss: 0.003532\n",
      "Training stage for Flod 3 Epoch: 30 [25600/37476                 (85%)]\tLoss: 0.021043\n",
      "Training stage for Flod 3 Epoch: 30 [28800/37476                 (96%)]\tLoss: 0.021052\n",
      "Test set for fold3: Average Loss:           0.9220, Accuracy: 14748/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 31 [0/37476                 (0%)]\tLoss: 0.024674\n",
      "Training stage for Flod 3 Epoch: 31 [3200/37476                 (11%)]\tLoss: 0.000338\n",
      "Training stage for Flod 3 Epoch: 31 [6400/37476                 (21%)]\tLoss: 0.010179\n",
      "Training stage for Flod 3 Epoch: 31 [9600/37476                 (32%)]\tLoss: 0.019852\n",
      "Training stage for Flod 3 Epoch: 31 [12800/37476                 (43%)]\tLoss: 0.029762\n",
      "Training stage for Flod 3 Epoch: 31 [16000/37476                 (53%)]\tLoss: 0.041883\n",
      "Training stage for Flod 3 Epoch: 31 [19200/37476                 (64%)]\tLoss: 0.376335\n",
      "Training stage for Flod 3 Epoch: 31 [22400/37476                 (75%)]\tLoss: 0.042214\n",
      "Training stage for Flod 3 Epoch: 31 [25600/37476                 (85%)]\tLoss: 0.024883\n",
      "Training stage for Flod 3 Epoch: 31 [28800/37476                 (96%)]\tLoss: 0.005420\n",
      "Test set for fold3: Average Loss:           0.9301, Accuracy: 14743/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 32 [0/37476                 (0%)]\tLoss: 0.032146\n",
      "Training stage for Flod 3 Epoch: 32 [3200/37476                 (11%)]\tLoss: 0.005958\n",
      "Training stage for Flod 3 Epoch: 32 [6400/37476                 (21%)]\tLoss: 0.101027\n",
      "Training stage for Flod 3 Epoch: 32 [9600/37476                 (32%)]\tLoss: 0.021435\n",
      "Training stage for Flod 3 Epoch: 32 [12800/37476                 (43%)]\tLoss: 0.151445\n",
      "Training stage for Flod 3 Epoch: 32 [16000/37476                 (53%)]\tLoss: 0.044462\n",
      "Training stage for Flod 3 Epoch: 32 [19200/37476                 (64%)]\tLoss: 0.008442\n",
      "Training stage for Flod 3 Epoch: 32 [22400/37476                 (75%)]\tLoss: 0.011007\n",
      "Training stage for Flod 3 Epoch: 32 [25600/37476                 (85%)]\tLoss: 0.072206\n",
      "Training stage for Flod 3 Epoch: 32 [28800/37476                 (96%)]\tLoss: 0.029448\n",
      "Test set for fold3: Average Loss:           0.9645, Accuracy: 14745/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 33 [0/37476                 (0%)]\tLoss: 0.037348\n",
      "Training stage for Flod 3 Epoch: 33 [3200/37476                 (11%)]\tLoss: 0.030778\n",
      "Training stage for Flod 3 Epoch: 33 [6400/37476                 (21%)]\tLoss: 0.000351\n",
      "Training stage for Flod 3 Epoch: 33 [9600/37476                 (32%)]\tLoss: 0.035681\n",
      "Training stage for Flod 3 Epoch: 33 [12800/37476                 (43%)]\tLoss: 0.045141\n",
      "Training stage for Flod 3 Epoch: 33 [16000/37476                 (53%)]\tLoss: 0.072505\n",
      "Training stage for Flod 3 Epoch: 33 [19200/37476                 (64%)]\tLoss: 0.001699\n",
      "Training stage for Flod 3 Epoch: 33 [22400/37476                 (75%)]\tLoss: 0.000489\n",
      "Training stage for Flod 3 Epoch: 33 [25600/37476                 (85%)]\tLoss: 0.070137\n",
      "Training stage for Flod 3 Epoch: 33 [28800/37476                 (96%)]\tLoss: 0.006114\n",
      "Test set for fold3: Average Loss:           0.9905, Accuracy: 14729/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 34 [0/37476                 (0%)]\tLoss: 0.001651\n",
      "Training stage for Flod 3 Epoch: 34 [3200/37476                 (11%)]\tLoss: 0.001470\n",
      "Training stage for Flod 3 Epoch: 34 [6400/37476                 (21%)]\tLoss: 0.094429\n",
      "Training stage for Flod 3 Epoch: 34 [9600/37476                 (32%)]\tLoss: 0.044405\n",
      "Training stage for Flod 3 Epoch: 34 [12800/37476                 (43%)]\tLoss: 0.083762\n",
      "Training stage for Flod 3 Epoch: 34 [16000/37476                 (53%)]\tLoss: 0.029108\n",
      "Training stage for Flod 3 Epoch: 34 [19200/37476                 (64%)]\tLoss: 0.063122\n",
      "Training stage for Flod 3 Epoch: 34 [22400/37476                 (75%)]\tLoss: 0.164807\n",
      "Training stage for Flod 3 Epoch: 34 [25600/37476                 (85%)]\tLoss: 0.002207\n",
      "Training stage for Flod 3 Epoch: 34 [28800/37476                 (96%)]\tLoss: 0.022957\n",
      "Test set for fold3: Average Loss:           0.9798, Accuracy: 14725/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 35 [0/37476                 (0%)]\tLoss: 0.003215\n",
      "Training stage for Flod 3 Epoch: 35 [3200/37476                 (11%)]\tLoss: 0.003804\n",
      "Training stage for Flod 3 Epoch: 35 [6400/37476                 (21%)]\tLoss: 0.169930\n",
      "Training stage for Flod 3 Epoch: 35 [9600/37476                 (32%)]\tLoss: 0.074927\n",
      "Training stage for Flod 3 Epoch: 35 [12800/37476                 (43%)]\tLoss: 0.011457\n",
      "Training stage for Flod 3 Epoch: 35 [16000/37476                 (53%)]\tLoss: 0.005577\n",
      "Training stage for Flod 3 Epoch: 35 [19200/37476                 (64%)]\tLoss: 0.072907\n",
      "Training stage for Flod 3 Epoch: 35 [22400/37476                 (75%)]\tLoss: 0.014495\n",
      "Training stage for Flod 3 Epoch: 35 [25600/37476                 (85%)]\tLoss: 0.013640\n",
      "Training stage for Flod 3 Epoch: 35 [28800/37476                 (96%)]\tLoss: 0.084894\n",
      "Test set for fold3: Average Loss:           1.0147, Accuracy: 14722/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 36 [0/37476                 (0%)]\tLoss: 0.006641\n",
      "Training stage for Flod 3 Epoch: 36 [3200/37476                 (11%)]\tLoss: 0.013754\n",
      "Training stage for Flod 3 Epoch: 36 [6400/37476                 (21%)]\tLoss: 0.003316\n",
      "Training stage for Flod 3 Epoch: 36 [9600/37476                 (32%)]\tLoss: 0.002638\n",
      "Training stage for Flod 3 Epoch: 36 [12800/37476                 (43%)]\tLoss: 0.000862\n",
      "Training stage for Flod 3 Epoch: 36 [16000/37476                 (53%)]\tLoss: 0.015359\n",
      "Training stage for Flod 3 Epoch: 36 [19200/37476                 (64%)]\tLoss: 0.001862\n",
      "Training stage for Flod 3 Epoch: 36 [22400/37476                 (75%)]\tLoss: 0.018139\n",
      "Training stage for Flod 3 Epoch: 36 [25600/37476                 (85%)]\tLoss: 0.011529\n",
      "Training stage for Flod 3 Epoch: 36 [28800/37476                 (96%)]\tLoss: 0.014101\n",
      "Test set for fold3: Average Loss:           0.9649, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 37 [0/37476                 (0%)]\tLoss: 0.010263\n",
      "Training stage for Flod 3 Epoch: 37 [3200/37476                 (11%)]\tLoss: 0.137905\n",
      "Training stage for Flod 3 Epoch: 37 [6400/37476                 (21%)]\tLoss: 0.003349\n",
      "Training stage for Flod 3 Epoch: 37 [9600/37476                 (32%)]\tLoss: 0.011660\n",
      "Training stage for Flod 3 Epoch: 37 [12800/37476                 (43%)]\tLoss: 0.000745\n",
      "Training stage for Flod 3 Epoch: 37 [16000/37476                 (53%)]\tLoss: 0.044958\n",
      "Training stage for Flod 3 Epoch: 37 [19200/37476                 (64%)]\tLoss: 0.005119\n",
      "Training stage for Flod 3 Epoch: 37 [22400/37476                 (75%)]\tLoss: 0.001456\n",
      "Training stage for Flod 3 Epoch: 37 [25600/37476                 (85%)]\tLoss: 0.031279\n",
      "Training stage for Flod 3 Epoch: 37 [28800/37476                 (96%)]\tLoss: 0.007461\n",
      "Test set for fold3: Average Loss:           0.9347, Accuracy: 14723/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 38 [0/37476                 (0%)]\tLoss: 0.001885\n",
      "Training stage for Flod 3 Epoch: 38 [3200/37476                 (11%)]\tLoss: 0.004610\n",
      "Training stage for Flod 3 Epoch: 38 [6400/37476                 (21%)]\tLoss: 0.021489\n",
      "Training stage for Flod 3 Epoch: 38 [9600/37476                 (32%)]\tLoss: 0.005722\n",
      "Training stage for Flod 3 Epoch: 38 [12800/37476                 (43%)]\tLoss: 0.002555\n",
      "Training stage for Flod 3 Epoch: 38 [16000/37476                 (53%)]\tLoss: 0.092912\n",
      "Training stage for Flod 3 Epoch: 38 [19200/37476                 (64%)]\tLoss: 0.012113\n",
      "Training stage for Flod 3 Epoch: 38 [22400/37476                 (75%)]\tLoss: 0.075163\n",
      "Training stage for Flod 3 Epoch: 38 [25600/37476                 (85%)]\tLoss: 0.001027\n",
      "Training stage for Flod 3 Epoch: 38 [28800/37476                 (96%)]\tLoss: 0.216081\n",
      "Test set for fold3: Average Loss:           0.8448, Accuracy: 14764/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 39 [0/37476                 (0%)]\tLoss: 0.006516\n",
      "Training stage for Flod 3 Epoch: 39 [3200/37476                 (11%)]\tLoss: 0.029490\n",
      "Training stage for Flod 3 Epoch: 39 [6400/37476                 (21%)]\tLoss: 0.007880\n",
      "Training stage for Flod 3 Epoch: 39 [9600/37476                 (32%)]\tLoss: 0.023678\n",
      "Training stage for Flod 3 Epoch: 39 [12800/37476                 (43%)]\tLoss: 0.003724\n",
      "Training stage for Flod 3 Epoch: 39 [16000/37476                 (53%)]\tLoss: 0.305768\n",
      "Training stage for Flod 3 Epoch: 39 [19200/37476                 (64%)]\tLoss: 0.007141\n",
      "Training stage for Flod 3 Epoch: 39 [22400/37476                 (75%)]\tLoss: 0.080816\n",
      "Training stage for Flod 3 Epoch: 39 [25600/37476                 (85%)]\tLoss: 0.083908\n",
      "Training stage for Flod 3 Epoch: 39 [28800/37476                 (96%)]\tLoss: 0.024827\n",
      "Test set for fold3: Average Loss:           0.9641, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 40 [0/37476                 (0%)]\tLoss: 0.034138\n",
      "Training stage for Flod 3 Epoch: 40 [3200/37476                 (11%)]\tLoss: 0.047561\n",
      "Training stage for Flod 3 Epoch: 40 [6400/37476                 (21%)]\tLoss: 0.025055\n",
      "Training stage for Flod 3 Epoch: 40 [9600/37476                 (32%)]\tLoss: 0.016195\n",
      "Training stage for Flod 3 Epoch: 40 [12800/37476                 (43%)]\tLoss: 0.003133\n",
      "Training stage for Flod 3 Epoch: 40 [16000/37476                 (53%)]\tLoss: 0.031139\n",
      "Training stage for Flod 3 Epoch: 40 [19200/37476                 (64%)]\tLoss: 0.017690\n",
      "Training stage for Flod 3 Epoch: 40 [22400/37476                 (75%)]\tLoss: 0.010262\n",
      "Training stage for Flod 3 Epoch: 40 [25600/37476                 (85%)]\tLoss: 0.013073\n",
      "Training stage for Flod 3 Epoch: 40 [28800/37476                 (96%)]\tLoss: 0.013719\n",
      "Test set for fold3: Average Loss:           0.8686, Accuracy: 14732/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 41 [0/37476                 (0%)]\tLoss: 0.003327\n",
      "Training stage for Flod 3 Epoch: 41 [3200/37476                 (11%)]\tLoss: 0.005121\n",
      "Training stage for Flod 3 Epoch: 41 [6400/37476                 (21%)]\tLoss: 0.027304\n",
      "Training stage for Flod 3 Epoch: 41 [9600/37476                 (32%)]\tLoss: 0.010426\n",
      "Training stage for Flod 3 Epoch: 41 [12800/37476                 (43%)]\tLoss: 0.020716\n",
      "Training stage for Flod 3 Epoch: 41 [16000/37476                 (53%)]\tLoss: 0.005880\n",
      "Training stage for Flod 3 Epoch: 41 [19200/37476                 (64%)]\tLoss: 0.000512\n",
      "Training stage for Flod 3 Epoch: 41 [22400/37476                 (75%)]\tLoss: 0.102510\n",
      "Training stage for Flod 3 Epoch: 41 [25600/37476                 (85%)]\tLoss: 0.016373\n",
      "Training stage for Flod 3 Epoch: 41 [28800/37476                 (96%)]\tLoss: 0.026606\n",
      "Test set for fold3: Average Loss:           0.8731, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 42 [0/37476                 (0%)]\tLoss: 0.042558\n",
      "Training stage for Flod 3 Epoch: 42 [3200/37476                 (11%)]\tLoss: 0.026019\n",
      "Training stage for Flod 3 Epoch: 42 [6400/37476                 (21%)]\tLoss: 0.012571\n",
      "Training stage for Flod 3 Epoch: 42 [9600/37476                 (32%)]\tLoss: 0.010745\n",
      "Training stage for Flod 3 Epoch: 42 [12800/37476                 (43%)]\tLoss: 0.037900\n",
      "Training stage for Flod 3 Epoch: 42 [16000/37476                 (53%)]\tLoss: 0.115268\n",
      "Training stage for Flod 3 Epoch: 42 [19200/37476                 (64%)]\tLoss: 0.019648\n",
      "Training stage for Flod 3 Epoch: 42 [22400/37476                 (75%)]\tLoss: 0.023842\n",
      "Training stage for Flod 3 Epoch: 42 [25600/37476                 (85%)]\tLoss: 0.048104\n",
      "Training stage for Flod 3 Epoch: 42 [28800/37476                 (96%)]\tLoss: 0.137191\n",
      "Test set for fold3: Average Loss:           0.9347, Accuracy: 14752/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 43 [0/37476                 (0%)]\tLoss: 0.013552\n",
      "Training stage for Flod 3 Epoch: 43 [3200/37476                 (11%)]\tLoss: 0.012650\n",
      "Training stage for Flod 3 Epoch: 43 [6400/37476                 (21%)]\tLoss: 0.000451\n",
      "Training stage for Flod 3 Epoch: 43 [9600/37476                 (32%)]\tLoss: 0.014096\n",
      "Training stage for Flod 3 Epoch: 43 [12800/37476                 (43%)]\tLoss: 0.010965\n",
      "Training stage for Flod 3 Epoch: 43 [16000/37476                 (53%)]\tLoss: 0.030044\n",
      "Training stage for Flod 3 Epoch: 43 [19200/37476                 (64%)]\tLoss: 0.028773\n",
      "Training stage for Flod 3 Epoch: 43 [22400/37476                 (75%)]\tLoss: 0.019928\n",
      "Training stage for Flod 3 Epoch: 43 [25600/37476                 (85%)]\tLoss: 0.005390\n",
      "Training stage for Flod 3 Epoch: 43 [28800/37476                 (96%)]\tLoss: 0.140351\n",
      "Test set for fold3: Average Loss:           1.1474, Accuracy: 14708/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 44 [0/37476                 (0%)]\tLoss: 0.005940\n",
      "Training stage for Flod 3 Epoch: 44 [3200/37476                 (11%)]\tLoss: 0.191076\n",
      "Training stage for Flod 3 Epoch: 44 [6400/37476                 (21%)]\tLoss: 0.002106\n",
      "Training stage for Flod 3 Epoch: 44 [9600/37476                 (32%)]\tLoss: 0.016251\n",
      "Training stage for Flod 3 Epoch: 44 [12800/37476                 (43%)]\tLoss: 0.030851\n",
      "Training stage for Flod 3 Epoch: 44 [16000/37476                 (53%)]\tLoss: 0.004388\n",
      "Training stage for Flod 3 Epoch: 44 [19200/37476                 (64%)]\tLoss: 0.053719\n",
      "Training stage for Flod 3 Epoch: 44 [22400/37476                 (75%)]\tLoss: 0.058717\n",
      "Training stage for Flod 3 Epoch: 44 [25600/37476                 (85%)]\tLoss: 0.008877\n",
      "Training stage for Flod 3 Epoch: 44 [28800/37476                 (96%)]\tLoss: 0.030853\n",
      "Test set for fold3: Average Loss:           0.8766, Accuracy: 14768/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 45 [0/37476                 (0%)]\tLoss: 0.015772\n",
      "Training stage for Flod 3 Epoch: 45 [3200/37476                 (11%)]\tLoss: 0.026093\n",
      "Training stage for Flod 3 Epoch: 45 [6400/37476                 (21%)]\tLoss: 0.010912\n",
      "Training stage for Flod 3 Epoch: 45 [9600/37476                 (32%)]\tLoss: 0.060794\n",
      "Training stage for Flod 3 Epoch: 45 [12800/37476                 (43%)]\tLoss: 0.002874\n",
      "Training stage for Flod 3 Epoch: 45 [16000/37476                 (53%)]\tLoss: 0.029667\n",
      "Training stage for Flod 3 Epoch: 45 [19200/37476                 (64%)]\tLoss: 0.001616\n",
      "Training stage for Flod 3 Epoch: 45 [22400/37476                 (75%)]\tLoss: 0.008888\n",
      "Training stage for Flod 3 Epoch: 45 [25600/37476                 (85%)]\tLoss: 0.035516\n",
      "Training stage for Flod 3 Epoch: 45 [28800/37476                 (96%)]\tLoss: 0.007299\n",
      "Test set for fold3: Average Loss:           0.8746, Accuracy: 14732/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 46 [0/37476                 (0%)]\tLoss: 0.027134\n",
      "Training stage for Flod 3 Epoch: 46 [3200/37476                 (11%)]\tLoss: 0.002343\n",
      "Training stage for Flod 3 Epoch: 46 [6400/37476                 (21%)]\tLoss: 0.022836\n",
      "Training stage for Flod 3 Epoch: 46 [9600/37476                 (32%)]\tLoss: 0.011288\n",
      "Training stage for Flod 3 Epoch: 46 [12800/37476                 (43%)]\tLoss: 0.004126\n",
      "Training stage for Flod 3 Epoch: 46 [16000/37476                 (53%)]\tLoss: 0.016308\n",
      "Training stage for Flod 3 Epoch: 46 [19200/37476                 (64%)]\tLoss: 0.008235\n",
      "Training stage for Flod 3 Epoch: 46 [22400/37476                 (75%)]\tLoss: 0.003374\n",
      "Training stage for Flod 3 Epoch: 46 [25600/37476                 (85%)]\tLoss: 0.011233\n",
      "Training stage for Flod 3 Epoch: 46 [28800/37476                 (96%)]\tLoss: 0.018219\n",
      "Test set for fold3: Average Loss:           1.0124, Accuracy: 14733/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 47 [0/37476                 (0%)]\tLoss: 0.042787\n",
      "Training stage for Flod 3 Epoch: 47 [3200/37476                 (11%)]\tLoss: 0.008313\n",
      "Training stage for Flod 3 Epoch: 47 [6400/37476                 (21%)]\tLoss: 0.014222\n",
      "Training stage for Flod 3 Epoch: 47 [9600/37476                 (32%)]\tLoss: 0.000524\n",
      "Training stage for Flod 3 Epoch: 47 [12800/37476                 (43%)]\tLoss: 0.039015\n",
      "Training stage for Flod 3 Epoch: 47 [16000/37476                 (53%)]\tLoss: 0.042286\n",
      "Training stage for Flod 3 Epoch: 47 [19200/37476                 (64%)]\tLoss: 0.083754\n",
      "Training stage for Flod 3 Epoch: 47 [22400/37476                 (75%)]\tLoss: 0.025482\n",
      "Training stage for Flod 3 Epoch: 47 [25600/37476                 (85%)]\tLoss: 0.051461\n",
      "Training stage for Flod 3 Epoch: 47 [28800/37476                 (96%)]\tLoss: 0.011536\n",
      "Test set for fold3: Average Loss:           0.9776, Accuracy: 14761/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 48 [0/37476                 (0%)]\tLoss: 0.007237\n",
      "Training stage for Flod 3 Epoch: 48 [3200/37476                 (11%)]\tLoss: 0.248958\n",
      "Training stage for Flod 3 Epoch: 48 [6400/37476                 (21%)]\tLoss: 0.008866\n",
      "Training stage for Flod 3 Epoch: 48 [9600/37476                 (32%)]\tLoss: 0.020369\n",
      "Training stage for Flod 3 Epoch: 48 [12800/37476                 (43%)]\tLoss: 0.002523\n",
      "Training stage for Flod 3 Epoch: 48 [16000/37476                 (53%)]\tLoss: 0.000348\n",
      "Training stage for Flod 3 Epoch: 48 [19200/37476                 (64%)]\tLoss: 0.079639\n",
      "Training stage for Flod 3 Epoch: 48 [22400/37476                 (75%)]\tLoss: 0.016036\n",
      "Training stage for Flod 3 Epoch: 48 [25600/37476                 (85%)]\tLoss: 0.011061\n",
      "Training stage for Flod 3 Epoch: 48 [28800/37476                 (96%)]\tLoss: 0.011658\n",
      "Test set for fold3: Average Loss:           1.0188, Accuracy: 14770/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 49 [0/37476                 (0%)]\tLoss: 0.005440\n",
      "Training stage for Flod 3 Epoch: 49 [3200/37476                 (11%)]\tLoss: 0.013712\n",
      "Training stage for Flod 3 Epoch: 49 [6400/37476                 (21%)]\tLoss: 0.002627\n",
      "Training stage for Flod 3 Epoch: 49 [9600/37476                 (32%)]\tLoss: 0.002884\n",
      "Training stage for Flod 3 Epoch: 49 [12800/37476                 (43%)]\tLoss: 0.107189\n",
      "Training stage for Flod 3 Epoch: 49 [16000/37476                 (53%)]\tLoss: 0.033076\n",
      "Training stage for Flod 3 Epoch: 49 [19200/37476                 (64%)]\tLoss: 0.027743\n",
      "Training stage for Flod 3 Epoch: 49 [22400/37476                 (75%)]\tLoss: 0.211724\n",
      "Training stage for Flod 3 Epoch: 49 [25600/37476                 (85%)]\tLoss: 0.011487\n",
      "Training stage for Flod 3 Epoch: 49 [28800/37476                 (96%)]\tLoss: 0.000396\n",
      "Test set for fold3: Average Loss:           0.8978, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 50 [0/37476                 (0%)]\tLoss: 0.027362\n",
      "Training stage for Flod 3 Epoch: 50 [3200/37476                 (11%)]\tLoss: 0.012679\n",
      "Training stage for Flod 3 Epoch: 50 [6400/37476                 (21%)]\tLoss: 0.001915\n",
      "Training stage for Flod 3 Epoch: 50 [9600/37476                 (32%)]\tLoss: 0.054809\n",
      "Training stage for Flod 3 Epoch: 50 [12800/37476                 (43%)]\tLoss: 0.047443\n",
      "Training stage for Flod 3 Epoch: 50 [16000/37476                 (53%)]\tLoss: 0.019192\n",
      "Training stage for Flod 3 Epoch: 50 [19200/37476                 (64%)]\tLoss: 0.001168\n",
      "Training stage for Flod 3 Epoch: 50 [22400/37476                 (75%)]\tLoss: 0.016859\n",
      "Training stage for Flod 3 Epoch: 50 [25600/37476                 (85%)]\tLoss: 0.008962\n",
      "Training stage for Flod 3 Epoch: 50 [28800/37476                 (96%)]\tLoss: 0.000383\n",
      "Test set for fold3: Average Loss:           1.0705, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 51 [0/37476                 (0%)]\tLoss: 0.006913\n",
      "Training stage for Flod 3 Epoch: 51 [3200/37476                 (11%)]\tLoss: 0.026999\n",
      "Training stage for Flod 3 Epoch: 51 [6400/37476                 (21%)]\tLoss: 0.001637\n",
      "Training stage for Flod 3 Epoch: 51 [9600/37476                 (32%)]\tLoss: 0.051809\n",
      "Training stage for Flod 3 Epoch: 51 [12800/37476                 (43%)]\tLoss: 0.120124\n",
      "Training stage for Flod 3 Epoch: 51 [16000/37476                 (53%)]\tLoss: 0.141592\n",
      "Training stage for Flod 3 Epoch: 51 [19200/37476                 (64%)]\tLoss: 0.001753\n",
      "Training stage for Flod 3 Epoch: 51 [22400/37476                 (75%)]\tLoss: 0.014915\n",
      "Training stage for Flod 3 Epoch: 51 [25600/37476                 (85%)]\tLoss: 0.008432\n",
      "Training stage for Flod 3 Epoch: 51 [28800/37476                 (96%)]\tLoss: 0.181838\n",
      "Test set for fold3: Average Loss:           0.9377, Accuracy: 14755/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 52 [0/37476                 (0%)]\tLoss: 0.001623\n",
      "Training stage for Flod 3 Epoch: 52 [3200/37476                 (11%)]\tLoss: 0.004019\n",
      "Training stage for Flod 3 Epoch: 52 [6400/37476                 (21%)]\tLoss: 0.057815\n",
      "Training stage for Flod 3 Epoch: 52 [9600/37476                 (32%)]\tLoss: 0.007916\n",
      "Training stage for Flod 3 Epoch: 52 [12800/37476                 (43%)]\tLoss: 0.067913\n",
      "Training stage for Flod 3 Epoch: 52 [16000/37476                 (53%)]\tLoss: 0.000627\n",
      "Training stage for Flod 3 Epoch: 52 [19200/37476                 (64%)]\tLoss: 0.000735\n",
      "Training stage for Flod 3 Epoch: 52 [22400/37476                 (75%)]\tLoss: 0.000268\n",
      "Training stage for Flod 3 Epoch: 52 [25600/37476                 (85%)]\tLoss: 0.013968\n",
      "Training stage for Flod 3 Epoch: 52 [28800/37476                 (96%)]\tLoss: 0.004488\n",
      "Test set for fold3: Average Loss:           0.9042, Accuracy: 14770/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 53 [0/37476                 (0%)]\tLoss: 0.028012\n",
      "Training stage for Flod 3 Epoch: 53 [3200/37476                 (11%)]\tLoss: 0.005630\n",
      "Training stage for Flod 3 Epoch: 53 [6400/37476                 (21%)]\tLoss: 0.157442\n",
      "Training stage for Flod 3 Epoch: 53 [9600/37476                 (32%)]\tLoss: 0.002569\n",
      "Training stage for Flod 3 Epoch: 53 [12800/37476                 (43%)]\tLoss: 0.004017\n",
      "Training stage for Flod 3 Epoch: 53 [16000/37476                 (53%)]\tLoss: 0.006410\n",
      "Training stage for Flod 3 Epoch: 53 [19200/37476                 (64%)]\tLoss: 0.014102\n",
      "Training stage for Flod 3 Epoch: 53 [22400/37476                 (75%)]\tLoss: 0.044047\n",
      "Training stage for Flod 3 Epoch: 53 [25600/37476                 (85%)]\tLoss: 0.066405\n",
      "Training stage for Flod 3 Epoch: 53 [28800/37476                 (96%)]\tLoss: 0.063121\n",
      "Test set for fold3: Average Loss:           1.1080, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 54 [0/37476                 (0%)]\tLoss: 0.042612\n",
      "Training stage for Flod 3 Epoch: 54 [3200/37476                 (11%)]\tLoss: 0.055623\n",
      "Training stage for Flod 3 Epoch: 54 [6400/37476                 (21%)]\tLoss: 0.001269\n",
      "Training stage for Flod 3 Epoch: 54 [9600/37476                 (32%)]\tLoss: 0.055423\n",
      "Training stage for Flod 3 Epoch: 54 [12800/37476                 (43%)]\tLoss: 0.008310\n",
      "Training stage for Flod 3 Epoch: 54 [16000/37476                 (53%)]\tLoss: 0.000077\n",
      "Training stage for Flod 3 Epoch: 54 [19200/37476                 (64%)]\tLoss: 0.007797\n",
      "Training stage for Flod 3 Epoch: 54 [22400/37476                 (75%)]\tLoss: 0.011686\n",
      "Training stage for Flod 3 Epoch: 54 [25600/37476                 (85%)]\tLoss: 0.003565\n",
      "Training stage for Flod 3 Epoch: 54 [28800/37476                 (96%)]\tLoss: 0.047759\n",
      "Test set for fold3: Average Loss:           1.0284, Accuracy: 14772/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 55 [0/37476                 (0%)]\tLoss: 0.002201\n",
      "Training stage for Flod 3 Epoch: 55 [3200/37476                 (11%)]\tLoss: 0.112170\n",
      "Training stage for Flod 3 Epoch: 55 [6400/37476                 (21%)]\tLoss: 0.011027\n",
      "Training stage for Flod 3 Epoch: 55 [9600/37476                 (32%)]\tLoss: 0.056315\n",
      "Training stage for Flod 3 Epoch: 55 [12800/37476                 (43%)]\tLoss: 0.000411\n",
      "Training stage for Flod 3 Epoch: 55 [16000/37476                 (53%)]\tLoss: 0.000663\n",
      "Training stage for Flod 3 Epoch: 55 [19200/37476                 (64%)]\tLoss: 0.010922\n",
      "Training stage for Flod 3 Epoch: 55 [22400/37476                 (75%)]\tLoss: 0.008825\n",
      "Training stage for Flod 3 Epoch: 55 [25600/37476                 (85%)]\tLoss: 0.004307\n",
      "Training stage for Flod 3 Epoch: 55 [28800/37476                 (96%)]\tLoss: 0.006964\n",
      "Test set for fold3: Average Loss:           1.0927, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 56 [0/37476                 (0%)]\tLoss: 0.013746\n",
      "Training stage for Flod 3 Epoch: 56 [3200/37476                 (11%)]\tLoss: 0.003581\n",
      "Training stage for Flod 3 Epoch: 56 [6400/37476                 (21%)]\tLoss: 0.000337\n",
      "Training stage for Flod 3 Epoch: 56 [9600/37476                 (32%)]\tLoss: 0.067325\n",
      "Training stage for Flod 3 Epoch: 56 [12800/37476                 (43%)]\tLoss: 0.016407\n",
      "Training stage for Flod 3 Epoch: 56 [16000/37476                 (53%)]\tLoss: 0.011812\n",
      "Training stage for Flod 3 Epoch: 56 [19200/37476                 (64%)]\tLoss: 0.114964\n",
      "Training stage for Flod 3 Epoch: 56 [22400/37476                 (75%)]\tLoss: 0.016208\n",
      "Training stage for Flod 3 Epoch: 56 [25600/37476                 (85%)]\tLoss: 0.009865\n",
      "Training stage for Flod 3 Epoch: 56 [28800/37476                 (96%)]\tLoss: 0.004878\n",
      "Test set for fold3: Average Loss:           0.9303, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 57 [0/37476                 (0%)]\tLoss: 0.017194\n",
      "Training stage for Flod 3 Epoch: 57 [3200/37476                 (11%)]\tLoss: 0.021183\n",
      "Training stage for Flod 3 Epoch: 57 [6400/37476                 (21%)]\tLoss: 0.019427\n",
      "Training stage for Flod 3 Epoch: 57 [9600/37476                 (32%)]\tLoss: 0.008942\n",
      "Training stage for Flod 3 Epoch: 57 [12800/37476                 (43%)]\tLoss: 0.129827\n",
      "Training stage for Flod 3 Epoch: 57 [16000/37476                 (53%)]\tLoss: 0.033098\n",
      "Training stage for Flod 3 Epoch: 57 [19200/37476                 (64%)]\tLoss: 0.000805\n",
      "Training stage for Flod 3 Epoch: 57 [22400/37476                 (75%)]\tLoss: 0.013686\n",
      "Training stage for Flod 3 Epoch: 57 [25600/37476                 (85%)]\tLoss: 0.006141\n",
      "Training stage for Flod 3 Epoch: 57 [28800/37476                 (96%)]\tLoss: 0.011919\n",
      "Test set for fold3: Average Loss:           1.0792, Accuracy: 14730/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 58 [0/37476                 (0%)]\tLoss: 0.021697\n",
      "Training stage for Flod 3 Epoch: 58 [3200/37476                 (11%)]\tLoss: 0.134437\n",
      "Training stage for Flod 3 Epoch: 58 [6400/37476                 (21%)]\tLoss: 0.021978\n",
      "Training stage for Flod 3 Epoch: 58 [9600/37476                 (32%)]\tLoss: 0.002741\n",
      "Training stage for Flod 3 Epoch: 58 [12800/37476                 (43%)]\tLoss: 0.002394\n",
      "Training stage for Flod 3 Epoch: 58 [16000/37476                 (53%)]\tLoss: 0.013921\n",
      "Training stage for Flod 3 Epoch: 58 [19200/37476                 (64%)]\tLoss: 0.005558\n",
      "Training stage for Flod 3 Epoch: 58 [22400/37476                 (75%)]\tLoss: 0.037521\n",
      "Training stage for Flod 3 Epoch: 58 [25600/37476                 (85%)]\tLoss: 0.003632\n",
      "Training stage for Flod 3 Epoch: 58 [28800/37476                 (96%)]\tLoss: 0.126029\n",
      "Test set for fold3: Average Loss:           1.1676, Accuracy: 14716/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 59 [0/37476                 (0%)]\tLoss: 0.000240\n",
      "Training stage for Flod 3 Epoch: 59 [3200/37476                 (11%)]\tLoss: 0.001685\n",
      "Training stage for Flod 3 Epoch: 59 [6400/37476                 (21%)]\tLoss: 0.024521\n",
      "Training stage for Flod 3 Epoch: 59 [9600/37476                 (32%)]\tLoss: 0.043392\n",
      "Training stage for Flod 3 Epoch: 59 [12800/37476                 (43%)]\tLoss: 0.004863\n",
      "Training stage for Flod 3 Epoch: 59 [16000/37476                 (53%)]\tLoss: 0.005064\n",
      "Training stage for Flod 3 Epoch: 59 [19200/37476                 (64%)]\tLoss: 0.003222\n",
      "Training stage for Flod 3 Epoch: 59 [22400/37476                 (75%)]\tLoss: 0.000582\n",
      "Training stage for Flod 3 Epoch: 59 [25600/37476                 (85%)]\tLoss: 0.000219\n",
      "Training stage for Flod 3 Epoch: 59 [28800/37476                 (96%)]\tLoss: 0.017379\n",
      "Test set for fold3: Average Loss:           1.0941, Accuracy: 14773/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 60 [0/37476                 (0%)]\tLoss: 0.051425\n",
      "Training stage for Flod 3 Epoch: 60 [3200/37476                 (11%)]\tLoss: 0.220744\n",
      "Training stage for Flod 3 Epoch: 60 [6400/37476                 (21%)]\tLoss: 0.016466\n",
      "Training stage for Flod 3 Epoch: 60 [9600/37476                 (32%)]\tLoss: 0.001024\n",
      "Training stage for Flod 3 Epoch: 60 [12800/37476                 (43%)]\tLoss: 0.007973\n",
      "Training stage for Flod 3 Epoch: 60 [16000/37476                 (53%)]\tLoss: 0.058844\n",
      "Training stage for Flod 3 Epoch: 60 [19200/37476                 (64%)]\tLoss: 0.003708\n",
      "Training stage for Flod 3 Epoch: 60 [22400/37476                 (75%)]\tLoss: 0.009583\n",
      "Training stage for Flod 3 Epoch: 60 [25600/37476                 (85%)]\tLoss: 0.006702\n",
      "Training stage for Flod 3 Epoch: 60 [28800/37476                 (96%)]\tLoss: 0.000069\n",
      "Test set for fold3: Average Loss:           0.9929, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 61 [0/37476                 (0%)]\tLoss: 0.019317\n",
      "Training stage for Flod 3 Epoch: 61 [3200/37476                 (11%)]\tLoss: 0.109711\n",
      "Training stage for Flod 3 Epoch: 61 [6400/37476                 (21%)]\tLoss: 0.000121\n",
      "Training stage for Flod 3 Epoch: 61 [9600/37476                 (32%)]\tLoss: 0.009537\n",
      "Training stage for Flod 3 Epoch: 61 [12800/37476                 (43%)]\tLoss: 0.075021\n",
      "Training stage for Flod 3 Epoch: 61 [16000/37476                 (53%)]\tLoss: 0.037353\n",
      "Training stage for Flod 3 Epoch: 61 [19200/37476                 (64%)]\tLoss: 0.020415\n",
      "Training stage for Flod 3 Epoch: 61 [22400/37476                 (75%)]\tLoss: 0.104933\n",
      "Training stage for Flod 3 Epoch: 61 [25600/37476                 (85%)]\tLoss: 0.005674\n",
      "Training stage for Flod 3 Epoch: 61 [28800/37476                 (96%)]\tLoss: 0.008046\n",
      "Test set for fold3: Average Loss:           0.9447, Accuracy: 14768/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 62 [0/37476                 (0%)]\tLoss: 0.006332\n",
      "Training stage for Flod 3 Epoch: 62 [3200/37476                 (11%)]\tLoss: 0.001478\n",
      "Training stage for Flod 3 Epoch: 62 [6400/37476                 (21%)]\tLoss: 0.012263\n",
      "Training stage for Flod 3 Epoch: 62 [9600/37476                 (32%)]\tLoss: 0.043282\n",
      "Training stage for Flod 3 Epoch: 62 [12800/37476                 (43%)]\tLoss: 0.001190\n",
      "Training stage for Flod 3 Epoch: 62 [16000/37476                 (53%)]\tLoss: 0.045333\n",
      "Training stage for Flod 3 Epoch: 62 [19200/37476                 (64%)]\tLoss: 0.089814\n",
      "Training stage for Flod 3 Epoch: 62 [22400/37476                 (75%)]\tLoss: 0.001903\n",
      "Training stage for Flod 3 Epoch: 62 [25600/37476                 (85%)]\tLoss: 0.078177\n",
      "Training stage for Flod 3 Epoch: 62 [28800/37476                 (96%)]\tLoss: 0.002691\n",
      "Test set for fold3: Average Loss:           1.1582, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 63 [0/37476                 (0%)]\tLoss: 0.000067\n",
      "Training stage for Flod 3 Epoch: 63 [3200/37476                 (11%)]\tLoss: 0.005697\n",
      "Training stage for Flod 3 Epoch: 63 [6400/37476                 (21%)]\tLoss: 0.003651\n",
      "Training stage for Flod 3 Epoch: 63 [9600/37476                 (32%)]\tLoss: 0.004922\n",
      "Training stage for Flod 3 Epoch: 63 [12800/37476                 (43%)]\tLoss: 0.033260\n",
      "Training stage for Flod 3 Epoch: 63 [16000/37476                 (53%)]\tLoss: 0.022734\n",
      "Training stage for Flod 3 Epoch: 63 [19200/37476                 (64%)]\tLoss: 0.010640\n",
      "Training stage for Flod 3 Epoch: 63 [22400/37476                 (75%)]\tLoss: 0.038487\n",
      "Training stage for Flod 3 Epoch: 63 [25600/37476                 (85%)]\tLoss: 0.003706\n",
      "Training stage for Flod 3 Epoch: 63 [28800/37476                 (96%)]\tLoss: 0.003714\n",
      "Test set for fold3: Average Loss:           1.0819, Accuracy: 14734/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 64 [0/37476                 (0%)]\tLoss: 0.002176\n",
      "Training stage for Flod 3 Epoch: 64 [3200/37476                 (11%)]\tLoss: 0.008911\n",
      "Training stage for Flod 3 Epoch: 64 [6400/37476                 (21%)]\tLoss: 0.022684\n",
      "Training stage for Flod 3 Epoch: 64 [9600/37476                 (32%)]\tLoss: 0.050556\n",
      "Training stage for Flod 3 Epoch: 64 [12800/37476                 (43%)]\tLoss: 0.002452\n",
      "Training stage for Flod 3 Epoch: 64 [16000/37476                 (53%)]\tLoss: 0.000147\n",
      "Training stage for Flod 3 Epoch: 64 [19200/37476                 (64%)]\tLoss: 0.019311\n",
      "Training stage for Flod 3 Epoch: 64 [22400/37476                 (75%)]\tLoss: 0.105458\n",
      "Training stage for Flod 3 Epoch: 64 [25600/37476                 (85%)]\tLoss: 0.060508\n",
      "Training stage for Flod 3 Epoch: 64 [28800/37476                 (96%)]\tLoss: 0.023060\n",
      "Test set for fold3: Average Loss:           1.0374, Accuracy: 14758/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 65 [0/37476                 (0%)]\tLoss: 0.019651\n",
      "Training stage for Flod 3 Epoch: 65 [3200/37476                 (11%)]\tLoss: 0.110153\n",
      "Training stage for Flod 3 Epoch: 65 [6400/37476                 (21%)]\tLoss: 0.054242\n",
      "Training stage for Flod 3 Epoch: 65 [9600/37476                 (32%)]\tLoss: 0.011643\n",
      "Training stage for Flod 3 Epoch: 65 [12800/37476                 (43%)]\tLoss: 0.024039\n",
      "Training stage for Flod 3 Epoch: 65 [16000/37476                 (53%)]\tLoss: 0.023521\n",
      "Training stage for Flod 3 Epoch: 65 [19200/37476                 (64%)]\tLoss: 0.007779\n",
      "Training stage for Flod 3 Epoch: 65 [22400/37476                 (75%)]\tLoss: 0.003397\n",
      "Training stage for Flod 3 Epoch: 65 [25600/37476                 (85%)]\tLoss: 0.003396\n",
      "Training stage for Flod 3 Epoch: 65 [28800/37476                 (96%)]\tLoss: 0.004112\n",
      "Test set for fold3: Average Loss:           1.1292, Accuracy: 14773/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 66 [0/37476                 (0%)]\tLoss: 0.044224\n",
      "Training stage for Flod 3 Epoch: 66 [3200/37476                 (11%)]\tLoss: 0.000855\n",
      "Training stage for Flod 3 Epoch: 66 [6400/37476                 (21%)]\tLoss: 0.032185\n",
      "Training stage for Flod 3 Epoch: 66 [9600/37476                 (32%)]\tLoss: 0.004345\n",
      "Training stage for Flod 3 Epoch: 66 [12800/37476                 (43%)]\tLoss: 0.001407\n",
      "Training stage for Flod 3 Epoch: 66 [16000/37476                 (53%)]\tLoss: 0.002657\n",
      "Training stage for Flod 3 Epoch: 66 [19200/37476                 (64%)]\tLoss: 0.002540\n",
      "Training stage for Flod 3 Epoch: 66 [22400/37476                 (75%)]\tLoss: 0.032367\n",
      "Training stage for Flod 3 Epoch: 66 [25600/37476                 (85%)]\tLoss: 0.010946\n",
      "Training stage for Flod 3 Epoch: 66 [28800/37476                 (96%)]\tLoss: 0.262837\n",
      "Test set for fold3: Average Loss:           0.9779, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 67 [0/37476                 (0%)]\tLoss: 0.042070\n",
      "Training stage for Flod 3 Epoch: 67 [3200/37476                 (11%)]\tLoss: 0.002448\n",
      "Training stage for Flod 3 Epoch: 67 [6400/37476                 (21%)]\tLoss: 0.064863\n",
      "Training stage for Flod 3 Epoch: 67 [9600/37476                 (32%)]\tLoss: 0.008216\n",
      "Training stage for Flod 3 Epoch: 67 [12800/37476                 (43%)]\tLoss: 0.007243\n",
      "Training stage for Flod 3 Epoch: 67 [16000/37476                 (53%)]\tLoss: 0.077724\n",
      "Training stage for Flod 3 Epoch: 67 [19200/37476                 (64%)]\tLoss: 0.052542\n",
      "Training stage for Flod 3 Epoch: 67 [22400/37476                 (75%)]\tLoss: 0.070635\n",
      "Training stage for Flod 3 Epoch: 67 [25600/37476                 (85%)]\tLoss: 0.083354\n",
      "Training stage for Flod 3 Epoch: 67 [28800/37476                 (96%)]\tLoss: 0.001446\n",
      "Test set for fold3: Average Loss:           1.0581, Accuracy: 14776/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 68 [0/37476                 (0%)]\tLoss: 0.000724\n",
      "Training stage for Flod 3 Epoch: 68 [3200/37476                 (11%)]\tLoss: 0.033617\n",
      "Training stage for Flod 3 Epoch: 68 [6400/37476                 (21%)]\tLoss: 0.015625\n",
      "Training stage for Flod 3 Epoch: 68 [9600/37476                 (32%)]\tLoss: 0.045749\n",
      "Training stage for Flod 3 Epoch: 68 [12800/37476                 (43%)]\tLoss: 0.005104\n",
      "Training stage for Flod 3 Epoch: 68 [16000/37476                 (53%)]\tLoss: 0.003907\n",
      "Training stage for Flod 3 Epoch: 68 [19200/37476                 (64%)]\tLoss: 0.018466\n",
      "Training stage for Flod 3 Epoch: 68 [22400/37476                 (75%)]\tLoss: 0.029255\n",
      "Training stage for Flod 3 Epoch: 68 [25600/37476                 (85%)]\tLoss: 0.112619\n",
      "Training stage for Flod 3 Epoch: 68 [28800/37476                 (96%)]\tLoss: 0.037164\n",
      "Test set for fold3: Average Loss:           1.0631, Accuracy: 14739/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 69 [0/37476                 (0%)]\tLoss: 0.002873\n",
      "Training stage for Flod 3 Epoch: 69 [3200/37476                 (11%)]\tLoss: 0.006246\n",
      "Training stage for Flod 3 Epoch: 69 [6400/37476                 (21%)]\tLoss: 0.025425\n",
      "Training stage for Flod 3 Epoch: 69 [9600/37476                 (32%)]\tLoss: 0.054604\n",
      "Training stage for Flod 3 Epoch: 69 [12800/37476                 (43%)]\tLoss: 0.026083\n",
      "Training stage for Flod 3 Epoch: 69 [16000/37476                 (53%)]\tLoss: 0.082599\n",
      "Training stage for Flod 3 Epoch: 69 [19200/37476                 (64%)]\tLoss: 0.001866\n",
      "Training stage for Flod 3 Epoch: 69 [22400/37476                 (75%)]\tLoss: 0.084107\n",
      "Training stage for Flod 3 Epoch: 69 [25600/37476                 (85%)]\tLoss: 0.037002\n",
      "Training stage for Flod 3 Epoch: 69 [28800/37476                 (96%)]\tLoss: 0.005252\n",
      "Test set for fold3: Average Loss:           1.1026, Accuracy: 14748/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 70 [0/37476                 (0%)]\tLoss: 0.004544\n",
      "Training stage for Flod 3 Epoch: 70 [3200/37476                 (11%)]\tLoss: 0.002399\n",
      "Training stage for Flod 3 Epoch: 70 [6400/37476                 (21%)]\tLoss: 0.011571\n",
      "Training stage for Flod 3 Epoch: 70 [9600/37476                 (32%)]\tLoss: 0.040376\n",
      "Training stage for Flod 3 Epoch: 70 [12800/37476                 (43%)]\tLoss: 0.004252\n",
      "Training stage for Flod 3 Epoch: 70 [16000/37476                 (53%)]\tLoss: 0.010711\n",
      "Training stage for Flod 3 Epoch: 70 [19200/37476                 (64%)]\tLoss: 0.073629\n",
      "Training stage for Flod 3 Epoch: 70 [22400/37476                 (75%)]\tLoss: 0.000146\n",
      "Training stage for Flod 3 Epoch: 70 [25600/37476                 (85%)]\tLoss: 0.005593\n",
      "Training stage for Flod 3 Epoch: 70 [28800/37476                 (96%)]\tLoss: 0.021563\n",
      "Test set for fold3: Average Loss:           1.1456, Accuracy: 14761/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 71 [0/37476                 (0%)]\tLoss: 0.001365\n",
      "Training stage for Flod 3 Epoch: 71 [3200/37476                 (11%)]\tLoss: 0.006275\n",
      "Training stage for Flod 3 Epoch: 71 [6400/37476                 (21%)]\tLoss: 0.000229\n",
      "Training stage for Flod 3 Epoch: 71 [9600/37476                 (32%)]\tLoss: 0.012010\n",
      "Training stage for Flod 3 Epoch: 71 [12800/37476                 (43%)]\tLoss: 0.048535\n",
      "Training stage for Flod 3 Epoch: 71 [16000/37476                 (53%)]\tLoss: 0.008771\n",
      "Training stage for Flod 3 Epoch: 71 [19200/37476                 (64%)]\tLoss: 0.094285\n",
      "Training stage for Flod 3 Epoch: 71 [22400/37476                 (75%)]\tLoss: 0.218461\n",
      "Training stage for Flod 3 Epoch: 71 [25600/37476                 (85%)]\tLoss: 0.016948\n",
      "Training stage for Flod 3 Epoch: 71 [28800/37476                 (96%)]\tLoss: 0.002992\n",
      "Test set for fold3: Average Loss:           1.0970, Accuracy: 14777/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 72 [0/37476                 (0%)]\tLoss: 0.018552\n",
      "Training stage for Flod 3 Epoch: 72 [3200/37476                 (11%)]\tLoss: 0.002463\n",
      "Training stage for Flod 3 Epoch: 72 [6400/37476                 (21%)]\tLoss: 0.048173\n",
      "Training stage for Flod 3 Epoch: 72 [9600/37476                 (32%)]\tLoss: 0.017183\n",
      "Training stage for Flod 3 Epoch: 72 [12800/37476                 (43%)]\tLoss: 0.002843\n",
      "Training stage for Flod 3 Epoch: 72 [16000/37476                 (53%)]\tLoss: 0.000706\n",
      "Training stage for Flod 3 Epoch: 72 [19200/37476                 (64%)]\tLoss: 0.020726\n",
      "Training stage for Flod 3 Epoch: 72 [22400/37476                 (75%)]\tLoss: 0.019711\n",
      "Training stage for Flod 3 Epoch: 72 [25600/37476                 (85%)]\tLoss: 0.013197\n",
      "Training stage for Flod 3 Epoch: 72 [28800/37476                 (96%)]\tLoss: 0.088017\n",
      "Test set for fold3: Average Loss:           1.0454, Accuracy: 14757/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 73 [0/37476                 (0%)]\tLoss: 0.001562\n",
      "Training stage for Flod 3 Epoch: 73 [3200/37476                 (11%)]\tLoss: 0.057227\n",
      "Training stage for Flod 3 Epoch: 73 [6400/37476                 (21%)]\tLoss: 0.046982\n",
      "Training stage for Flod 3 Epoch: 73 [9600/37476                 (32%)]\tLoss: 0.014997\n",
      "Training stage for Flod 3 Epoch: 73 [12800/37476                 (43%)]\tLoss: 0.007526\n",
      "Training stage for Flod 3 Epoch: 73 [16000/37476                 (53%)]\tLoss: 0.042809\n",
      "Training stage for Flod 3 Epoch: 73 [19200/37476                 (64%)]\tLoss: 0.037004\n",
      "Training stage for Flod 3 Epoch: 73 [22400/37476                 (75%)]\tLoss: 0.009574\n",
      "Training stage for Flod 3 Epoch: 73 [25600/37476                 (85%)]\tLoss: 0.024838\n",
      "Training stage for Flod 3 Epoch: 73 [28800/37476                 (96%)]\tLoss: 0.016262\n",
      "Test set for fold3: Average Loss:           1.0572, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 74 [0/37476                 (0%)]\tLoss: 0.008004\n",
      "Training stage for Flod 3 Epoch: 74 [3200/37476                 (11%)]\tLoss: 0.061900\n",
      "Training stage for Flod 3 Epoch: 74 [6400/37476                 (21%)]\tLoss: 0.002764\n",
      "Training stage for Flod 3 Epoch: 74 [9600/37476                 (32%)]\tLoss: 0.003187\n",
      "Training stage for Flod 3 Epoch: 74 [12800/37476                 (43%)]\tLoss: 0.005907\n",
      "Training stage for Flod 3 Epoch: 74 [16000/37476                 (53%)]\tLoss: 0.066217\n",
      "Training stage for Flod 3 Epoch: 74 [19200/37476                 (64%)]\tLoss: 0.012759\n",
      "Training stage for Flod 3 Epoch: 74 [22400/37476                 (75%)]\tLoss: 0.004626\n",
      "Training stage for Flod 3 Epoch: 74 [25600/37476                 (85%)]\tLoss: 0.001989\n",
      "Training stage for Flod 3 Epoch: 74 [28800/37476                 (96%)]\tLoss: 0.001614\n",
      "Test set for fold3: Average Loss:           1.1154, Accuracy: 14795/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 75 [0/37476                 (0%)]\tLoss: 0.052825\n",
      "Training stage for Flod 3 Epoch: 75 [3200/37476                 (11%)]\tLoss: 0.014840\n",
      "Training stage for Flod 3 Epoch: 75 [6400/37476                 (21%)]\tLoss: 0.001378\n",
      "Training stage for Flod 3 Epoch: 75 [9600/37476                 (32%)]\tLoss: 0.024607\n",
      "Training stage for Flod 3 Epoch: 75 [12800/37476                 (43%)]\tLoss: 0.027254\n",
      "Training stage for Flod 3 Epoch: 75 [16000/37476                 (53%)]\tLoss: 0.006892\n",
      "Training stage for Flod 3 Epoch: 75 [19200/37476                 (64%)]\tLoss: 0.000750\n",
      "Training stage for Flod 3 Epoch: 75 [22400/37476                 (75%)]\tLoss: 0.002886\n",
      "Training stage for Flod 3 Epoch: 75 [25600/37476                 (85%)]\tLoss: 0.003423\n",
      "Training stage for Flod 3 Epoch: 75 [28800/37476                 (96%)]\tLoss: 0.007357\n",
      "Test set for fold3: Average Loss:           1.1777, Accuracy: 14758/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 76 [0/37476                 (0%)]\tLoss: 0.000285\n",
      "Training stage for Flod 3 Epoch: 76 [3200/37476                 (11%)]\tLoss: 0.002051\n",
      "Training stage for Flod 3 Epoch: 76 [6400/37476                 (21%)]\tLoss: 0.011647\n",
      "Training stage for Flod 3 Epoch: 76 [9600/37476                 (32%)]\tLoss: 0.203070\n",
      "Training stage for Flod 3 Epoch: 76 [12800/37476                 (43%)]\tLoss: 0.007574\n",
      "Training stage for Flod 3 Epoch: 76 [16000/37476                 (53%)]\tLoss: 0.032646\n",
      "Training stage for Flod 3 Epoch: 76 [19200/37476                 (64%)]\tLoss: 0.015299\n",
      "Training stage for Flod 3 Epoch: 76 [22400/37476                 (75%)]\tLoss: 0.014900\n",
      "Training stage for Flod 3 Epoch: 76 [25600/37476                 (85%)]\tLoss: 0.009722\n",
      "Training stage for Flod 3 Epoch: 76 [28800/37476                 (96%)]\tLoss: 0.005788\n",
      "Test set for fold3: Average Loss:           1.2670, Accuracy: 14787/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 77 [0/37476                 (0%)]\tLoss: 0.001628\n",
      "Training stage for Flod 3 Epoch: 77 [3200/37476                 (11%)]\tLoss: 0.003073\n",
      "Training stage for Flod 3 Epoch: 77 [6400/37476                 (21%)]\tLoss: 0.171535\n",
      "Training stage for Flod 3 Epoch: 77 [9600/37476                 (32%)]\tLoss: 0.056944\n",
      "Training stage for Flod 3 Epoch: 77 [12800/37476                 (43%)]\tLoss: 0.002297\n",
      "Training stage for Flod 3 Epoch: 77 [16000/37476                 (53%)]\tLoss: 0.183975\n",
      "Training stage for Flod 3 Epoch: 77 [19200/37476                 (64%)]\tLoss: 0.000019\n",
      "Training stage for Flod 3 Epoch: 77 [22400/37476                 (75%)]\tLoss: 0.210876\n",
      "Training stage for Flod 3 Epoch: 77 [25600/37476                 (85%)]\tLoss: 0.000107\n",
      "Training stage for Flod 3 Epoch: 77 [28800/37476                 (96%)]\tLoss: 0.134175\n",
      "Test set for fold3: Average Loss:           1.0396, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 78 [0/37476                 (0%)]\tLoss: 0.051250\n",
      "Training stage for Flod 3 Epoch: 78 [3200/37476                 (11%)]\tLoss: 0.002167\n",
      "Training stage for Flod 3 Epoch: 78 [6400/37476                 (21%)]\tLoss: 0.014058\n",
      "Training stage for Flod 3 Epoch: 78 [9600/37476                 (32%)]\tLoss: 0.001765\n",
      "Training stage for Flod 3 Epoch: 78 [12800/37476                 (43%)]\tLoss: 0.026972\n",
      "Training stage for Flod 3 Epoch: 78 [16000/37476                 (53%)]\tLoss: 0.023527\n",
      "Training stage for Flod 3 Epoch: 78 [19200/37476                 (64%)]\tLoss: 0.136470\n",
      "Training stage for Flod 3 Epoch: 78 [22400/37476                 (75%)]\tLoss: 0.000819\n",
      "Training stage for Flod 3 Epoch: 78 [25600/37476                 (85%)]\tLoss: 0.085391\n",
      "Training stage for Flod 3 Epoch: 78 [28800/37476                 (96%)]\tLoss: 0.049026\n",
      "Test set for fold3: Average Loss:           1.0115, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 79 [0/37476                 (0%)]\tLoss: 0.006151\n",
      "Training stage for Flod 3 Epoch: 79 [3200/37476                 (11%)]\tLoss: 0.008886\n",
      "Training stage for Flod 3 Epoch: 79 [6400/37476                 (21%)]\tLoss: 0.005052\n",
      "Training stage for Flod 3 Epoch: 79 [9600/37476                 (32%)]\tLoss: 0.008868\n",
      "Training stage for Flod 3 Epoch: 79 [12800/37476                 (43%)]\tLoss: 0.020252\n",
      "Training stage for Flod 3 Epoch: 79 [16000/37476                 (53%)]\tLoss: 0.002245\n",
      "Training stage for Flod 3 Epoch: 79 [19200/37476                 (64%)]\tLoss: 0.042897\n",
      "Training stage for Flod 3 Epoch: 79 [22400/37476                 (75%)]\tLoss: 0.001175\n",
      "Training stage for Flod 3 Epoch: 79 [25600/37476                 (85%)]\tLoss: 0.001474\n",
      "Training stage for Flod 3 Epoch: 79 [28800/37476                 (96%)]\tLoss: 0.000499\n",
      "Test set for fold3: Average Loss:           1.1950, Accuracy: 14748/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 80 [0/37476                 (0%)]\tLoss: 0.034234\n",
      "Training stage for Flod 3 Epoch: 80 [3200/37476                 (11%)]\tLoss: 0.001665\n",
      "Training stage for Flod 3 Epoch: 80 [6400/37476                 (21%)]\tLoss: 0.006605\n",
      "Training stage for Flod 3 Epoch: 80 [9600/37476                 (32%)]\tLoss: 0.005765\n",
      "Training stage for Flod 3 Epoch: 80 [12800/37476                 (43%)]\tLoss: 0.015727\n",
      "Training stage for Flod 3 Epoch: 80 [16000/37476                 (53%)]\tLoss: 0.005903\n",
      "Training stage for Flod 3 Epoch: 80 [19200/37476                 (64%)]\tLoss: 0.006314\n",
      "Training stage for Flod 3 Epoch: 80 [22400/37476                 (75%)]\tLoss: 0.002139\n",
      "Training stage for Flod 3 Epoch: 80 [25600/37476                 (85%)]\tLoss: 0.113824\n",
      "Training stage for Flod 3 Epoch: 80 [28800/37476                 (96%)]\tLoss: 0.000164\n",
      "Test set for fold3: Average Loss:           0.9660, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 81 [0/37476                 (0%)]\tLoss: 0.029434\n",
      "Training stage for Flod 3 Epoch: 81 [3200/37476                 (11%)]\tLoss: 0.002679\n",
      "Training stage for Flod 3 Epoch: 81 [6400/37476                 (21%)]\tLoss: 0.010057\n",
      "Training stage for Flod 3 Epoch: 81 [9600/37476                 (32%)]\tLoss: 0.041211\n",
      "Training stage for Flod 3 Epoch: 81 [12800/37476                 (43%)]\tLoss: 0.002026\n",
      "Training stage for Flod 3 Epoch: 81 [16000/37476                 (53%)]\tLoss: 0.041342\n",
      "Training stage for Flod 3 Epoch: 81 [19200/37476                 (64%)]\tLoss: 0.003572\n",
      "Training stage for Flod 3 Epoch: 81 [22400/37476                 (75%)]\tLoss: 0.029379\n",
      "Training stage for Flod 3 Epoch: 81 [25600/37476                 (85%)]\tLoss: 0.004891\n",
      "Training stage for Flod 3 Epoch: 81 [28800/37476                 (96%)]\tLoss: 0.020036\n",
      "Test set for fold3: Average Loss:           1.1609, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 82 [0/37476                 (0%)]\tLoss: 0.051897\n",
      "Training stage for Flod 3 Epoch: 82 [3200/37476                 (11%)]\tLoss: 0.006161\n",
      "Training stage for Flod 3 Epoch: 82 [6400/37476                 (21%)]\tLoss: 0.002917\n",
      "Training stage for Flod 3 Epoch: 82 [9600/37476                 (32%)]\tLoss: 0.046231\n",
      "Training stage for Flod 3 Epoch: 82 [12800/37476                 (43%)]\tLoss: 0.000081\n",
      "Training stage for Flod 3 Epoch: 82 [16000/37476                 (53%)]\tLoss: 0.020326\n",
      "Training stage for Flod 3 Epoch: 82 [19200/37476                 (64%)]\tLoss: 0.034433\n",
      "Training stage for Flod 3 Epoch: 82 [22400/37476                 (75%)]\tLoss: 0.005657\n",
      "Training stage for Flod 3 Epoch: 82 [25600/37476                 (85%)]\tLoss: 0.010002\n",
      "Training stage for Flod 3 Epoch: 82 [28800/37476                 (96%)]\tLoss: 0.004799\n",
      "Test set for fold3: Average Loss:           1.2582, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 83 [0/37476                 (0%)]\tLoss: 0.002245\n",
      "Training stage for Flod 3 Epoch: 83 [3200/37476                 (11%)]\tLoss: 0.089613\n",
      "Training stage for Flod 3 Epoch: 83 [6400/37476                 (21%)]\tLoss: 0.027386\n",
      "Training stage for Flod 3 Epoch: 83 [9600/37476                 (32%)]\tLoss: 0.029507\n",
      "Training stage for Flod 3 Epoch: 83 [12800/37476                 (43%)]\tLoss: 0.000121\n",
      "Training stage for Flod 3 Epoch: 83 [16000/37476                 (53%)]\tLoss: 0.009698\n",
      "Training stage for Flod 3 Epoch: 83 [19200/37476                 (64%)]\tLoss: 0.004715\n",
      "Training stage for Flod 3 Epoch: 83 [22400/37476                 (75%)]\tLoss: 0.006063\n",
      "Training stage for Flod 3 Epoch: 83 [25600/37476                 (85%)]\tLoss: 0.006038\n",
      "Training stage for Flod 3 Epoch: 83 [28800/37476                 (96%)]\tLoss: 0.016477\n",
      "Test set for fold3: Average Loss:           1.2235, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 84 [0/37476                 (0%)]\tLoss: 0.010118\n",
      "Training stage for Flod 3 Epoch: 84 [3200/37476                 (11%)]\tLoss: 0.004977\n",
      "Training stage for Flod 3 Epoch: 84 [6400/37476                 (21%)]\tLoss: 0.005519\n",
      "Training stage for Flod 3 Epoch: 84 [9600/37476                 (32%)]\tLoss: 0.000102\n",
      "Training stage for Flod 3 Epoch: 84 [12800/37476                 (43%)]\tLoss: 0.008648\n",
      "Training stage for Flod 3 Epoch: 84 [16000/37476                 (53%)]\tLoss: 0.127512\n",
      "Training stage for Flod 3 Epoch: 84 [19200/37476                 (64%)]\tLoss: 0.001681\n",
      "Training stage for Flod 3 Epoch: 84 [22400/37476                 (75%)]\tLoss: 0.002935\n",
      "Training stage for Flod 3 Epoch: 84 [25600/37476                 (85%)]\tLoss: 0.007004\n",
      "Training stage for Flod 3 Epoch: 84 [28800/37476                 (96%)]\tLoss: 0.029338\n",
      "Test set for fold3: Average Loss:           0.9615, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 85 [0/37476                 (0%)]\tLoss: 0.005967\n",
      "Training stage for Flod 3 Epoch: 85 [3200/37476                 (11%)]\tLoss: 0.032043\n",
      "Training stage for Flod 3 Epoch: 85 [6400/37476                 (21%)]\tLoss: 0.010814\n",
      "Training stage for Flod 3 Epoch: 85 [9600/37476                 (32%)]\tLoss: 0.032181\n",
      "Training stage for Flod 3 Epoch: 85 [12800/37476                 (43%)]\tLoss: 0.021389\n",
      "Training stage for Flod 3 Epoch: 85 [16000/37476                 (53%)]\tLoss: 0.007773\n",
      "Training stage for Flod 3 Epoch: 85 [19200/37476                 (64%)]\tLoss: 0.004143\n",
      "Training stage for Flod 3 Epoch: 85 [22400/37476                 (75%)]\tLoss: 0.005384\n",
      "Training stage for Flod 3 Epoch: 85 [25600/37476                 (85%)]\tLoss: 0.005213\n",
      "Training stage for Flod 3 Epoch: 85 [28800/37476                 (96%)]\tLoss: 0.067454\n",
      "Test set for fold3: Average Loss:           1.1172, Accuracy: 14744/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 86 [0/37476                 (0%)]\tLoss: 0.003959\n",
      "Training stage for Flod 3 Epoch: 86 [3200/37476                 (11%)]\tLoss: 0.001327\n",
      "Training stage for Flod 3 Epoch: 86 [6400/37476                 (21%)]\tLoss: 0.006044\n",
      "Training stage for Flod 3 Epoch: 86 [9600/37476                 (32%)]\tLoss: 0.139340\n",
      "Training stage for Flod 3 Epoch: 86 [12800/37476                 (43%)]\tLoss: 0.135487\n",
      "Training stage for Flod 3 Epoch: 86 [16000/37476                 (53%)]\tLoss: 0.004566\n",
      "Training stage for Flod 3 Epoch: 86 [19200/37476                 (64%)]\tLoss: 0.037150\n",
      "Training stage for Flod 3 Epoch: 86 [22400/37476                 (75%)]\tLoss: 0.014633\n",
      "Training stage for Flod 3 Epoch: 86 [25600/37476                 (85%)]\tLoss: 0.025177\n",
      "Training stage for Flod 3 Epoch: 86 [28800/37476                 (96%)]\tLoss: 0.122507\n",
      "Test set for fold3: Average Loss:           1.1597, Accuracy: 14744/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 87 [0/37476                 (0%)]\tLoss: 0.024944\n",
      "Training stage for Flod 3 Epoch: 87 [3200/37476                 (11%)]\tLoss: 0.000073\n",
      "Training stage for Flod 3 Epoch: 87 [6400/37476                 (21%)]\tLoss: 0.000699\n",
      "Training stage for Flod 3 Epoch: 87 [9600/37476                 (32%)]\tLoss: 0.012117\n",
      "Training stage for Flod 3 Epoch: 87 [12800/37476                 (43%)]\tLoss: 0.000555\n",
      "Training stage for Flod 3 Epoch: 87 [16000/37476                 (53%)]\tLoss: 0.004108\n",
      "Training stage for Flod 3 Epoch: 87 [19200/37476                 (64%)]\tLoss: 0.003267\n",
      "Training stage for Flod 3 Epoch: 87 [22400/37476                 (75%)]\tLoss: 0.019412\n",
      "Training stage for Flod 3 Epoch: 87 [25600/37476                 (85%)]\tLoss: 0.004148\n",
      "Training stage for Flod 3 Epoch: 87 [28800/37476                 (96%)]\tLoss: 0.001654\n",
      "Test set for fold3: Average Loss:           1.0921, Accuracy: 14761/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 88 [0/37476                 (0%)]\tLoss: 0.083657\n",
      "Training stage for Flod 3 Epoch: 88 [3200/37476                 (11%)]\tLoss: 0.002316\n",
      "Training stage for Flod 3 Epoch: 88 [6400/37476                 (21%)]\tLoss: 0.011879\n",
      "Training stage for Flod 3 Epoch: 88 [9600/37476                 (32%)]\tLoss: 0.001051\n",
      "Training stage for Flod 3 Epoch: 88 [12800/37476                 (43%)]\tLoss: 0.003967\n",
      "Training stage for Flod 3 Epoch: 88 [16000/37476                 (53%)]\tLoss: 0.055397\n",
      "Training stage for Flod 3 Epoch: 88 [19200/37476                 (64%)]\tLoss: 0.074892\n",
      "Training stage for Flod 3 Epoch: 88 [22400/37476                 (75%)]\tLoss: 0.006620\n",
      "Training stage for Flod 3 Epoch: 88 [25600/37476                 (85%)]\tLoss: 0.016835\n",
      "Training stage for Flod 3 Epoch: 88 [28800/37476                 (96%)]\tLoss: 0.020438\n",
      "Test set for fold3: Average Loss:           1.1809, Accuracy: 14732/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 89 [0/37476                 (0%)]\tLoss: 0.027309\n",
      "Training stage for Flod 3 Epoch: 89 [3200/37476                 (11%)]\tLoss: 0.020739\n",
      "Training stage for Flod 3 Epoch: 89 [6400/37476                 (21%)]\tLoss: 0.000151\n",
      "Training stage for Flod 3 Epoch: 89 [9600/37476                 (32%)]\tLoss: 0.021047\n",
      "Training stage for Flod 3 Epoch: 89 [12800/37476                 (43%)]\tLoss: 0.001977\n",
      "Training stage for Flod 3 Epoch: 89 [16000/37476                 (53%)]\tLoss: 0.004600\n",
      "Training stage for Flod 3 Epoch: 89 [19200/37476                 (64%)]\tLoss: 0.044803\n",
      "Training stage for Flod 3 Epoch: 89 [22400/37476                 (75%)]\tLoss: 0.165373\n",
      "Training stage for Flod 3 Epoch: 89 [25600/37476                 (85%)]\tLoss: 0.084761\n",
      "Training stage for Flod 3 Epoch: 89 [28800/37476                 (96%)]\tLoss: 0.012675\n",
      "Test set for fold3: Average Loss:           1.2007, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 90 [0/37476                 (0%)]\tLoss: 0.213775\n",
      "Training stage for Flod 3 Epoch: 90 [3200/37476                 (11%)]\tLoss: 0.009197\n",
      "Training stage for Flod 3 Epoch: 90 [6400/37476                 (21%)]\tLoss: 0.017260\n",
      "Training stage for Flod 3 Epoch: 90 [9600/37476                 (32%)]\tLoss: 0.006609\n",
      "Training stage for Flod 3 Epoch: 90 [12800/37476                 (43%)]\tLoss: 0.007541\n",
      "Training stage for Flod 3 Epoch: 90 [16000/37476                 (53%)]\tLoss: 0.003545\n",
      "Training stage for Flod 3 Epoch: 90 [19200/37476                 (64%)]\tLoss: 0.001290\n",
      "Training stage for Flod 3 Epoch: 90 [22400/37476                 (75%)]\tLoss: 0.037182\n",
      "Training stage for Flod 3 Epoch: 90 [25600/37476                 (85%)]\tLoss: 0.001544\n",
      "Training stage for Flod 3 Epoch: 90 [28800/37476                 (96%)]\tLoss: 0.006002\n",
      "Test set for fold3: Average Loss:           1.1536, Accuracy: 14761/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 91 [0/37476                 (0%)]\tLoss: 0.008017\n",
      "Training stage for Flod 3 Epoch: 91 [3200/37476                 (11%)]\tLoss: 0.004694\n",
      "Training stage for Flod 3 Epoch: 91 [6400/37476                 (21%)]\tLoss: 0.016257\n",
      "Training stage for Flod 3 Epoch: 91 [9600/37476                 (32%)]\tLoss: 0.011828\n",
      "Training stage for Flod 3 Epoch: 91 [12800/37476                 (43%)]\tLoss: 0.002236\n",
      "Training stage for Flod 3 Epoch: 91 [16000/37476                 (53%)]\tLoss: 0.001216\n",
      "Training stage for Flod 3 Epoch: 91 [19200/37476                 (64%)]\tLoss: 0.000992\n",
      "Training stage for Flod 3 Epoch: 91 [22400/37476                 (75%)]\tLoss: 0.007964\n",
      "Training stage for Flod 3 Epoch: 91 [25600/37476                 (85%)]\tLoss: 0.038944\n",
      "Training stage for Flod 3 Epoch: 91 [28800/37476                 (96%)]\tLoss: 0.096658\n",
      "Test set for fold3: Average Loss:           1.2699, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 92 [0/37476                 (0%)]\tLoss: 0.008972\n",
      "Training stage for Flod 3 Epoch: 92 [3200/37476                 (11%)]\tLoss: 0.050445\n",
      "Training stage for Flod 3 Epoch: 92 [6400/37476                 (21%)]\tLoss: 0.013771\n",
      "Training stage for Flod 3 Epoch: 92 [9600/37476                 (32%)]\tLoss: 0.000726\n",
      "Training stage for Flod 3 Epoch: 92 [12800/37476                 (43%)]\tLoss: 0.037262\n",
      "Training stage for Flod 3 Epoch: 92 [16000/37476                 (53%)]\tLoss: 0.072094\n",
      "Training stage for Flod 3 Epoch: 92 [19200/37476                 (64%)]\tLoss: 0.008186\n",
      "Training stage for Flod 3 Epoch: 92 [22400/37476                 (75%)]\tLoss: 0.045799\n",
      "Training stage for Flod 3 Epoch: 92 [25600/37476                 (85%)]\tLoss: 0.000010\n",
      "Training stage for Flod 3 Epoch: 92 [28800/37476                 (96%)]\tLoss: 0.060555\n",
      "Test set for fold3: Average Loss:           1.0294, Accuracy: 14798/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 93 [0/37476                 (0%)]\tLoss: 0.021613\n",
      "Training stage for Flod 3 Epoch: 93 [3200/37476                 (11%)]\tLoss: 0.004065\n",
      "Training stage for Flod 3 Epoch: 93 [6400/37476                 (21%)]\tLoss: 0.034272\n",
      "Training stage for Flod 3 Epoch: 93 [9600/37476                 (32%)]\tLoss: 0.014609\n",
      "Training stage for Flod 3 Epoch: 93 [12800/37476                 (43%)]\tLoss: 0.081568\n",
      "Training stage for Flod 3 Epoch: 93 [16000/37476                 (53%)]\tLoss: 0.105084\n",
      "Training stage for Flod 3 Epoch: 93 [19200/37476                 (64%)]\tLoss: 0.119198\n",
      "Training stage for Flod 3 Epoch: 93 [22400/37476                 (75%)]\tLoss: 0.031652\n",
      "Training stage for Flod 3 Epoch: 93 [25600/37476                 (85%)]\tLoss: 0.056348\n",
      "Training stage for Flod 3 Epoch: 93 [28800/37476                 (96%)]\tLoss: 0.099998\n",
      "Test set for fold3: Average Loss:           1.2295, Accuracy: 14780/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 94 [0/37476                 (0%)]\tLoss: 0.031217\n",
      "Training stage for Flod 3 Epoch: 94 [3200/37476                 (11%)]\tLoss: 0.010404\n",
      "Training stage for Flod 3 Epoch: 94 [6400/37476                 (21%)]\tLoss: 0.003000\n",
      "Training stage for Flod 3 Epoch: 94 [9600/37476                 (32%)]\tLoss: 0.018405\n",
      "Training stage for Flod 3 Epoch: 94 [12800/37476                 (43%)]\tLoss: 0.027861\n",
      "Training stage for Flod 3 Epoch: 94 [16000/37476                 (53%)]\tLoss: 0.000040\n",
      "Training stage for Flod 3 Epoch: 94 [19200/37476                 (64%)]\tLoss: 0.060548\n",
      "Training stage for Flod 3 Epoch: 94 [22400/37476                 (75%)]\tLoss: 0.005530\n",
      "Training stage for Flod 3 Epoch: 94 [25600/37476                 (85%)]\tLoss: 0.009232\n",
      "Training stage for Flod 3 Epoch: 94 [28800/37476                 (96%)]\tLoss: 0.009757\n",
      "Test set for fold3: Average Loss:           1.4602, Accuracy: 14773/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 95 [0/37476                 (0%)]\tLoss: 0.122370\n",
      "Training stage for Flod 3 Epoch: 95 [3200/37476                 (11%)]\tLoss: 0.108290\n",
      "Training stage for Flod 3 Epoch: 95 [6400/37476                 (21%)]\tLoss: 0.033667\n",
      "Training stage for Flod 3 Epoch: 95 [9600/37476                 (32%)]\tLoss: 0.024943\n",
      "Training stage for Flod 3 Epoch: 95 [12800/37476                 (43%)]\tLoss: 0.009054\n",
      "Training stage for Flod 3 Epoch: 95 [16000/37476                 (53%)]\tLoss: 0.023943\n",
      "Training stage for Flod 3 Epoch: 95 [19200/37476                 (64%)]\tLoss: 0.215666\n",
      "Training stage for Flod 3 Epoch: 95 [22400/37476                 (75%)]\tLoss: 0.013110\n",
      "Training stage for Flod 3 Epoch: 95 [25600/37476                 (85%)]\tLoss: 0.006999\n",
      "Training stage for Flod 3 Epoch: 95 [28800/37476                 (96%)]\tLoss: 0.000136\n",
      "Test set for fold3: Average Loss:           1.1221, Accuracy: 14769/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 96 [0/37476                 (0%)]\tLoss: 0.008607\n",
      "Training stage for Flod 3 Epoch: 96 [3200/37476                 (11%)]\tLoss: 0.006128\n",
      "Training stage for Flod 3 Epoch: 96 [6400/37476                 (21%)]\tLoss: 0.020600\n",
      "Training stage for Flod 3 Epoch: 96 [9600/37476                 (32%)]\tLoss: 0.008378\n",
      "Training stage for Flod 3 Epoch: 96 [12800/37476                 (43%)]\tLoss: 0.037384\n",
      "Training stage for Flod 3 Epoch: 96 [16000/37476                 (53%)]\tLoss: 0.005435\n",
      "Training stage for Flod 3 Epoch: 96 [19200/37476                 (64%)]\tLoss: 0.082150\n",
      "Training stage for Flod 3 Epoch: 96 [22400/37476                 (75%)]\tLoss: 0.001501\n",
      "Training stage for Flod 3 Epoch: 96 [25600/37476                 (85%)]\tLoss: 0.007593\n",
      "Training stage for Flod 3 Epoch: 96 [28800/37476                 (96%)]\tLoss: 0.002462\n",
      "Test set for fold3: Average Loss:           1.1827, Accuracy: 14725/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 97 [0/37476                 (0%)]\tLoss: 0.003212\n",
      "Training stage for Flod 3 Epoch: 97 [3200/37476                 (11%)]\tLoss: 0.021482\n",
      "Training stage for Flod 3 Epoch: 97 [6400/37476                 (21%)]\tLoss: 0.092044\n",
      "Training stage for Flod 3 Epoch: 97 [9600/37476                 (32%)]\tLoss: 0.061422\n",
      "Training stage for Flod 3 Epoch: 97 [12800/37476                 (43%)]\tLoss: 0.001037\n",
      "Training stage for Flod 3 Epoch: 97 [16000/37476                 (53%)]\tLoss: 0.063525\n",
      "Training stage for Flod 3 Epoch: 97 [19200/37476                 (64%)]\tLoss: 0.004426\n",
      "Training stage for Flod 3 Epoch: 97 [22400/37476                 (75%)]\tLoss: 0.055914\n",
      "Training stage for Flod 3 Epoch: 97 [25600/37476                 (85%)]\tLoss: 0.050618\n",
      "Training stage for Flod 3 Epoch: 97 [28800/37476                 (96%)]\tLoss: 0.013594\n",
      "Test set for fold3: Average Loss:           1.1136, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 98 [0/37476                 (0%)]\tLoss: 0.001046\n",
      "Training stage for Flod 3 Epoch: 98 [3200/37476                 (11%)]\tLoss: 0.062951\n",
      "Training stage for Flod 3 Epoch: 98 [6400/37476                 (21%)]\tLoss: 0.014482\n",
      "Training stage for Flod 3 Epoch: 98 [9600/37476                 (32%)]\tLoss: 0.053816\n",
      "Training stage for Flod 3 Epoch: 98 [12800/37476                 (43%)]\tLoss: 0.091007\n",
      "Training stage for Flod 3 Epoch: 98 [16000/37476                 (53%)]\tLoss: 0.001218\n",
      "Training stage for Flod 3 Epoch: 98 [19200/37476                 (64%)]\tLoss: 0.039648\n",
      "Training stage for Flod 3 Epoch: 98 [22400/37476                 (75%)]\tLoss: 0.087161\n",
      "Training stage for Flod 3 Epoch: 98 [25600/37476                 (85%)]\tLoss: 0.005697\n",
      "Training stage for Flod 3 Epoch: 98 [28800/37476                 (96%)]\tLoss: 0.040511\n",
      "Test set for fold3: Average Loss:           1.1292, Accuracy: 14751/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 99 [0/37476                 (0%)]\tLoss: 0.010611\n",
      "Training stage for Flod 3 Epoch: 99 [3200/37476                 (11%)]\tLoss: 0.016098\n",
      "Training stage for Flod 3 Epoch: 99 [6400/37476                 (21%)]\tLoss: 0.028116\n",
      "Training stage for Flod 3 Epoch: 99 [9600/37476                 (32%)]\tLoss: 0.002209\n",
      "Training stage for Flod 3 Epoch: 99 [12800/37476                 (43%)]\tLoss: 0.000154\n",
      "Training stage for Flod 3 Epoch: 99 [16000/37476                 (53%)]\tLoss: 0.019524\n",
      "Training stage for Flod 3 Epoch: 99 [19200/37476                 (64%)]\tLoss: 0.000006\n",
      "Training stage for Flod 3 Epoch: 99 [22400/37476                 (75%)]\tLoss: 0.008569\n",
      "Training stage for Flod 3 Epoch: 99 [25600/37476                 (85%)]\tLoss: 0.078005\n",
      "Training stage for Flod 3 Epoch: 99 [28800/37476                 (96%)]\tLoss: 0.006648\n",
      "Test set for fold3: Average Loss:           1.0987, Accuracy: 14790/37476           (39%)\n",
      "Training stage for Flod 3 Epoch: 100 [0/37476                 (0%)]\tLoss: 0.027246\n",
      "Training stage for Flod 3 Epoch: 100 [3200/37476                 (11%)]\tLoss: 0.042562\n",
      "Training stage for Flod 3 Epoch: 100 [6400/37476                 (21%)]\tLoss: 0.001361\n",
      "Training stage for Flod 3 Epoch: 100 [9600/37476                 (32%)]\tLoss: 0.000400\n",
      "Training stage for Flod 3 Epoch: 100 [12800/37476                 (43%)]\tLoss: 0.011623\n",
      "Training stage for Flod 3 Epoch: 100 [16000/37476                 (53%)]\tLoss: 0.017535\n",
      "Training stage for Flod 3 Epoch: 100 [19200/37476                 (64%)]\tLoss: 0.003382\n",
      "Training stage for Flod 3 Epoch: 100 [22400/37476                 (75%)]\tLoss: 0.075110\n",
      "Training stage for Flod 3 Epoch: 100 [25600/37476                 (85%)]\tLoss: 0.016443\n",
      "Training stage for Flod 3 Epoch: 100 [28800/37476                 (96%)]\tLoss: 0.000199\n",
      "Test set for fold3: Average Loss:           1.1418, Accuracy: 14749/37476           (39%)\n",
      "-------------------Fold 4-------------------\n",
      "Training stage for Flod 4 Epoch: 1 [0/37476                 (0%)]\tLoss: 0.657697\n",
      "Training stage for Flod 4 Epoch: 1 [3200/37476                 (11%)]\tLoss: 0.020783\n",
      "Training stage for Flod 4 Epoch: 1 [6400/37476                 (21%)]\tLoss: 0.133873\n",
      "Training stage for Flod 4 Epoch: 1 [9600/37476                 (32%)]\tLoss: 0.135181\n",
      "Training stage for Flod 4 Epoch: 1 [12800/37476                 (43%)]\tLoss: 0.016276\n",
      "Training stage for Flod 4 Epoch: 1 [16000/37476                 (53%)]\tLoss: 0.117140\n",
      "Training stage for Flod 4 Epoch: 1 [19200/37476                 (64%)]\tLoss: 0.136628\n",
      "Training stage for Flod 4 Epoch: 1 [22400/37476                 (75%)]\tLoss: 0.006775\n",
      "Training stage for Flod 4 Epoch: 1 [25600/37476                 (85%)]\tLoss: 0.011328\n",
      "Training stage for Flod 4 Epoch: 1 [28800/37476                 (96%)]\tLoss: 0.019281\n",
      "Test set for fold4: Average Loss:           0.7928, Accuracy: 14590/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 2 [0/37476                 (0%)]\tLoss: 0.073758\n",
      "Training stage for Flod 4 Epoch: 2 [3200/37476                 (11%)]\tLoss: 0.003647\n",
      "Training stage for Flod 4 Epoch: 2 [6400/37476                 (21%)]\tLoss: 0.090357\n",
      "Training stage for Flod 4 Epoch: 2 [9600/37476                 (32%)]\tLoss: 0.099558\n",
      "Training stage for Flod 4 Epoch: 2 [12800/37476                 (43%)]\tLoss: 0.003153\n",
      "Training stage for Flod 4 Epoch: 2 [16000/37476                 (53%)]\tLoss: 0.060818\n",
      "Training stage for Flod 4 Epoch: 2 [19200/37476                 (64%)]\tLoss: 0.028071\n",
      "Training stage for Flod 4 Epoch: 2 [22400/37476                 (75%)]\tLoss: 0.052080\n",
      "Training stage for Flod 4 Epoch: 2 [25600/37476                 (85%)]\tLoss: 0.066071\n",
      "Training stage for Flod 4 Epoch: 2 [28800/37476                 (96%)]\tLoss: 0.005618\n",
      "Test set for fold4: Average Loss:           0.7903, Accuracy: 14620/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 3 [0/37476                 (0%)]\tLoss: 0.008492\n",
      "Training stage for Flod 4 Epoch: 3 [3200/37476                 (11%)]\tLoss: 0.033174\n",
      "Training stage for Flod 4 Epoch: 3 [6400/37476                 (21%)]\tLoss: 0.049192\n",
      "Training stage for Flod 4 Epoch: 3 [9600/37476                 (32%)]\tLoss: 0.105629\n",
      "Training stage for Flod 4 Epoch: 3 [12800/37476                 (43%)]\tLoss: 0.030627\n",
      "Training stage for Flod 4 Epoch: 3 [16000/37476                 (53%)]\tLoss: 0.092361\n",
      "Training stage for Flod 4 Epoch: 3 [19200/37476                 (64%)]\tLoss: 0.017880\n",
      "Training stage for Flod 4 Epoch: 3 [22400/37476                 (75%)]\tLoss: 0.011682\n",
      "Training stage for Flod 4 Epoch: 3 [25600/37476                 (85%)]\tLoss: 0.092514\n",
      "Training stage for Flod 4 Epoch: 3 [28800/37476                 (96%)]\tLoss: 0.012755\n",
      "Test set for fold4: Average Loss:           0.8525, Accuracy: 14644/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 4 [0/37476                 (0%)]\tLoss: 0.133727\n",
      "Training stage for Flod 4 Epoch: 4 [3200/37476                 (11%)]\tLoss: 0.016371\n",
      "Training stage for Flod 4 Epoch: 4 [6400/37476                 (21%)]\tLoss: 0.067072\n",
      "Training stage for Flod 4 Epoch: 4 [9600/37476                 (32%)]\tLoss: 0.005714\n",
      "Training stage for Flod 4 Epoch: 4 [12800/37476                 (43%)]\tLoss: 0.016264\n",
      "Training stage for Flod 4 Epoch: 4 [16000/37476                 (53%)]\tLoss: 0.057570\n",
      "Training stage for Flod 4 Epoch: 4 [19200/37476                 (64%)]\tLoss: 0.025816\n",
      "Training stage for Flod 4 Epoch: 4 [22400/37476                 (75%)]\tLoss: 0.024850\n",
      "Training stage for Flod 4 Epoch: 4 [25600/37476                 (85%)]\tLoss: 0.056713\n",
      "Training stage for Flod 4 Epoch: 4 [28800/37476                 (96%)]\tLoss: 0.074529\n",
      "Test set for fold4: Average Loss:           0.8828, Accuracy: 14604/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 5 [0/37476                 (0%)]\tLoss: 0.045410\n",
      "Training stage for Flod 4 Epoch: 5 [3200/37476                 (11%)]\tLoss: 0.025343\n",
      "Training stage for Flod 4 Epoch: 5 [6400/37476                 (21%)]\tLoss: 0.089283\n",
      "Training stage for Flod 4 Epoch: 5 [9600/37476                 (32%)]\tLoss: 0.015314\n",
      "Training stage for Flod 4 Epoch: 5 [12800/37476                 (43%)]\tLoss: 0.083979\n",
      "Training stage for Flod 4 Epoch: 5 [16000/37476                 (53%)]\tLoss: 0.136952\n",
      "Training stage for Flod 4 Epoch: 5 [19200/37476                 (64%)]\tLoss: 0.107469\n",
      "Training stage for Flod 4 Epoch: 5 [22400/37476                 (75%)]\tLoss: 0.006157\n",
      "Training stage for Flod 4 Epoch: 5 [25600/37476                 (85%)]\tLoss: 0.006386\n",
      "Training stage for Flod 4 Epoch: 5 [28800/37476                 (96%)]\tLoss: 0.021510\n",
      "Test set for fold4: Average Loss:           0.9218, Accuracy: 14685/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 6 [0/37476                 (0%)]\tLoss: 0.272187\n",
      "Training stage for Flod 4 Epoch: 6 [3200/37476                 (11%)]\tLoss: 0.016630\n",
      "Training stage for Flod 4 Epoch: 6 [6400/37476                 (21%)]\tLoss: 0.002500\n",
      "Training stage for Flod 4 Epoch: 6 [9600/37476                 (32%)]\tLoss: 0.236135\n",
      "Training stage for Flod 4 Epoch: 6 [12800/37476                 (43%)]\tLoss: 0.058228\n",
      "Training stage for Flod 4 Epoch: 6 [16000/37476                 (53%)]\tLoss: 0.075732\n",
      "Training stage for Flod 4 Epoch: 6 [19200/37476                 (64%)]\tLoss: 0.038183\n",
      "Training stage for Flod 4 Epoch: 6 [22400/37476                 (75%)]\tLoss: 0.070151\n",
      "Training stage for Flod 4 Epoch: 6 [25600/37476                 (85%)]\tLoss: 0.018878\n",
      "Training stage for Flod 4 Epoch: 6 [28800/37476                 (96%)]\tLoss: 0.031125\n",
      "Test set for fold4: Average Loss:           1.0107, Accuracy: 14702/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 7 [0/37476                 (0%)]\tLoss: 0.079194\n",
      "Training stage for Flod 4 Epoch: 7 [3200/37476                 (11%)]\tLoss: 0.010506\n",
      "Training stage for Flod 4 Epoch: 7 [6400/37476                 (21%)]\tLoss: 0.000540\n",
      "Training stage for Flod 4 Epoch: 7 [9600/37476                 (32%)]\tLoss: 0.029509\n",
      "Training stage for Flod 4 Epoch: 7 [12800/37476                 (43%)]\tLoss: 0.146095\n",
      "Training stage for Flod 4 Epoch: 7 [16000/37476                 (53%)]\tLoss: 0.043931\n",
      "Training stage for Flod 4 Epoch: 7 [19200/37476                 (64%)]\tLoss: 0.012522\n",
      "Training stage for Flod 4 Epoch: 7 [22400/37476                 (75%)]\tLoss: 0.058628\n",
      "Training stage for Flod 4 Epoch: 7 [25600/37476                 (85%)]\tLoss: 0.036781\n",
      "Training stage for Flod 4 Epoch: 7 [28800/37476                 (96%)]\tLoss: 0.051135\n",
      "Test set for fold4: Average Loss:           1.0160, Accuracy: 14710/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 8 [0/37476                 (0%)]\tLoss: 0.015806\n",
      "Training stage for Flod 4 Epoch: 8 [3200/37476                 (11%)]\tLoss: 0.104459\n",
      "Training stage for Flod 4 Epoch: 8 [6400/37476                 (21%)]\tLoss: 0.000565\n",
      "Training stage for Flod 4 Epoch: 8 [9600/37476                 (32%)]\tLoss: 0.191161\n",
      "Training stage for Flod 4 Epoch: 8 [12800/37476                 (43%)]\tLoss: 0.109740\n",
      "Training stage for Flod 4 Epoch: 8 [16000/37476                 (53%)]\tLoss: 0.029528\n",
      "Training stage for Flod 4 Epoch: 8 [19200/37476                 (64%)]\tLoss: 0.019963\n",
      "Training stage for Flod 4 Epoch: 8 [22400/37476                 (75%)]\tLoss: 0.114500\n",
      "Training stage for Flod 4 Epoch: 8 [25600/37476                 (85%)]\tLoss: 0.018117\n",
      "Training stage for Flod 4 Epoch: 8 [28800/37476                 (96%)]\tLoss: 0.181993\n",
      "Test set for fold4: Average Loss:           0.9543, Accuracy: 14657/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 9 [0/37476                 (0%)]\tLoss: 0.002634\n",
      "Training stage for Flod 4 Epoch: 9 [3200/37476                 (11%)]\tLoss: 0.113959\n",
      "Training stage for Flod 4 Epoch: 9 [6400/37476                 (21%)]\tLoss: 0.087262\n",
      "Training stage for Flod 4 Epoch: 9 [9600/37476                 (32%)]\tLoss: 0.053940\n",
      "Training stage for Flod 4 Epoch: 9 [12800/37476                 (43%)]\tLoss: 0.002078\n",
      "Training stage for Flod 4 Epoch: 9 [16000/37476                 (53%)]\tLoss: 0.001276\n",
      "Training stage for Flod 4 Epoch: 9 [19200/37476                 (64%)]\tLoss: 0.207297\n",
      "Training stage for Flod 4 Epoch: 9 [22400/37476                 (75%)]\tLoss: 0.013536\n",
      "Training stage for Flod 4 Epoch: 9 [25600/37476                 (85%)]\tLoss: 0.001429\n",
      "Training stage for Flod 4 Epoch: 9 [28800/37476                 (96%)]\tLoss: 0.051388\n",
      "Test set for fold4: Average Loss:           0.9753, Accuracy: 14614/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 10 [0/37476                 (0%)]\tLoss: 0.272325\n",
      "Training stage for Flod 4 Epoch: 10 [3200/37476                 (11%)]\tLoss: 0.160796\n",
      "Training stage for Flod 4 Epoch: 10 [6400/37476                 (21%)]\tLoss: 0.243786\n",
      "Training stage for Flod 4 Epoch: 10 [9600/37476                 (32%)]\tLoss: 0.225906\n",
      "Training stage for Flod 4 Epoch: 10 [12800/37476                 (43%)]\tLoss: 0.016793\n",
      "Training stage for Flod 4 Epoch: 10 [16000/37476                 (53%)]\tLoss: 0.026252\n",
      "Training stage for Flod 4 Epoch: 10 [19200/37476                 (64%)]\tLoss: 0.118347\n",
      "Training stage for Flod 4 Epoch: 10 [22400/37476                 (75%)]\tLoss: 0.009320\n",
      "Training stage for Flod 4 Epoch: 10 [25600/37476                 (85%)]\tLoss: 0.038600\n",
      "Training stage for Flod 4 Epoch: 10 [28800/37476                 (96%)]\tLoss: 0.003061\n",
      "Test set for fold4: Average Loss:           0.8811, Accuracy: 14506/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 11 [0/37476                 (0%)]\tLoss: 0.221925\n",
      "Training stage for Flod 4 Epoch: 11 [3200/37476                 (11%)]\tLoss: 0.711103\n",
      "Training stage for Flod 4 Epoch: 11 [6400/37476                 (21%)]\tLoss: 0.012802\n",
      "Training stage for Flod 4 Epoch: 11 [9600/37476                 (32%)]\tLoss: 0.127681\n",
      "Training stage for Flod 4 Epoch: 11 [12800/37476                 (43%)]\tLoss: 0.043997\n",
      "Training stage for Flod 4 Epoch: 11 [16000/37476                 (53%)]\tLoss: 0.110097\n",
      "Training stage for Flod 4 Epoch: 11 [19200/37476                 (64%)]\tLoss: 0.000650\n",
      "Training stage for Flod 4 Epoch: 11 [22400/37476                 (75%)]\tLoss: 0.186664\n",
      "Training stage for Flod 4 Epoch: 11 [25600/37476                 (85%)]\tLoss: 0.015255\n",
      "Training stage for Flod 4 Epoch: 11 [28800/37476                 (96%)]\tLoss: 0.097583\n",
      "Test set for fold4: Average Loss:           1.0142, Accuracy: 14679/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 12 [0/37476                 (0%)]\tLoss: 0.065529\n",
      "Training stage for Flod 4 Epoch: 12 [3200/37476                 (11%)]\tLoss: 0.012965\n",
      "Training stage for Flod 4 Epoch: 12 [6400/37476                 (21%)]\tLoss: 0.002363\n",
      "Training stage for Flod 4 Epoch: 12 [9600/37476                 (32%)]\tLoss: 0.164542\n",
      "Training stage for Flod 4 Epoch: 12 [12800/37476                 (43%)]\tLoss: 0.073151\n",
      "Training stage for Flod 4 Epoch: 12 [16000/37476                 (53%)]\tLoss: 0.101722\n",
      "Training stage for Flod 4 Epoch: 12 [19200/37476                 (64%)]\tLoss: 0.038255\n",
      "Training stage for Flod 4 Epoch: 12 [22400/37476                 (75%)]\tLoss: 0.006152\n",
      "Training stage for Flod 4 Epoch: 12 [25600/37476                 (85%)]\tLoss: 0.010812\n",
      "Training stage for Flod 4 Epoch: 12 [28800/37476                 (96%)]\tLoss: 0.057168\n",
      "Test set for fold4: Average Loss:           0.9508, Accuracy: 14734/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 13 [0/37476                 (0%)]\tLoss: 0.164125\n",
      "Training stage for Flod 4 Epoch: 13 [3200/37476                 (11%)]\tLoss: 0.064821\n",
      "Training stage for Flod 4 Epoch: 13 [6400/37476                 (21%)]\tLoss: 0.001262\n",
      "Training stage for Flod 4 Epoch: 13 [9600/37476                 (32%)]\tLoss: 0.034256\n",
      "Training stage for Flod 4 Epoch: 13 [12800/37476                 (43%)]\tLoss: 0.000209\n",
      "Training stage for Flod 4 Epoch: 13 [16000/37476                 (53%)]\tLoss: 0.008846\n",
      "Training stage for Flod 4 Epoch: 13 [19200/37476                 (64%)]\tLoss: 0.063194\n",
      "Training stage for Flod 4 Epoch: 13 [22400/37476                 (75%)]\tLoss: 0.040194\n",
      "Training stage for Flod 4 Epoch: 13 [25600/37476                 (85%)]\tLoss: 0.030789\n",
      "Training stage for Flod 4 Epoch: 13 [28800/37476                 (96%)]\tLoss: 0.010904\n",
      "Test set for fold4: Average Loss:           1.0157, Accuracy: 14697/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 14 [0/37476                 (0%)]\tLoss: 0.001457\n",
      "Training stage for Flod 4 Epoch: 14 [3200/37476                 (11%)]\tLoss: 0.169523\n",
      "Training stage for Flod 4 Epoch: 14 [6400/37476                 (21%)]\tLoss: 0.010597\n",
      "Training stage for Flod 4 Epoch: 14 [9600/37476                 (32%)]\tLoss: 0.029858\n",
      "Training stage for Flod 4 Epoch: 14 [12800/37476                 (43%)]\tLoss: 0.009815\n",
      "Training stage for Flod 4 Epoch: 14 [16000/37476                 (53%)]\tLoss: 0.110505\n",
      "Training stage for Flod 4 Epoch: 14 [19200/37476                 (64%)]\tLoss: 0.078778\n",
      "Training stage for Flod 4 Epoch: 14 [22400/37476                 (75%)]\tLoss: 0.009327\n",
      "Training stage for Flod 4 Epoch: 14 [25600/37476                 (85%)]\tLoss: 0.086881\n",
      "Training stage for Flod 4 Epoch: 14 [28800/37476                 (96%)]\tLoss: 0.034989\n",
      "Test set for fold4: Average Loss:           1.0109, Accuracy: 14680/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 15 [0/37476                 (0%)]\tLoss: 0.003071\n",
      "Training stage for Flod 4 Epoch: 15 [3200/37476                 (11%)]\tLoss: 0.011529\n",
      "Training stage for Flod 4 Epoch: 15 [6400/37476                 (21%)]\tLoss: 0.056266\n",
      "Training stage for Flod 4 Epoch: 15 [9600/37476                 (32%)]\tLoss: 0.024483\n",
      "Training stage for Flod 4 Epoch: 15 [12800/37476                 (43%)]\tLoss: 0.059871\n",
      "Training stage for Flod 4 Epoch: 15 [16000/37476                 (53%)]\tLoss: 0.006433\n",
      "Training stage for Flod 4 Epoch: 15 [19200/37476                 (64%)]\tLoss: 0.029917\n",
      "Training stage for Flod 4 Epoch: 15 [22400/37476                 (75%)]\tLoss: 0.041838\n",
      "Training stage for Flod 4 Epoch: 15 [25600/37476                 (85%)]\tLoss: 0.142781\n",
      "Training stage for Flod 4 Epoch: 15 [28800/37476                 (96%)]\tLoss: 0.106506\n",
      "Test set for fold4: Average Loss:           1.0749, Accuracy: 14729/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 16 [0/37476                 (0%)]\tLoss: 0.005071\n",
      "Training stage for Flod 4 Epoch: 16 [3200/37476                 (11%)]\tLoss: 0.052358\n",
      "Training stage for Flod 4 Epoch: 16 [6400/37476                 (21%)]\tLoss: 0.015506\n",
      "Training stage for Flod 4 Epoch: 16 [9600/37476                 (32%)]\tLoss: 0.059846\n",
      "Training stage for Flod 4 Epoch: 16 [12800/37476                 (43%)]\tLoss: 0.049800\n",
      "Training stage for Flod 4 Epoch: 16 [16000/37476                 (53%)]\tLoss: 0.015943\n",
      "Training stage for Flod 4 Epoch: 16 [19200/37476                 (64%)]\tLoss: 0.057726\n",
      "Training stage for Flod 4 Epoch: 16 [22400/37476                 (75%)]\tLoss: 0.053333\n",
      "Training stage for Flod 4 Epoch: 16 [25600/37476                 (85%)]\tLoss: 0.083305\n",
      "Training stage for Flod 4 Epoch: 16 [28800/37476                 (96%)]\tLoss: 0.093115\n",
      "Test set for fold4: Average Loss:           1.0016, Accuracy: 14658/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 17 [0/37476                 (0%)]\tLoss: 0.000715\n",
      "Training stage for Flod 4 Epoch: 17 [3200/37476                 (11%)]\tLoss: 0.027812\n",
      "Training stage for Flod 4 Epoch: 17 [6400/37476                 (21%)]\tLoss: 0.065228\n",
      "Training stage for Flod 4 Epoch: 17 [9600/37476                 (32%)]\tLoss: 0.006036\n",
      "Training stage for Flod 4 Epoch: 17 [12800/37476                 (43%)]\tLoss: 0.181431\n",
      "Training stage for Flod 4 Epoch: 17 [16000/37476                 (53%)]\tLoss: 0.014710\n",
      "Training stage for Flod 4 Epoch: 17 [19200/37476                 (64%)]\tLoss: 0.014576\n",
      "Training stage for Flod 4 Epoch: 17 [22400/37476                 (75%)]\tLoss: 0.001375\n",
      "Training stage for Flod 4 Epoch: 17 [25600/37476                 (85%)]\tLoss: 0.061390\n",
      "Training stage for Flod 4 Epoch: 17 [28800/37476                 (96%)]\tLoss: 0.005062\n",
      "Test set for fold4: Average Loss:           1.0036, Accuracy: 14702/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 18 [0/37476                 (0%)]\tLoss: 0.144893\n",
      "Training stage for Flod 4 Epoch: 18 [3200/37476                 (11%)]\tLoss: 0.072027\n",
      "Training stage for Flod 4 Epoch: 18 [6400/37476                 (21%)]\tLoss: 0.028696\n",
      "Training stage for Flod 4 Epoch: 18 [9600/37476                 (32%)]\tLoss: 0.089318\n",
      "Training stage for Flod 4 Epoch: 18 [12800/37476                 (43%)]\tLoss: 0.029769\n",
      "Training stage for Flod 4 Epoch: 18 [16000/37476                 (53%)]\tLoss: 0.046909\n",
      "Training stage for Flod 4 Epoch: 18 [19200/37476                 (64%)]\tLoss: 0.015103\n",
      "Training stage for Flod 4 Epoch: 18 [22400/37476                 (75%)]\tLoss: 0.093305\n",
      "Training stage for Flod 4 Epoch: 18 [25600/37476                 (85%)]\tLoss: 0.086754\n",
      "Training stage for Flod 4 Epoch: 18 [28800/37476                 (96%)]\tLoss: 0.091815\n",
      "Test set for fold4: Average Loss:           0.9409, Accuracy: 14700/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 19 [0/37476                 (0%)]\tLoss: 0.028122\n",
      "Training stage for Flod 4 Epoch: 19 [3200/37476                 (11%)]\tLoss: 0.091083\n",
      "Training stage for Flod 4 Epoch: 19 [6400/37476                 (21%)]\tLoss: 0.110168\n",
      "Training stage for Flod 4 Epoch: 19 [9600/37476                 (32%)]\tLoss: 0.018536\n",
      "Training stage for Flod 4 Epoch: 19 [12800/37476                 (43%)]\tLoss: 0.014481\n",
      "Training stage for Flod 4 Epoch: 19 [16000/37476                 (53%)]\tLoss: 0.046299\n",
      "Training stage for Flod 4 Epoch: 19 [19200/37476                 (64%)]\tLoss: 0.020540\n",
      "Training stage for Flod 4 Epoch: 19 [22400/37476                 (75%)]\tLoss: 0.019450\n",
      "Training stage for Flod 4 Epoch: 19 [25600/37476                 (85%)]\tLoss: 0.046625\n",
      "Training stage for Flod 4 Epoch: 19 [28800/37476                 (96%)]\tLoss: 0.041487\n",
      "Test set for fold4: Average Loss:           1.0137, Accuracy: 14736/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 20 [0/37476                 (0%)]\tLoss: 0.007346\n",
      "Training stage for Flod 4 Epoch: 20 [3200/37476                 (11%)]\tLoss: 0.002598\n",
      "Training stage for Flod 4 Epoch: 20 [6400/37476                 (21%)]\tLoss: 0.147115\n",
      "Training stage for Flod 4 Epoch: 20 [9600/37476                 (32%)]\tLoss: 0.003476\n",
      "Training stage for Flod 4 Epoch: 20 [12800/37476                 (43%)]\tLoss: 0.012279\n",
      "Training stage for Flod 4 Epoch: 20 [16000/37476                 (53%)]\tLoss: 0.076979\n",
      "Training stage for Flod 4 Epoch: 20 [19200/37476                 (64%)]\tLoss: 0.063449\n",
      "Training stage for Flod 4 Epoch: 20 [22400/37476                 (75%)]\tLoss: 0.000190\n",
      "Training stage for Flod 4 Epoch: 20 [25600/37476                 (85%)]\tLoss: 0.010662\n",
      "Training stage for Flod 4 Epoch: 20 [28800/37476                 (96%)]\tLoss: 0.000102\n",
      "Test set for fold4: Average Loss:           1.0971, Accuracy: 14695/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 21 [0/37476                 (0%)]\tLoss: 0.251285\n",
      "Training stage for Flod 4 Epoch: 21 [3200/37476                 (11%)]\tLoss: 0.006152\n",
      "Training stage for Flod 4 Epoch: 21 [6400/37476                 (21%)]\tLoss: 0.017279\n",
      "Training stage for Flod 4 Epoch: 21 [9600/37476                 (32%)]\tLoss: 0.014941\n",
      "Training stage for Flod 4 Epoch: 21 [12800/37476                 (43%)]\tLoss: 0.038306\n",
      "Training stage for Flod 4 Epoch: 21 [16000/37476                 (53%)]\tLoss: 0.027533\n",
      "Training stage for Flod 4 Epoch: 21 [19200/37476                 (64%)]\tLoss: 0.036214\n",
      "Training stage for Flod 4 Epoch: 21 [22400/37476                 (75%)]\tLoss: 0.064217\n",
      "Training stage for Flod 4 Epoch: 21 [25600/37476                 (85%)]\tLoss: 0.028850\n",
      "Training stage for Flod 4 Epoch: 21 [28800/37476                 (96%)]\tLoss: 0.017435\n",
      "Test set for fold4: Average Loss:           0.9439, Accuracy: 14649/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 22 [0/37476                 (0%)]\tLoss: 0.034782\n",
      "Training stage for Flod 4 Epoch: 22 [3200/37476                 (11%)]\tLoss: 0.016506\n",
      "Training stage for Flod 4 Epoch: 22 [6400/37476                 (21%)]\tLoss: 0.054517\n",
      "Training stage for Flod 4 Epoch: 22 [9600/37476                 (32%)]\tLoss: 0.086331\n",
      "Training stage for Flod 4 Epoch: 22 [12800/37476                 (43%)]\tLoss: 0.052075\n",
      "Training stage for Flod 4 Epoch: 22 [16000/37476                 (53%)]\tLoss: 0.014604\n",
      "Training stage for Flod 4 Epoch: 22 [19200/37476                 (64%)]\tLoss: 0.000068\n",
      "Training stage for Flod 4 Epoch: 22 [22400/37476                 (75%)]\tLoss: 0.031277\n",
      "Training stage for Flod 4 Epoch: 22 [25600/37476                 (85%)]\tLoss: 0.072175\n",
      "Training stage for Flod 4 Epoch: 22 [28800/37476                 (96%)]\tLoss: 0.021506\n",
      "Test set for fold4: Average Loss:           1.0741, Accuracy: 14701/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 23 [0/37476                 (0%)]\tLoss: 0.039559\n",
      "Training stage for Flod 4 Epoch: 23 [3200/37476                 (11%)]\tLoss: 0.005029\n",
      "Training stage for Flod 4 Epoch: 23 [6400/37476                 (21%)]\tLoss: 0.041511\n",
      "Training stage for Flod 4 Epoch: 23 [9600/37476                 (32%)]\tLoss: 0.001650\n",
      "Training stage for Flod 4 Epoch: 23 [12800/37476                 (43%)]\tLoss: 0.035448\n",
      "Training stage for Flod 4 Epoch: 23 [16000/37476                 (53%)]\tLoss: 0.026090\n",
      "Training stage for Flod 4 Epoch: 23 [19200/37476                 (64%)]\tLoss: 0.004592\n",
      "Training stage for Flod 4 Epoch: 23 [22400/37476                 (75%)]\tLoss: 0.041870\n",
      "Training stage for Flod 4 Epoch: 23 [25600/37476                 (85%)]\tLoss: 0.103336\n",
      "Training stage for Flod 4 Epoch: 23 [28800/37476                 (96%)]\tLoss: 0.053161\n",
      "Test set for fold4: Average Loss:           1.0800, Accuracy: 14702/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 24 [0/37476                 (0%)]\tLoss: 0.001301\n",
      "Training stage for Flod 4 Epoch: 24 [3200/37476                 (11%)]\tLoss: 0.005058\n",
      "Training stage for Flod 4 Epoch: 24 [6400/37476                 (21%)]\tLoss: 0.018917\n",
      "Training stage for Flod 4 Epoch: 24 [9600/37476                 (32%)]\tLoss: 0.034825\n",
      "Training stage for Flod 4 Epoch: 24 [12800/37476                 (43%)]\tLoss: 0.023133\n",
      "Training stage for Flod 4 Epoch: 24 [16000/37476                 (53%)]\tLoss: 0.009311\n",
      "Training stage for Flod 4 Epoch: 24 [19200/37476                 (64%)]\tLoss: 0.040197\n",
      "Training stage for Flod 4 Epoch: 24 [22400/37476                 (75%)]\tLoss: 0.021964\n",
      "Training stage for Flod 4 Epoch: 24 [25600/37476                 (85%)]\tLoss: 0.002682\n",
      "Training stage for Flod 4 Epoch: 24 [28800/37476                 (96%)]\tLoss: 0.020590\n",
      "Test set for fold4: Average Loss:           1.0677, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 25 [0/37476                 (0%)]\tLoss: 0.004107\n",
      "Training stage for Flod 4 Epoch: 25 [3200/37476                 (11%)]\tLoss: 0.002837\n",
      "Training stage for Flod 4 Epoch: 25 [6400/37476                 (21%)]\tLoss: 0.003581\n",
      "Training stage for Flod 4 Epoch: 25 [9600/37476                 (32%)]\tLoss: 0.002976\n",
      "Training stage for Flod 4 Epoch: 25 [12800/37476                 (43%)]\tLoss: 0.039983\n",
      "Training stage for Flod 4 Epoch: 25 [16000/37476                 (53%)]\tLoss: 0.006114\n",
      "Training stage for Flod 4 Epoch: 25 [19200/37476                 (64%)]\tLoss: 0.049927\n",
      "Training stage for Flod 4 Epoch: 25 [22400/37476                 (75%)]\tLoss: 0.003855\n",
      "Training stage for Flod 4 Epoch: 25 [25600/37476                 (85%)]\tLoss: 0.034727\n",
      "Training stage for Flod 4 Epoch: 25 [28800/37476                 (96%)]\tLoss: 0.009633\n",
      "Test set for fold4: Average Loss:           1.1024, Accuracy: 14753/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 26 [0/37476                 (0%)]\tLoss: 0.001581\n",
      "Training stage for Flod 4 Epoch: 26 [3200/37476                 (11%)]\tLoss: 0.007223\n",
      "Training stage for Flod 4 Epoch: 26 [6400/37476                 (21%)]\tLoss: 0.076519\n",
      "Training stage for Flod 4 Epoch: 26 [9600/37476                 (32%)]\tLoss: 0.007301\n",
      "Training stage for Flod 4 Epoch: 26 [12800/37476                 (43%)]\tLoss: 0.035655\n",
      "Training stage for Flod 4 Epoch: 26 [16000/37476                 (53%)]\tLoss: 0.044171\n",
      "Training stage for Flod 4 Epoch: 26 [19200/37476                 (64%)]\tLoss: 0.016314\n",
      "Training stage for Flod 4 Epoch: 26 [22400/37476                 (75%)]\tLoss: 0.000738\n",
      "Training stage for Flod 4 Epoch: 26 [25600/37476                 (85%)]\tLoss: 0.060209\n",
      "Training stage for Flod 4 Epoch: 26 [28800/37476                 (96%)]\tLoss: 0.018271\n",
      "Test set for fold4: Average Loss:           1.0386, Accuracy: 14725/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 27 [0/37476                 (0%)]\tLoss: 0.040773\n",
      "Training stage for Flod 4 Epoch: 27 [3200/37476                 (11%)]\tLoss: 0.040879\n",
      "Training stage for Flod 4 Epoch: 27 [6400/37476                 (21%)]\tLoss: 0.035355\n",
      "Training stage for Flod 4 Epoch: 27 [9600/37476                 (32%)]\tLoss: 0.082034\n",
      "Training stage for Flod 4 Epoch: 27 [12800/37476                 (43%)]\tLoss: 0.003351\n",
      "Training stage for Flod 4 Epoch: 27 [16000/37476                 (53%)]\tLoss: 0.004747\n",
      "Training stage for Flod 4 Epoch: 27 [19200/37476                 (64%)]\tLoss: 0.006182\n",
      "Training stage for Flod 4 Epoch: 27 [22400/37476                 (75%)]\tLoss: 0.003043\n",
      "Training stage for Flod 4 Epoch: 27 [25600/37476                 (85%)]\tLoss: 0.008053\n",
      "Training stage for Flod 4 Epoch: 27 [28800/37476                 (96%)]\tLoss: 0.176154\n",
      "Test set for fold4: Average Loss:           1.1256, Accuracy: 14733/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 28 [0/37476                 (0%)]\tLoss: 0.029067\n",
      "Training stage for Flod 4 Epoch: 28 [3200/37476                 (11%)]\tLoss: 0.129428\n",
      "Training stage for Flod 4 Epoch: 28 [6400/37476                 (21%)]\tLoss: 0.032632\n",
      "Training stage for Flod 4 Epoch: 28 [9600/37476                 (32%)]\tLoss: 0.005142\n",
      "Training stage for Flod 4 Epoch: 28 [12800/37476                 (43%)]\tLoss: 0.023429\n",
      "Training stage for Flod 4 Epoch: 28 [16000/37476                 (53%)]\tLoss: 0.128560\n",
      "Training stage for Flod 4 Epoch: 28 [19200/37476                 (64%)]\tLoss: 0.000750\n",
      "Training stage for Flod 4 Epoch: 28 [22400/37476                 (75%)]\tLoss: 0.013142\n",
      "Training stage for Flod 4 Epoch: 28 [25600/37476                 (85%)]\tLoss: 0.007199\n",
      "Training stage for Flod 4 Epoch: 28 [28800/37476                 (96%)]\tLoss: 0.006644\n",
      "Test set for fold4: Average Loss:           1.1518, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 29 [0/37476                 (0%)]\tLoss: 0.006469\n",
      "Training stage for Flod 4 Epoch: 29 [3200/37476                 (11%)]\tLoss: 0.114189\n",
      "Training stage for Flod 4 Epoch: 29 [6400/37476                 (21%)]\tLoss: 0.023840\n",
      "Training stage for Flod 4 Epoch: 29 [9600/37476                 (32%)]\tLoss: 0.005050\n",
      "Training stage for Flod 4 Epoch: 29 [12800/37476                 (43%)]\tLoss: 0.012074\n",
      "Training stage for Flod 4 Epoch: 29 [16000/37476                 (53%)]\tLoss: 0.055253\n",
      "Training stage for Flod 4 Epoch: 29 [19200/37476                 (64%)]\tLoss: 0.033626\n",
      "Training stage for Flod 4 Epoch: 29 [22400/37476                 (75%)]\tLoss: 0.002724\n",
      "Training stage for Flod 4 Epoch: 29 [25600/37476                 (85%)]\tLoss: 0.181434\n",
      "Training stage for Flod 4 Epoch: 29 [28800/37476                 (96%)]\tLoss: 0.003966\n",
      "Test set for fold4: Average Loss:           1.1326, Accuracy: 14710/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 30 [0/37476                 (0%)]\tLoss: 0.010952\n",
      "Training stage for Flod 4 Epoch: 30 [3200/37476                 (11%)]\tLoss: 0.018823\n",
      "Training stage for Flod 4 Epoch: 30 [6400/37476                 (21%)]\tLoss: 0.039580\n",
      "Training stage for Flod 4 Epoch: 30 [9600/37476                 (32%)]\tLoss: 0.010721\n",
      "Training stage for Flod 4 Epoch: 30 [12800/37476                 (43%)]\tLoss: 0.003018\n",
      "Training stage for Flod 4 Epoch: 30 [16000/37476                 (53%)]\tLoss: 0.045862\n",
      "Training stage for Flod 4 Epoch: 30 [19200/37476                 (64%)]\tLoss: 0.002542\n",
      "Training stage for Flod 4 Epoch: 30 [22400/37476                 (75%)]\tLoss: 0.053501\n",
      "Training stage for Flod 4 Epoch: 30 [25600/37476                 (85%)]\tLoss: 0.014774\n",
      "Training stage for Flod 4 Epoch: 30 [28800/37476                 (96%)]\tLoss: 0.009119\n",
      "Test set for fold4: Average Loss:           1.0257, Accuracy: 14760/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 31 [0/37476                 (0%)]\tLoss: 0.022017\n",
      "Training stage for Flod 4 Epoch: 31 [3200/37476                 (11%)]\tLoss: 0.001999\n",
      "Training stage for Flod 4 Epoch: 31 [6400/37476                 (21%)]\tLoss: 0.059384\n",
      "Training stage for Flod 4 Epoch: 31 [9600/37476                 (32%)]\tLoss: 0.072030\n",
      "Training stage for Flod 4 Epoch: 31 [12800/37476                 (43%)]\tLoss: 0.020052\n",
      "Training stage for Flod 4 Epoch: 31 [16000/37476                 (53%)]\tLoss: 0.036840\n",
      "Training stage for Flod 4 Epoch: 31 [19200/37476                 (64%)]\tLoss: 0.001679\n",
      "Training stage for Flod 4 Epoch: 31 [22400/37476                 (75%)]\tLoss: 0.000499\n",
      "Training stage for Flod 4 Epoch: 31 [25600/37476                 (85%)]\tLoss: 0.033002\n",
      "Training stage for Flod 4 Epoch: 31 [28800/37476                 (96%)]\tLoss: 0.021721\n",
      "Test set for fold4: Average Loss:           1.2397, Accuracy: 14776/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 32 [0/37476                 (0%)]\tLoss: 0.004690\n",
      "Training stage for Flod 4 Epoch: 32 [3200/37476                 (11%)]\tLoss: 0.029637\n",
      "Training stage for Flod 4 Epoch: 32 [6400/37476                 (21%)]\tLoss: 0.006821\n",
      "Training stage for Flod 4 Epoch: 32 [9600/37476                 (32%)]\tLoss: 0.060207\n",
      "Training stage for Flod 4 Epoch: 32 [12800/37476                 (43%)]\tLoss: 0.012608\n",
      "Training stage for Flod 4 Epoch: 32 [16000/37476                 (53%)]\tLoss: 0.028230\n",
      "Training stage for Flod 4 Epoch: 32 [19200/37476                 (64%)]\tLoss: 0.001382\n",
      "Training stage for Flod 4 Epoch: 32 [22400/37476                 (75%)]\tLoss: 0.060244\n",
      "Training stage for Flod 4 Epoch: 32 [25600/37476                 (85%)]\tLoss: 0.027370\n",
      "Training stage for Flod 4 Epoch: 32 [28800/37476                 (96%)]\tLoss: 0.045878\n",
      "Test set for fold4: Average Loss:           1.0843, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 33 [0/37476                 (0%)]\tLoss: 0.001822\n",
      "Training stage for Flod 4 Epoch: 33 [3200/37476                 (11%)]\tLoss: 0.017366\n",
      "Training stage for Flod 4 Epoch: 33 [6400/37476                 (21%)]\tLoss: 0.006523\n",
      "Training stage for Flod 4 Epoch: 33 [9600/37476                 (32%)]\tLoss: 0.053157\n",
      "Training stage for Flod 4 Epoch: 33 [12800/37476                 (43%)]\tLoss: 0.208389\n",
      "Training stage for Flod 4 Epoch: 33 [16000/37476                 (53%)]\tLoss: 0.172036\n",
      "Training stage for Flod 4 Epoch: 33 [19200/37476                 (64%)]\tLoss: 0.008948\n",
      "Training stage for Flod 4 Epoch: 33 [22400/37476                 (75%)]\tLoss: 0.009233\n",
      "Training stage for Flod 4 Epoch: 33 [25600/37476                 (85%)]\tLoss: 0.017603\n",
      "Training stage for Flod 4 Epoch: 33 [28800/37476                 (96%)]\tLoss: 0.019840\n",
      "Test set for fold4: Average Loss:           1.1441, Accuracy: 14667/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 34 [0/37476                 (0%)]\tLoss: 0.000158\n",
      "Training stage for Flod 4 Epoch: 34 [3200/37476                 (11%)]\tLoss: 0.013099\n",
      "Training stage for Flod 4 Epoch: 34 [6400/37476                 (21%)]\tLoss: 0.061574\n",
      "Training stage for Flod 4 Epoch: 34 [9600/37476                 (32%)]\tLoss: 0.093468\n",
      "Training stage for Flod 4 Epoch: 34 [12800/37476                 (43%)]\tLoss: 0.000262\n",
      "Training stage for Flod 4 Epoch: 34 [16000/37476                 (53%)]\tLoss: 0.059043\n",
      "Training stage for Flod 4 Epoch: 34 [19200/37476                 (64%)]\tLoss: 0.070583\n",
      "Training stage for Flod 4 Epoch: 34 [22400/37476                 (75%)]\tLoss: 0.003579\n",
      "Training stage for Flod 4 Epoch: 34 [25600/37476                 (85%)]\tLoss: 0.016145\n",
      "Training stage for Flod 4 Epoch: 34 [28800/37476                 (96%)]\tLoss: 0.002586\n",
      "Test set for fold4: Average Loss:           1.0668, Accuracy: 14728/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 35 [0/37476                 (0%)]\tLoss: 0.071478\n",
      "Training stage for Flod 4 Epoch: 35 [3200/37476                 (11%)]\tLoss: 0.063534\n",
      "Training stage for Flod 4 Epoch: 35 [6400/37476                 (21%)]\tLoss: 0.002697\n",
      "Training stage for Flod 4 Epoch: 35 [9600/37476                 (32%)]\tLoss: 0.079381\n",
      "Training stage for Flod 4 Epoch: 35 [12800/37476                 (43%)]\tLoss: 0.029192\n",
      "Training stage for Flod 4 Epoch: 35 [16000/37476                 (53%)]\tLoss: 0.001643\n",
      "Training stage for Flod 4 Epoch: 35 [19200/37476                 (64%)]\tLoss: 0.000205\n",
      "Training stage for Flod 4 Epoch: 35 [22400/37476                 (75%)]\tLoss: 0.006874\n",
      "Training stage for Flod 4 Epoch: 35 [25600/37476                 (85%)]\tLoss: 0.012188\n",
      "Training stage for Flod 4 Epoch: 35 [28800/37476                 (96%)]\tLoss: 0.134018\n",
      "Test set for fold4: Average Loss:           1.1602, Accuracy: 14744/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 36 [0/37476                 (0%)]\tLoss: 0.002126\n",
      "Training stage for Flod 4 Epoch: 36 [3200/37476                 (11%)]\tLoss: 0.004333\n",
      "Training stage for Flod 4 Epoch: 36 [6400/37476                 (21%)]\tLoss: 0.052407\n",
      "Training stage for Flod 4 Epoch: 36 [9600/37476                 (32%)]\tLoss: 0.007641\n",
      "Training stage for Flod 4 Epoch: 36 [12800/37476                 (43%)]\tLoss: 0.053667\n",
      "Training stage for Flod 4 Epoch: 36 [16000/37476                 (53%)]\tLoss: 0.023546\n",
      "Training stage for Flod 4 Epoch: 36 [19200/37476                 (64%)]\tLoss: 0.109093\n",
      "Training stage for Flod 4 Epoch: 36 [22400/37476                 (75%)]\tLoss: 0.024018\n",
      "Training stage for Flod 4 Epoch: 36 [25600/37476                 (85%)]\tLoss: 0.002084\n",
      "Training stage for Flod 4 Epoch: 36 [28800/37476                 (96%)]\tLoss: 0.013912\n",
      "Test set for fold4: Average Loss:           1.1838, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 37 [0/37476                 (0%)]\tLoss: 0.098148\n",
      "Training stage for Flod 4 Epoch: 37 [3200/37476                 (11%)]\tLoss: 0.000322\n",
      "Training stage for Flod 4 Epoch: 37 [6400/37476                 (21%)]\tLoss: 0.007235\n",
      "Training stage for Flod 4 Epoch: 37 [9600/37476                 (32%)]\tLoss: 0.031387\n",
      "Training stage for Flod 4 Epoch: 37 [12800/37476                 (43%)]\tLoss: 0.094474\n",
      "Training stage for Flod 4 Epoch: 37 [16000/37476                 (53%)]\tLoss: 0.039636\n",
      "Training stage for Flod 4 Epoch: 37 [19200/37476                 (64%)]\tLoss: 0.250900\n",
      "Training stage for Flod 4 Epoch: 37 [22400/37476                 (75%)]\tLoss: 0.032908\n",
      "Training stage for Flod 4 Epoch: 37 [25600/37476                 (85%)]\tLoss: 0.011331\n",
      "Training stage for Flod 4 Epoch: 37 [28800/37476                 (96%)]\tLoss: 0.109535\n",
      "Test set for fold4: Average Loss:           1.2023, Accuracy: 14780/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 38 [0/37476                 (0%)]\tLoss: 0.006906\n",
      "Training stage for Flod 4 Epoch: 38 [3200/37476                 (11%)]\tLoss: 0.029061\n",
      "Training stage for Flod 4 Epoch: 38 [6400/37476                 (21%)]\tLoss: 0.021313\n",
      "Training stage for Flod 4 Epoch: 38 [9600/37476                 (32%)]\tLoss: 0.006578\n",
      "Training stage for Flod 4 Epoch: 38 [12800/37476                 (43%)]\tLoss: 0.023044\n",
      "Training stage for Flod 4 Epoch: 38 [16000/37476                 (53%)]\tLoss: 0.002067\n",
      "Training stage for Flod 4 Epoch: 38 [19200/37476                 (64%)]\tLoss: 0.016714\n",
      "Training stage for Flod 4 Epoch: 38 [22400/37476                 (75%)]\tLoss: 0.003151\n",
      "Training stage for Flod 4 Epoch: 38 [25600/37476                 (85%)]\tLoss: 0.030414\n",
      "Training stage for Flod 4 Epoch: 38 [28800/37476                 (96%)]\tLoss: 0.018614\n",
      "Test set for fold4: Average Loss:           0.9426, Accuracy: 14767/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 39 [0/37476                 (0%)]\tLoss: 0.016360\n",
      "Training stage for Flod 4 Epoch: 39 [3200/37476                 (11%)]\tLoss: 0.001305\n",
      "Training stage for Flod 4 Epoch: 39 [6400/37476                 (21%)]\tLoss: 0.008682\n",
      "Training stage for Flod 4 Epoch: 39 [9600/37476                 (32%)]\tLoss: 0.013840\n",
      "Training stage for Flod 4 Epoch: 39 [12800/37476                 (43%)]\tLoss: 0.046346\n",
      "Training stage for Flod 4 Epoch: 39 [16000/37476                 (53%)]\tLoss: 0.033981\n",
      "Training stage for Flod 4 Epoch: 39 [19200/37476                 (64%)]\tLoss: 0.019680\n",
      "Training stage for Flod 4 Epoch: 39 [22400/37476                 (75%)]\tLoss: 0.025677\n",
      "Training stage for Flod 4 Epoch: 39 [25600/37476                 (85%)]\tLoss: 0.012008\n",
      "Training stage for Flod 4 Epoch: 39 [28800/37476                 (96%)]\tLoss: 0.010197\n",
      "Test set for fold4: Average Loss:           1.1545, Accuracy: 14753/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 40 [0/37476                 (0%)]\tLoss: 0.002191\n",
      "Training stage for Flod 4 Epoch: 40 [3200/37476                 (11%)]\tLoss: 0.041153\n",
      "Training stage for Flod 4 Epoch: 40 [6400/37476                 (21%)]\tLoss: 0.094102\n",
      "Training stage for Flod 4 Epoch: 40 [9600/37476                 (32%)]\tLoss: 0.000849\n",
      "Training stage for Flod 4 Epoch: 40 [12800/37476                 (43%)]\tLoss: 0.011370\n",
      "Training stage for Flod 4 Epoch: 40 [16000/37476                 (53%)]\tLoss: 0.002629\n",
      "Training stage for Flod 4 Epoch: 40 [19200/37476                 (64%)]\tLoss: 0.009630\n",
      "Training stage for Flod 4 Epoch: 40 [22400/37476                 (75%)]\tLoss: 0.038580\n",
      "Training stage for Flod 4 Epoch: 40 [25600/37476                 (85%)]\tLoss: 0.000867\n",
      "Training stage for Flod 4 Epoch: 40 [28800/37476                 (96%)]\tLoss: 0.054251\n",
      "Test set for fold4: Average Loss:           1.0154, Accuracy: 14746/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 41 [0/37476                 (0%)]\tLoss: 0.012099\n",
      "Training stage for Flod 4 Epoch: 41 [3200/37476                 (11%)]\tLoss: 0.024136\n",
      "Training stage for Flod 4 Epoch: 41 [6400/37476                 (21%)]\tLoss: 0.056451\n",
      "Training stage for Flod 4 Epoch: 41 [9600/37476                 (32%)]\tLoss: 0.053740\n",
      "Training stage for Flod 4 Epoch: 41 [12800/37476                 (43%)]\tLoss: 0.080881\n",
      "Training stage for Flod 4 Epoch: 41 [16000/37476                 (53%)]\tLoss: 0.090744\n",
      "Training stage for Flod 4 Epoch: 41 [19200/37476                 (64%)]\tLoss: 0.044984\n",
      "Training stage for Flod 4 Epoch: 41 [22400/37476                 (75%)]\tLoss: 0.099505\n",
      "Training stage for Flod 4 Epoch: 41 [25600/37476                 (85%)]\tLoss: 0.017943\n",
      "Training stage for Flod 4 Epoch: 41 [28800/37476                 (96%)]\tLoss: 0.047068\n",
      "Test set for fold4: Average Loss:           1.1681, Accuracy: 14765/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 42 [0/37476                 (0%)]\tLoss: 0.009011\n",
      "Training stage for Flod 4 Epoch: 42 [3200/37476                 (11%)]\tLoss: 0.054916\n",
      "Training stage for Flod 4 Epoch: 42 [6400/37476                 (21%)]\tLoss: 0.009688\n",
      "Training stage for Flod 4 Epoch: 42 [9600/37476                 (32%)]\tLoss: 0.044442\n",
      "Training stage for Flod 4 Epoch: 42 [12800/37476                 (43%)]\tLoss: 0.075659\n",
      "Training stage for Flod 4 Epoch: 42 [16000/37476                 (53%)]\tLoss: 0.005735\n",
      "Training stage for Flod 4 Epoch: 42 [19200/37476                 (64%)]\tLoss: 0.030800\n",
      "Training stage for Flod 4 Epoch: 42 [22400/37476                 (75%)]\tLoss: 0.001266\n",
      "Training stage for Flod 4 Epoch: 42 [25600/37476                 (85%)]\tLoss: 0.007340\n",
      "Training stage for Flod 4 Epoch: 42 [28800/37476                 (96%)]\tLoss: 0.153312\n",
      "Test set for fold4: Average Loss:           1.0405, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 43 [0/37476                 (0%)]\tLoss: 0.044831\n",
      "Training stage for Flod 4 Epoch: 43 [3200/37476                 (11%)]\tLoss: 0.012502\n",
      "Training stage for Flod 4 Epoch: 43 [6400/37476                 (21%)]\tLoss: 0.006751\n",
      "Training stage for Flod 4 Epoch: 43 [9600/37476                 (32%)]\tLoss: 0.011689\n",
      "Training stage for Flod 4 Epoch: 43 [12800/37476                 (43%)]\tLoss: 0.004236\n",
      "Training stage for Flod 4 Epoch: 43 [16000/37476                 (53%)]\tLoss: 0.006118\n",
      "Training stage for Flod 4 Epoch: 43 [19200/37476                 (64%)]\tLoss: 0.096056\n",
      "Training stage for Flod 4 Epoch: 43 [22400/37476                 (75%)]\tLoss: 0.005469\n",
      "Training stage for Flod 4 Epoch: 43 [25600/37476                 (85%)]\tLoss: 0.001291\n",
      "Training stage for Flod 4 Epoch: 43 [28800/37476                 (96%)]\tLoss: 0.009307\n",
      "Test set for fold4: Average Loss:           1.0842, Accuracy: 14759/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 44 [0/37476                 (0%)]\tLoss: 0.107224\n",
      "Training stage for Flod 4 Epoch: 44 [3200/37476                 (11%)]\tLoss: 0.016591\n",
      "Training stage for Flod 4 Epoch: 44 [6400/37476                 (21%)]\tLoss: 0.035179\n",
      "Training stage for Flod 4 Epoch: 44 [9600/37476                 (32%)]\tLoss: 0.016764\n",
      "Training stage for Flod 4 Epoch: 44 [12800/37476                 (43%)]\tLoss: 0.027284\n",
      "Training stage for Flod 4 Epoch: 44 [16000/37476                 (53%)]\tLoss: 0.007043\n",
      "Training stage for Flod 4 Epoch: 44 [19200/37476                 (64%)]\tLoss: 0.003357\n",
      "Training stage for Flod 4 Epoch: 44 [22400/37476                 (75%)]\tLoss: 0.015856\n",
      "Training stage for Flod 4 Epoch: 44 [25600/37476                 (85%)]\tLoss: 0.002540\n",
      "Training stage for Flod 4 Epoch: 44 [28800/37476                 (96%)]\tLoss: 0.039093\n",
      "Test set for fold4: Average Loss:           1.0965, Accuracy: 14744/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 45 [0/37476                 (0%)]\tLoss: 0.001286\n",
      "Training stage for Flod 4 Epoch: 45 [3200/37476                 (11%)]\tLoss: 0.009908\n",
      "Training stage for Flod 4 Epoch: 45 [6400/37476                 (21%)]\tLoss: 0.033906\n",
      "Training stage for Flod 4 Epoch: 45 [9600/37476                 (32%)]\tLoss: 0.229645\n",
      "Training stage for Flod 4 Epoch: 45 [12800/37476                 (43%)]\tLoss: 0.038326\n",
      "Training stage for Flod 4 Epoch: 45 [16000/37476                 (53%)]\tLoss: 0.003806\n",
      "Training stage for Flod 4 Epoch: 45 [19200/37476                 (64%)]\tLoss: 0.001689\n",
      "Training stage for Flod 4 Epoch: 45 [22400/37476                 (75%)]\tLoss: 0.018333\n",
      "Training stage for Flod 4 Epoch: 45 [25600/37476                 (85%)]\tLoss: 0.017861\n",
      "Training stage for Flod 4 Epoch: 45 [28800/37476                 (96%)]\tLoss: 0.008347\n",
      "Test set for fold4: Average Loss:           1.0159, Accuracy: 14720/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 46 [0/37476                 (0%)]\tLoss: 0.032031\n",
      "Training stage for Flod 4 Epoch: 46 [3200/37476                 (11%)]\tLoss: 0.075760\n",
      "Training stage for Flod 4 Epoch: 46 [6400/37476                 (21%)]\tLoss: 0.056037\n",
      "Training stage for Flod 4 Epoch: 46 [9600/37476                 (32%)]\tLoss: 0.000570\n",
      "Training stage for Flod 4 Epoch: 46 [12800/37476                 (43%)]\tLoss: 0.031048\n",
      "Training stage for Flod 4 Epoch: 46 [16000/37476                 (53%)]\tLoss: 0.016662\n",
      "Training stage for Flod 4 Epoch: 46 [19200/37476                 (64%)]\tLoss: 0.248542\n",
      "Training stage for Flod 4 Epoch: 46 [22400/37476                 (75%)]\tLoss: 0.027558\n",
      "Training stage for Flod 4 Epoch: 46 [25600/37476                 (85%)]\tLoss: 0.001074\n",
      "Training stage for Flod 4 Epoch: 46 [28800/37476                 (96%)]\tLoss: 0.023571\n",
      "Test set for fold4: Average Loss:           1.0552, Accuracy: 14770/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 47 [0/37476                 (0%)]\tLoss: 0.032897\n",
      "Training stage for Flod 4 Epoch: 47 [3200/37476                 (11%)]\tLoss: 0.003102\n",
      "Training stage for Flod 4 Epoch: 47 [6400/37476                 (21%)]\tLoss: 0.116831\n",
      "Training stage for Flod 4 Epoch: 47 [9600/37476                 (32%)]\tLoss: 0.076084\n",
      "Training stage for Flod 4 Epoch: 47 [12800/37476                 (43%)]\tLoss: 0.031076\n",
      "Training stage for Flod 4 Epoch: 47 [16000/37476                 (53%)]\tLoss: 0.027324\n",
      "Training stage for Flod 4 Epoch: 47 [19200/37476                 (64%)]\tLoss: 0.012962\n",
      "Training stage for Flod 4 Epoch: 47 [22400/37476                 (75%)]\tLoss: 0.075806\n",
      "Training stage for Flod 4 Epoch: 47 [25600/37476                 (85%)]\tLoss: 0.000631\n",
      "Training stage for Flod 4 Epoch: 47 [28800/37476                 (96%)]\tLoss: 0.000503\n",
      "Test set for fold4: Average Loss:           1.5199, Accuracy: 14741/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 48 [0/37476                 (0%)]\tLoss: 0.000018\n",
      "Training stage for Flod 4 Epoch: 48 [3200/37476                 (11%)]\tLoss: 0.003973\n",
      "Training stage for Flod 4 Epoch: 48 [6400/37476                 (21%)]\tLoss: 0.019367\n",
      "Training stage for Flod 4 Epoch: 48 [9600/37476                 (32%)]\tLoss: 0.002425\n",
      "Training stage for Flod 4 Epoch: 48 [12800/37476                 (43%)]\tLoss: 0.004349\n",
      "Training stage for Flod 4 Epoch: 48 [16000/37476                 (53%)]\tLoss: 0.007283\n",
      "Training stage for Flod 4 Epoch: 48 [19200/37476                 (64%)]\tLoss: 0.007210\n",
      "Training stage for Flod 4 Epoch: 48 [22400/37476                 (75%)]\tLoss: 0.000661\n",
      "Training stage for Flod 4 Epoch: 48 [25600/37476                 (85%)]\tLoss: 0.019745\n",
      "Training stage for Flod 4 Epoch: 48 [28800/37476                 (96%)]\tLoss: 0.001255\n",
      "Test set for fold4: Average Loss:           1.1042, Accuracy: 14777/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 49 [0/37476                 (0%)]\tLoss: 0.032218\n",
      "Training stage for Flod 4 Epoch: 49 [3200/37476                 (11%)]\tLoss: 0.013808\n",
      "Training stage for Flod 4 Epoch: 49 [6400/37476                 (21%)]\tLoss: 0.011637\n",
      "Training stage for Flod 4 Epoch: 49 [9600/37476                 (32%)]\tLoss: 0.011905\n",
      "Training stage for Flod 4 Epoch: 49 [12800/37476                 (43%)]\tLoss: 0.006016\n",
      "Training stage for Flod 4 Epoch: 49 [16000/37476                 (53%)]\tLoss: 0.090653\n",
      "Training stage for Flod 4 Epoch: 49 [19200/37476                 (64%)]\tLoss: 0.012754\n",
      "Training stage for Flod 4 Epoch: 49 [22400/37476                 (75%)]\tLoss: 0.062571\n",
      "Training stage for Flod 4 Epoch: 49 [25600/37476                 (85%)]\tLoss: 0.001701\n",
      "Training stage for Flod 4 Epoch: 49 [28800/37476                 (96%)]\tLoss: 0.027814\n",
      "Test set for fold4: Average Loss:           1.0270, Accuracy: 14780/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 50 [0/37476                 (0%)]\tLoss: 0.009501\n",
      "Training stage for Flod 4 Epoch: 50 [3200/37476                 (11%)]\tLoss: 0.002644\n",
      "Training stage for Flod 4 Epoch: 50 [6400/37476                 (21%)]\tLoss: 0.045659\n",
      "Training stage for Flod 4 Epoch: 50 [9600/37476                 (32%)]\tLoss: 0.000973\n",
      "Training stage for Flod 4 Epoch: 50 [12800/37476                 (43%)]\tLoss: 0.009240\n",
      "Training stage for Flod 4 Epoch: 50 [16000/37476                 (53%)]\tLoss: 0.005640\n",
      "Training stage for Flod 4 Epoch: 50 [19200/37476                 (64%)]\tLoss: 0.003133\n",
      "Training stage for Flod 4 Epoch: 50 [22400/37476                 (75%)]\tLoss: 0.010561\n",
      "Training stage for Flod 4 Epoch: 50 [25600/37476                 (85%)]\tLoss: 0.001101\n",
      "Training stage for Flod 4 Epoch: 50 [28800/37476                 (96%)]\tLoss: 0.023488\n",
      "Test set for fold4: Average Loss:           1.1513, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 51 [0/37476                 (0%)]\tLoss: 0.008308\n",
      "Training stage for Flod 4 Epoch: 51 [3200/37476                 (11%)]\tLoss: 0.144414\n",
      "Training stage for Flod 4 Epoch: 51 [6400/37476                 (21%)]\tLoss: 0.005216\n",
      "Training stage for Flod 4 Epoch: 51 [9600/37476                 (32%)]\tLoss: 0.005158\n",
      "Training stage for Flod 4 Epoch: 51 [12800/37476                 (43%)]\tLoss: 0.008740\n",
      "Training stage for Flod 4 Epoch: 51 [16000/37476                 (53%)]\tLoss: 0.139101\n",
      "Training stage for Flod 4 Epoch: 51 [19200/37476                 (64%)]\tLoss: 0.185536\n",
      "Training stage for Flod 4 Epoch: 51 [22400/37476                 (75%)]\tLoss: 0.008260\n",
      "Training stage for Flod 4 Epoch: 51 [25600/37476                 (85%)]\tLoss: 0.006013\n",
      "Training stage for Flod 4 Epoch: 51 [28800/37476                 (96%)]\tLoss: 0.001738\n",
      "Test set for fold4: Average Loss:           1.0597, Accuracy: 14719/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 52 [0/37476                 (0%)]\tLoss: 0.022267\n",
      "Training stage for Flod 4 Epoch: 52 [3200/37476                 (11%)]\tLoss: 0.011139\n",
      "Training stage for Flod 4 Epoch: 52 [6400/37476                 (21%)]\tLoss: 0.000897\n",
      "Training stage for Flod 4 Epoch: 52 [9600/37476                 (32%)]\tLoss: 0.020452\n",
      "Training stage for Flod 4 Epoch: 52 [12800/37476                 (43%)]\tLoss: 0.003284\n",
      "Training stage for Flod 4 Epoch: 52 [16000/37476                 (53%)]\tLoss: 0.032705\n",
      "Training stage for Flod 4 Epoch: 52 [19200/37476                 (64%)]\tLoss: 0.001967\n",
      "Training stage for Flod 4 Epoch: 52 [22400/37476                 (75%)]\tLoss: 0.014484\n",
      "Training stage for Flod 4 Epoch: 52 [25600/37476                 (85%)]\tLoss: 0.045319\n",
      "Training stage for Flod 4 Epoch: 52 [28800/37476                 (96%)]\tLoss: 0.015083\n",
      "Test set for fold4: Average Loss:           1.2580, Accuracy: 14775/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 53 [0/37476                 (0%)]\tLoss: 0.001739\n",
      "Training stage for Flod 4 Epoch: 53 [3200/37476                 (11%)]\tLoss: 0.000196\n",
      "Training stage for Flod 4 Epoch: 53 [6400/37476                 (21%)]\tLoss: 0.010454\n",
      "Training stage for Flod 4 Epoch: 53 [9600/37476                 (32%)]\tLoss: 0.032925\n",
      "Training stage for Flod 4 Epoch: 53 [12800/37476                 (43%)]\tLoss: 0.051283\n",
      "Training stage for Flod 4 Epoch: 53 [16000/37476                 (53%)]\tLoss: 0.129089\n",
      "Training stage for Flod 4 Epoch: 53 [19200/37476                 (64%)]\tLoss: 0.006987\n",
      "Training stage for Flod 4 Epoch: 53 [22400/37476                 (75%)]\tLoss: 0.056354\n",
      "Training stage for Flod 4 Epoch: 53 [25600/37476                 (85%)]\tLoss: 0.037692\n",
      "Training stage for Flod 4 Epoch: 53 [28800/37476                 (96%)]\tLoss: 0.075126\n",
      "Test set for fold4: Average Loss:           1.3313, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 54 [0/37476                 (0%)]\tLoss: 0.012137\n",
      "Training stage for Flod 4 Epoch: 54 [3200/37476                 (11%)]\tLoss: 0.001515\n",
      "Training stage for Flod 4 Epoch: 54 [6400/37476                 (21%)]\tLoss: 0.057394\n",
      "Training stage for Flod 4 Epoch: 54 [9600/37476                 (32%)]\tLoss: 0.092573\n",
      "Training stage for Flod 4 Epoch: 54 [12800/37476                 (43%)]\tLoss: 0.130802\n",
      "Training stage for Flod 4 Epoch: 54 [16000/37476                 (53%)]\tLoss: 0.204618\n",
      "Training stage for Flod 4 Epoch: 54 [19200/37476                 (64%)]\tLoss: 0.001705\n",
      "Training stage for Flod 4 Epoch: 54 [22400/37476                 (75%)]\tLoss: 0.005735\n",
      "Training stage for Flod 4 Epoch: 54 [25600/37476                 (85%)]\tLoss: 0.017773\n",
      "Training stage for Flod 4 Epoch: 54 [28800/37476                 (96%)]\tLoss: 0.019931\n",
      "Test set for fold4: Average Loss:           1.0459, Accuracy: 14752/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 55 [0/37476                 (0%)]\tLoss: 0.027312\n",
      "Training stage for Flod 4 Epoch: 55 [3200/37476                 (11%)]\tLoss: 0.011281\n",
      "Training stage for Flod 4 Epoch: 55 [6400/37476                 (21%)]\tLoss: 0.002490\n",
      "Training stage for Flod 4 Epoch: 55 [9600/37476                 (32%)]\tLoss: 0.024447\n",
      "Training stage for Flod 4 Epoch: 55 [12800/37476                 (43%)]\tLoss: 0.031961\n",
      "Training stage for Flod 4 Epoch: 55 [16000/37476                 (53%)]\tLoss: 0.003078\n",
      "Training stage for Flod 4 Epoch: 55 [19200/37476                 (64%)]\tLoss: 0.027531\n",
      "Training stage for Flod 4 Epoch: 55 [22400/37476                 (75%)]\tLoss: 0.038950\n",
      "Training stage for Flod 4 Epoch: 55 [25600/37476                 (85%)]\tLoss: 0.007301\n",
      "Training stage for Flod 4 Epoch: 55 [28800/37476                 (96%)]\tLoss: 0.002423\n",
      "Test set for fold4: Average Loss:           1.0250, Accuracy: 14786/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 56 [0/37476                 (0%)]\tLoss: 0.006497\n",
      "Training stage for Flod 4 Epoch: 56 [3200/37476                 (11%)]\tLoss: 0.030011\n",
      "Training stage for Flod 4 Epoch: 56 [6400/37476                 (21%)]\tLoss: 0.004455\n",
      "Training stage for Flod 4 Epoch: 56 [9600/37476                 (32%)]\tLoss: 0.029873\n",
      "Training stage for Flod 4 Epoch: 56 [12800/37476                 (43%)]\tLoss: 0.002212\n",
      "Training stage for Flod 4 Epoch: 56 [16000/37476                 (53%)]\tLoss: 0.027144\n",
      "Training stage for Flod 4 Epoch: 56 [19200/37476                 (64%)]\tLoss: 0.005097\n",
      "Training stage for Flod 4 Epoch: 56 [22400/37476                 (75%)]\tLoss: 0.001232\n",
      "Training stage for Flod 4 Epoch: 56 [25600/37476                 (85%)]\tLoss: 0.002194\n",
      "Training stage for Flod 4 Epoch: 56 [28800/37476                 (96%)]\tLoss: 0.000741\n",
      "Test set for fold4: Average Loss:           1.2060, Accuracy: 14765/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 57 [0/37476                 (0%)]\tLoss: 0.093468\n",
      "Training stage for Flod 4 Epoch: 57 [3200/37476                 (11%)]\tLoss: 0.004417\n",
      "Training stage for Flod 4 Epoch: 57 [6400/37476                 (21%)]\tLoss: 0.035730\n",
      "Training stage for Flod 4 Epoch: 57 [9600/37476                 (32%)]\tLoss: 0.138094\n",
      "Training stage for Flod 4 Epoch: 57 [12800/37476                 (43%)]\tLoss: 0.001161\n",
      "Training stage for Flod 4 Epoch: 57 [16000/37476                 (53%)]\tLoss: 0.063125\n",
      "Training stage for Flod 4 Epoch: 57 [19200/37476                 (64%)]\tLoss: 0.060634\n",
      "Training stage for Flod 4 Epoch: 57 [22400/37476                 (75%)]\tLoss: 0.007472\n",
      "Training stage for Flod 4 Epoch: 57 [25600/37476                 (85%)]\tLoss: 0.034409\n",
      "Training stage for Flod 4 Epoch: 57 [28800/37476                 (96%)]\tLoss: 0.052352\n",
      "Test set for fold4: Average Loss:           1.1013, Accuracy: 14719/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 58 [0/37476                 (0%)]\tLoss: 0.012591\n",
      "Training stage for Flod 4 Epoch: 58 [3200/37476                 (11%)]\tLoss: 0.023387\n",
      "Training stage for Flod 4 Epoch: 58 [6400/37476                 (21%)]\tLoss: 0.002397\n",
      "Training stage for Flod 4 Epoch: 58 [9600/37476                 (32%)]\tLoss: 0.009241\n",
      "Training stage for Flod 4 Epoch: 58 [12800/37476                 (43%)]\tLoss: 0.008525\n",
      "Training stage for Flod 4 Epoch: 58 [16000/37476                 (53%)]\tLoss: 0.084607\n",
      "Training stage for Flod 4 Epoch: 58 [19200/37476                 (64%)]\tLoss: 0.017431\n",
      "Training stage for Flod 4 Epoch: 58 [22400/37476                 (75%)]\tLoss: 0.036580\n",
      "Training stage for Flod 4 Epoch: 58 [25600/37476                 (85%)]\tLoss: 0.106721\n",
      "Training stage for Flod 4 Epoch: 58 [28800/37476                 (96%)]\tLoss: 0.012769\n",
      "Test set for fold4: Average Loss:           1.3805, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 59 [0/37476                 (0%)]\tLoss: 0.000721\n",
      "Training stage for Flod 4 Epoch: 59 [3200/37476                 (11%)]\tLoss: 0.046118\n",
      "Training stage for Flod 4 Epoch: 59 [6400/37476                 (21%)]\tLoss: 0.000494\n",
      "Training stage for Flod 4 Epoch: 59 [9600/37476                 (32%)]\tLoss: 0.040219\n",
      "Training stage for Flod 4 Epoch: 59 [12800/37476                 (43%)]\tLoss: 0.002641\n",
      "Training stage for Flod 4 Epoch: 59 [16000/37476                 (53%)]\tLoss: 0.024345\n",
      "Training stage for Flod 4 Epoch: 59 [19200/37476                 (64%)]\tLoss: 0.100724\n",
      "Training stage for Flod 4 Epoch: 59 [22400/37476                 (75%)]\tLoss: 0.000247\n",
      "Training stage for Flod 4 Epoch: 59 [25600/37476                 (85%)]\tLoss: 0.022525\n",
      "Training stage for Flod 4 Epoch: 59 [28800/37476                 (96%)]\tLoss: 0.037093\n",
      "Test set for fold4: Average Loss:           1.3050, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 60 [0/37476                 (0%)]\tLoss: 0.000665\n",
      "Training stage for Flod 4 Epoch: 60 [3200/37476                 (11%)]\tLoss: 0.001942\n",
      "Training stage for Flod 4 Epoch: 60 [6400/37476                 (21%)]\tLoss: 0.010037\n",
      "Training stage for Flod 4 Epoch: 60 [9600/37476                 (32%)]\tLoss: 0.008095\n",
      "Training stage for Flod 4 Epoch: 60 [12800/37476                 (43%)]\tLoss: 0.098390\n",
      "Training stage for Flod 4 Epoch: 60 [16000/37476                 (53%)]\tLoss: 0.118972\n",
      "Training stage for Flod 4 Epoch: 60 [19200/37476                 (64%)]\tLoss: 0.023918\n",
      "Training stage for Flod 4 Epoch: 60 [22400/37476                 (75%)]\tLoss: 0.003772\n",
      "Training stage for Flod 4 Epoch: 60 [25600/37476                 (85%)]\tLoss: 0.052845\n",
      "Training stage for Flod 4 Epoch: 60 [28800/37476                 (96%)]\tLoss: 0.001588\n",
      "Test set for fold4: Average Loss:           1.1635, Accuracy: 14739/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 61 [0/37476                 (0%)]\tLoss: 0.000243\n",
      "Training stage for Flod 4 Epoch: 61 [3200/37476                 (11%)]\tLoss: 0.004648\n",
      "Training stage for Flod 4 Epoch: 61 [6400/37476                 (21%)]\tLoss: 0.006146\n",
      "Training stage for Flod 4 Epoch: 61 [9600/37476                 (32%)]\tLoss: 0.046961\n",
      "Training stage for Flod 4 Epoch: 61 [12800/37476                 (43%)]\tLoss: 0.001674\n",
      "Training stage for Flod 4 Epoch: 61 [16000/37476                 (53%)]\tLoss: 0.114280\n",
      "Training stage for Flod 4 Epoch: 61 [19200/37476                 (64%)]\tLoss: 0.071299\n",
      "Training stage for Flod 4 Epoch: 61 [22400/37476                 (75%)]\tLoss: 0.041861\n",
      "Training stage for Flod 4 Epoch: 61 [25600/37476                 (85%)]\tLoss: 0.019662\n",
      "Training stage for Flod 4 Epoch: 61 [28800/37476                 (96%)]\tLoss: 0.000083\n",
      "Test set for fold4: Average Loss:           1.1037, Accuracy: 14804/37476           (40%)\n",
      "Training stage for Flod 4 Epoch: 62 [0/37476                 (0%)]\tLoss: 0.002168\n",
      "Training stage for Flod 4 Epoch: 62 [3200/37476                 (11%)]\tLoss: 0.060639\n",
      "Training stage for Flod 4 Epoch: 62 [6400/37476                 (21%)]\tLoss: 0.046483\n",
      "Training stage for Flod 4 Epoch: 62 [9600/37476                 (32%)]\tLoss: 0.010106\n",
      "Training stage for Flod 4 Epoch: 62 [12800/37476                 (43%)]\tLoss: 0.009910\n",
      "Training stage for Flod 4 Epoch: 62 [16000/37476                 (53%)]\tLoss: 0.018458\n",
      "Training stage for Flod 4 Epoch: 62 [19200/37476                 (64%)]\tLoss: 0.000393\n",
      "Training stage for Flod 4 Epoch: 62 [22400/37476                 (75%)]\tLoss: 0.008978\n",
      "Training stage for Flod 4 Epoch: 62 [25600/37476                 (85%)]\tLoss: 0.009921\n",
      "Training stage for Flod 4 Epoch: 62 [28800/37476                 (96%)]\tLoss: 0.000113\n",
      "Test set for fold4: Average Loss:           1.2874, Accuracy: 14618/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 63 [0/37476                 (0%)]\tLoss: 0.058894\n",
      "Training stage for Flod 4 Epoch: 63 [3200/37476                 (11%)]\tLoss: 0.000429\n",
      "Training stage for Flod 4 Epoch: 63 [6400/37476                 (21%)]\tLoss: 0.000480\n",
      "Training stage for Flod 4 Epoch: 63 [9600/37476                 (32%)]\tLoss: 0.003122\n",
      "Training stage for Flod 4 Epoch: 63 [12800/37476                 (43%)]\tLoss: 0.000284\n",
      "Training stage for Flod 4 Epoch: 63 [16000/37476                 (53%)]\tLoss: 0.046593\n",
      "Training stage for Flod 4 Epoch: 63 [19200/37476                 (64%)]\tLoss: 0.013918\n",
      "Training stage for Flod 4 Epoch: 63 [22400/37476                 (75%)]\tLoss: 0.007379\n",
      "Training stage for Flod 4 Epoch: 63 [25600/37476                 (85%)]\tLoss: 0.116444\n",
      "Training stage for Flod 4 Epoch: 63 [28800/37476                 (96%)]\tLoss: 0.035412\n",
      "Test set for fold4: Average Loss:           1.2666, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 64 [0/37476                 (0%)]\tLoss: 0.012138\n",
      "Training stage for Flod 4 Epoch: 64 [3200/37476                 (11%)]\tLoss: 0.120883\n",
      "Training stage for Flod 4 Epoch: 64 [6400/37476                 (21%)]\tLoss: 0.006970\n",
      "Training stage for Flod 4 Epoch: 64 [9600/37476                 (32%)]\tLoss: 0.024602\n",
      "Training stage for Flod 4 Epoch: 64 [12800/37476                 (43%)]\tLoss: 0.047357\n",
      "Training stage for Flod 4 Epoch: 64 [16000/37476                 (53%)]\tLoss: 0.060791\n",
      "Training stage for Flod 4 Epoch: 64 [19200/37476                 (64%)]\tLoss: 0.184128\n",
      "Training stage for Flod 4 Epoch: 64 [22400/37476                 (75%)]\tLoss: 0.178599\n",
      "Training stage for Flod 4 Epoch: 64 [25600/37476                 (85%)]\tLoss: 0.003988\n",
      "Training stage for Flod 4 Epoch: 64 [28800/37476                 (96%)]\tLoss: 0.000140\n",
      "Test set for fold4: Average Loss:           1.4028, Accuracy: 14794/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 65 [0/37476                 (0%)]\tLoss: 0.056396\n",
      "Training stage for Flod 4 Epoch: 65 [3200/37476                 (11%)]\tLoss: 0.012848\n",
      "Training stage for Flod 4 Epoch: 65 [6400/37476                 (21%)]\tLoss: 0.014376\n",
      "Training stage for Flod 4 Epoch: 65 [9600/37476                 (32%)]\tLoss: 0.014550\n",
      "Training stage for Flod 4 Epoch: 65 [12800/37476                 (43%)]\tLoss: 0.039017\n",
      "Training stage for Flod 4 Epoch: 65 [16000/37476                 (53%)]\tLoss: 0.009466\n",
      "Training stage for Flod 4 Epoch: 65 [19200/37476                 (64%)]\tLoss: 0.000390\n",
      "Training stage for Flod 4 Epoch: 65 [22400/37476                 (75%)]\tLoss: 0.014114\n",
      "Training stage for Flod 4 Epoch: 65 [25600/37476                 (85%)]\tLoss: 0.064874\n",
      "Training stage for Flod 4 Epoch: 65 [28800/37476                 (96%)]\tLoss: 0.011244\n",
      "Test set for fold4: Average Loss:           1.4343, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 66 [0/37476                 (0%)]\tLoss: 0.003191\n",
      "Training stage for Flod 4 Epoch: 66 [3200/37476                 (11%)]\tLoss: 0.154465\n",
      "Training stage for Flod 4 Epoch: 66 [6400/37476                 (21%)]\tLoss: 0.048776\n",
      "Training stage for Flod 4 Epoch: 66 [9600/37476                 (32%)]\tLoss: 0.012396\n",
      "Training stage for Flod 4 Epoch: 66 [12800/37476                 (43%)]\tLoss: 0.110035\n",
      "Training stage for Flod 4 Epoch: 66 [16000/37476                 (53%)]\tLoss: 0.199326\n",
      "Training stage for Flod 4 Epoch: 66 [19200/37476                 (64%)]\tLoss: 0.002161\n",
      "Training stage for Flod 4 Epoch: 66 [22400/37476                 (75%)]\tLoss: 0.025468\n",
      "Training stage for Flod 4 Epoch: 66 [25600/37476                 (85%)]\tLoss: 0.042916\n",
      "Training stage for Flod 4 Epoch: 66 [28800/37476                 (96%)]\tLoss: 0.001427\n",
      "Test set for fold4: Average Loss:           1.1422, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 67 [0/37476                 (0%)]\tLoss: 0.125925\n",
      "Training stage for Flod 4 Epoch: 67 [3200/37476                 (11%)]\tLoss: 0.008534\n",
      "Training stage for Flod 4 Epoch: 67 [6400/37476                 (21%)]\tLoss: 0.019136\n",
      "Training stage for Flod 4 Epoch: 67 [9600/37476                 (32%)]\tLoss: 0.000709\n",
      "Training stage for Flod 4 Epoch: 67 [12800/37476                 (43%)]\tLoss: 0.042071\n",
      "Training stage for Flod 4 Epoch: 67 [16000/37476                 (53%)]\tLoss: 0.012993\n",
      "Training stage for Flod 4 Epoch: 67 [19200/37476                 (64%)]\tLoss: 0.006731\n",
      "Training stage for Flod 4 Epoch: 67 [22400/37476                 (75%)]\tLoss: 0.057655\n",
      "Training stage for Flod 4 Epoch: 67 [25600/37476                 (85%)]\tLoss: 0.008804\n",
      "Training stage for Flod 4 Epoch: 67 [28800/37476                 (96%)]\tLoss: 0.016779\n",
      "Test set for fold4: Average Loss:           1.1379, Accuracy: 14800/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 68 [0/37476                 (0%)]\tLoss: 0.002667\n",
      "Training stage for Flod 4 Epoch: 68 [3200/37476                 (11%)]\tLoss: 0.130056\n",
      "Training stage for Flod 4 Epoch: 68 [6400/37476                 (21%)]\tLoss: 0.056674\n",
      "Training stage for Flod 4 Epoch: 68 [9600/37476                 (32%)]\tLoss: 0.013603\n",
      "Training stage for Flod 4 Epoch: 68 [12800/37476                 (43%)]\tLoss: 0.065956\n",
      "Training stage for Flod 4 Epoch: 68 [16000/37476                 (53%)]\tLoss: 0.008501\n",
      "Training stage for Flod 4 Epoch: 68 [19200/37476                 (64%)]\tLoss: 0.083378\n",
      "Training stage for Flod 4 Epoch: 68 [22400/37476                 (75%)]\tLoss: 0.008906\n",
      "Training stage for Flod 4 Epoch: 68 [25600/37476                 (85%)]\tLoss: 0.004853\n",
      "Training stage for Flod 4 Epoch: 68 [28800/37476                 (96%)]\tLoss: 0.000889\n",
      "Test set for fold4: Average Loss:           1.1185, Accuracy: 14774/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 69 [0/37476                 (0%)]\tLoss: 0.002334\n",
      "Training stage for Flod 4 Epoch: 69 [3200/37476                 (11%)]\tLoss: 0.052676\n",
      "Training stage for Flod 4 Epoch: 69 [6400/37476                 (21%)]\tLoss: 0.004260\n",
      "Training stage for Flod 4 Epoch: 69 [9600/37476                 (32%)]\tLoss: 0.077519\n",
      "Training stage for Flod 4 Epoch: 69 [12800/37476                 (43%)]\tLoss: 0.049065\n",
      "Training stage for Flod 4 Epoch: 69 [16000/37476                 (53%)]\tLoss: 0.000647\n",
      "Training stage for Flod 4 Epoch: 69 [19200/37476                 (64%)]\tLoss: 0.009543\n",
      "Training stage for Flod 4 Epoch: 69 [22400/37476                 (75%)]\tLoss: 0.003937\n",
      "Training stage for Flod 4 Epoch: 69 [25600/37476                 (85%)]\tLoss: 0.039749\n",
      "Training stage for Flod 4 Epoch: 69 [28800/37476                 (96%)]\tLoss: 0.007275\n",
      "Test set for fold4: Average Loss:           1.2205, Accuracy: 14786/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 70 [0/37476                 (0%)]\tLoss: 0.005773\n",
      "Training stage for Flod 4 Epoch: 70 [3200/37476                 (11%)]\tLoss: 0.007308\n",
      "Training stage for Flod 4 Epoch: 70 [6400/37476                 (21%)]\tLoss: 0.001448\n",
      "Training stage for Flod 4 Epoch: 70 [9600/37476                 (32%)]\tLoss: 0.127952\n",
      "Training stage for Flod 4 Epoch: 70 [12800/37476                 (43%)]\tLoss: 0.005724\n",
      "Training stage for Flod 4 Epoch: 70 [16000/37476                 (53%)]\tLoss: 0.002053\n",
      "Training stage for Flod 4 Epoch: 70 [19200/37476                 (64%)]\tLoss: 0.000771\n",
      "Training stage for Flod 4 Epoch: 70 [22400/37476                 (75%)]\tLoss: 0.039237\n",
      "Training stage for Flod 4 Epoch: 70 [25600/37476                 (85%)]\tLoss: 0.010185\n",
      "Training stage for Flod 4 Epoch: 70 [28800/37476                 (96%)]\tLoss: 0.004495\n",
      "Test set for fold4: Average Loss:           1.3554, Accuracy: 14756/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 71 [0/37476                 (0%)]\tLoss: 0.043527\n",
      "Training stage for Flod 4 Epoch: 71 [3200/37476                 (11%)]\tLoss: 0.039166\n",
      "Training stage for Flod 4 Epoch: 71 [6400/37476                 (21%)]\tLoss: 0.005261\n",
      "Training stage for Flod 4 Epoch: 71 [9600/37476                 (32%)]\tLoss: 0.060110\n",
      "Training stage for Flod 4 Epoch: 71 [12800/37476                 (43%)]\tLoss: 0.003687\n",
      "Training stage for Flod 4 Epoch: 71 [16000/37476                 (53%)]\tLoss: 0.008705\n",
      "Training stage for Flod 4 Epoch: 71 [19200/37476                 (64%)]\tLoss: 0.001328\n",
      "Training stage for Flod 4 Epoch: 71 [22400/37476                 (75%)]\tLoss: 0.201270\n",
      "Training stage for Flod 4 Epoch: 71 [25600/37476                 (85%)]\tLoss: 0.002593\n",
      "Training stage for Flod 4 Epoch: 71 [28800/37476                 (96%)]\tLoss: 0.025767\n",
      "Test set for fold4: Average Loss:           1.4422, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 72 [0/37476                 (0%)]\tLoss: 0.049850\n",
      "Training stage for Flod 4 Epoch: 72 [3200/37476                 (11%)]\tLoss: 0.009870\n",
      "Training stage for Flod 4 Epoch: 72 [6400/37476                 (21%)]\tLoss: 0.001454\n",
      "Training stage for Flod 4 Epoch: 72 [9600/37476                 (32%)]\tLoss: 0.010980\n",
      "Training stage for Flod 4 Epoch: 72 [12800/37476                 (43%)]\tLoss: 0.002595\n",
      "Training stage for Flod 4 Epoch: 72 [16000/37476                 (53%)]\tLoss: 0.026247\n",
      "Training stage for Flod 4 Epoch: 72 [19200/37476                 (64%)]\tLoss: 0.071363\n",
      "Training stage for Flod 4 Epoch: 72 [22400/37476                 (75%)]\tLoss: 0.016151\n",
      "Training stage for Flod 4 Epoch: 72 [25600/37476                 (85%)]\tLoss: 0.006921\n",
      "Training stage for Flod 4 Epoch: 72 [28800/37476                 (96%)]\tLoss: 0.004839\n",
      "Test set for fold4: Average Loss:           1.2576, Accuracy: 14773/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 73 [0/37476                 (0%)]\tLoss: 0.001287\n",
      "Training stage for Flod 4 Epoch: 73 [3200/37476                 (11%)]\tLoss: 0.008406\n",
      "Training stage for Flod 4 Epoch: 73 [6400/37476                 (21%)]\tLoss: 0.016205\n",
      "Training stage for Flod 4 Epoch: 73 [9600/37476                 (32%)]\tLoss: 0.006919\n",
      "Training stage for Flod 4 Epoch: 73 [12800/37476                 (43%)]\tLoss: 0.041414\n",
      "Training stage for Flod 4 Epoch: 73 [16000/37476                 (53%)]\tLoss: 0.003070\n",
      "Training stage for Flod 4 Epoch: 73 [19200/37476                 (64%)]\tLoss: 0.180612\n",
      "Training stage for Flod 4 Epoch: 73 [22400/37476                 (75%)]\tLoss: 0.026500\n",
      "Training stage for Flod 4 Epoch: 73 [25600/37476                 (85%)]\tLoss: 0.079694\n",
      "Training stage for Flod 4 Epoch: 73 [28800/37476                 (96%)]\tLoss: 0.023298\n",
      "Test set for fold4: Average Loss:           1.1959, Accuracy: 14797/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 74 [0/37476                 (0%)]\tLoss: 0.013957\n",
      "Training stage for Flod 4 Epoch: 74 [3200/37476                 (11%)]\tLoss: 0.000337\n",
      "Training stage for Flod 4 Epoch: 74 [6400/37476                 (21%)]\tLoss: 0.003118\n",
      "Training stage for Flod 4 Epoch: 74 [9600/37476                 (32%)]\tLoss: 0.075833\n",
      "Training stage for Flod 4 Epoch: 74 [12800/37476                 (43%)]\tLoss: 0.001313\n",
      "Training stage for Flod 4 Epoch: 74 [16000/37476                 (53%)]\tLoss: 0.007549\n",
      "Training stage for Flod 4 Epoch: 74 [19200/37476                 (64%)]\tLoss: 0.015900\n",
      "Training stage for Flod 4 Epoch: 74 [22400/37476                 (75%)]\tLoss: 0.000210\n",
      "Training stage for Flod 4 Epoch: 74 [25600/37476                 (85%)]\tLoss: 0.074418\n",
      "Training stage for Flod 4 Epoch: 74 [28800/37476                 (96%)]\tLoss: 0.026326\n",
      "Test set for fold4: Average Loss:           1.1351, Accuracy: 14790/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 75 [0/37476                 (0%)]\tLoss: 0.003367\n",
      "Training stage for Flod 4 Epoch: 75 [3200/37476                 (11%)]\tLoss: 0.016476\n",
      "Training stage for Flod 4 Epoch: 75 [6400/37476                 (21%)]\tLoss: 0.000886\n",
      "Training stage for Flod 4 Epoch: 75 [9600/37476                 (32%)]\tLoss: 0.041724\n",
      "Training stage for Flod 4 Epoch: 75 [12800/37476                 (43%)]\tLoss: 0.003206\n",
      "Training stage for Flod 4 Epoch: 75 [16000/37476                 (53%)]\tLoss: 0.028931\n",
      "Training stage for Flod 4 Epoch: 75 [19200/37476                 (64%)]\tLoss: 0.071034\n",
      "Training stage for Flod 4 Epoch: 75 [22400/37476                 (75%)]\tLoss: 0.151251\n",
      "Training stage for Flod 4 Epoch: 75 [25600/37476                 (85%)]\tLoss: 0.038463\n",
      "Training stage for Flod 4 Epoch: 75 [28800/37476                 (96%)]\tLoss: 0.012746\n",
      "Test set for fold4: Average Loss:           1.3114, Accuracy: 14721/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 76 [0/37476                 (0%)]\tLoss: 0.157810\n",
      "Training stage for Flod 4 Epoch: 76 [3200/37476                 (11%)]\tLoss: 0.000846\n",
      "Training stage for Flod 4 Epoch: 76 [6400/37476                 (21%)]\tLoss: 0.001573\n",
      "Training stage for Flod 4 Epoch: 76 [9600/37476                 (32%)]\tLoss: 0.005704\n",
      "Training stage for Flod 4 Epoch: 76 [12800/37476                 (43%)]\tLoss: 0.100012\n",
      "Training stage for Flod 4 Epoch: 76 [16000/37476                 (53%)]\tLoss: 0.060899\n",
      "Training stage for Flod 4 Epoch: 76 [19200/37476                 (64%)]\tLoss: 0.005900\n",
      "Training stage for Flod 4 Epoch: 76 [22400/37476                 (75%)]\tLoss: 0.062410\n",
      "Training stage for Flod 4 Epoch: 76 [25600/37476                 (85%)]\tLoss: 0.010875\n",
      "Training stage for Flod 4 Epoch: 76 [28800/37476                 (96%)]\tLoss: 0.004831\n",
      "Test set for fold4: Average Loss:           1.1406, Accuracy: 14789/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 77 [0/37476                 (0%)]\tLoss: 0.002286\n",
      "Training stage for Flod 4 Epoch: 77 [3200/37476                 (11%)]\tLoss: 0.010350\n",
      "Training stage for Flod 4 Epoch: 77 [6400/37476                 (21%)]\tLoss: 0.053493\n",
      "Training stage for Flod 4 Epoch: 77 [9600/37476                 (32%)]\tLoss: 0.006984\n",
      "Training stage for Flod 4 Epoch: 77 [12800/37476                 (43%)]\tLoss: 0.006641\n",
      "Training stage for Flod 4 Epoch: 77 [16000/37476                 (53%)]\tLoss: 0.003153\n",
      "Training stage for Flod 4 Epoch: 77 [19200/37476                 (64%)]\tLoss: 0.020616\n",
      "Training stage for Flod 4 Epoch: 77 [22400/37476                 (75%)]\tLoss: 0.011864\n",
      "Training stage for Flod 4 Epoch: 77 [25600/37476                 (85%)]\tLoss: 0.005635\n",
      "Training stage for Flod 4 Epoch: 77 [28800/37476                 (96%)]\tLoss: 0.005589\n",
      "Test set for fold4: Average Loss:           1.4451, Accuracy: 14762/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 78 [0/37476                 (0%)]\tLoss: 0.000624\n",
      "Training stage for Flod 4 Epoch: 78 [3200/37476                 (11%)]\tLoss: 0.017892\n",
      "Training stage for Flod 4 Epoch: 78 [6400/37476                 (21%)]\tLoss: 0.082062\n",
      "Training stage for Flod 4 Epoch: 78 [9600/37476                 (32%)]\tLoss: 0.000646\n",
      "Training stage for Flod 4 Epoch: 78 [12800/37476                 (43%)]\tLoss: 0.029184\n",
      "Training stage for Flod 4 Epoch: 78 [16000/37476                 (53%)]\tLoss: 0.000062\n",
      "Training stage for Flod 4 Epoch: 78 [19200/37476                 (64%)]\tLoss: 0.020679\n",
      "Training stage for Flod 4 Epoch: 78 [22400/37476                 (75%)]\tLoss: 0.009848\n",
      "Training stage for Flod 4 Epoch: 78 [25600/37476                 (85%)]\tLoss: 0.018036\n",
      "Training stage for Flod 4 Epoch: 78 [28800/37476                 (96%)]\tLoss: 0.062803\n",
      "Test set for fold4: Average Loss:           1.2714, Accuracy: 14790/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 79 [0/37476                 (0%)]\tLoss: 0.017909\n",
      "Training stage for Flod 4 Epoch: 79 [3200/37476                 (11%)]\tLoss: 0.007192\n",
      "Training stage for Flod 4 Epoch: 79 [6400/37476                 (21%)]\tLoss: 0.000443\n",
      "Training stage for Flod 4 Epoch: 79 [9600/37476                 (32%)]\tLoss: 0.002865\n",
      "Training stage for Flod 4 Epoch: 79 [12800/37476                 (43%)]\tLoss: 0.121169\n",
      "Training stage for Flod 4 Epoch: 79 [16000/37476                 (53%)]\tLoss: 0.009478\n",
      "Training stage for Flod 4 Epoch: 79 [19200/37476                 (64%)]\tLoss: 0.003303\n",
      "Training stage for Flod 4 Epoch: 79 [22400/37476                 (75%)]\tLoss: 0.007725\n",
      "Training stage for Flod 4 Epoch: 79 [25600/37476                 (85%)]\tLoss: 0.050905\n",
      "Training stage for Flod 4 Epoch: 79 [28800/37476                 (96%)]\tLoss: 0.006425\n",
      "Test set for fold4: Average Loss:           1.0526, Accuracy: 14782/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 80 [0/37476                 (0%)]\tLoss: 0.056393\n",
      "Training stage for Flod 4 Epoch: 80 [3200/37476                 (11%)]\tLoss: 0.015046\n",
      "Training stage for Flod 4 Epoch: 80 [6400/37476                 (21%)]\tLoss: 0.000447\n",
      "Training stage for Flod 4 Epoch: 80 [9600/37476                 (32%)]\tLoss: 0.033732\n",
      "Training stage for Flod 4 Epoch: 80 [12800/37476                 (43%)]\tLoss: 0.088953\n",
      "Training stage for Flod 4 Epoch: 80 [16000/37476                 (53%)]\tLoss: 0.032488\n",
      "Training stage for Flod 4 Epoch: 80 [19200/37476                 (64%)]\tLoss: 0.001156\n",
      "Training stage for Flod 4 Epoch: 80 [22400/37476                 (75%)]\tLoss: 0.015248\n",
      "Training stage for Flod 4 Epoch: 80 [25600/37476                 (85%)]\tLoss: 0.110260\n",
      "Training stage for Flod 4 Epoch: 80 [28800/37476                 (96%)]\tLoss: 0.014817\n",
      "Test set for fold4: Average Loss:           1.1866, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 81 [0/37476                 (0%)]\tLoss: 0.031011\n",
      "Training stage for Flod 4 Epoch: 81 [3200/37476                 (11%)]\tLoss: 0.131678\n",
      "Training stage for Flod 4 Epoch: 81 [6400/37476                 (21%)]\tLoss: 0.006734\n",
      "Training stage for Flod 4 Epoch: 81 [9600/37476                 (32%)]\tLoss: 0.008058\n",
      "Training stage for Flod 4 Epoch: 81 [12800/37476                 (43%)]\tLoss: 0.011660\n",
      "Training stage for Flod 4 Epoch: 81 [16000/37476                 (53%)]\tLoss: 0.012874\n",
      "Training stage for Flod 4 Epoch: 81 [19200/37476                 (64%)]\tLoss: 0.014788\n",
      "Training stage for Flod 4 Epoch: 81 [22400/37476                 (75%)]\tLoss: 0.044405\n",
      "Training stage for Flod 4 Epoch: 81 [25600/37476                 (85%)]\tLoss: 0.049399\n",
      "Training stage for Flod 4 Epoch: 81 [28800/37476                 (96%)]\tLoss: 0.004408\n",
      "Test set for fold4: Average Loss:           1.3555, Accuracy: 14718/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 82 [0/37476                 (0%)]\tLoss: 0.028056\n",
      "Training stage for Flod 4 Epoch: 82 [3200/37476                 (11%)]\tLoss: 0.020001\n",
      "Training stage for Flod 4 Epoch: 82 [6400/37476                 (21%)]\tLoss: 0.011117\n",
      "Training stage for Flod 4 Epoch: 82 [9600/37476                 (32%)]\tLoss: 0.012732\n",
      "Training stage for Flod 4 Epoch: 82 [12800/37476                 (43%)]\tLoss: 0.110235\n",
      "Training stage for Flod 4 Epoch: 82 [16000/37476                 (53%)]\tLoss: 0.002709\n",
      "Training stage for Flod 4 Epoch: 82 [19200/37476                 (64%)]\tLoss: 0.036096\n",
      "Training stage for Flod 4 Epoch: 82 [22400/37476                 (75%)]\tLoss: 0.178337\n",
      "Training stage for Flod 4 Epoch: 82 [25600/37476                 (85%)]\tLoss: 0.125799\n",
      "Training stage for Flod 4 Epoch: 82 [28800/37476                 (96%)]\tLoss: 0.014033\n",
      "Test set for fold4: Average Loss:           1.5042, Accuracy: 14768/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 83 [0/37476                 (0%)]\tLoss: 0.004478\n",
      "Training stage for Flod 4 Epoch: 83 [3200/37476                 (11%)]\tLoss: 0.023086\n",
      "Training stage for Flod 4 Epoch: 83 [6400/37476                 (21%)]\tLoss: 0.024563\n",
      "Training stage for Flod 4 Epoch: 83 [9600/37476                 (32%)]\tLoss: 0.007108\n",
      "Training stage for Flod 4 Epoch: 83 [12800/37476                 (43%)]\tLoss: 0.000639\n",
      "Training stage for Flod 4 Epoch: 83 [16000/37476                 (53%)]\tLoss: 0.008009\n",
      "Training stage for Flod 4 Epoch: 83 [19200/37476                 (64%)]\tLoss: 0.006730\n",
      "Training stage for Flod 4 Epoch: 83 [22400/37476                 (75%)]\tLoss: 0.068356\n",
      "Training stage for Flod 4 Epoch: 83 [25600/37476                 (85%)]\tLoss: 0.005494\n",
      "Training stage for Flod 4 Epoch: 83 [28800/37476                 (96%)]\tLoss: 0.002533\n",
      "Test set for fold4: Average Loss:           1.2490, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 84 [0/37476                 (0%)]\tLoss: 0.002320\n",
      "Training stage for Flod 4 Epoch: 84 [3200/37476                 (11%)]\tLoss: 0.038186\n",
      "Training stage for Flod 4 Epoch: 84 [6400/37476                 (21%)]\tLoss: 0.080738\n",
      "Training stage for Flod 4 Epoch: 84 [9600/37476                 (32%)]\tLoss: 0.035546\n",
      "Training stage for Flod 4 Epoch: 84 [12800/37476                 (43%)]\tLoss: 0.001362\n",
      "Training stage for Flod 4 Epoch: 84 [16000/37476                 (53%)]\tLoss: 0.015646\n",
      "Training stage for Flod 4 Epoch: 84 [19200/37476                 (64%)]\tLoss: 0.000704\n",
      "Training stage for Flod 4 Epoch: 84 [22400/37476                 (75%)]\tLoss: 0.003911\n",
      "Training stage for Flod 4 Epoch: 84 [25600/37476                 (85%)]\tLoss: 0.006192\n",
      "Training stage for Flod 4 Epoch: 84 [28800/37476                 (96%)]\tLoss: 0.000684\n",
      "Test set for fold4: Average Loss:           1.2999, Accuracy: 14803/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 85 [0/37476                 (0%)]\tLoss: 0.036427\n",
      "Training stage for Flod 4 Epoch: 85 [3200/37476                 (11%)]\tLoss: 0.019411\n",
      "Training stage for Flod 4 Epoch: 85 [6400/37476                 (21%)]\tLoss: 0.025173\n",
      "Training stage for Flod 4 Epoch: 85 [9600/37476                 (32%)]\tLoss: 0.005233\n",
      "Training stage for Flod 4 Epoch: 85 [12800/37476                 (43%)]\tLoss: 0.022789\n",
      "Training stage for Flod 4 Epoch: 85 [16000/37476                 (53%)]\tLoss: 0.004241\n",
      "Training stage for Flod 4 Epoch: 85 [19200/37476                 (64%)]\tLoss: 0.005422\n",
      "Training stage for Flod 4 Epoch: 85 [22400/37476                 (75%)]\tLoss: 0.000352\n",
      "Training stage for Flod 4 Epoch: 85 [25600/37476                 (85%)]\tLoss: 0.000950\n",
      "Training stage for Flod 4 Epoch: 85 [28800/37476                 (96%)]\tLoss: 0.015592\n",
      "Test set for fold4: Average Loss:           1.4188, Accuracy: 14779/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 86 [0/37476                 (0%)]\tLoss: 0.119488\n",
      "Training stage for Flod 4 Epoch: 86 [3200/37476                 (11%)]\tLoss: 0.006364\n",
      "Training stage for Flod 4 Epoch: 86 [6400/37476                 (21%)]\tLoss: 0.006852\n",
      "Training stage for Flod 4 Epoch: 86 [9600/37476                 (32%)]\tLoss: 0.014271\n",
      "Training stage for Flod 4 Epoch: 86 [12800/37476                 (43%)]\tLoss: 0.003879\n",
      "Training stage for Flod 4 Epoch: 86 [16000/37476                 (53%)]\tLoss: 0.019008\n",
      "Training stage for Flod 4 Epoch: 86 [19200/37476                 (64%)]\tLoss: 0.000235\n",
      "Training stage for Flod 4 Epoch: 86 [22400/37476                 (75%)]\tLoss: 0.000510\n",
      "Training stage for Flod 4 Epoch: 86 [25600/37476                 (85%)]\tLoss: 0.011810\n",
      "Training stage for Flod 4 Epoch: 86 [28800/37476                 (96%)]\tLoss: 0.002769\n",
      "Test set for fold4: Average Loss:           1.2186, Accuracy: 14776/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 87 [0/37476                 (0%)]\tLoss: 0.037321\n",
      "Training stage for Flod 4 Epoch: 87 [3200/37476                 (11%)]\tLoss: 0.066988\n",
      "Training stage for Flod 4 Epoch: 87 [6400/37476                 (21%)]\tLoss: 0.082252\n",
      "Training stage for Flod 4 Epoch: 87 [9600/37476                 (32%)]\tLoss: 0.006434\n",
      "Training stage for Flod 4 Epoch: 87 [12800/37476                 (43%)]\tLoss: 0.081192\n",
      "Training stage for Flod 4 Epoch: 87 [16000/37476                 (53%)]\tLoss: 0.027262\n",
      "Training stage for Flod 4 Epoch: 87 [19200/37476                 (64%)]\tLoss: 0.000825\n",
      "Training stage for Flod 4 Epoch: 87 [22400/37476                 (75%)]\tLoss: 0.003161\n",
      "Training stage for Flod 4 Epoch: 87 [25600/37476                 (85%)]\tLoss: 0.001544\n",
      "Training stage for Flod 4 Epoch: 87 [28800/37476                 (96%)]\tLoss: 0.001010\n",
      "Test set for fold4: Average Loss:           1.2199, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 88 [0/37476                 (0%)]\tLoss: 0.000953\n",
      "Training stage for Flod 4 Epoch: 88 [3200/37476                 (11%)]\tLoss: 0.002792\n",
      "Training stage for Flod 4 Epoch: 88 [6400/37476                 (21%)]\tLoss: 0.010654\n",
      "Training stage for Flod 4 Epoch: 88 [9600/37476                 (32%)]\tLoss: 0.001694\n",
      "Training stage for Flod 4 Epoch: 88 [12800/37476                 (43%)]\tLoss: 0.014790\n",
      "Training stage for Flod 4 Epoch: 88 [16000/37476                 (53%)]\tLoss: 0.000351\n",
      "Training stage for Flod 4 Epoch: 88 [19200/37476                 (64%)]\tLoss: 0.009938\n",
      "Training stage for Flod 4 Epoch: 88 [22400/37476                 (75%)]\tLoss: 0.002239\n",
      "Training stage for Flod 4 Epoch: 88 [25600/37476                 (85%)]\tLoss: 0.061866\n",
      "Training stage for Flod 4 Epoch: 88 [28800/37476                 (96%)]\tLoss: 0.010139\n",
      "Test set for fold4: Average Loss:           1.5588, Accuracy: 14778/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 89 [0/37476                 (0%)]\tLoss: 0.004480\n",
      "Training stage for Flod 4 Epoch: 89 [3200/37476                 (11%)]\tLoss: 0.006208\n",
      "Training stage for Flod 4 Epoch: 89 [6400/37476                 (21%)]\tLoss: 0.052823\n",
      "Training stage for Flod 4 Epoch: 89 [9600/37476                 (32%)]\tLoss: 0.000522\n",
      "Training stage for Flod 4 Epoch: 89 [12800/37476                 (43%)]\tLoss: 0.016917\n",
      "Training stage for Flod 4 Epoch: 89 [16000/37476                 (53%)]\tLoss: 0.010436\n",
      "Training stage for Flod 4 Epoch: 89 [19200/37476                 (64%)]\tLoss: 0.109872\n",
      "Training stage for Flod 4 Epoch: 89 [22400/37476                 (75%)]\tLoss: 0.013522\n",
      "Training stage for Flod 4 Epoch: 89 [25600/37476                 (85%)]\tLoss: 0.003949\n",
      "Training stage for Flod 4 Epoch: 89 [28800/37476                 (96%)]\tLoss: 0.010032\n",
      "Test set for fold4: Average Loss:           1.4429, Accuracy: 14803/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 90 [0/37476                 (0%)]\tLoss: 0.082394\n",
      "Training stage for Flod 4 Epoch: 90 [3200/37476                 (11%)]\tLoss: 0.003937\n",
      "Training stage for Flod 4 Epoch: 90 [6400/37476                 (21%)]\tLoss: 0.100939\n",
      "Training stage for Flod 4 Epoch: 90 [9600/37476                 (32%)]\tLoss: 0.045755\n",
      "Training stage for Flod 4 Epoch: 90 [12800/37476                 (43%)]\tLoss: 0.012443\n",
      "Training stage for Flod 4 Epoch: 90 [16000/37476                 (53%)]\tLoss: 0.150879\n",
      "Training stage for Flod 4 Epoch: 90 [19200/37476                 (64%)]\tLoss: 0.000877\n",
      "Training stage for Flod 4 Epoch: 90 [22400/37476                 (75%)]\tLoss: 0.028203\n",
      "Training stage for Flod 4 Epoch: 90 [25600/37476                 (85%)]\tLoss: 0.016367\n",
      "Training stage for Flod 4 Epoch: 90 [28800/37476                 (96%)]\tLoss: 0.030889\n",
      "Test set for fold4: Average Loss:           1.3742, Accuracy: 14771/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 91 [0/37476                 (0%)]\tLoss: 0.001545\n",
      "Training stage for Flod 4 Epoch: 91 [3200/37476                 (11%)]\tLoss: 0.160199\n",
      "Training stage for Flod 4 Epoch: 91 [6400/37476                 (21%)]\tLoss: 0.028626\n",
      "Training stage for Flod 4 Epoch: 91 [9600/37476                 (32%)]\tLoss: 0.093782\n",
      "Training stage for Flod 4 Epoch: 91 [12800/37476                 (43%)]\tLoss: 0.008264\n",
      "Training stage for Flod 4 Epoch: 91 [16000/37476                 (53%)]\tLoss: 0.000267\n",
      "Training stage for Flod 4 Epoch: 91 [19200/37476                 (64%)]\tLoss: 0.003553\n",
      "Training stage for Flod 4 Epoch: 91 [22400/37476                 (75%)]\tLoss: 0.052369\n",
      "Training stage for Flod 4 Epoch: 91 [25600/37476                 (85%)]\tLoss: 0.025209\n",
      "Training stage for Flod 4 Epoch: 91 [28800/37476                 (96%)]\tLoss: 0.000311\n",
      "Test set for fold4: Average Loss:           1.6209, Accuracy: 14791/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 92 [0/37476                 (0%)]\tLoss: 0.016407\n",
      "Training stage for Flod 4 Epoch: 92 [3200/37476                 (11%)]\tLoss: 0.004363\n",
      "Training stage for Flod 4 Epoch: 92 [6400/37476                 (21%)]\tLoss: 0.052411\n",
      "Training stage for Flod 4 Epoch: 92 [9600/37476                 (32%)]\tLoss: 0.003125\n",
      "Training stage for Flod 4 Epoch: 92 [12800/37476                 (43%)]\tLoss: 0.001000\n",
      "Training stage for Flod 4 Epoch: 92 [16000/37476                 (53%)]\tLoss: 0.049625\n",
      "Training stage for Flod 4 Epoch: 92 [19200/37476                 (64%)]\tLoss: 0.078881\n",
      "Training stage for Flod 4 Epoch: 92 [22400/37476                 (75%)]\tLoss: 0.045214\n",
      "Training stage for Flod 4 Epoch: 92 [25600/37476                 (85%)]\tLoss: 0.001388\n",
      "Training stage for Flod 4 Epoch: 92 [28800/37476                 (96%)]\tLoss: 0.000417\n",
      "Test set for fold4: Average Loss:           1.3025, Accuracy: 14800/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 93 [0/37476                 (0%)]\tLoss: 0.004645\n",
      "Training stage for Flod 4 Epoch: 93 [3200/37476                 (11%)]\tLoss: 0.014646\n",
      "Training stage for Flod 4 Epoch: 93 [6400/37476                 (21%)]\tLoss: 0.001491\n",
      "Training stage for Flod 4 Epoch: 93 [9600/37476                 (32%)]\tLoss: 0.008185\n",
      "Training stage for Flod 4 Epoch: 93 [12800/37476                 (43%)]\tLoss: 0.010901\n",
      "Training stage for Flod 4 Epoch: 93 [16000/37476                 (53%)]\tLoss: 0.002318\n",
      "Training stage for Flod 4 Epoch: 93 [19200/37476                 (64%)]\tLoss: 0.003059\n",
      "Training stage for Flod 4 Epoch: 93 [22400/37476                 (75%)]\tLoss: 0.000061\n",
      "Training stage for Flod 4 Epoch: 93 [25600/37476                 (85%)]\tLoss: 0.003049\n",
      "Training stage for Flod 4 Epoch: 93 [28800/37476                 (96%)]\tLoss: 0.013476\n",
      "Test set for fold4: Average Loss:           1.2789, Accuracy: 14784/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 94 [0/37476                 (0%)]\tLoss: 0.025600\n",
      "Training stage for Flod 4 Epoch: 94 [3200/37476                 (11%)]\tLoss: 0.000967\n",
      "Training stage for Flod 4 Epoch: 94 [6400/37476                 (21%)]\tLoss: 0.048998\n",
      "Training stage for Flod 4 Epoch: 94 [9600/37476                 (32%)]\tLoss: 0.001902\n",
      "Training stage for Flod 4 Epoch: 94 [12800/37476                 (43%)]\tLoss: 0.025761\n",
      "Training stage for Flod 4 Epoch: 94 [16000/37476                 (53%)]\tLoss: 0.085391\n",
      "Training stage for Flod 4 Epoch: 94 [19200/37476                 (64%)]\tLoss: 0.000481\n",
      "Training stage for Flod 4 Epoch: 94 [22400/37476                 (75%)]\tLoss: 0.019632\n",
      "Training stage for Flod 4 Epoch: 94 [25600/37476                 (85%)]\tLoss: 0.002234\n",
      "Training stage for Flod 4 Epoch: 94 [28800/37476                 (96%)]\tLoss: 0.012800\n",
      "Test set for fold4: Average Loss:           1.5979, Accuracy: 14751/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 95 [0/37476                 (0%)]\tLoss: 0.038878\n",
      "Training stage for Flod 4 Epoch: 95 [3200/37476                 (11%)]\tLoss: 0.004284\n",
      "Training stage for Flod 4 Epoch: 95 [6400/37476                 (21%)]\tLoss: 0.005344\n",
      "Training stage for Flod 4 Epoch: 95 [9600/37476                 (32%)]\tLoss: 0.000102\n",
      "Training stage for Flod 4 Epoch: 95 [12800/37476                 (43%)]\tLoss: 0.000624\n",
      "Training stage for Flod 4 Epoch: 95 [16000/37476                 (53%)]\tLoss: 0.000279\n",
      "Training stage for Flod 4 Epoch: 95 [19200/37476                 (64%)]\tLoss: 0.000047\n",
      "Training stage for Flod 4 Epoch: 95 [22400/37476                 (75%)]\tLoss: 0.002268\n",
      "Training stage for Flod 4 Epoch: 95 [25600/37476                 (85%)]\tLoss: 0.000700\n",
      "Training stage for Flod 4 Epoch: 95 [28800/37476                 (96%)]\tLoss: 0.000909\n",
      "Test set for fold4: Average Loss:           1.4288, Accuracy: 14747/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 96 [0/37476                 (0%)]\tLoss: 0.028918\n",
      "Training stage for Flod 4 Epoch: 96 [3200/37476                 (11%)]\tLoss: 0.001909\n",
      "Training stage for Flod 4 Epoch: 96 [6400/37476                 (21%)]\tLoss: 0.003213\n",
      "Training stage for Flod 4 Epoch: 96 [9600/37476                 (32%)]\tLoss: 0.003844\n",
      "Training stage for Flod 4 Epoch: 96 [12800/37476                 (43%)]\tLoss: 0.002626\n",
      "Training stage for Flod 4 Epoch: 96 [16000/37476                 (53%)]\tLoss: 0.021573\n",
      "Training stage for Flod 4 Epoch: 96 [19200/37476                 (64%)]\tLoss: 0.009882\n",
      "Training stage for Flod 4 Epoch: 96 [22400/37476                 (75%)]\tLoss: 0.043725\n",
      "Training stage for Flod 4 Epoch: 96 [25600/37476                 (85%)]\tLoss: 0.003267\n",
      "Training stage for Flod 4 Epoch: 96 [28800/37476                 (96%)]\tLoss: 0.003290\n",
      "Test set for fold4: Average Loss:           1.4035, Accuracy: 14788/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 97 [0/37476                 (0%)]\tLoss: 0.003218\n",
      "Training stage for Flod 4 Epoch: 97 [3200/37476                 (11%)]\tLoss: 0.000364\n",
      "Training stage for Flod 4 Epoch: 97 [6400/37476                 (21%)]\tLoss: 0.024916\n",
      "Training stage for Flod 4 Epoch: 97 [9600/37476                 (32%)]\tLoss: 0.001486\n",
      "Training stage for Flod 4 Epoch: 97 [12800/37476                 (43%)]\tLoss: 0.055242\n",
      "Training stage for Flod 4 Epoch: 97 [16000/37476                 (53%)]\tLoss: 0.000303\n",
      "Training stage for Flod 4 Epoch: 97 [19200/37476                 (64%)]\tLoss: 0.144115\n",
      "Training stage for Flod 4 Epoch: 97 [22400/37476                 (75%)]\tLoss: 0.000308\n",
      "Training stage for Flod 4 Epoch: 97 [25600/37476                 (85%)]\tLoss: 0.018034\n",
      "Training stage for Flod 4 Epoch: 97 [28800/37476                 (96%)]\tLoss: 0.000274\n",
      "Test set for fold4: Average Loss:           1.0876, Accuracy: 14748/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 98 [0/37476                 (0%)]\tLoss: 0.039031\n",
      "Training stage for Flod 4 Epoch: 98 [3200/37476                 (11%)]\tLoss: 0.076904\n",
      "Training stage for Flod 4 Epoch: 98 [6400/37476                 (21%)]\tLoss: 0.018429\n",
      "Training stage for Flod 4 Epoch: 98 [9600/37476                 (32%)]\tLoss: 0.000450\n",
      "Training stage for Flod 4 Epoch: 98 [12800/37476                 (43%)]\tLoss: 0.005163\n",
      "Training stage for Flod 4 Epoch: 98 [16000/37476                 (53%)]\tLoss: 0.011750\n",
      "Training stage for Flod 4 Epoch: 98 [19200/37476                 (64%)]\tLoss: 0.065621\n",
      "Training stage for Flod 4 Epoch: 98 [22400/37476                 (75%)]\tLoss: 0.028869\n",
      "Training stage for Flod 4 Epoch: 98 [25600/37476                 (85%)]\tLoss: 0.021970\n",
      "Training stage for Flod 4 Epoch: 98 [28800/37476                 (96%)]\tLoss: 0.000667\n",
      "Test set for fold4: Average Loss:           1.2778, Accuracy: 14783/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 99 [0/37476                 (0%)]\tLoss: 0.085537\n",
      "Training stage for Flod 4 Epoch: 99 [3200/37476                 (11%)]\tLoss: 0.002065\n",
      "Training stage for Flod 4 Epoch: 99 [6400/37476                 (21%)]\tLoss: 0.000701\n",
      "Training stage for Flod 4 Epoch: 99 [9600/37476                 (32%)]\tLoss: 0.004219\n",
      "Training stage for Flod 4 Epoch: 99 [12800/37476                 (43%)]\tLoss: 0.029210\n",
      "Training stage for Flod 4 Epoch: 99 [16000/37476                 (53%)]\tLoss: 0.037925\n",
      "Training stage for Flod 4 Epoch: 99 [19200/37476                 (64%)]\tLoss: 0.053222\n",
      "Training stage for Flod 4 Epoch: 99 [22400/37476                 (75%)]\tLoss: 0.000086\n",
      "Training stage for Flod 4 Epoch: 99 [25600/37476                 (85%)]\tLoss: 0.206043\n",
      "Training stage for Flod 4 Epoch: 99 [28800/37476                 (96%)]\tLoss: 0.050316\n",
      "Test set for fold4: Average Loss:           1.4535, Accuracy: 14765/37476           (39%)\n",
      "Training stage for Flod 4 Epoch: 100 [0/37476                 (0%)]\tLoss: 0.014121\n",
      "Training stage for Flod 4 Epoch: 100 [3200/37476                 (11%)]\tLoss: 0.006768\n",
      "Training stage for Flod 4 Epoch: 100 [6400/37476                 (21%)]\tLoss: 0.019315\n",
      "Training stage for Flod 4 Epoch: 100 [9600/37476                 (32%)]\tLoss: 0.000769\n",
      "Training stage for Flod 4 Epoch: 100 [12800/37476                 (43%)]\tLoss: 0.004036\n",
      "Training stage for Flod 4 Epoch: 100 [16000/37476                 (53%)]\tLoss: 0.008327\n",
      "Training stage for Flod 4 Epoch: 100 [19200/37476                 (64%)]\tLoss: 0.000450\n",
      "Training stage for Flod 4 Epoch: 100 [22400/37476                 (75%)]\tLoss: 0.050169\n",
      "Training stage for Flod 4 Epoch: 100 [25600/37476                 (85%)]\tLoss: 0.007827\n",
      "Training stage for Flod 4 Epoch: 100 [28800/37476                 (96%)]\tLoss: 0.020076\n",
      "Test set for fold4: Average Loss:           1.3449, Accuracy: 14779/37476           (39%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcd2001eb80>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAE/CAYAAACEto0QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAC+AklEQVR4nOzdeVxU1f/48deZYR0RBJfCMHFLFGZAFsUFhVRSMcPMTxmU+kn9mGZ+7Ksf9VOU1ddvln7KbLGfn3LJXFvMSi1TQa1cUTS3QsVCcAFklUVm5vz+mGECQRRFET3Px2MeM3PvueeeO6OXM+e+73kLKSWKoiiKoijKjdPUdQMURVEURVHuFKpjpSiKoiiKUktUx0pRFEVRFKWWqI6VoiiKoihKLVEdK0VRFEVRlFqiOlaKoiiKoii1RHWslJtCCHFKCNGnrtuhKIqiKLeS6lgpiqIoiqLUEtWxUhRFUZRaIizU39a7mPrylZtKCOEohJgrhEi3PuYKIRyt65oIIb4TQuQIIS4IIbaXnZCEEFOFEGlCiHwhxG9CiN51eySKotQnQohpQogT1nPIESHE4HLrRgshjpZbF2hd3kII8ZUQIkMIkSWEeN+6fIYQ4rNy23sLIaQQws76PkEIMVMI8TNQCLQWQowst4+TQoh/XNa+R4QQSUKIPGs7+wkhhgohEi8r9z9CiK9v2gel1Dq7um6Acsd7EQgFAgAJrAVeAuKA/wFOA02tZUMBKYRoDzwHhEgp04UQ3oD21jZbUZR67gQQBpwFhgKfCSHaAj2AGUA0sBdoA5QKIbTAd8AW4CnABATXYH9PAf2B3wABtAcGAieBnsAGIcQeKeU+IURn4FPgMWAz4Ak0BFKA/yeE6CClPGqtNxb43+s4fqWOqBEr5WaLAV6TUp6XUmYAr2I5AQGUYjmhtJRSlkopt0tL8koT4Ah0FELYSylPSSlP1EnrFUWpl6SUn0sp06WUZinlKiAZ6AyMAt6SUu6RFsellH9Y1zUHpkgpL0opi6WUP9Vgl4ullIellEbr+WydlPKEdR9bgY1YOnoAzwALpZQ/WtuXJqU8JqUsAVZh6UwhhPAFvLF0+JR6QnWslJutOfBHufd/WJcBzAaOAxutQ+XTAKSUx4F/YvlVeV4IsVII0RxFUZRrJIR42nqpLUcIkQP4AU2AFlhGsy7XAvhDSmm8zl2mXrb//kKIndYwhxxggHX/Zfu60o/FJcCTQgiB5UfoamuHS6knVMdKudnSgZbl3t9vXYaUMl9K+T9SytbAw8ALZbFUUsrlUsoe1m0l8OatbbaiKPWVEKIl8F8sIQWNpZSNgENYLtGlYrn8d7lU4P6yuKnLXAR05d7fW0UZWW7/jsCXwBzgHuv+11v3X7avqtqAlHIncAnL6NaTwNKqyim3L9WxUm62FcBLQoimQogmwMvAZwBCiIFCiLbWX2Z5WC4BmoQQ7YUQD1pPTsVAkXWdoijKtWiApaOTASCEGIllxArgY2CyECLIegdfW2tHbDdwBpglhGgghHASQnS3bpME9BRC3C+EcAOmX2X/DljCGTIAoxCiPxBZbv0nwEghRG8hhEYIcZ8Qwqfc+k+B9wFjDS9HKrcB1bFSbrb/xRIgehD4FdjHX4GY7YBNQAGwA/hQSpmA5YQ0C8jEEnjaDPj3LW21oij1lpTyCPAfLOeVc4Ae+Nm67nNgJrAcyAe+BjyklCYsI+dtgT+x3FjzuHWbH7HEPh0EErlKzJOUMh94HlgNZGMZefqm3PrdwEjgHSAX2ErFkf2lWDqCarSqHhKWWGFFURRFUW4HQghn4DwQKKVMruv2KDWjRqwURVEU5fbyLLBHdarqJzWPlaIoiqLcJoQQp7AEuUfXbUuU66UuBSqKoiiKotQSdSlQURRFURSllqiOlaIoiqIoSi25LWKsmjRpIr29veu6GYqi3EKJiYmZUsqmVy95+1PnMEW5u1R3/rotOlbe3t7s3bu3rpuhKMotJIT44+ql6gd1DlOUu0t15y91KVBRFEVRFKWWqI6VoiiKoihKLVEdK0VRFEVRlFpyW8RYKcq1Ki0t5fTp0xQXF9d1U5Rr5OTkhJeXF/b29nXdFEVRlJtOdayUeuX06dM0bNgQb29vhBB13RzlKqSUZGVlcfr0aVq1alUrdQohGgEfY0lSK4G/A79hSZLrDZwC/ialzLaWnw48A5iA56WUP1iXBwGLAWdgPTBRSimFEI7Ap0AQkAU8LqU8VSuNVxTljqcuBSr1SnFxMY0bN1adqnpCCEHjxo1re4TxXeB7KaUP4A8cBaYBm6WU7YDN1vcIIToCTwC+QD/gQyGE1lrPfGAM0M766Gdd/gyQLaVsC7wDvFmbjVcU5c6mOlZKvaM6VfVLbX5fQghXoCfwCYCU8pKUMgd4BFhiLbaEv/KsPQKslFKWSClTgONAZyGEJ+AqpdwhLXm9Pr1sm7K6vgB6C/WPTlGUa6Q6VopSAzk5OXz44YfXte2AAQPIycm55vIzZsxgzpw517Wvq5k3bx4dOnQgJibmimUWL17Mc889V+U6FxcX2+slS5bQrl072rVrx5IlS6osX4taAxnAIiHEfiHEx0KIBsA9UsozANbnZtby9wGp5bY/bV12n/X15csrbCOlNAK5QOObcziKotxpVMdKUWqguo6VyWSqdtv169fTqFGjm9Cqmvvwww9Zv349y5Ytu6F6Lly4wKuvvsquXbvYvXs3r776KtnZ2bXUyirZAYHAfCllJ+Ai1st+V1DVSJOsZnl121SsWIgxQoi9Qoi9GRkZ1bdaUZS7Rr0LXv9t11kaejjSvJ17XTdFuQtNmzaNEydOEBAQQN++fYmKiuLVV1/F09OTpKQkjhw5QnR0NKmpqRQXFzNx4kTGjBkD/DU7d0FBAf3796dHjx788ssv3HfffaxduxZnZ+cr7jcpKYmxY8dSWFhImzZtWLhwIe7u7sybN4+PPvoIOzs7OnbsyMqVK9m6dSsTJ04ELJfhtm3bRsOGDW11jR07lpMnTzJo0CD+/ve/M3z4cP7+979z8uRJdDodCxYswGAwVNh/SkoKTz75JEajkX79+tmW//DDD/Tt2xcPDw8A+vbty/fff8+wYcNq7TO/zGngtJRyl/X9F1g6VueEEJ5SyjPWy3zny5VvUW57LyDdutyriuXltzkthLAD3IALlzdESrkAWAAQHBxcqeOlKHcbU6mZ0ksmHJy0yCIjZgcoksXIUoG4pAUpcHTSohEmjKWF5BVlYSxxQlxyRCMFDdwd0ThIimUO+YWZGEskpiJHuOSIRmtPiUniqDNi51SCKJZoL2rRlAhMwohRlGLv1ACnBi5ohCPmEi3GEhMZOfmcy8sGp0LsnYtw1uhoUOwORfbkmAvJN+Xj0gBcnQXNPO/Fs227G/4c6l3HasdXx2np11h1rJQ6MWvWLA4dOkRSUhIACQkJ7N69m0OHDtnuelu4cCEeHh4UFRUREhLCkCFDaNy44pWk5ORkVqxYwX//+1/+9re/8eWXXxIbG3vF/T799NO899579OrVi5dffplXX32VuXPnMmvWLFJSUnB0dLRdZpwzZw4ffPAB3bt3p6CgACcnpwp1ffTRR3z//ffEx8fTpEkTJkyYQKdOnfj666/ZsmULTz/9tO34ykycOJFnn32Wp59+mg8++MC2PC0tjRYt/uq3eHl5kZaWVtOP9ZpJKc8KIVKFEO2llL8BvYEj1sdwYJb1ea11k2+A5UKIt4HmWILUd0spTUKIfCFEKLALeBp4r9w2w4EdwGPAFmsclnIdzGYzGs1fF0eklEhZikbjUKGclJK8S3kUlBag0zRAW+yARmgxacwYRSmXjMUUG4swShMmcxElF3MpvZiH2WzEztEBs50gNy+HvLw8MJnRmsEOB+w1DbHT6jDbmSmhhJL8Asy5hWAsBG0eUluExA6JPUI4odW4IKTEZMxHyEK0WoHGToDRjLlUIswlaDUFCEooNTag1NQAiUQjSkGYkVoNAi0U6dAUO2Fvdwl7hzxwzEU6ZiHtijGXuCJLGmEWEmlfgkZTgp2mBC0SCpsgCu7BJLWUao0I+yK0Drlo7IrQoEFILWajI6ZSRwQCO40RjdaIENKyf/tCzPYFaIQJjdQghMRsfxFpXwhSIMx2CJMD0uQIUgv2hWBXCNpS0JQipBZNaQOEyQkprcO7woxGmK3flAaQmLUlmDWXQFj+awiTI9oSVzQmJ4TGiNRewuiQh8khF2FyxK6oCRqTE0bnDIxO2bbtkAK7Yg+0pQ0wOmVjcsiv/h9UpZ845RQAmWX/oITloTGjsUYumEotRQoEoLMsc5CCS0KSWQpnforAs+3H1e//GtS7jpXWXoOx1Hz1gsod79VvD3MkPa9W6+zY3JVXHvat0TadO3euMJXAvHnzWLNmDQCpqakkJydX6li1atWKgIAAAIKCgjh16tQV68/NzSUnJ4devXoBMHz4cIYOHQqAwWAgJiaG6OhooqOjAejevTsvvPACMTExPProo3h5eV2pagB++uknvvzySwAefPBBsrKyyM3NrVDm559/tpV56qmnmDp1KmD5Y3i5WxDnPQFYJoRwAE4CI7Gc7VcLIZ4B/gSGWtt3WAixGkvHywiMl1KWXbN9lr+mW9hgfYAlMH6pEOI4ltP4Ezf7gG6ElBKjNGKvscwTZpZm0nPToRh0ZheKC82kZ2dyIT8bFxd3mnk0JjM3h+NnTlBQcg6cShH2l3AuKMGpMB+NqRjMZrQmE/YYsROXMNsXIO0LMJs1mI06MGkR2ktoNZew15jQaowAmKQAYUZrV2JZZv0jjsaEtCu2/NF3zAG7ErjkgraoMRqTAyAtf2g1RqSmFGn9wyw1paAxQtkfdSGR1n1diSuA1vqwMgMYwRFwdMbyjd/Qhy4QZnuctJeueRON0Qm7kkYIkyPGBr9jcrScu4TJDo3JGW1pA5AaShsfQ2pL0QJlXU9hskNT6mI5fmHCbFeEneav0ANhcrB2JLRoS3U4lDYAswNmLiGxdlzyWyCFxKwpRWougV0JQmOEkkaIAk/MJgeMZjtMwgj2hQhtCQLQCDBKgUkK2zV0gUCa7MFsD1KDkALsi8EhD+zzMJntkaWOUNgKbakbaIspcc5Eaksw5rbk0ll/pMkZjXREOF5E43wejV0Rpqx7kUWNEKUuaE06TBoTpfZFSLsSHDRgJ0Ca7DEbHSjFTLHdJYwYLS0yY+keC7P135ERNBI7jQP2GkfsL+mwu+REieYS+Q7ZGO2KcRaOOAh7SkvtKSqxQ9vI5wb/YVjUw46VFpPqWCm3kQYNGtheJyQksGnTJnbs2IFOpyM8PLzKqQYcHR1tr7VaLUVFRde173Xr1rFt2za++eYbXn/9dQ4fPsy0adOIiopi/fr1hIaGsmnTJnx8rnzCuNbOUVXLvLy8SEhIsL0/ffo04eHh13Us10pKmQQEV7Gq9xXKzwRmVrF8L5a5sC5fXoy1Y3YrSSkpyD/LH2lHSUs9SVFmJhTlIc0mSowaNJhxsSvAyb4QgUQC0iEPdJkIu2I0Rh0aozMahGVMQVuKWVuM2a4Qk30BWq2R4lw7Tmc1QGou4akrsv1qB6CR9VEFjdEJbakLUhgx2RcgNSY0JkeE9YHJHhA4IAEN0uiA2eRg6SA5FoBZC0YnRLE7Thl+2BtduOR4gVLnLIwaI1IKpBRgdLaMopjtrc8OCGmHlFqMSIwSMNshzQ6YzU6YzE6YpR0aWYoQZrTCAa3GCZPUUmICk+YSGoeLlo4eWrRSi9Q6YrLXgdYVe+GBHS6YjJcwmYuR4iJQgNBosLN3R6NpSPElSUmJEXtHOxq6OOHc0A37BvdglnYU5eZRejEDZ2cHPNxcsbPXUlhQTHHJRbQupQinEhwc3TE7NMDR3g1nXCgsMpKeW8SZ3Dx0GjvcnR1wdrLH0dURe2c7Ss0mLl3KoLCoiKL8S6DR4eLWFFcXRxzttDjYCRy0Ghy0JUgkpSZHJAKdgx0NHLXYazVohcAsJcVGM4UlRnKLSskrLsXRTktDJzuauDjSwLHyn/+CEiOXjGa0GoGjnQYne22lMsrV1buOlZ29BqNRdawUajyyVBsaNmxIfv6Vh6pzc3Nxd3dHp9Nx7Ngxdu7cecP7dHNzw93dne3btxMWFsbSpUvp1asXZrOZ1NRUIiIi6NGjB8uXL6egoICsrCz0ej16vZ4dO3Zw7NixajtWPXv2ZNmyZcTFxZGQkECTJk1wdXWtUKZ79+6sXLmS2NjYCgHvDz30EP/+979tAesbN27kjTfeuOFjvtOZTCUcP7aVE79vAuNvODhmIpwvgHUExM4OGt57hY0v6ZBSaxlNKHVBW9QECpwx2RdRaleIWYAJYenYlDbALD3B3BCNbICgEKG5CNIBrfDATrhiNpkwmc2gaYqdzhP7hu7YOzRAODnh2MgdOydXnIQDdiaJg1aDo70GB3sNjk722Dto0dhVvMyHBKGpftTSbJZorlKmfrgP6FDjrSr15iu5lptQG1S7VoPARavBxdGOZq5O1ZYt4+JoZxnWU25I/exYXVIdK6VuNG7cmO7du+Pn50f//v2JioqqsL5fv3589NFHGAwG2rdvT2hoaK3sd8mSJbbg9datW7No0SJMJhOxsbHk5uYipWTSpEk0atSIuLg44uPj0Wq1dOzYkf79+1db94wZMxg5ciQGgwGdTlfllAnvvvsuTz75JO+++y5DhgyxLffw8CAuLo6QkBAAXn75ZVsgu1LZl99Pwql4N04umaAx4ugG9hfvwb7gPoozO1JQ2hAjrki7Rtg3aopHi+Z46NyxLzbjoHXgnnatcXTRXX1HdUQIUfU9lZe5MzpVilI1cTvEZAYHB8u9e/deU9kZL8TTyNGOf74RdpNbpdyOjh49SocONf+FqNStqr43IUSilLKqS3r1zrWew9Z89j80aXAAMtuTnX8/Fxu3om3HQDp0vB+XBg5X3V5RlNtDdeevq45YCSEWAgOB81JKP+syD2qYl6u2XJISs7oUqChKPWRfOpKDP5zngdB7iZrog1arphJUlDvNtfyvXsxfObTKXE9erlohtWCJYFQURalfukZ6Exrdmj7DO6hOlaLcoa76P1tKuY3KM0fUKC9X7TTVSiPArDpWiqLUP43vcyGon7fKd6kod7Dr/clU07xctUbYCduUJoqiKIqiKLeT2h6LvqYcW3D9eba8hQZ1z5GiKIqiKLej6+1YnbPm4+Ia83JVIqVcIKUMllIGN23a9Jp33MskaG2vYhMURVEURbn9XG8PpSyXFlTOy/WEEMJRCNEKa16uG2tiRUYhsafq2aIV5WbLycnhww8/vO7t586dS2FhYZXrwsPDudZpR2pq2LBhGAwG3nnnnSuWGTFiBF988UWl5QkJCQwcOBCw/L97/vnnadu2LQaDgX379t2U9iqKotRXV+1YCSFWYElG2l4Icdqai2sW0FcIkQz0tb5HSnkYKMvL9T0V83LViiKRj1aAWd0ZqNSBm9mxulnOnj3LL7/8wsGDB5k0adIN1bVhwwaSk5NJTk5mwYIFPPvss7XUSkVRlDvDtdwVOExK6SmltJdSekkpP5FSZkkpe0sp21mfL5QrP1NK2UZK2V5KuaG6uq/HJWFCCxhLa7W/pijXZNq0aZw4cYKAgACmTJkCwOzZswkJCcFgMPDKK68AcPHiRaKiovD398fPz49Vq1Yxb9480tPTiYiIICIiotr9rFixAr1ej5+fny3hsclkYsSIEfj5+aHX622jT/PmzaNjx44YDAaeeKJyvuDIyEjOnz9PQEAA27dvJykpidDQUAwGA4MHD7aloynv+++/x8fHhx49evDVV1/Zlq9du5ann34aIQShoaHk5ORw5syZ6/swFUVR7kD1LqVNqcZEAwHGUrNKaaTccrNmzeLQoUMkJSUBltx4ycnJ7N69GyklgwYNYtu2bWRkZNC8eXPWrVsHWHIIurm58fbbbxMfH0+TJk2uuI/09HSmTp1KYmIi7u7uREZG8vXXX9OiRQvS0tI4dOgQYBk9K2tTSkoKjo6OtmXlffPNNwwcONDWZoPBwHvvvUevXr14+eWXefXVV5k7d66tfHFxMaNHj2bLli20bduWxx9/3LYuLS2NFi3+CqP08vIiLS0NT0/P6/g0FUVR7jz1r2P1wJeUZLXHVFq702Mp9dCGaXD219qt81499J91zcU3btzIxo0b6dSpEwAFBQUkJycTFhbG5MmTmTp1KgMHDiQs7NpTMO3Zs4fw8HDKbuqIiYlh27ZtxMXFcfLkSSZMmEBUVBSRkZGApaMUExNDdHQ00dHR1dadm5tLTk4OvXr1AmD48OEMHTq0Qpljx47RqlUr2rVrB0BsbCwLFiwAqo5tVHMyKYqi/KX+3V7X+Agmj+NcKlGXApW6J6Vk+vTpJCUlkZSUxPHjx3nmmWd44IEHSExMRK/XM336dF577bUa1VkVd3d3Dhw4QHh4OB988AGjRo0CYN26dYwfP57ExESCgoIwGo03fFxX6ix5eXmRmvrXVHWnT5+mefPmN7w/RVGUO0W9G7GSZnvQllBYfON/PJR6rgYjS7WlYcOG5Ofn294/9NBDxMXFERMTg4uLC2lpadjb22M0GvHw8CA2NhYXFxcWL15cYfvqLgV26dKFiRMnkpmZibu7OytWrGDChAlkZmbi4ODAkCFDaNOmDSNGjMBsNpOamkpERAQ9evRg+fLlFBQU0KhRoyrrdnNzw93dne3btxMWFsbSpUtto1dlfHx8SElJ4cSJE7Rp04YVK1bY1g0aNIj333+fJ554gl27duHm5qYuAyqKopRTDztWDkjtJYqKSuu6KcpdqHHjxnTv3h0/Pz/69+/P7NmzOXr0KF27dgXAxcWFzz77jOPHjzNlyhQ0Gg329vbMnz8fgDFjxtC/f388PT2Jj4+vch+enp688cYbREREIKVkwIABPPLIIxw4cICRI0diNltSD7zxxhuYTCZiY2PJzc1FSsmkSZOu2Kkqs2TJEsaOHUthYSGtW7dm0aJFFdY7OTmxYMECoqKiaNKkCT169LDFdQ0YMID169fTtm1bdDpdpW0VRVHuduJ2mA8qODhYXuv8Pd+vfRDXUmd03p8QEHzvTW6Zcrs5evQoHTp0qOtmKDVU1fcmhEiUUgbXUZNqVU3OYYqi1H/Vnb/qXYyVlPZI7SWKi9WIlaIoiqIot5d617Ey44BZW0JxwaW6boqiKIqiKEoF9a5jBQ5IzSWKL5bUdUMURVEURVEqqHcdKykcMWtLuFRUXNdNURRFURRFqaDedayE1hGz9hKXitWIlaIoiqIot5d617HSaJ2R2hKMJSrGSlEURVGU20u961hp7XUgJLK0qK6botyFcnJy+PDDD69r2wEDBlSZy+9KZsyYwZw5c65rX1czb948OnToQExMzBXLLF68mOeee67KdS4uLrbX/fr1o1GjRgwcOLDW26koilLf1LuOlYNjA8sLo+pYKbdedR0rk6n6NEvr16+/6uSdt8qHH37I+vXrWbZs2Q3XNWXKFJYuXVoLrVIURan/6mHHyvJLWUgVvK7cetOmTePEiRMEBAQwZcoUEhISiIiI4Mknn0Sv1wMQHR1NUFAQvr6+tuTFAN7e3mRmZnLq1Ck6dOjA6NGj8fX1JTIykqKi6n8oJCUlERoaisFgYPDgwWRnZwOWkaeOHTtiMBh44oknANi6dSsBAQEEBATQqVOnCil4AMaOHcvJkycZNGgQ77zzDhcuXCA6OhqDwUBoaCgHDx6stP+UlBS6du1KSEgIcXFxFdb17t2bhg0b1vzDVBRFuQPVu46Vs6PlBK5FdayUW2/WrFm0adOGpKQkZs+eDcDu3buZOXMmR44cAWDhwoUkJiayd+9e5s2bR1ZWVqV6kpOTGT9+PIcPH6ZRo0Z8+eWX1e736aef5s033+TgwYPo9XpeffVVW3v279/PwYMH+eijjwCYM2cOH3zwAUlJSWzfvh1nZ+cKdX300Uc0b96c+Ph4Jk2axCuvvEKnTp04ePAg//d//8fTTz9daf8TJ07k2WefZc+ePdx7r8p4oCiKciX1LlegTufGhSzQSBW8frd7c/ebHLtwrFbr9PHwYWrnqTXapnPnzrRq1cr2ft68eaxZswaA1NRUkpOTady4cYVtWrVqRUBAAABBQUGcOnXqivXn5uaSk5NjS5Y8fPhwhg4dCoDBYCAmJobo6Giio6MB6N69Oy+88AIxMTE8+uijeHl5Vdv+n376ydaxe/DBB8nKyiI3N7dCmZ9//tlW5qmnnmLq1Jp9RoqiKHeL+jdi5dQIAK1Q0y0ot4cGDRrYXickJLBp0yZ27NjBgQMH6NSpE8XFlUdXHR0dba+1Wi1Go/G69r1u3TrGjx9PYmIiQUFBGI1Gpk2bxscff0xRURGhoaEcO1Z957OqfKFCiGtapiiKolRU/0asnD0AsBMqV+DdrqYjS7WhYcOGlWKWysvNzcXd3R2dTsexY8fYuXPnDe/Tzc0Nd3d3tm/fTlhYGEuXLqVXr16YzWZSU1OJiIigR48eLF++nIKCArKystDr9ej1enbs2MGxY8fw8fG5Yv09e/Zk2bJlxMXFkZCQQJMmTXB1da1Qpnv37qxcuZLY2NhaCXhXFEW5U9W7jpWjo+WEr9WoS4HKrde4cWO6d++On58f/fv3JyoqqsL6fv368dFHH2EwGGjfvj2hoaG1st8lS5YwduxYCgsLad26NYsWLcJkMhEbG0tubi5SSiZNmkSjRo2Ii4sjPj4erVZLx44d6d+/f7V1z5gxg5EjR2IwGNDpdCxZsqRSmXfffZcnn3ySd999lyFDhlRYFxYWxrFjxygoKMDLy4tPPvmEhx56qFaOW1EUpb4RVV0GuNWCg4Pl3r17r6lsYWEKO3b2wXRwOJH/fPkmt0y53Rw9epQOHTrUdTOUGqrqexNCJEopg+uoSbWqJucwRVHqv+rOX/Uuxkqr1Vmf1YiVotyNhBCnhBC/CiGShBB7rcs8hBA/CiGSrc/u5cpPF0IcF0L8JoR4qNzyIGs9x4UQ84Q1iEwI4SiEWGVdvksI4X3LD1JRlHqr3nWsNBpn67PqWCnKXSxCShlQ7hfjNGCzlLIdsNn6HiFER+AJwBfoB3wohNBat5kPjAHaWR/9rMufAbKllG2Bd4A3b8HxKIpyh6h3HSut1tn6rILXFUWxeQQoCw5bAkSXW75SSlkipUwBjgOdhRCegKuUcoe0xEN8etk2ZXV9AfQW6pZIRVGuUb3rWGk09mDWINSlQEW5W0lgoxAiUQgxxrrsHinlGQDrczPr8vuA1HLbnrYuu8/6+vLlFbaRUhqBXKDiRGSKoihXUO/uCgTA5IhQI1aKcrfqLqVMF0I0A34UQlQ3UVdVI02ymuXVbVOxYkunbgzA/fffX32LFUW5a9S7ESsATA5qxEpR7lJSynTr83lgDdAZOGe9vIf1+by1+GmgRbnNvYB063KvKpZX2EYIYQe4AReqaMcCKWWwlDK4adOmtXNwiqLUe/WyYyXN9qpjpdSJnJwcPvzww+vefu7cuRQWFla5Ljw8nJt1y/6wYcMwGAy88847VywzYsQIvvjii0rLExISGDhwIADHjh2ja9euODo6MmfOnJvS1uoIIRoIIRqWvQYigUPAN8Bwa7HhwFrr62+AJ6x3+rXCEqS+23q5MF8IEWqNn3r6sm3K6noM2CJvh3lpFEWpF+rlpUBpcgBtCdIsERoVU6rcOmUdq3Hjxl3X9nPnziU2NhadTlfLLbuys2fP8ssvv/DHH3/ccF0eHh7MmzePr7/++sYbdn3uAdZYY8ntgOVSyu+FEHuA1UKIZ4A/gaEAUsrDQojVwBHACIyXUpqsdT0LLAacgQ3WB8AnwFIhxHEsI1VP3IoDUxTlzlBPR6wckNpLmIzmum6KcpeZNm0aJ06cICAggClTpgAwe/ZsQkJCMBgMvPLKKwBcvHiRqKgo/P398fPzY9WqVcybN4/09HQiIiKIiIiodj8rVqxAr9fj5+dnS3hsMpkYMWIEfn5+6PV62+jTvHnz6NixIwaDgSeeqNwHiIyM5Pz58wQEBLB9+3aSkpIIDQ3FYDAwePBgsrOzK23z/fff4+PjQ48ePfjqq69sy5s1a0ZISAj29vbX9wHeICnlSSmlv/XhK6WcaV2eJaXsLaVsZ32+UG6bmVLKNlLK9lLKDeWW75VS+lnXPVc2KiWlLJZSDpVStpVSdpZSnrz1R6ooSn1VP0eszJYRq6LiEho63Lpf/ooya9YsDh06RFJSEgAbN24kOTmZ3bt3I6Vk0KBBbNu2jYyMDJo3b866desASw5BNzc33n77beLj42nSpMkV95Gens7UqVNJTEzE3d2dyMhIvv76a1q0aEFaWhqHDh0CLKNnZW1KSUnB0dHRtqy8b775hoEDB9rabDAYeO+99+jVqxcvv/wyr776KnPnzrWVLy4uZvTo0WzZsoW2bdvy+OOP3/DnpiiKcreonx0raY+0KyI3P5+Grqpjdbc6+3//R8nR6m4IqznHDj7c++9/X3P5jRs3snHjRjp16gRAQUEBycnJhIWFMXnyZKZOncrAgQMJCwu75jr37NlDeHg4ZQHRMTExbNu2jbi4OE6ePMmECROIiooiMjISsHSUYmJiiI6OJjo6utq6c3NzycnJoVevXgAMHz6coUOHVihz7NgxWrVqRbt27QCIjY1lwYIF19x+RVGUu1m97FiZpSNm7SWycy/idd/VyyvKzSKlZPr06fzjH/+otC4xMZH169czffp0IiMjefnla8tteaU4aXd3dw4cOMAPP/zABx98wOrVq1m4cCHr1q1j27ZtfPPNN7z++uscPnwYO7sb+6+t5sNU7ialpaWcPn2a4uLium6KcptxcnLCy8urRuEP9bJjJXFAakvIyy2o66YodagmI0u1pWHDhuTn59veP/TQQ8TFxRETE4OLiwtpaWnY29tjNBrx8PAgNjYWFxcXFi9eXGH76i4FdunShYkTJ5KZmYm7uzsrVqxgwoQJZGZm4uDgwJAhQ2jTpg0jRozAbDaTmppKREQEPXr0YPny5RQUFNCoUaMq63Zzc8Pd3Z3t27cTFhbG0qVLbaNXZXx8fEhJSeHEiRO0adOGFStW3PDnpii3s9OnT9OwYUO8vb3VjwrFRkpJVlYWp0+fplWrVte8XT3tWDlh1pZQVFBU101R7jKNGzeme/fu+Pn50b9/f2bPns3Ro0fp2rUrAC4uLnz22WccP36cKVOmoNFosLe3Z/78+QCMGTOG/v374+npSXx8fJX78PT05I033iAiIgIpJQMGDOCRRx7hwIEDjBw5ErPZctPGG2+8gclkIjY2ltzcXKSUTJo06YqdqjJLlixh7NixFBYW0rp1axYtWlRhvZOTEwsWLCAqKoomTZrQo0cPW1zX2bNnCQ4OJi8vD41Gw9y5czly5Aiurq438rEqSp0qLi5WnSqlEiEEjRs3JiMjo2bb3Q7TswQHB8uazN/z3ZfP4uy2maKLHzPwkZ43sWXK7ebo0aN06NChrpuh1FBV35sQIrFcEuV6rabnMOX2os4rSnVqev6ql9MtCI0ONCZKiqqeaFFRFEVRFKUu1MuOlVbrDIBRdawURVGUeu5GMjoMGDCgymlW7mS1lUlCSsnzzz9P27ZtMRgM7Nu3r1baVy9jrLT2DSwvjKpjpSiKotRv1WV0MJlMaLXaK267fv36m9m06yalREqJRlO74ze1mUliw4YNJCcnk5yczK5du3j22WfZtWvXDdd7Q0cshJgkhDgshDgkhFghhHASQngIIX4UQiRbn91vuJWXsXdsCIA0qVtjFUVRlPrt8owOCQkJRERE8OSTT6LX6wGIjo4mKCgIX1/fCvPKeXt7k5mZyalTp+jQoQOjR4/G19eXyMhIiooq3+D17bff0qVLFzp16kSfPn04d+4cYJmDb+TIkej1egwGA19++SVgycIQGBiIv78/vXv3BmDGjBkVcoX6+flx6tQpWxvGjRtHYGAgqampPPvsswQHB+Pr62vLTAGW+fq6deuGv78/nTt3Jj8/n7CwMNtExgDdu3fn4MGDFdpfm5kk1q5dy9NPP40QgtDQUHJycjhz5sw1f29Xct0jVkKI+4DngY5SyiJrPq4ngI7AZinlLCHENGAaMPWGW1qOs5MrBcUgzKpjpSiKotSeV789zJH0vFqts2NzV1552PeK6y/P6JCQkMDu3bs5dOiQ7Tb/hQsX4uHhQVFRESEhIQwZMoTGjRtXqCc5OZkVK1bw3//+l7/97W98+eWXxMbGVijTo0cPdu7ciRCCjz/+mLfeeov//Oc/vP7667i5ufHrr78CkJ2dTUZGBqNHj2bbtm20atWKCxcucDW//fYbixYtsl3anDlzJh4eHphMJnr37s3Bgwfx8fHh8ccfZ9WqVYSEhJCXl4ezszOjRo1i8eLFzJ07l99//52SkhIMBkOF+mszk0RaWhotWrSwvffy8iItLQ1PT8+rHmd1bnSMzg5wFkLYATogHXgEWGJdvwSIvsF9VOLcoBEAGvOl2q5aURRFUepc586dK8ydNG/ePPz9/QkNDSU1NZXk5ORK27Rq1YqAgAAAgoKCOHXqVKUyp0+f5qGHHkKv1zN79mwOHz4MwKZNmxg/frytnLu7Ozt37qRnz562dnh4eFy13S1btiQ0NNT2fvXq1QQGBtKpUycOHz7MkSNH+O233/D09CQkJAQAV1dX7OzsGDp0KN999x2lpaUsXLiQESNGVLuvqjJJbNu2rUKZ8pkkhBAVOppVzYpQG1NuXPeIlZQyTQgxB0sm+SJgo5RyoxDiHinlGWuZM0KIZjfcysvoXNwgC7SU1HbVilKtnJwcli9fXmUsxNUMGDCA5cuXX3WeqTIzZszAxcWFyZMn13hfVzNv3jzmz59PYGAgy5Ytq7LM4sWL2bt3L++//36ldS4uLhQUFJCUlMSzzz5LXl4eWq2WF198UeUWVOq16kaWbqUGDRrYXickJLBp0yZ27NiBTqcjPDy8ylniHR0dba+1Wm2VlwInTJjACy+8wKBBg0hISGDGjBmApZNxeaeiqmUAdnZ2tvn0gAptKd/ulJQU5syZw549e3B3d2fEiBEUFxdfsV6dTkffvn1Zu3Ytq1evpramMLlSZ8nLy4vU1FTb+9OnT9O8efMb3t91j1hZY6ceAVoBzYEGQojY6reqsP0YIcReIcTemk6+pXOxTEaolaU12k5RblR1d++YTKZqt12/fv01d6putg8//JD169dfsVN1rXQ6HZ9++imHDx/m+++/55///Oddd4eSotyoyzM6XC43Nxd3d3d0Oh3Hjh1j586d172v3Nxc7rvPkgtuyZIltuWRkZEVfkRlZ2fTtWtXtm7dSkpKCoDtUqC3t7ftDrp9+/bZ1l8uLy+PBg0a4Obmxrlz59iwYQNgye6Qnp7Onj17AMjPz8doNAIwatQonn/+eUJCQq46QlY+kwRw1UwSQIVMEoMGDeLTTz9FSsnOnTtxc3O74cuAcGOXAvsAKVLKDCllKfAV0A04J4TwBLA+n69qYynlAillsJQyuCzZ7LVqoHMBQCtUx0q5tW5lkGl5VwrQnDdvHh07dsRgMPDEE08AsHXrVgICAggICKBTp06VTthjx47l5MmTDBo0iHfeeYcLFy4QHR2NwWAgNDS0UrAoWH55du3alZCQEOLi4mzLH3jgAVuy5ubNm9OsWbMaz1KsKHe78hkdpkyZUml9v379MBqNGAwG4uLiKlxqq6kZM2YwdOhQwsLCKqTWeumll8jOzsbPzw9/f3/i4+Np2rQpCxYs4NFHH8Xf3982Gj1kyBAuXLhAQEAA8+fP54EHHqhyX/7+/nTq1AlfX1/+/ve/0717dwAcHBxYtWoVEyZMwN/fn759+9pGvYKCgnB1dWXkyJHXdDxLlixhypQpGAwGkpKSKuVkLZ9JokePHrRs2dK2bsCAAbRu3Zq2bdsyevTo657yopKyWyJr+gC6AIexxFYJLPFUE4DZwDRrmWnAW1erKygoSNZEUVGa3LS5tVz3wdQabafUf0eOHKnT/aekpEhfX1/b+/j4eKnT6eTJkydty7KysqSUUhYWFkpfX1+ZmZkppZSyZcuWMiMjQ6akpEitViv3798vpZRy6NChcunSpZX29corr8jZs2dLKaXU6/UyISFBSillXFycnDhxopRSSk9PT1lcXCyllDI7O1tKKeXAgQPlTz/9JKWUMj8/X5aWllaqu6wtUkr53HPPyRkzZkgppdy8ebP09/eXUkq5aNEiOX78eCmllA8//LBcsmSJlFLK999/XzZo0KBSnbt27ZI+Pj7SZDJVWlfV9wbsldd5/rndHjU9hym3l7o+ryh/SUtLk+3atavyPFJXanr+upEYq11CiC+AfYAR2A8sAFyA1UKIZ7DEXw293n1cSdkEoXZqxOqutn3172Sm1m4i7iYtXAj7W9W/vq6kqiDTNWvWANiCTC+/e+dagkzLVBWgOXSo5b+VwWAgJiaG6OhooqOjAcstyi+88AIxMTE8+uijeHl5Vdv+n376yXZr9YMPPkhWVha5ubkVyvz888+2Mk899RRTp1a80ffMmTM89dRTLFmypNbnrVEU5e7w6aef8uKLL/L222/X6/PIDbVcSvmKlNJHSuknpXxKSlkipcySUvaWUrazPl/9/swa0mh0AGi1qmOl1L0rBZkeOHCATp06XVOQaVl8QU2tW7eO8ePHk5iYSFBQEEajkWnTpvHxxx9TVFREaGgox44dq7YOeY13xlwpADQvL4+oqCj+93//94YuUSiKcnd7+umnSU1Ntf1wrK/q5czrGo0DSIHQXN8fI+XOUNORpdpwK4NMy5QP0AwLC7MFaJrNZlJTU4mIiKBHjx4sX76cgoICsrKy0Ov16PV6duzYwbFjx/Dx8bli/T179mTZsmXExcWRkJBAkyZNcHV1rVCme/furFy5ktjY2AoB75cuXWLw4ME8/fTT9f5kqCiKUhvqZcdKCAEmB7QaNY+VcmuVDzLt378/UVFRFdb369ePjz76CIPBQPv27WttBGfJkiWMHTuWwsJCWrduzaJFizCZTMTGxpKbm4uUkkmTJtGoUSPi4uKIj49Hq9XSsWNH+vfvX23dM2bMYOTIkRgMBnQ6XYU7hcq8++67PPnkk7z77rsMGTLEtnz16tVs27aNrKwsFi9eDFimaSi7zKkoinK3EVVdBrjVgoODZU3nq9iyIQC78/70HF75j4By5zp69CgdOnSo62YoNVTV9yaESJRSBtdRk2rV9ZzDlNuHOq8o1anp+aveRodJkyNCW1plfIiiKIqiKEpdqLcdK0z2CO0lLpZerOuWKIqiKMp1q27i4Wsxd+5cCgsLa7FFt4+MjAxb0uiyiUCrUjZP4OXKJ4y+cOECffv2pV27dvTt27fKhM21od52rKTZEbSXyCrOquumKIqiKMp1uxM6Vtd7Z/PVbN68GR8fH/bv309YWNgN1TVr1ix69+5NcnIyvXv3ZtasWbXUyorqb8fK5IDQlJJZVLmHqiiKoij1xeUZHQBmz55NSEgIBoOBV155BYCLFy8SFRWFv78/fn5+rFq1innz5pGenk5ERAQRERGV6n7ttdcICQnBz8+PMWPG2MJnjh8/Tp8+ffD39ycwMNCW8uWtt95Cr9fj7+/PtGnTAAgPD7fl7cvMzMTb2xuw3KgydOhQHn74YSIjIykoKKB3794EBgai1+tZu3atrR2ffvopBoMBf39/nnrqKfLz82nVqhWlpZZpk/Ly8vD29ra9B0vGiX/961+sX7+egIAAioqKWLFiBXq9Hj8/v0rz6ZWZOXMm7du3p0+fPvz222+25WvXrmX48OGAZT7Ar7/+umZf1DWql3cFApjMjkiHPC7kZcI9dd0aRVEU5Y6wYRqc/bV267xXD/2vPDoya9YsDh06RFJSEgAbN24kOTmZ3bt3I6Vk0KBBbNu2jYyMDJo3b866desAy/Qubm5uvP3228THx1dIUVPmueees6V5eeqpp/juu+94+OGHiYmJYdq0aQwePJji4mLMZjMbNmzg66+/ZteuXeh0OltuwOrs2LGDgwcP4uHhgdFoZM2aNbi6upKZmUloaCiDBg3iyJEjzJw5k59//pkmTZpw4cIFGjZsSHh4OOvWrSM6OpqVK1cyZMgQ7O3tbXUHBATw2muv2ZLBp6enM3XqVBITE3F3dycyMpKvv/7aNjkyQGJiIitXrmT//v0YjUYCAwMJCgoC4Ny5c7ZcgJ6enpw/X2XGvRtWb0esTGZHpOYS+Rdy6ropiqIoilJrNm7cyMaNG+nUqROBgYEcO3aM5ORk9Ho9mzZtYurUqWzfvh03N7er1hUfH0+XLl3Q6/Vs2bKFw4cPk5+fT1paGoMHDwYs+fR0Oh2bNm1i5MiR6HSWSbivlgQZoG/fvrZyUkr+/e9/YzAY6NOnD2lpaZw7d44tW7bw2GOP2Tp+ZeVHjRrFokWLAFi0aNFV8wPu2bOH8PBwmjZtip2dHTExMWzbtq1Cme3btzN48GB0Oh2urq4MGjToqsdQ2+rtiJVZOGPWllCYVbspTRSlOjk5OSxfvpxx48Zd1/Zz585lzJgxthNXeeHh4cyZM4fg4NqfgWDYsGEcPnyYkSNHMmnSpCrLjBgxgoEDB/LYY49VWJ6QkMCcOXP47rvvWLZsGW+++SYALi4uzJ8/H39//1pvr6LUmWpGlm4VKSXTp0/nH//4R6V1iYmJrF+/nunTpxMZGVkp6XB5xcXFjBs3jr1799KiRQtmzJhBcXHxFe+ml1JWmWHBzs4Os9lsq7O88pknli1bRkZGBomJidjb2+Pt7W3bX1X1du/enVOnTrF161ZMJhN+fn5XPJay9l2LK2WJuOeeezhz5gyenp6cOXOGZs2aXVN9NVVvR6zsnBpitiuhMF2ltVFunfoYZHr27Fl++eUXDh48eMVO1bVq1aoVW7du5eDBg8TFxTFmzJhaaqWi3L0uz+jw0EMPsXDhQgoKLAMHaWlpnD9/nvT0dHQ6HbGxsUyePJl9+/ZVuX2Zsk5QkyZNKCgo4IsvvgDA1dUVLy8vW4xRSUkJhYWFREZGsnDhQts5quxSoLe3N4mJiQC2OqqSm5tLs2bNsLe3Jz4+nj/++AOA3r17s3r1arKysirUC5Y0NsOGDbvqaBVAly5d2Lp1K5mZmZhMJlasWGHLoVqmZ8+erFmzhqKiIvLz8/n2229t6wYNGmSbAHnJkiU88sgjV93n9ai3HStX9+aY7S9il2Oq66Yod5GbGWRaXlUBmiaTiREjRuDn54der+edd94BLEmfO3bsiMFg4IknnqhUV2RkJOfPnycgIIDt27eTlJREaGgoBoOBwYMHV3nL8ffff4+Pjw89evTgq6++si3v1q0b7u7uAISGhnL69Onr+BQVRSmvfEaHKVOmEBkZyZNPPknXrl3R6/U89thj5Ofn8+uvv9K5c2cCAgKYOXMmL730EgBjxoyhf//+lc4rjRo1YvTo0ej1eqKjowkJCbGtW7p0KfPmzcNgMNCtWzfOnj1Lv379GDRoEMHBwQQEBNimKZg8eTLz58+nW7duVU5pUCYmJoa9e/cSHBzMsmXLbKm0fH19efHFF+nVqxf+/v688MILFbbJzs5m2LBhV/2cPD09eeONN4iIiLAF3V/eOQoMDOTxxx8nICCAIUOGVLiTcNq0afz444+0a9eOH3/80RacX+uklHX+CAoKkjV14vfP5abNreXqN+fUeFul/jpy5Eid7j8lJUX6+vra3v/www9y9OjR0mw2S5PJJKOiouTWrVvlF198IUeNGmUrl5OTI6WUsmXLljIjI6PKunv16iX37Nkj09LSZIsWLeT58+dlaWmpjIiIkGvWrJF79+6Vffr0sZXPzs6WUkrp6ekpi4uLKyyrrs16vV4mJCRIKaWMi4uTEydOlFJKOXz4cPn555/LoqIi6eXlJX///XdpNpvl0KFDZVRUVKV6Z8+eLZ955plr+NSq/t6AvfI2OP/UxuN6zmHK7aOuzyt3s88//1zGxsbWdTOqVdPzV72NsfJo0oqUVHC1uzMnRVOuLn7xAs7/cbJW62zWsjURI6798lb5IFOAgoICkpOTCQsLY/LkyUydOpWBAwfWaP6V8gGagC1AMy4ujpMnTzJhwgSioqKIjIwEwGAwEBMTQ3R0dIW7Y6qSm5tLTk6Obfh8+PDhlZInHzt2jFatWtGuXTsAYmNjWbBgQYUy8fHxfPLJJ/z000/XfFyKoijlTZgwgQ0bNrB+/fq6bkqtqreXAp119wOgc7xIyU2amExRrkZag0yTkpJISkri+PHjPPPMMzzwwAMkJiai1+uZPn06r732Wo3qrIq7uzsHDhwgPDycDz74gFGjRgGwbt06xo8fT2JiIkFBQbUyUd+Vgj8BDh48yKhRo1i7di2NGze+4X3VlBBCK4TYL4T4zvreQwjxoxAi2frsXq7sdCHEcSHEb0KIh8otDxJC/GpdN09YD1gI4SiEWGVdvksI4X3LD1BR7hLvvfcex48f54EHHqjrptSqejti5eDQBEz22Dvlsu/0abpaJyxT7h41GVmqLVUFmcbFxRETE4OLiwtpaWnY29tjNBrx8PAgNjYWFxcXFi9eXGH7quabKdOlSxcmTpxIZmYm7u7urFixggkTJpCZmYmDgwNDhgyhTZs2jBgxArPZTGpqKhEREfTo0YPly5dTUFBAo0aNqqzbzc0Nd3d3tm/fTlhYGEuXLq0U/Onj40NKSgonTpygTZs2rFixwrbuzz//5NFHH2Xp0qV1eTKcCBwFXK3vpwGbpZSzhBDTrO+nCiE6Ak8AvkBzYJMQ4gEppQmYD4wBdgLrgX7ABuAZIFtK2VYI8QTwJvD4rTs0RVHqu3rbsRJCQElj0GWx988/VcdKuSXKB5n279+f2bNnc/ToUbp27QpYpiD47LPPOH78OFOmTEGj0WBvb8/8+fOBv4JMPT09iY+Pr3If5QM0pZQMGDCARx55hAMHDjBy5Ejbbc9vvPEGJpOJ2NhYcnNzkVIyadKkK3aqyixZsoSxY8dSWFhI69atbfPIlHFycmLBggVERUXRpEkTevTowaFDhwDLLM5ZWVm26Sbs7OxsMzLfCkIILyAKmAmURcA+AoRbXy8BEoCp1uUrpZQlQIoQ4jjQWQhxCnCVUu6w1vkpEI2lY/UIMMNa1xfA+0IIIa80jKgoinIZcTucL4KDg+X1nJwTvhmMnTmXL/LGMPfpyndDKXeeo0eP0qFDh7puhlJDVX1vQohEKWWNJu0SQnwBvAE0BCZLKQcKIXKklI3KlcmWUroLId4HdkopP7Mu/wRL5+kUMEtK2ce6PAyYaq3rENBPSnnauu4E0EVKWW3urOs9hym3B3VeUapT0/NXvY2xArC386TUOZNLZy/WdVMURbnJhBADgfNSysRr3aSKZbKa5dVtU1V7xggh9goh9mZkZFxjkxRFudPV646Vi0tLzPYXcSss4pLRXNfNURTl5uoODLJeylsJPCiE+Aw4J4TwBLA+lyUAOw20KLe9F5BuXe5VxfIK2wgh7AA3oMqEaVLKBVLKYCllcNkdnIpyPW5k4uEBAwaQk5NTuw26zQ0bNgyDwWCby68qI0aMqHIy04SEBAYOHAhY7oDu2rUrjo6Otjm7akO97lh5NLPcDu7lUMTv5yrPOqsoyp1DSjldSuklpfTGEpS+RUoZC3wDDLcWGw6stb7+BnjCeqdfK6AdsFtKeQbIF0KEWu8GfPqybcrqesy6j7qPl1DuaNV1rEym6ifBXr9+/VXjKuuClNIWD1qbajOThIeHB/PmzWPy5Mm11DqLet2xcm3WGoBmjoX8kaXms1KUu9QsoK8QIhnoa32PlPIwsBo4AnwPjLfeEQjwLPAxcBw4gSX2CuAToLE10P0FLHcYKspNdXlGh4SEBCIiInjyySfR6/UAREdHExQUhK+vb4V55by9vcnMzOTUqVN06NCB0aNH4+vrS2RkJEVFRZX29e2339KlSxc6depEnz59OHfuHGCZg2/kyJHo9XoMBgNffvklYMnCEBgYiL+/P7179wZgxowZFUZ4/Pz8OHXqlK0N48aNIzAwkNTUVJ599lmCg4Px9fW1ZaYAy3x93bp1w9/fn86dO5Ofn09YWBhJSUm2Mt27d+fgwYMV2l+bmSSaNWtGSEgI9vb21/xdXYt6e1cggK5hSwBcnfJJza38D0hRlDuTlDIBy91/SCmzgN5XKDcTyx2Ely/fC1TK+CqlLAaGXr5cuXu8uftNjl04Vqt1+nj4MLXz1CuunzVrFocOHbJ1KhISEti9ezeHDh2iVatWACxcuBAPDw+KiooICQlhyJAhleaRS05OZsWKFfz3v//lb3/7G19++SWxsbEVyvTo0YOdO3cihODjjz/mrbfe4j//+Q+vv/46bm5u/PrrrwBkZ2eTkZHB6NGj2bZtG61ataqQ4+9KfvvtNxYtWmQbgZs5cyYeHh6YTCZ69+7NwYMH8fHx4fHHH2fVqlWEhISQl5eHs7Mzo0aNYvHixcydO5fff/+dkpISDAZDhfq/+eYbBg4caPusDAYD7733Hr169eLll1/m1VdfZe7cubbyxcXFjB49mi1bttC2bVsef/zmz55Sr0es7OwaIYyOODrlkp5TfPUNFEVRFKUe6Ny5s61TBZacoP7+/oSGhpKamkpycnKlbVq1akVAQAAAQUFBnDp1qlKZ06dP89BDD6HX65k9ezaHDx8GYNOmTYwfP95Wzt3dnZ07d9KzZ09bOzw8PK7a7pYtWxIaGmp7v3r1agIDA+nUqROHDx/myJEj/Pbbb3h6etpyF7q6umJnZ8fQoUP57rvvKC0tZeHChYwYMaLafVWVSWLbtm0VypTPJCGEqNTRvBnq9YiVEAJR3Bit7gJnstWlQOXmy8nJYfny5bZ5nGpiwIABLF++/JrjIWbMmIGLi0utX/8Hy0l6/vz5BAYGsmzZsirLLF68mL179/L+++9XWufi4kJBQQF//PEHjz76KCaTidLSUiZMmMDYsWNrvb2KcqtUN7J0KzVo0MD2OiEhgU2bNrFjxw50Oh3h4eEUF1ceTHB0dLS91mq1VV4KnDBhAi+88AKDBg0iISGBGTNmAJaYqMszLlS1DCzz15WPnyrflvLtTklJYc6cOezZswd3d3dGjBhBcXHxFevV6XT07duXtWvXsnr16lqbI6+6TBI3Q70esQKgtDFSl8mFzLy6bolyF7hTgkw//PBD1q9ff8VO1bXy9PTkl19+ISkpiV27djFr1izS09OvvqGiKDaXZ3S4XG5uLu7u7uh0Oo4dO8bOnTuve1+5ubncd999gGWy4DKRkZEVfkRlZ2fTtWtXtm7dSkpKCoDtUqC3tzf79u0DYN++fbb1l8vLy6NBgwa4ublx7tw5NmywhDL6+PiQnp7Onj17AMjPz7el4ho1ahTPP/88ISEhVx0hK59JArhqJgmgQiaJm6Xed6wEjTE6Z1Kck1vXTVHuArcyyLS8KwVozps3j44dO2IwGHjiCcskuVu3biUgIICAgAA6depU6YQ9duxYTp48yaBBg3jnnXe4cOEC0dHRGAwGQkNDKwWLguWXZ9euXQkJCSEuLs623MHBwfYruaSk5KbcBaQod7ryGR2mTJlSaX2/fv0wGo0YDAbi4uIqXGqrqRkzZjB06FDCwsIqpNZ66aWXyM7Oxs/PD39/f+Lj42natCkLFizg0Ucfxd/f3xafNGTIEC5cuEBAQADz58+/Ynorf39/OnXqhK+vL3//+9/p3r07YDlvrFq1igkTJuDv70/fvn1to15BQUG4uroycuTIazqeJUuWMGXKFAwGA0lJSbz88ssV1pfPJNGjRw9atmxpW3f27Fm8vLx4++23+d///V+8vLzIy6uFQRopZZ0/goKC5PX6ad2/5abNreXY6YtkSanpuutR6ocjR47U6f5TUlKkr6+v7X18fLzU6XTy5MmTtmVZWVlSSikLCwulr6+vzMzMlFJK2bJlS5mRkSFTUlKkVquV+/fvl1JKOXToULl06dJK+3rllVfk7NmzpZRS6vV6mZCQIKWUMi4uTk6cOFFKKaWnp6csLi6WUkqZnZ0tpZRy4MCB8qeffpJSSpmfny9LS0sr1V3WFimlfO655+SMGTOklFJu3rxZ+vv7SymlXLRokRw/fryUUsqHH35YLlmyREop5fvvvy8bNGhgq+vPP/+Uer1eOjs7y/fff7/Kz62q7w3YK2+D809tPG7kHKbUvbo+ryh/SUtLk+3atZMm0+3z97ym5696HWMF0NDNm+ISaOeUw7m8Ylp46Oq6ScotkvPtCS6l1+6s+w7NG9Do4TY12qaqINM1a9YA2IJML79751qCTMtUFaA5dKjlxjWDwUBMTAzR0dFER0cDlluUX3jhBWJiYnj00Ufx8vK6UtUA/PTTT7Zbqx988EGysrLIza04Avzzzz/byjz11FNMnfpXHEqLFi04ePAg6enpREdH89hjj3HPPfdUu09FUZTLffrpp7z44ou8/fbbaDT194Ja/W25lbd3BAC+TU+RnqOmXFBuvSsFmR44cIBOnTpdU5BpWXxBTa1bt47x48eTmJhIUFAQRqORadOm8fHHH1NUVERoaCjHjlV/67isYv7LqoI9rxYA2rx5c3x9fW3xDoqiKDXx9NNPk5qaavvhWF/V+xEr1/vaIne1QndfIhf+yIHWja+6jXJnqOnIUm24lUGmZcoHaIaFhdkCNM1mM6mpqURERNCjRw+WL19OQUEBWVlZ6PV69Ho9O3bs4NixY/j4+Fyx/p49e7Js2TLi4uJISEigSZMmuLq6VijTvXt3Vq5cSWxsbIWA99OnT9O4cWOcnZ3Jzs7m559/5oUXXrjhY1YURamv6n3HCuCsuQOeLuvx+HUPRNz6P7bK3aN8kGn//v2JioqqsL5fv3589NFHGAwG2rdvf0NBpuUtWbKEsWPHUlhYSOvWrVm0aBEmk4nY2Fhyc3ORUjJp0iQaNWpEXFwc8fHxaLVaOnbsSP/+/aute8aMGYwcORKDwYBOp6twp1CZd999lyeffJJ3332XIUOG2JYfPXqU//mf/0EIgZSSyZMn24L4FUVR7kaiqssAt1pwcLC8kfkq1u74kgYXp+PxRyQdw17H6QH3Wmydcjs5evQoHTp0qOtmKDVU1fcmhEiUUgbXUZNq1Y2ew5S6pc4rSnVqev6q9zFWAJ18OnPxbEdyPXdSsEfNoaMoiqIoSt24IzpWLRp5cSa9I2anbLIv7KkyGFdRFEVRbkfVTTx8LebOnUth4Z2ZfSQjI8OWNLq6G2PK5gm8XPmE0Z9//jm+vr5oNJpam9W9KndEx0oIwemC+5FGB/Ia/ozpgsobqCiKotQPd0LH6nrvbL6azZs34+Pjw/79+wkLC7uhuvz8/Pjqq6/o2bNnLbWuandExwrA7OpMSWY7Cj2OUfKHSm+jKIqi1A+XZ3QAmD17NiEhIRgMBl555RUALl68SFRUFP7+/vj5+bFq1SrmzZtHeno6ERERREREVKr7tddeIyQkBD8/P8aMGWO7onP8+HH69OmDv78/gYGBtpQvb731Fnq9Hn9/f6ZNmwZAeHi4bYQnMzMTb29vwJJPdOjQoTz88MNERkZSUFBA7969CQwMRK/Xs3btWls7Pv30UwwGA/7+/jz11FPk5+fTqlUrSktLAUv6G29vb9t7sGSc+Ne//sX69esJCAigqKiIFStWoNfr8fPzqzCfXnkzZ86kffv29OnTh99++822vEOHDrRv377mX1AN3RF3BQJ43NuYvPPtcbr3Ky7++ScNAtUEhYqiKErNnP2//6PkaPVzv9WUYwcf7v33v6+4ftasWRw6dIikpCQANm7cSHJyMrt370ZKyaBBg9i2bRsZGRk0b96cdevWAZbpXdzc3Hj77beJj4+vkKKmzHPPPWdL8/LUU0/x3Xff8fDDDxMTE8O0adMYPHgwxcXFmM1mNmzYwNdff82uXbvQ6XS23IDV2bFjBwcPHsTDwwOj0ciaNWtwdXUlMzOT0NBQBg0axJEjR5g5cyY///wzTZo04cKFCzRs2JDw8HDWrVtHdHQ0K1euZMiQIdjb29vqDggI4LXXXrMlg09PT2fq1KkkJibi7u5OZGQkX3/9tW1yZIDExERWrlzJ/v37MRqNBAYGEhQUdC1fU625Y0asOrRtR1FGOwByLuyp49YoiqIoyvXZuHEjGzdupFOnTgQGBnLs2DGSk5PR6/Vs2rSJqVOnsn37dtzc3K5aV3x8PF26dEGv17NlyxYOHz5Mfn4+aWlpDB48GLDk09PpdGzatImRI0ei01kymFwtCTJA3759beWklPz73//GYDDQp08f0tLSOHfuHFu2bOGxxx6zdfzKyo8aNYpFixYBsGjRoqvmB9yzZw/h4eE0bdoUOzs7YmJi2LZtW4Uy27dvZ/Dgweh0OlxdXRk0aNBVj6G23dCIlRCiEfAx4AdI4O/Ab8AqwBs4BfxNSpl9I/u5Fr5t7uN4TnOkyZ58cRBzYSkanf3VN1SUGsjJyWH58uWMGzfuurafO3cuY8aMsZ24ygsPD2fOnDkEB9f+DATDhg3j8OHDjBw5kkmTJlVZZsSIEQwcOJDHHnuswvKEhATmzJnDd999x9q1a4mLi0Oj0WBnZ8fcuXPp0aNHrbdXUepKdSNLt4qUkunTp/OPf/yj0rrExETWr1/P9OnTiYyMrJR0uLzi4mLGjRvH3r17adGiBTNmzKC4uPiKN3hJKavMsGBnZ2dLsH55JonymSeWLVtGRkYGiYmJ2Nvb4+3tbdtfVfV2796dU6dOsXXrVkwmE35+flc8lrL2XYurZYm42W50xOpd4HsppQ/gDxwFpgGbpZTtgM3W9zedp7szvzsWcimrNUXuv1Hy55Vnx1aU61Ufg0zPnj3LL7/8wsGDB6/YqbpWvXv35sCBAyQlJbFw4UJGjRpVS61UlLvX5RkdHnroIRYuXEhBQQEAaWlpnD9/nvT0dHQ6HbGxsUyePJl9+/ZVuX2Zsk5QkyZNKCgo4IsvvgDA1dUVLy8vvv76awBKSkooLCwkMjKShQsX2s5RZZcCvb29SUxMBLDVUZXc3FyaNWuGvb098fHx/PHHH4DlvLF69WqysrIq1AuWNDbDhg276mgVQJcuXdi6dSuZmZmYTCZWrFhhy6FapmfPnqxZs4aioiLy8/P59ttvr1pvbbvujpUQwhXoCXwCIKW8JKXMAR4ByqZuXgJE31gTr42jnZZTbh4UnG9PScNU8lNSb8VulbvMzQwyLa+qAE2TycSIESPw8/NDr9fzzjvvAJakzx07dsRgMPDEE09UqisyMpLz588TEBDA9u3bSUpKIjQ0FIPBwODBg8nOrjyg/P333+Pj40OPHj346quvbMtdXFxsvwYvXrxY578MFeVOUD6jw5QpU4iMjOTJJ5+ka9eu6PV6HnvsMfLz8/n111/p3LkzAQEBzJw5k5deegmAMWPG0L9//0rnlUaNGjF69Gj0ej3R0dGEhITY1i1dupR58+ZhMBjo1q0bZ8+epV+/fgwaNIjg4GACAgJs0xRMnjyZ+fPn061btyqnNCgTExPD3r17CQ4OZtmyZbZUWr6+vrz44ov06tULf3//CmmvYmJiyM7OZtiwYVf9nDw9PXnjjTeIiIiwBd0/8sgjFcoEBgby+OOPExAQwJAhQyrcSbhmzRq8vLzYsWMHUVFRPPTQQ1fd53WRUl7XAwgAdgOLgf1YLgk2AHIuK5d9he3HAHuBvffff7+sDV8mpso3Xp0tN21uLRP/+3at1KncXo4cOVKn+09JSZG+vr629z/88IMcPXq0NJvN0mQyyaioKLl161b5xRdfyFGjRtnK5eTkSCmlbNmypczIyKiy7l69esk9e/bItLQ02aJFC3n+/HlZWloqIyIi5Jo1a+TevXtlnz59bOWzs7OllFJ6enrK4uLiCsuqa7Ner5cJCQlSSinj4uLkxIkTpZRSDh8+XH7++eeyqKhIenl5yd9//12azWY5dOhQGRUVZdv+q6++ku3bt5fu7u7yl19+uabPrarvDdgrr/P8c7s9goKCrulzUG5PdX1euZt9/vnnMjY2tq6bUa2anr9uJMbKDggEJkgpdwkh3qUGl/2klAuABWBJB3ED7bAZ3Ok+Jm8LBpMdZnGcSyUlODg61kbVym1ow4YNnD17tlbrvPfee6+aW6+88kGmAAUFBSQnJxMWFsbkyZOZOnUqAwcOrNH8K+UDNAFbgGZcXBwnT55kwoQJREVFERkZCYDBYCAmJobo6OgKd8dUJTc3l5ycHNvw+fDhwytlkj927BitWrWiXTvLzSCxsbEsWLDAtn7w4MEMHjzY1qZNmzZd87EpiqKUmTBhAhs2bGD9+vV13ZRadSMxVqeB01LKXdb3X2DpaJ0TQngCWJ/P31gTr50QgsnDu1CS05ISj9/ZvvNH27qSP/Iw5ZbcqqYodwlpDTJNSkoiKSmJ48eP88wzz/DAAw+QmJiIXq9n+vTpvPbaazWqsyru7u4cOHCA8PBwPvjgA1t807p16xg/fjyJiYkEBQXVykR913KJr2fPnpw4caLaSwOKoihX8t5773H8+HEeeOCBum5KrbruESsp5VkhRKoQor2U8jegN3DE+hgOzLI+r62mmlrn6e7MLzIIR9fPObE/kfCw/ohLkpNrFuPm5Uvzx3rfyuYoN1FNRpZqS1VBpnFxccTExODi4kJaWhr29vYYjUY8PDyIjY3FxcWFxYsXV9i+qvlmynTp0oWJEyeSmZmJu7s7K1asYMKECWRmZuLg4MCQIUNo06YNI0aMwGw2k5qaSkREBD169GD58uUUFBTQqFGjKut2c3PD3d2d7du3ExYWxtKlSysFf/r4+JCSksKJEydo06YNK1assK07fvw4bdq0QQjBvn37uHTpEo0bN77+D1RRFOUOc6MThE4AlgkhHICTwEgso2CrhRDPAH8CQ6vZ/qbw7ziYP86vpq1jHj+c+pFuOS1I93uPguxQmqM6Vsr1Kx9k2r9/f2bPns3Ro0fp2rUrYAnu/uyzzzh+/DhTpkxBo9Fgb2/P/Pnzgb+CTD09PYmPj69yH+UDNKWUDBgwgEceeYQDBw4wcuRI223Pb7zxBiaTidjYWHJzc5FSMmnSpCt2qsosWbKEsWPHUlhYSOvWrW3zyJRxcnJiwYIFREVF0aRJE3r06MGhQ4cA+PLLL/n000+xt7fH2dmZVatWqQB2RVGUcsSVLjvcSsHBwbI2EyKWXjKy7fteOJe4M7PUibec/TjrthT7i/fQLWwLdo2cam1fyq119OhROnToUNfNUGqoqu9NCJEopaz9SbvqQG2fw5RbS51XlOrU9Px1x8y8Xp69gx0ypytFHsdol2NHlv0PAJQ2OEfhidN13DpFURRFUe5Ud2THCkDn2heEZLB7KaW68zRrYJnWPjs9sY5bpiiKoih/uZGJhwcMGEBOTk7tNug2N2zYMAwGg20uv6qMGDGiyslMExISGDhwIGCZKd5gMNjm8jpw4ECttO+O7Vh5tg5Ec/EejPckIUqdaeNnmWQxL/dgHbdMURRFUf5SXcfKZDJVu+369euvGldZF6SUtnjQ2lSbmSRatWrF1q1bOXjwIHFxcYwZM6ZW2njHdqyat22EKc1y+fPChZZ8fzwPR7wo1P6O6WJpHbdOURRFUSwuz+iQkJBAREQETz75JHq9HoDo6GiCgoLw9fWtMK+ct7c3mZmZnDp1ig4dOjB69Gh8fX2JjIykqKio0r6+/fZbunTpQqdOnejTpw/nzp0DLHPwjRw5Er1ej8Fg4MsvvwQsWRgCAwPx9/end2/LzV8zZsywzcoO4Ofnx6lTp2xtGDduHIGBgaSmpvLss88SHByMr6+vLTMFWObr69atG/7+/nTu3Jn8/HzCwsJISkqylenevTsHD1YcDKnNTBLdunXD3d0dgNDQUE6frp1QoRu9K/C25aizpzgvAreC/WwovETqrh+YE6Anu3gnl07l4eyrbhFXFEVRKtq++ncyUwtqtc4mLVwI+9uV52qaNWsWhw4dsnUqEhIS2L17N4cOHaJVq1YALFy4EA8PD4qKiggJCWHIkCGVpjpJTk5mxYoV/Pe//+Vvf/sbX375JbGxsRXK9OjRg507dyKE4OOPP+att97iP//5D6+//jpubm78+uuvAGRnZ5ORkcHo0aPZtm0brVq1qpDj70p+++03Fi1aZBuBmzlzJh4eHphMJnr37s3Bgwfx8fHh8ccfZ9WqVYSEhJCXl4ezszOjRo1i8eLFzJ07l99//52SkhIMBkOF+r/55hsGDhxo+6wMBgPvvfcevXr14uWXX+bVV19l7ty5tvLFxcWMHj2aLVu20LZtWx5//PEq2/3JJ5/U2hQ+d+yIFYDr/b4c2/w6B+0v8kdhEvn27TE6ZVNw6mRdN01RFEVRrqhz5862ThVYcoL6+/sTGhpKamoqycnJlbZp1aoVAQEBAAQFBXHq1KlKZU6fPs1DDz2EXq9n9uzZHD58GIBNmzYxfvx4Wzl3d3d27txJz549be3w8PC4artbtmxJaGio7f3q1asJDAykU6dOHD58mCNHjvDbb7/h6elpy13o6uqKnZ0dQ4cO5bvvvqO0tJSFCxcyYsSIavdVVSaJbdu2VShTPpOEEKJSRxMgPj6eTz75hDfffPOqx3ct7tgRKwDPtm78mnCaB7X9We/yI/EpfenaCHIy9tNEBqv5d5Qay8nJYfny5YwbN67G2w4YMIDly5dfczzEjBkzcHFxYfLkyTXe19XMmzeP+fPnExgYyLJly6oss3jxYvbu3cv7779faZ2LiwsFBX/9qs/Ly6NDhw4MHjy4yvKKUl9UN7J0KzVo0MD2OiEhgU2bNrFjxw50Oh3h4eEUFxdX2saxXAo3rVZb5aXACRMm8MILLzBo0CASEhKYMWMGYImJuvxvYlXLAOzs7CrET5VvS/l2p6SkMGfOHPbs2YO7uzsjRoyguLj4ivXqdDr69u3L2rVrWb16NbU1hUl1f+sPHjzIqFGj2LBhQ61NdnxHj1h565vg2sQJ7wOh2GsK+f7kJUDLxdIjFPycfsXUIYpyJXdKkOmHH37I+vXrr9ipqqm4uLhKM7grinJtLs/ocLnc3Fzc3d3R6XQcO3aMnTt3Xve+cnNzue+++wDLZMFlIiMjK/woys7OpmvXrmzdupWUlBQA26VAb29v9u3bB8C+ffts6y+Xl5dHgwYNcHNz49y5c2zYsAGwZHdIT09nz549AOTn59tScY0aNYrnn3+ekJCQq46Qlc8kAVw1kwRQIZPEn3/+yaOPPsrSpUtrNa3OHd2xsnfU0mdER8z5WrqdGsw50zEcnNpw6b40ctad5Mm4H0k+d+V/zIpyuVsZZFrelQI0582bR8eOHTEYDDzxxBMAbN26lYCAAAICAujUqVOlE/bYsWM5efIkgwYN4p133uHChQtER0djMBgIDQ2tFCwKll+eXbt2JSQkhLi4uArrEhMTOXfunC0p9M0khHASQuwWQhwQQhwWQrxqXe4hhPhRCJFsfXYvt810IcRxIcRvQoiHyi0PEkL8al03T1h/1gohHIUQq6zLdwkhvG/6gSl3tfIZHaZMmVJpfb9+/TAajRgMBuLi4ipcaqupGTNmMHToUMLCwiqk1nrppZfIzs7Gz88Pf39/4uPjadq0KQsWLODRRx/F39/fFp80ZMgQLly4QEBAAPPnz79ip8Tf359OnTrh6+vL3//+d7p37w6Ag4MDq1atYsKECfj7+9O3b1/bqFdQUBCurq6MHDnymo5nyZIlTJkyBYPBQFJSEi+//HKF9eUzSfTo0YOWLVva1r322mtkZWUxbtw4AgICCA6upfmKpZR1/ggKCpI30y9fJcv3/7FZjnl5hvzsqydkfLxBJr26WR6cmiCX/Pj7Td23UruOHDlSp/tPSUmRvr6+tvfx8fFSp9PJkydP2pZlZWVJKaUsLCyUvr6+MjMzU0opZcuWLWVGRoZMSUmRWq1W7t+/X0op5dChQ+XSpUsr7euVV16Rs2fPllJKqdfrZUJCgpRSyri4ODlx4kQppZSenp6yuLhYSilldna2lFLKgQMHyp9++klKKWV+fr4sLS2tVHdZW6SU8rnnnpMzZsyQUkq5efNm6e/vL6WUctGiRXL8+PFSSikffvhhuWTJEimllO+//75s0KCBlFJKk8kke/XqJf/8888K5S9X1fcG7JU1PFcAAnCxvrYHdgGhwFvANOvyacCb1tcdgQOAI9AKOAForet2A12tdW4A+luXjwM+sr5+Alh1tXbd7HOYcnPV9XlF+UtaWpps166dNJlMdd0Um5qev+7oGKsynR9uzdnUHMxHulGyrxmmiN2savYN404NwfhrJvRpV9dNVK7D77+/Tn7B0Vqts6FLBx54IO7qBcupKsh0zZo1ALYg08uv3V9LkGmZqgI0hw61pOA0GAzExMQQHR1NdHQ0YLlF+YUXXiAmJoZHH30ULy+vatv/008/2W6tfvDBB8nKyiI3N7dCmZ9//tlW5qmnnmLqVMu8cB9++CEDBgygRYsW1e6jtlhPaGXBXfbWhwQeAcKty5cACcBU6/KVUsoSIEUIcRzoLIQ4BbhKKXcACCE+BaKxdLAeAWZY6/oCeF8IIaz7VhTlJvn000958cUXefvtt9Fo6u8Ftfrb8hrQ2mkY/Hwwf0S4s6rBn5Tk3YPh3l/43elPWmVeVLFWyg25UpDpgQMH6NSp0zUFmZbFF9TUunXrGD9+PImJiQQFBWE0Gpk2bRoff/wxRUVFhIaGcuzYsWrrqOrff1XBnlUt27FjB++//z7e3t5MnjyZTz/9lGnTpl3XsVwrIYRWCJEEnAd+lFLuAu6RUp4BsD43sxa/D0gtt/lp67L7rK8vX15hGymlEcgF1PwsinKTPf3006Smptp+ONZXd8WIVZmePs35Yn8PUtLM+HT4gv2Nt/N4Wgwnf8+iTfsmV69Aua3UdGSpNtzKINMy5QM0w8LCbAGaZrOZ1NRUIiIi6NGjB8uXL6egoICsrCz0ej16vZ4dO3Zw7NgxfHx8rlh/z549WbZsGXFxcSQkJNCkSRNcXV0rlOnevTsrV64kNja2QsB7+ddldxHOmjXrho+5OlJKExAghGgErBFC+FVTvKrbgWQ1y6vbpmLFQowBxgDcf//91TVZUZS7yF0xYlWmWxvLj87laUGYTXY0cT+HGcnZnekA5OYmcfr0Z0hZ+9PwK3eGWxlkWl5VAZomk4nY2Fj0ej2dOnVi0qRJNGrUiLlz59oCUJ2dna866d2MGTPYu3cvBoOBadOmVbhTqMy7777LBx98QEhISKXLhHVFSpmD5ZJfP+CcEMITwPp83lrsNFD+OqUXkG5d7lXF8grbCCHsADeg0syIUsoFUspgKWVw06ZNa+egFEWp98TtcBksODhY1tZ8FVfT/93tHD2Tx4ehn2PntJeG2/8XV1NjPMa04tjpaMzGbBo3Dse34zvY27tevULlljp69CgdOnSo62YoNVTV9yaESJRS1ug2HCFEU6BUSpkjhHAGNgJvAr2ALCnlLCHENMBDSvkvIYQvsBzoDDQHNgPtpJQmIcQeYAKWAPj1wHtSyvVCiPGAXko5VgjxBPColPJv1bXrVp7DlNqnzitKdWp6/rqrRqwAhgTeR2hrDzoF/QOtQxHZvgtxFSYO7HgZY2k2WwucuHDhJ/bsHUxJyfmrV6goyq3kCcQLIQ4Ce7DEWH0HzAL6CiGSgb7W90gpDwOrgSPA98B466VEgGeBj4HjWO4W3GBd/gnQ2Bro/gKWuwwVRVGuyV3XsRoV1pqVY7ri2aIr93lPxvHew/wZ9Bb2nltJOdOONdka7nvgbUpKznDs2IsqsF1RbiNSyoNSyk5SSoOU0k9K+Zp1eZaUsreUsp31+UK5bWZKKdtIKdtLKTeUW77XWkcbKeVzZXf9SSmLpZRDpZRtpZSdpZQqB5ZyU1U38fC1mDt3LoWFhbXYottHRkaGLWl02USgVSmbJ/By5RNGT5kyBR8fH9t8gDk5OTelzXddx6o8n9bPsr2wGcUev2F3sRn9jkzhrVOTSN1nRxvv/yEzawtnzn551XrM5lJMpsp3fimKoijK1dwJHavrvbP5ajZv3oyPjw/79+8nLCzshurq27cvhw4d4uDBgzzwwAO88cYbtdTKiu7qjhWAXeMolmbqWHG8PYcLtNx7qRk+P+so+aQdrrpAfv/9dYqL06ut4/fk19izd7Aa3VIURVFq7PKMDgCzZ88mJCQEg8HAK6+8AsDFixeJiorC398fPz8/Vq1axbx580hPTyciIoKIiIhKdb/22muEhITg5+fHmDFjbH+njh8/Tp8+ffD39ycwMNCW8uWtt95Cr9fj7+9vmzolPDzclrcvMzMTb29vwHIn8NChQ3n44YeJjIykoKCA3r17ExgYiF6vZ+3atbZ2fPrppxgMBvz9/XnqqafIz8+nVatWlJaWApb0N97e3rb3YMk48a9//Yv169cTEBBAUVERK1asQK/X4+fnZ5tP73IzZ86kffv29OnTh99++822PDIyEjs7y2QIoaGhnD59usrtb9RdNd1CVbo278qq31ehbXSAIEfYV5pHZos9PJvWG5JGIH2mcvz4m/j5vVvl9mazkXPn1mM05nDx4u+4uLS/xUegKIqi1Jb4xQs4/0ftXv1t1rI1ESPGXHH9rFmzOHToEElJSQBs3LiR5ORkdu/ejZSSQYMGsW3bNjIyMmjevDnr1q0DLNO7uLm58fbbbxMfH18hRU2Z5557zpbm5amnnuK7777j4YcfJiYmhmnTpjF48GCKi4sxm81s2LCBr7/+ml27dqHT6Wy5AauzY8cODh48iIeHB0ajkTVr1uDq6kpmZiahoaEMGjSII0eOMHPmTH7++WeaNGnChQsXaNiwIeHh4axbt47o6GhWrlzJkCFDsLe3t9UdEBDAa6+9ZksGn56eztSpU0lMTMTd3Z3IyEi+/vpr2+TIYEmxtXLlSvbv34/RaCQwMJCgoKBK7V64cKEtRU9tu+tHrILvDUYg6NDUh/DHO+BW0JQWvweT6CjwPOXKhVI/zp1fx8WLVf9Hy83dh9GYA0Bm5uZb2HJFURTlTrRx40Y2btxIp06dCAwM5NixYyQnJ6PX69m0aRNTp05l+/btuLm5XbWu+Ph4unTpgl6vZ8uWLRw+fJj8/HzS0tIYPHgwYMmnp9Pp2LRpEyNHjkSn0wFcNQkyWC6vlZWTUvLvf/8bg8FAnz59SEtL49y5c2zZsoXHHnvM1vErKz9q1CgWLVoEwKJFi66aH3DPnj2Eh4fTtGlT7OzsiImJYdu2bRXKbN++ncGDB6PT6XB1dWXQoEGV6pk5c6Zt+5vhrh+xcnN0418h/8KviR/tmt3D5rzTZK7T0OBcAwoaadEmBmMMSeTUHx/i23FOpe0zMzchhD06nTcZmZvx9h5XB0eh3Co5OTksX76cceOu73ueO3cuY8aMsZ24ygsPD2fOnDm1lwi0nGHDhnH48GFGjhzJpEmTqiwzYsQIBg4cyGOPPVZheUJCAnPmzOG7776zLduzZw+hoaGsWrWqUnlFqc+qG1m6VaSUTJ8+nX/84x+V1iUmJrJ+/XqmT59OZGRkpaTD5RUXFzNu3Dj27t1LixYtmDFjBsXFxVcMW5FSVplhwc7ODrPZbKuzvPKZJ5YtW0ZGRgaJiYnY29vj7e1t219V9Xbv3p1Tp06xdetWTCYTfn7VzfVbdZaIqlS1rzJLlizhu+++Y/PmzdWWuxF3/YgVQGzHWAKaBQDg064Va/zeQeOm4aipiOCcLvyR1ZyzZ7/hfPZxzhz9gR0/jqek8AJSSjIyN+HuHso99zxMXl4SJSUZdXswyk1VH4NMz549yy+//MLBgwev2KmqCZPJxNSpU3nooYdqoXWKolye0eGhhx5i4cKFFBRY0mKmpaVx/vx50tPT0el0xMbGMnnyZPbt21fl9mXKOkFNmjShoKCAL774AgBXV1e8vLz4+uuvASgpKaGwsJDIyEgWLlxoO0eVXQr09vYmMTERwFZHVXJzc2nWrBn29vbEx8fzxx9/ANC7d29Wr15NVlZWhXrBksZm2LBhVx2tAujSpQtbt24lMzMTk8nEihUrbDlUy/Ts2ZM1a9ZQVFREfn4+3377rW3d999/z5tvvsk333xT5Y/b2qI6Vpd5wP0BzBoTDkF5nMt34Jw2l8hDk8Ek+HX3YI6cGUeh9nt2/jKGwsITFBX9gWuDcBzM3QDIzNpSx0eg3Ew3M8i0vKoCNE0mEyNGjMDPzw+9Xs8777wDWJI+d+zYEYPBwBNPPFGprsjISM6fP09AQADbt28nKSmJ0NBQ2y3H2dnZlbb5/vvv8fHxoUePHnz11VcV1r333nsMGTKEZs2aVdpOUZSauzyjQ2RkJE8++SRdu3ZFr9fz2GOPkZ+fz6+//krnzp0JCAhg5syZvPTSSwCMGTOG/v37VzqvNGrUiNGjR6PX64mOjiYkJMS2bunSpcybNw+DwUC3bt04e/Ys/fr1Y9CgQQQHBxMQEGCbpmDy5MnMnz+fbt26VTmlQZmYmBj27t1LcHAwy5Yts6XS8vX15cUXX6RXr174+/vzwgsvVNgmOzubYcOGXfVz8vT05I033iAiIsIWdP/II49UKBMYGMjjjz9OQEAAQ4YMqXAn4XPPPUd+fj59+/YlICCAsWPHXnWf10VKWeePoKAgebswm80ydFmo/MeGsXLWhC/kf6atlePfGy3XLx0pN3/XXb6zeLBM+O+zctPm1nLnrii5aXNr+dlr38j/NzFebv8pTCYdGF3Xh3BHO3LkSJ3uPyUlRfr6+tre//DDD3L06NHSbDZLk8kko6Ki5NatW+UXX3whR40aZSuXk5MjpZSyZcuWMiMjo8q6e/XqJffs2SPT0tJkixYt5Pnz52VpaamMiIiQa9askXv37pV9+vSxlc/OzpZSSunp6SmLi4srLKuuzXq9XiYkJEgppYyLi5MTJ06UUko5fPhw+fnnn8uioiLp5eUlf//9d2k2m+XQoUNlVFSUlFLK06dPy549e0qj0Wgrfy2q+t6AvfI2OP/UxuN2OocpNVfX55W72eeffy5jY2PruhnVqun5666PsbqcEIIH3B/g53M/kXMfhKUM5Z4gH16/9AWv+73OJwn/JCq7A9EZ/hRwAFnShtz0BkhpRhZ25kLpekymQrTamzfMqFjEJZ/mUEFRrdbp5+LM6+28rl7QqnyQKUBBQQHJycmEhYUxefJkpk6dysCBA2s0/0r5AE3AFqAZFxfHyZMnmTBhAlFRUURGRgJgMBiIiYkhOjq6wt0xVcnNzSUnJ8c2fD58+PBKmeSPHTtGq1ataNeuHQCxsbEsWLAAgH/+85+8+eabaLXaaz4eRVGUqkyYMIENGzawfv36um5KrVKXAqvQzt3yB0Xjk4+LuyP3JgVQWFLE6ztf5x7dPeTdE4Hbkaeg6B7OHe5Ml0dac7+vB6n72mA2l5CTo3KG3S2kNcg0KSmJpKQkjh8/zjPPPMMDDzxAYmIier2e6dOn89prr9Wozqq4u7tz4MABwsPD+eCDDxg1ahQA69atY/z48SQmJhIUFFQrE/VdKahz7969PPHEE3h7e/PFF18wbtw4W5yGoihKTbz33nscP36cBx54oK6bUqvUiFUVHnC3fMl92jxIeFsfvnvvAF0cB/LL/V8zKWgS90pf9v2eQI9tsyi8R8eawlzSCvMJSW3JPSFacnL30rhxz0r1mvIvoW3ocKsP545Vk5Gl2lJVkGlcXBwxMTG4uLiQlpaGvb09RqMRDw8PYmNjcXFxYfHixRW2r2q+mTJdunRh4sSJZGZm4u7uzooVK5gwYQKZmZk4ODgwZMgQ2rRpw4gRIzCbzaSmphIREUGPHj1Yvnw5BQUFNGrUqMq63dzccHd3Z/v27YSFhbF06dJKwZ8+Pj6kpKRw4sQJ2rRpw4oVK2zrUlJSbK/L7iK82iiZoijK3UR1rKoQdl8Yne/tzMDWA2mma0zHHs2RP4VzxuM4D3kM5I/EfDIv3IPWVWDIvIj/eUG6lJzxaMSlvPvJyd5Tqc6iYxfIWnKYZs91wuE+lzo4KqU2lA8y7d+/P7Nnz+bo0aN07doVABcXFz777DOOHz/OlClT0Gg02NvbM3/+fOCvIFNPT0/i4+Or3Ef5AE0pJQMGDOCRRx7hwIEDjBw50nbb8xtvvIHJZCI2Npbc3FyklEyaNOmKnaoyS5YsYezYsRQWFtK6dWvbPDJlnJycWLBgAVFRUTRp0oQePXpw6NChG/zkFEVR7g7iSpcdbqXg4GBZNl3+7ehSsZFlr+6gMPuvqfZzmtpT1PBHmkkd5qIudCvR4Ck1JLdZBm22ER6+H43G0VY+e00yF3edxSXsPhpFta6Lw7gjHD16lA4dOtR1M5Qaqup7E0IkSilrf9KuOnC7n8OU6qnzilKdmp6/1IjVNXBwsuPRF4JIOZCJg7MdLu6OHCgt5p/fuqFrsZhhre5lZ2FXNPGn6Z3fnnR+JC/3EI3c/5pGv+RELgBFBzNx698Kobk5E5MpiqIoilJ3VPD6NXJrqiOgz/107N6c+zs25kGfe3Au9aUB3mzPWMXQrvcxz+4SJy9a4rPSf9qANFtGA425JRgzi7D3csGUW8Kl1MoTuSmKoih3pxuZeHjAgAHk5OTUboNuc8OGDcNgMNjm8qvKiBEjqpzMNCEhgYEDBwKwdu1aDAYDAQEBBAcH89NPP9VK+1TH6jo1cLRjw8SevNrzn5wuOM2ezC0M9Pdk3kU7KLiHvIv7yd92GiklxcmWCRgbDWwNdoKiA2p2dkVRFMWiuo6VyWSqdtv169dfNa6yLkgpbfGgtak2M0n07t2bAwcOkJSUxMKFC213Wt8o1bG6AS08dER6P4iPhw8fJn3IiO73kuEsyMxoy0X337mQkMzBA2PZn/0YRc1/w+F+V5zae1D4a6ZtNEupudshLlC5dur7UpTqXZ7RISEhgYiICJ588kn0ej0A0dHRBAUF4evra5tXDizpZjIzMzl16hQdOnRg9OjR+Pr6EhkZSVFR5Xn+vv32W7p06UKnTp3o06cP586dAyxz8I0cORK9Xo/BYODLL78ELFkYAgMD8ff3p3fv3gDMmDHDNis7gJ+fH6dOnbK1Ydy4cQQGBpKamsqzzz5LcHAwvr6+tswUYJmvr1u3bvj7+9O5c2fy8/MJCwsjKSnJVqZ79+4cPHiwQvtrM5OEi4uLbWqZixcv1lruQBVjdYOEELzY5UVGfD+Cz46/Tfy//pfPPv2RJvaFpBlmUXzhBHZGD/70e4MD8btp0uQ+GhXlk7VrBcX2f+Li4kMHn/+r68OoN5ycnMjKyqJx48Y3LYGmUnuklGRlZeHk5FTXTVGUa5Lz7QkupV+s1Todmjeg0cNtrrh+1qxZHDp0yNapSEhIYPfu3Rw6dIhWrVoBsHDhQjw8PCgqKiIkJIQhQ4bQuHHjCvUkJyezYsUK/vvf//K3v/2NL7/8ktjY2AplevTowc6dOxFC8PHHH/PWW2/xn//8h9dffx03Nzd+/fVXALKzs8nIyGD06NFs27aNVq1aVcjxdyW//fYbixYtso3AzZw5Ew8PD0wmE7179+bgwYP4+Pjw+OOPs2rVKkJCQsjLy8PZ2ZlRo0axePFi5s6dy++//05JSQkGg6FC/d988w0DBw60fVYGg4H33nuPXr168fLLL/Pqq68yd+5cW/ni4mJGjx7Nli1baNu2LY8//niF+tasWcP06dM5f/4869atu+rxXQvVsaoFAc0CGB8wnnn75yGR/CEP4QMUNzqBx9EnaZwWzg/tPuf+FpsxOZrJagvaHFe0NCAv7wAt7/8HOl3Luj6MesHLy4vTp0+TkaEup9YXTk5OeHnd+jnHFKU+69y5s61TBZacoGvWrAEgNTWV5OTkSh2rVq1aERAQAEBQUBCnTp2qVO/p06d5/PHHOXPmDJcuXbLtY9OmTaxcudJWzt3dnW+//ZaePXvaynh4eFy13S1btiQ0NNT2fvXq1SxYsACj0ciZM2c4cuQIQgg8PT1tuQtdXV0BGDp0KK+//jqzZ89m4cKFjBgxotp93WgmCYDBgwczePBgW3aLTZs2XfUYr0Z1rGrJ3/3+zq6zu/j+1Pd0adWF0gv3UnTqftpn9gHgWO4TPDRoNrlFJayKP0HR4VxGeZpI95vAqe2L6BD5ihqBuQb29vYVTjaKoii1qbqRpVupQYMGttcJCQls2rSJHTt2oNPpCA8Pp7i4uNI2jo5/TfGj1WqrvBQ4YcIEXnjhBQYNGkRCQgIzZswALKPLl/8NqmoZgJ2dXYX4qfJtKd/ulJQU5syZw549e3B3d2fEiBEUFxdfsV6dTkffvn1Zu3Ytq1evpramMLmWv609e/bkxIkTZGZmVjuB87VQMVa1RKvR8mbYmzzf6XneiXiHfo8tov/YlzjdsIA8swnPk7nsTN6CzMmhn4sHGa6ODMvWoisNINO4gayVRzn33Ql+/HgB+3f/iqnUTGlJ9UGLiqIoSv13eUaHy+Xm5uLu7o5Op+PYsWPs3LnzuveVm5vLfffdB1gmCy4TGRnJ+++/b3ufnZ1N165d2bp1qy3jQtmlQG9vb/bt2wfAvn37KmRkKC8vL48GDRrg5ubGuXPn2LBhA2DJ7pCens6ePZbJtPPz822puEaNGsXzzz9PSEjIVUfIymeSAK6aSQKokEni+PHjthjQffv2cenSpUqjgNdDjVjVosbOjRltGG177+RiT+fJkez9NQnt0gLOL3Yigz8B6AK4ORtZ/kcQ0e32c+HQT5TozqHxXUzuWU82//ctcnPtGTo9xFaflGZAqJEtRVGUO8jlGR2ioqIqrO/Xrx8fffQRBoOB9u3bV7jUVlMzZsxg6NCh3HfffYSGhto6RS+99BLjx4/Hz88PrVbLK6+8wqOPPsqCBQt49NFHMZvNNGvWjB9//JEhQ4bw6aefEhAQQEhIyBVz/fn7+9OpUyd8fX1p3bo13bt3B8DBwYFVq1YxYcIEioqKcHZ2ZtOmTbi4uBAUFISrqysjR468puO5kUwSX375JZ9++in29vY4OzuzatWqWvn7esMzrwshtMBeIE1KOVAI4QGsAryBU8DfpJSVw/TLuRtmLb5wtoDN3+6nuFk2fzQ8wh+/5GFIi0BoSmk3aDK5hR40angOKdpiJ0/gcOEBDic8z/A3wygxHSYtfSXnz6/nfu9/cbKwP4109oR4/9Wb//HIOTp4NsTLXVeHR6ko107NvK7cLtTM67eP9PR0wsPDOXbsGBrN7XFRrS5mXp8IHAVcre+nAZullLOEENOs76fWwn7qNY97XRg6Osz6bhDpndP5KH4ROUckjc51wNMriYISdyIfXMZ3Sz7Go+3/44EBL/HTrjw0mlJKTE6UmnXsOvQRKT/dxz1Ci/bZTgTe786K3X8y/atfcXG0I25gB/4W3EKNaimKoij1yqeffsqLL77I22+/fdt0qq7HDXWshBBeQBQwE3jBuvgRINz6egmQgOpYVdLcpTmvPfwiPAwXLx5ny/cv8MfuaGbYz6HjnwNxLiqgyb2/cvZsEAddHyCtKJhmDjsJv+cjgtz/oEF2eyYv3sfTgzvy8tpDdG/bGLMZpn75K0mpObzxqOHqjVAURVGU28TTTz/N008/XdfNuGE3OmI1F/gX0LDcsnuklGcApJRnhBDNqtpQCDEGGANw//3332Az6rcGDdrSOegz/vxxL+LHIgpyi/HqNhx+OkOwWRL5pA86Q1NMpiC2xS8hz2sbuoIODNKeYce+xQQ1Hch/7Fvi2sWT/2uWxrJdfzIuvC0tPNRlQUVRFEW5la57rE0IMRA4L6VMvJ7tpZQLpJTBUsrgpk2bXm8z7hj3eLvSulNTWuR2oERbSFbLfcige7hglFxY/TsFu8+Q+3sRDdO6kH/vXuxCL3Ffp3cJb/ELY/xeIyt3LTlrfufZbq0QwPLdfyJNtZ9OQFEURVGUK7uREavuwCAhxADACXAVQnwGnBNCeFpHqzyB87XR0LtB6COtSTmQQYF3Ov+X+CFv3P8ehy4a6d/ChZyvjnNJSlzcwshtkcBRpwkIk6TpwRco8FrHWb9POF+6Et2vrZjW9R4c/2jK8dkP4H5PELrAe3D2a4LQqrgrRVEURbmZrnvESko5XUrpJaX0Bp4AtkgpY4FvgOHWYsOBtTfcyruE+70N+Nu/Q5jwj8dp3ag1M1KmcklKku+3Y4fIRAPsvSgozvXELEtw3D8Oj7MGmqa8wqXGMbhe6oEpt5S2zrvxareSPzu/RnbhTi6sOMaF1b9hNpu5WGLkYomRUjWapSiKoii17maE3c8C+gohkoG+1vfKNWri1RB3VzcWRi6km3cXsnRn2Lf5D85nu3GwC7QZ144Nds35f+mN2GLI4IyTYOPpTKbv+5bVwpn7971Iu83zOL1rBpml9vxpWIxDt2YUHchg+bzd+L7yA76v/ECX/9tM8rm/JqSTUlZIlmsyS5buOMW5vMqz+yqKoii1Jycnx5Zb73rMnTuXwsLCWmzR7SMjI8OWNLpsItCqlCWjvlz5hNFxcXEYDAYCAgKIjIwkPT39prS5VjpWUsoEKeVA6+ssKWVvKWU76/PVszYqlTRyasTb4W9z/wNNcDI2wOVeOwY+0oOu93Vj3sCVRAaM4fOMNbzk8R8oceYR43B+Or2PfcWXOFpiZqs4zNdZDlB6hk3HF5HlpiXQvIM43xSm9ffBLCVTvjiIySwpumRi2H93MvC9nzifZ0k38Nq3h4lbe5g31h+t649CURTljnYndKzKZk6vbZs3b8bHx4f9+/cTFhZ29Q2qMWXKFA4ePEhSUhIDBw7ktddeq6VWVlR/J4q4Cwgh6BkahBDQN8aARmv5ujRCw2jDaD6O/JjgTh1xb+lI65MhRB97nhMlhRxunEHQqUg6//QfCtL1NPH9htT20zgT8D7e971DWKP5zBjYmqTUHBZsO8n45fvYlXKBkxkXGfr/dvDWD7+xZMcfeLo58d3BM2rUSlEU5SaaNm0aJ06cIOD/t3ff8VFV6ePHP2daZtI7CYSQIJ2EhEDoSEdBaaIigm0VdVV0dy2A+lV0l7Wui90fuoiggojSBBSRqlIDiPSABEhCeq/Tzu+PhAhShUCIPu/Xa16ZOffeM8+9zByeOffce+LjefzxxwF45ZVXSExMpF27djz77LMAlJaWct111xEXF0dMTAyfffYZb7zxBunp6fTp04c+ffqcUvfzzz9PYmIiMTEx3HvvvTVnJg4cOED//v2Ji4sjISGhZsqXl19+mdjYWOLi4pg4cSIAvXv3rpm3Lycnh6ioKABmzJjBTTfdxJAhQxg4cCAlJSX069ePhIQEYmNjWbjw15FAM2fOpF27dsTFxXHbbbdRXFxMdHQ0DocDqJr+JioqquY1wPbt23niiSdYunQp8fHxlJeXM3v2bGJjY4mJiWHChNPfyWnKlCm0bNmS/v37s2/fvpry45M9Hz+Wl+p+jzKlzRUuOi6Yu17ugc3HcsqyxLBEEsMSOeSXw9J3duDt4cvcVq+S45XKXZH/wDu5JTP2D+f+sCkYjQ7S0x4k1nGUDL2AUM9tPJLYlcPfZ+IVsI93r9mGh9HJ98kxeGzsyi3Nm3HfsDb0/c9qPl6/j7/1b47RKLdvEHVLKdUYmAmEAW5gmtb69bPN+KCUmgTcDbiAh7XW31SXdwBmADZgKfCI1lorpTyq36MDkAuM0lqnXKZdFHVs2bJlZGRk1GqdYWFhDBo06IzLX3zxRXbu3Mn27dsBWL58OcnJyWzatAmtNUOHDmXt2rVkZ2fTsGFDlixZAlTN++fn58drr73GqlWrTjt58EMPPcQzzzwDwG233cZXX33FkCFDGDNmDBMnTmTEiBFUVFTgdrtZtmwZCxYsYOPGjXh6etbMDXg269evZ8eOHQQGBuJ0Opk/fz6+vr7k5OTQpUsXhg4dyu7du5kyZQo//PADwcHB5OXl4ePjQ+/evVmyZAnDhw9nzpw5jBw5ErPZXFN3fHw8zz//PFu2bOGtt94iPT2dCRMmkJSUREBAAAMHDmTBggUMHz68ZpukpCTmzJnDtm3bcDqdJCQk0KFDh5rlTz31FDNnzsTPz49Vq1adc/8uhPRYXeGUUqdNqk4UFRtE56HRjPhbR6KuCiPSN5L7R4/inue64gptxsTtw9i26mHcRxJp1vpxIpIeQ5VaaBcwmwHdpjC89TwCbTZ8reF0a7KU2O7Pck/2XgI2ZzG4ZSAhlQ+zYdNQnM7Sy7TXQpyRE3hUa90a6AI8qJRqw68zPjQHvqt+TfWyW4C2wLXAO9XTcAG8S9W99JpXP66tLr8byNdaNwP+C7x0OXZMiOOWL1/O8uXLad++PQkJCezdu5fk5GRiY2NZsWIFEyZMYN26dfj5+Z2zrlWrVtG5c2diY2NZuXIlu3btori4mLS0NEaMGAFUzafn6enJihUruOuuu/D0rPoRfa5JkAEGDBhQs57WmieffJJ27drRv39/0tLSyMzMZOXKldx44401id/x9e+5556auf0+/PDDc84PuHnzZnr37k1ISAgmk4kxY8awdu3ak9ZZt24dI0aMwNPTE19fX4YOHXrS8ilTpnD06FHGjBlz0qTTtUl6rP4AlFJ0HBwNwLTIaTjcDjzNVV+Mp69rw6gPUtndYD1+h4dREGBFVcbTeEVbHIEF6MR03Bu98fFuA0oRlH+Yoz1eIDNhBh6rIrgrbjmF3qmUlyuSk6fQPPT/SEfz76V7uKNbFF2vuviZwIU4X9U3Hz5+A+JipdQeoBFnnvFhGDBHa10JHFJKHQA6KaVSAF+t9XoApdRMYDiwrHqbydV1zQPeUkopfbETq4p64Ww9S5eL1ppJkyZx3333nbIsKSmJpUuXMmnSJAYOHFjTG3U6FRUVPPDAA2zZsoXGjRszefJkKioqONNHWWt92tNjJpMJt9tdU+eJvLy8ap5/8sknZGdnk5SUhNlsJioqqub9Tldv9+7dSUlJYc2aNbhcLmJiYs64L8fjOx/nc4rv1ltv5brrruO55547rzp/D+mx+oMxG801SRVAp+hAPr6rP+bWFbgMThZO3c66jHL2OjU/5gcQ1uEuGg27Fkd6CY7UYhqM6EHrmH9TYUohb/A8CoO/oCKrCxtTBpB+7DOSZ0xjyn9/5OtdGTzwSRLpBeWnxFBRkX7eXwAhLpRSKgpoD2zkNzM+AMdnfGgEHD1hs9TqskbVz39bftI2WmsnUAjILwhxyfj4+FBc/OtV2tdccw3Tp0+npKQEgLS0NLKyskhPT8fT05OxY8fy2GOPsXXr1tNuf9zxJCg4OJiSkhLmzZsHVI01ioiIYMGCBQBUVlZSVlbGwIEDmT59es1A+OOnAqOiokhKqroX+PE6TqewsJDQ0FDMZjOrVq3i8OHDAPTr14+5c+eSm5t7Ur1QNY3N6NGjz9lbBdC5c2fWrFlDTk4OLpeL2bNn06tXr5PWufrqq5k/fz7l5eUUFxezePHimmXJyck1zxctWkSrVq3O+Z4XQnqs/gS6NwvmsKMX3+5cwSCvEfQbFYvL4Wb+q1v54q0NpPfZQHrTA0TYGvFo6+7okh4E+l1HTuESTMqXtrtuJcbpyeGAvRyLncadWe0ZfVVP/vZNOA99upX/3BzP26sOsPVwPv8ZVkbe0Ye5quljREXdT6XTxaz1h9mVXsRLI9thMUkuLy6eUsob+AL4m9a66Cy/UE+3QJ+l/Gzb/DYGmZZL1IqgoCC6d+9OTEwMgwYN4pVXXmHPnj107doVAG9vbz7++GMOHDjA448/jsFgwGw28+677wJw7733MmjQIMLDw08aN+Tv78+4ceOIjY0lKiqKxMTEmmWzZs3ivvvu45lnnsFsNvP5559z7bXXsn37djp27IjFYmHw4MH8+9//5rHHHuPmm29m1qxZ9O3b94z7MWbMGIYMGULHjh2Jj4+vSVzatm3LU089Ra9evTAajbRv354ZM2bUbPP0008zevTocx6n8PBwXnjhBfr06YPWmsGDBzNs2LCT1klISGDUqFHEx8fTpEmTk64knDhxIvv27cNgMNCkSRPee++9c77nhVBXQs9Cx44d9fErDsSlkV2WTb/P+3FD8xvo0rALvxT8QtKPySTsGEK+LZNjMT+xv2wPo4sfpvyIwmgpplG39wj0vZW2rm6U55Tz9a5dBMV+iW/oflweRbjMPbl3yY2AorHRyAhbCS0Tn8dqLsVsCSHLOps3VqZwJK/q18+n93SmW7OTB1e63ZqckkpCfa11cFREXVJKJWmtO17AdmbgK+AbrfVr1WX7gN4nzPiwWmvdsnrgOlrrF6rX+4aq03wpwCqtdavq8tHV2993fB2t9XqllAnIAELOdipQ2rD6bc+ePbRu3bquw/hTmjdvHgsXLmTWrFl1HcoZne7zcbb2S3qs/iRCPEPoGNaRL5K/4IvkL1AoEq9KJLBpKT4/RBKwuQFtGEi+pYSeQ2PwDbBxcFs0O9bl8v3wxdiKGmKuCMXT/Sz5X2dyVatvyW75GS8OaMGBguu541ApR6OnUqFc+O0dTUGr2Xy+5SNs5r68N7ohizfMYlMydL1qcM35b601j37+E4t/Smf+A92JjTj3QMzfy+lyYzJKL9kfhar68PwP2HM8qap2fMaHFzl5xodFwKdKqdeAhlQNUt+ktXYppYqVUl2oOpV4O/Dmb+paD9xI1awSdf8LVIg/mPHjx7Ns2TKWLl1a16HUKkms/kRevvplDhUewt/DnxBbCP5WfwBcA9zs25DBvuz9PJ83mdCov9MioAUbSr4mYFdHSr71xlLiTVqDn2jaJoH0jZp2XmOoyPgFGkwnovQAv7TZgdOajy3vUYy/tMbceBV/jdtE504PkrT+Bm5ong4sZMPG/9Ki+bMEBfXk/XW/MH9bKr4WO5Pm72DBA91rkiCtNSUle3E48ggI6HZB9xvZfrSA0dM28PaY9vRt1aAWj6SoQ92B24CflVLbq8uepCqhmquUuhs4AtwEoLXepZSaC+ym6orCB7XWrurt/sqvt1tYVv2AqsRtVvVA9zyqrioUQtSyN99889wr1UNyKlDU0Fozbvk4NmZsBMBmsjHKcT/WDVEAbOg6h+16Aw/tmkpEVBCRzmIKoidh98zEnRlLedkAjia1AL8yOkdsobDtLCyVDXCY8gjb8SBpHrlY2/+Iw34Y/CfxzBIn4zt+SbBlH4cKI/EP6E37xt6UlR+msGALlfZMAEJCBtK61YuYzb/2aC3ZcYwvtqbyn5viCPA69XYUJRVlDHlrE4dySunfOpQP7kg8ZR1Rty70VOCVSNqw+k1OBYqzkVOB4oIppXiqy1N8tOsjujXsRs+InngYrHyRkYTBoHh/zBvc8fUd/Oz7Pa5dPUhxQwf7/xESZeZgQSBpu/Mwdsvmbfe/2JPRh5EOT+yWLMqSHiClpD3t7Jpf3H3x6vb/KMn/J890NeJh8iYs4E5yKtcR4J5JymEjNmsj/PwSCArujcOey8FfXmPTputp334mnp7RJB3O4++fbcfucjNp+mae8/Ql8LqmmMOqLvtNTf2EnftewmV/gK5N41m9L5vckkqCvD3q+AgLIYT4o5PESpwk2i+ayd0mn1Q24h/t0YDZZGRq76k8mP4YrdN6UOKbw/3BL2Cxm5l5y0wSjG0YvmQofSP68MjwR1i7xpOctCJSSsrwq9iNt6M5TdNNFM2/B1dzGxaDm+DkkZgcvnT3HsAqey5eykbrIH+soZ749m+CKdyGf0AXtm27nQMHXyEw4lXum5VEVKDm/oRKGn7jwEEBxT+kETiyBQA7D8zGrEqZ1Hk6kS0+5bq3c/lqxzHu6BZ12Y+nEEKIPxdJrMQ5mSzGmufh3uFMHPIIb+R9THnjLB5PeIzpP0/nkVWPEBMcg91t59GOjxLpG0nTYS+gtWZ33m5WH13Nv7a9y0P7biaoPIjKvHtp2zYUY8uqXqTKgwVcnaxJL7ezN6eUFtnllO3IwbtLOH6DY/EIuo7srM94+YcumBwhPNvrYyocGyG+I7t2jqVJEvzU2o91yXvp7rOHA8XdaOG3ndKMR4ltOJ4vt6WdMbH6fMtRwvys9GwechmOphBCiD8yuVxK/G6dwjsx9ZFn+Gz0x4xpPYY3+r5BbkUuyw8v59ZWtxLp++s9fZRStA1qy4PxDzKhy5s8H7CBLeWVZFsc+F0ThaVjEEkN9/O5+2cWZ5WwtcDFniwH8/Ir2GhyU/JjOjvf+5FHtyyl0mXhTv+FvN/gRyocG/HO6kBZ6DaMvf+F2ZrO/5u5nYNHvwbglj5PExvzOsXFu7gn9lN+OprPvoxiVu3N4vMtR3G4qu4i/PmWozw+bwfjZm5hz7GiOjmeQog/t4KCAt55550L2nbw4MEUFBTUbkBXuNGjR9OuXTv++9//nnGdO++887Q3M129ejXXX3/9SWWbN2/GaDSe9eanv4f0WIkLEmz79X5UbYPb8mLPF5mzbw73trv3jNt0bxZMTNgQdrtWYNrZhw0r9zI7+RM8f2lIRGFLUgP2kN/gKL3CRpG9sZBjWU6+8oHrU808b/0H/vbtFDZbSHboTgIsPYnt/w7FhmS2b7uDrLaf8FjmsxTGHcFmiMTT1QRd3pAmfuM5XPgGQ5p6c90bBpzuqos1Pt10hDu6RvHk/J/pHB3IoZxSHvhkKwsf6o6v1XxK7Gv2Z7NyTyZ3do8mOtjrlOVCCHGhjidWDzzwwCnLXC4XRqPxNFtVuVJvVaC1RmuNwVC7/TcZGRn8+OOPNXd1v1gul4sJEyZwzTXX1Ep9ID1Wopb0b9KfDwZ+gJ/Hme9FZTUbmXFHL/oOjaXEkk/S3HRabOtD47IWdLslmlv+3p3NQctZE/Qe907pSuuuYbiKYWt5Ja0qosnJaI1b2TCZ/Yjp9CrKz4PvZ7nJ2D6YsoBdeLlXo+xJeOd0IPOlzWS9vR2Pz9vjm9md4c2W8FSnJbw/4ifeHfYzLTw/ZU3Sv2jboIz3hsby9g3tOJJXxhOfb8ftPvlK2cU/pXP3jM18tP4w/V9bwxPzfqKownGpD6kQ4k9i4sSJHDx4kPj4eB5//HFWr15Nnz59uPXWW4mNjQVg+PDhdOjQgbZt2zJt2rSabaOiosjJySElJYXWrVszbtw42rZty8CBAykvP3XKscWLF9O5c2fat29P//79ycysuvq6pKSEu+66i9jYWNq1a8cXX3wBwNdff01CQgJxcXH069cPgMmTJ/Pqq6/W1BkTE0NKSkpNDA888AAJCQkcPXqUv/71r3Ts2JG2bdvy7LPP1myzefNmunXrRlxcHJ06daK4uJiePXuyffv2mnW6d+/Ojh07Top/4MCBZGVlER8fz7p169i+fTtdunShXbt2jBgxgvz8/FP2+euvv6ZVq1b06NGDL7/88qRlb775JiNHjiQ0NPSU7S6U9FiJy+7GNiP5S4/7cRcYebT3w8REtawZx/VU56eYvH4y7+5+m3/c8Q/sbTNYsmgD2fndqCi4Cv3dQ7Ru05xfzA4Obt3F0T35xA+8ndKS1Rxr+T5aO7DuboVP7wjcwTZ2/ZCK776/4GksIDJwOZS6sQADIw1oDUbDBrLn5BLqH8IbA2ZSWZHCS0umMvH6qvmnZm04zLOLdpEYFchLI9sxc30Ks9Yf5lhhBR/emSg3HxXiD2b//n9SXLKnVuv08W5Nixb/d8blL774Ijt37qxJKlavXs2mTZvYuXMn0dHRAEyfPp3AwEDKy8tJTExk5MiRBAWdPIVlcnIys2fP5v333+fmm2/miy++YOzYsSet06NHDzZs2IBSig8++ICXX36Z//znP/zzn//Ez8+Pn3/+GYD8/Hyys7MZN24ca9euJTo6+qQ5/s5k3759fPjhhzWnNqdMmUJgYCAul4t+/fqxY8cOWrVqxahRo/jss89ITEykqKgIm83GPffcw4wZM5g6dSr79++nsrKSdu3anVT/okWLuP7662uOVbt27XjzzTfp1asXzzzzDM899xxTp06tWb+iooJx48axcuVKmjVrxqhRo2qWpaWlMX/+fFauXMnmzZvPuW/nSxIrcdkZDUamjXwLozJiMpz8ERzZYiR78vbw4a4Pucr/KtaUrOHntkm8dP2jfLt2A+vWlWPYCHvXVjV8vUa3IKZXBLs2P0pG8RMY7N74duyHoX04n73yA2WFdnwamQn75f8w/VBKdKAZW4WdMpsHP5UeIbz7uxxt9woAPsofs8VJQNm/eHR+GUey/NiSUkDvliG8O6YDNouRZ4e0pWUDHyZ++TP/WrKHyUPbXvbjJ4T44+vUqVNNUgXwxhtvMH/+fACOHj1KcnLyKYlVdHQ08fHxAHTo0IGUlJRT6k1NTWXUqFEcO3YMu91e8x4rVqxgzpw5NesFBASwePFirr766pp1AgMDzxl3kyZN6NKlS83ruXPnMm3aNJxOJ8eOHWP37t0opQgPD6+Zu9DX1xeAm266iX/+85+88sorTJ8+nTvvvPOs71VYWEhBQUHNRMx33HEHN91000nr7N27l+joaJo3bw7A2LFja3r8/va3v/HSSy+d9VTrhZDEStQJD+OZ7yk1odMEUopSmLx+MgC3tLwFLy8bQ6/txSfOd1iR9xGjQm7j5hajaNYsAgDP5glkrWyKR2ZLvtx5CNOyw5SWV7CkzXv4R3ow9/q5bFp8iG+XHaaJzUiM20k31Yjy9U+TmrgQ/2ITDQ23sr/j9zRXb3O46O8kl97BSyPHclOHxjiOFFF0oACffpHc0imS/ZklTP/hENHBXjVXG7rdmjXJ2eSW2Cl3uGgV5kPHJgEXdNd4IUTdOFvP0uXk5fXrWM7Vq1ezYsUK1q9fj6enJ71796aiouKUbTw8fm1XjUbjaU8Fjh8/nn/84x8MHTqU1atXM3nyZKBqTNRv26rTlQGYTCbcbnfN6xNjOTHuQ4cO8eqrr7J582YCAgK48847qaioOGO9np6eDBgwgIULFzJ37lxq66a7Z2qDt2zZwi23VE2skJOTw9KlSzGZTAwfPvyi3k8SK3HFMRvM/KfXfxizdAyHiw4z5KohABiUgfcHvM+7P73LZ/tmsSD/MwZlDSI+JJ5XtryCURmxeO2nfcBPNCiOZlOHL7i54xDe3PYmSVlJdBmWyKqMFSQl22g2pAvBKU6yjUZ2L7+BNl4mQsyV7EiKwtAjkv6BRwiNWcKIuPvQ5U5yP96Du8SBRzN/PKL8eHJwK47klfHsol0EHi2ls78nE9OzWbkv+6R9aRbqzf29ruLGDhGn7KfD5abS6cbbQ76GQvyZ+fj4UFxcfMblhYWFBAQE4Onpyd69e9mwYcMFv1dhYSGNGjUC4KOPPqopHzhwIG+99VbNabT8/Hy6du3Kgw8+yKFDh2pOBQYGBhIVFcVXX30FwNatWzl06NBp36uoqAgvLy/8/PzIzMxk2bJl9O7dm1atWpGens7mzZtJTEykuLgYm82GyWTinnvuYciQIfTs2fOcPWR+fn4EBASwbt06evbsyaxZs2p6r45r1aoVhw4d4uDBg1x11VXMnj27ZtmJcd95551cf/31F51UgSRW4grl5+HH+wPeZ2vWVloH/jqVgL/Vn0mdJzGq1Shm7JzBskPL+DL5S5r5N+PNvm9iMVr4vx/+jx+zPufDQR/SxLcJs3bP4pM9n1BUWcSn1rcgFrytd/Lo2EcJBmzRvhzdewzH3iLaWX34edUECnvMIiZ0A9//2IuQ4sF4ubtgMAdR8mM6HlF+GJSTB2M+YLSy0WLbCOzkYVPlTB7SpmpeQlc6m39JZ8YmF499/hPNQ72Ja+wPwM+phXy66QjLdh5Da5hzbxdah1d1hR+fYkp6uYT48wgKCqJ79+7ExMQwaNAgrrvuupOWX3vttbz33nu0a9eOli1bnnSq7feaPHkyN910E40aNaJLly41ycXTTz/Ngw8+SExMDEajkWeffZYbbriBadOmccMNN+B2uwkNDeXbb79l5MiRzJw5k/j4eBITE2nRosVp3ysuLo727dvTtm1bmjZtSvfu3QGwWCx89tlnjB8/nvLycmw2GytWrMDb25sOHTrg6+vLXXfddV7789FHH3H//fdTVlZG06ZN+fDDD09abrVamTZtGtdddx3BwcH06NGDnTt3XvDxOx8yV6Co10rsJWzP3k770PZ4mau6oLXWONwOLMaqOQRf3/o603dOx9fiS7hXOJG+kaxNXcu3N35bcxXj42se56qN/lyfdzVOXwt7cyrI8t5NSOIyLLY9aKUpyo8k4efHafxoX5KPTCEt65OqGLJG4/XzAFqYzIQ8EkVK1lukp38OuFHKTF6FDzaTHW8PE16hTzHmYxtGg6J/6wZsPJSL1jDv/m5sO5rPv5bswely06FJIEPiwhkW3+i0++12awyG+p18yVyB4kohcwVeOdLT0+nduzd79+6t9Vs1XCiZK1D8qXhbvOnRqMdJZUqpmqQKYFTLUXy480NKHaVM6TEFjeablG/4dO+n/DXuryRlJvF1ytc82O8B/B3NKN1wjBiLHewx8EMMaf6/sDrqf7QPPsLhTs9StnEDx4yf4H9kIK6rXBA6m4ibrRzb8zMHtu5EG5yEBN6Kn18sdudBStNT+OFQOb2iMqlMnUhi+EO8euvN5Ge8zs3RB3lq1Q0MnLoGE0U83GEeZksIc/YM5JE5mQR6WU66I/zRvDIe/fwncksqWfbI1VhMJzc8pZVO8krtNA70vLQHXgghatnMmTN56qmneO21166YpOpCSI+V+FOYu28uAdYABjQZAMD478azLXsbfRr3YeWRlXiaPVk0fBE2kw2tNc7scjas207aqnIKnAoXYA1IoUnPt1HWAmx5LTm27mEyHYom/V7HGrAPygPwy4kjdW9/0vIbAKAMCk9fE4crHWQZCunR87+E+uViMlpxuUowGDxwY2PRoZsZ1GQ+ZnIBN8rgwdeHh7I2rQ/f/P1qvMyKb374C0v3hrH8SB/sTjev3NiOmzo2BiA17VPQbp77NpS1B10sHt+DFg186uhonx/psRJXCumxEmfze3usJLESf0o/Zf/EbUtvq+nxuqvtXbQOOvmLo7Xm4Y8nEeJoxD09x7Jq7m4MeRm0abGJnMOdSSpRpEQnEZIRQYTVRpPoOFplOnAVl/Fvv3ko5clfHYPxKzOx3mQgI6MML0sxkb1fQzt9Cd4zFn+DJj1xKm5zDmZjINE5T+PZNIKDlVMpLvuRV7c8TMPQboSZv6Fv+Ie4taJJi094+ItKnG7N8r9dTWVlKj+u7w2A221g/5HuLCr8Cwsf6o7VXLuXEdcmSazElUISK3E2klgJcZ4ySjMIsgVhNpw6hc1xbu3GoKq6pIvzKpj+3EoCHVbytJ1md1ro1b4Tz69/nmUpy7AarXS1JjI+aQRZfoUUqGLi8pvhUm5KG7hw3tSIkPIGHNqRjd/+fPwKKik1gocpn+zG3xKc2hdzRTBYjawsKyOkx5O4zVae+OEhpvR4AZstAi9TPkajJ1ke7/HIZ3t4//aONLV+QkrKO3x16FG6G9cSELmJqWufJD6+M/8cHnO5DufvJomVuFJIYiXO5vcmVvX3JKYQFynMK+ysSRVQk1QB+ARaaXOLHxnGYuhzjOs698Xb4s3LvV5m7vVzGdx0MHs5wMEuhTQpaEBcfjO2tj/MOxFz8c0ws/qT2ew6vI6WlS78Cir5JTaPUS3G89+ApTh/uY69eQG8b/sBKlxEaQvk34fVI43/Xv0WPpYimu+8lfD991BWdpCWHh8TEWDjvdX7SE+fh7Z0puynq8jZdgvabWRM023M2nCYW6at5+udx3C5NZWHCinZeOyCj5erxE7ht4fRDve5VxZCiD8pGbwuxO8woGt3WrY5QmPfxieVtw5qzXPdngOqTiEW+6ViCvBgaFxPrte3cnTudm7Y1g++hTJDNnntXTxY/jR9I/vyxMgnCLeF8830nYQmdeKIp5Moq5EGg0ezfs1yCNyCz7EueJvaogsd+B/pSxof8bfOjZm5JQu7PZO524YQZzfRtl8bXK6uBAR/z78T7ubtg/m8uewTfj7UltuSQ3Fml2Fu4su2sgraR/rjYTq/U4Vaa/I/30/FvnzMYV54xgafeyMhhPgTkh4rIX6nSL/Is95nSimFb+/GeMZVTeppUAYiR8Zjvi6cl5p9xIPxLzHO+QTtQtrxcq+XaeTdCIPRwDV/iSFhcCRLWqzFpV1kv7mVJj+PwZLag/SDowi4qy2hjyTQoOlErPnNCHS9woMdV+Nw+9H2WCfM3mY6D4nmqla3YrIW0z51DZ/0TuMfHd4l3vYwaQHv4zKWsvXzPdwybQN9X13D7I2HcbhO3wNVZnfW3FerdHMGFfuqJjet2Hvu+cKEEOevoKCgZm69CzF16lTKyspqMaIrR3Z2ds2k0evWrTvjescno/6t304YDfDqq6+ilDrt+rVBeqyEuAyUyUCDns24s8VfufubuwmyBTG1z9STpvYxGA10HdqMsMLbmf/BdPrmd+L9Bl9yIL+IoenebF58iAbNfFj62UFivB/AePW/0Lb95CVfizcWBt7TFrPFiOfmSEye/mRfNR9naSY27wRy9/lQELmCkoifaPj9RAZEBFNmzsGddQvvL+1Lk7bdSMpM4uar7mXm6rVQsYGGtj008A+nS5N+uJb74dWsCQabiYr9eWi3RhnOb/oLIcTZHU+sHnjggQvafurUqYwdOxZPz7q7zYrT6cRkqv2U4rvvvqNVq1Yn3SX+Yhw9epRvv/2WyMjIWqnvdKTHSojLqEODDvzvmv/x0bUfEeIZctp1ov2iaT6sE+/3/pr+1w5jYPce7An9kW0rDrPsvZ3kWNN5v+Fi/Lc/iEdWLA0qB+JotZmsLUlkvbGV8i05BKhrcPikYioPpGnaBOL23MW+zY9ToQrJbD+VSQ014+PeIcwri6tsc3lr4zN8vv9z3lw7mH7Bk+jbaBHBXpUYndvYn/oMB7v8g8yOM6hs9gtZoXPZtH4IW7eO4cDBVyks3M6BrBJuePdHDmaXXOYjKkT9N3HiRA4ePEh8fDyPP/44AK+88gqJiYm0a9eOZ599FoDS0lKuu+464uLiiImJ4bPPPuONN94gPT2dPn360KdPn1Pqfv7550lMTCQmJoZ77723phf6wIED9O/fn7i4OBISEjh48CAAL7/8MrGxscTFxTFx4kQAevfuXTNvX05ODlFRUQDMmDGDm266iSFDhjBw4EBKSkro168fCQkJxMbGsnDhwpo4Zs6cSbt27YiLi+O2226juLiY6OhoHA4HUDX9TVRUVM1rgO3bt/PEE0+wdOlS4uPjKS8vZ/bs2cTGxhITE8OECRNOezynTJlCy5Yt6d+/P/v27Ttp2d///ndefvnlS/ojUHqshLjMOjTocM51hjQbwpBmVXMkOtwO/pI6juJVrSiwZuFzfSGTWj3Cf7/4N0//9DAWbaYpgdiVA6JN+I9oRnDcw1RuK8K9+mqc9nIUsC07mr2b72ZYp/dIcY+DMk3r1q+zdeej3Orlxit3HH6N3sKWFYe23Etizx5M/c8P/M07j4peP5GZv5hMvQCaKrwr2uK0lHDkyPscPjyNVcdu51BOJ3ys0qSI+u3/klPZWXLq5MUXI8bbxj+bnzpf6HEvvvgiO3fuZPv27QAsX76c5ORkNm3ahNaaoUOHsnbtWrKzs2nYsCFLliwBqub98/Pz47XXXmPVqlUEB5869vGhhx7imWeeAeC2227jq6++YsiQIYwZM4aJEycyYsQIKioqcLvdLFu2jAULFrBx40Y8PT3Jyzv3af/169ezY8cOAgMDcTqdzJ8/H19fX3JycujSpQtDhw5l9+7dTJkyhR9++IHg4GDy8vLw8fGhd+/eLFmyhOHDhzNnzhxGjhyJ2fzrBUXx8fE8//zzbNmyhbfeeov09HQmTJhAUlISAQEBDBw4kAULFpw0v19SUhJz5sxh27ZtOJ1OEhIS6NChqs1dtGgRjRo1Ii4u7pz7dTGkx0qIK5zZYGZK3+dZ0PE/HL36B57o9hixIbHcdM3tjG41kdnX/MC+0WXc2OJRNvU7gnfncPYXZ/FM+kHmBCTh1poMh5v4UhMtD7cnN2kMAGE/3UPZPH98fxlEQ+8i/JtPQ1X647vjHhpt9MT3UC6PGD3ZVxpOUfhjPL3+Od7dNRy99kWsmx6nY4f5XN1zC2WqPX3CP+SF/hsI8fY4x94IIc5l+fLlLF++nPbt25OQkMDevXtJTk4mNjaWFStWMGHCBNatW4efn98561q1ahWdO3cmNjaWlStXsmvXLoqLi0lLS2PEiBFA1Xx6np6erFixgrvuuqvmlOK5JkEGGDBgQM16WmuefPJJ2rVrR//+/UlLSyMzM5OVK1dy44031iR+x9e/5557aub2+/DDD885P+DmzZvp3bs3ISEhmEwmxowZw9q1a09aZ926dYwYMQJPT098fX0ZOnQoAGVlZUyZMoXnn3/+nPt0seTnpRD1QKRvJAtvWICfhx9mY9UvukHRg+jfpD9mgxmtNY0PvcOs3bPoG9mXCWsn4HA78O/iyZLkn9lXdpgNhvVYXDYSinrR9OsXsDv9ucrDhcnaE3vTHbhNaRxZcx+5IT60TdeYvjyCh1Hxki7n6Mdf4NNoMfssh9lSaebWnEGMe/1lxpf3Irj4Xg528iGcjygouJaAgE51fLSEuHBn61m6XLTWTJo0ifvuu++UZUlJSSxdupRJkyYxcODAmt6o06moqOCBBx5gy5YtNG7cmMmTJ1NRUcGZ7l95pnGSJpMJt9tdU+eJvLy8ap5/8sknZGdnk5SUhNlsJioqqub9Tldv9+7dSUlJYc2aNbhcLmJizn7fvfO97+bp3uvgwYMcOnSoprcqNTWVhIQENm3aRFhY2HnVe76kx0qIeqKBVwOsJutJZcfvw6WUYmybsezJ28P9395PWkkaL139Es92fZa/3v4gU+9/mSW3LeCTsf9j6oSnGP/KMLxH+7HYnMv23ABaNpmGTn8Zd1kzht3flR+C8slzuvim0Vacbd7BK+ptfLyLeLLzk3Tt2Q8DBp7L7I6xyI4VE70230Er4+v4+yfWxaERol7z8fGhuLi45vU111zD9OnTKSmpGrOYlpZGVlYW6enpeHp6MnbsWB577DG2bt162u2PO54EBQcHU1JSwrx58wDw9fUlIiKCBQsWAFBZWUlZWRkDBw5k+vTpNVcYHj8VGBUVRVJSEkBNHadTWFhIaGgoZrOZVatWcfjwYQD69evH3Llzyc3NPalegNtvv53Ro0efs7cKoHPnzqxZs4acnBxcLhezZ8+mV69eJ61z9dVXM3/+fMrLyykuLmbx4sUAxMbGkpWVRUpKCikpKURERLB169ZaT6pAeqyE+MO4vun1vL71dbZnb+fumLtPGcsVZAsiiCCgag7DHl3a0zEmhs9f3My3H2RQUeJLXN+GWGwmbhrXj1nP/kh+thexzSNoEdCPce3uxsfig3ZrDidt5LvCNZT3CGBY01swLj6EexnkeGYRktigLnZfiHorKCiI7t27ExMTw6BBg3jllVfYs2cPXbt2BcDb25uPP/6YAwcO8Pjjj2MwGDCbzbz77rsA3HvvvQwaNIjw8HBWrVpVU6+/vz/jxo0jNjaWqKgoEhN//eEza9Ys7rvvPp555hnMZjOff/451157Ldu3b6djx45YLBYGDx7Mv//9bx577DFuvvlmZs2aRd++fc+4H2PGjGHIkCF07NiR+Ph4WrVqBUDbtm156qmn6NWrF0ajkfbt2zNjxoyabZ5++mlGjx59zuMUHh7OCy+8QJ8+fdBaM3jwYIYNG3bSOgkJCYwaNYr4+HiaNGlCz549z+8foRbJlDZC/IHM2z+P79O+55WrX6k5ZXguuWklzHs5CZfDzdh/dsE3yAbAxkW/sGVpCjc/mUhIpA9utyYrpYiQSB+MJgP3f3s/e/P2snDwYmb+cy2+lW5aPRJO56bnN0uNTGkjrhQypU3dmTdvHgsXLmTWrFl1HcoZ/d4pbaTHSog/kBtb3MiNLW78XdsENfJmyPg4SvIqapIqgPgBkfy8OpWNi37hugfasWrmHvZuyMDqbaZl5zBuuepOxqeN48X/zKBhUWuyB+9nbNTg2t4lIcQf1Pjx41m2bBlLly6t61BqlSRWQggaNvM/pczDZqL9wEg2LPiFJe/s4PDOXGKubkR5sZ2fV6fi/k7zF49/Y6n0wrNLKeOHXtjNDYUQf05vvvlmXYdwSVxwYqWUagzMBMIANzBNa/26UioQ+AyIAlKAm7XW+RcfqhDicmvXpzE/fXeUwztzie/fmG4jm6GUoqLEwS8/ZbN3Sxra4mTE7afemFCI+kRmDhCncyHDpS6mx8oJPKq13qqU8gGSlFLfAncC32mtX1RKTQQmAqe/PaoQ4opm9jDS/6425KaWEj+gcc1/PFZvM226N6RN94Z1HKEQF89qtZKbm0tQUJAkV6KG1prc3FysVuu5Vz7BBSdWWutjwLHq58VKqT1AI2AY0Lt6tY+A1UhiJUS9FdkmiMg2QXUdhhCXTEREBKmpqWRnZ9d1KOIKY7VaiYj4ffc2q5UxVkqpKKA9sBFoUJ10obU+ppQKPcM29wL3Apd0MkQhhBDibMxmM9HR0XUdhviDuOgbhCqlvIEvgL9prYvOdzut9TStdUetdceQkNNPRiuEECdSSk1XSmUppXaeUBaolPpWKZVc/TfghGWTlFIHlFL7lFLXnFDeQSn1c/WyN1T1+R+llIdS6rPq8o3VPxqFEOK8XVRipZQyU5VUfaK1/rK6OFMpFV69PBzIurgQhRCixgzg2t+UTaRqXGdz4Lvq1yil2gC3AG2rt3lHKWWs3uZdqnrMm1c/jtd5N5CvtW4G/Bd46ZLtiRDiD+mCE6vqX3j/A/ZorV87YdEi4I7q53cACy88PCGE+JXWei2Q95viYVSN56T67/ATyudorSu11oeAA0Cn6h98vlrr9brqkp+Zv9nmeF3zgH7He7OEEOJ8XEyPVXfgNqCvUmp79WMw8CIwQCmVDAyofi2EEJfKSeM6gePjOhsBR09YL7W6rFH189+Wn7SN1toJFAIycl8Icd4u5qrA74Ez/ZLrd6H1CiFELTld+6TPUn62bU6tXC7AEUKcxkUPXhdCiDp2pnGdqUDjE9aLANKryyNOU37SNkopE+DHqaceAbkARwhxepJYCSHquzON61wE3FJ9pV80VYPUN1WfLixWSnWpHj91+2+2OV7XjcBKfSXMVC+EqDdkrkAhRL2hlJpN1Q2Ig5VSqcCzVI3jnKuUuhs4AtwEoLXepZSaC+ymaqaIB7XWruqq/krVFYY2YFn1A6ouyJmllDpAVU/VLZdht4QQfyCSWAkh6g2t9egzLDrtuE6t9RRgymnKtwAxpymvoDoxE0KICyGnAoUQQgghaokkVkIIIYQQtUQSKyGEEEKIWiKJlRBCCCFELZHESgghhBCilkhiJYQQQghRSySxEkIIIYSoJZJYCSGEEELUEkmshBBCCCFqiSRWQgghhBC1RBIrIYQQQohaIomVEEIIIUQtkcRKCCGEEKKWSGIlhBBCCFFLJLESQgghhKglklgJIYQQQtQSSayEEEIIIWqJJFZCCCGEELVEEishhBBCiFoiiZUQQgghRC2RxEoIIYQQopZIYiWEEEIIUUsksRJCCCGEqCWSWAkhhBBC1BJJrIQQQgghaokkVkIIIYQQtUQSKyGEEEKIWiKJlRBCCCFELZHESgghhBCilkhiJYQQQghRSySxEkIIIYSoJZJYCSGEEELUEkmshKintMOB/fBhtNN5yjJnTg75c+ZQumEj7vLyC3uDymJwnVo3gKsg/7TvK4QQf3amug5AXCLFGZD3CwS3AK/gS/Y27spK3GVlGH18UKaTP06lleXgduFl876gurXTibukBIOfHwqgPB8Kj4JXKNonDHdpGQYvT5RSZ6/H5cJ+6BDa7cYSGYkym7EfPoI95RBmSzEexRtBgduvNTqkDaYmbas2/OlT2PAeGC3Qdji0uBbtG0lF8iHc5RWAxhgQgKVxYwyenmi7HVdJCbhcaJeLsqQkir/5BmfqIbybeeITVoCpZTdU7AjcXo1wFRWhHQ6Mvr4YfXyqYtUag82GMhrB5cCR+ws5afsIzduHyjtIRXo5JUfslO1LpyIlG+3UKJMbW5ADWyMvrK1aUekKJX3ZFnY2aEhAcRENC3Lx9TfhEWjAEGTjYOueHAhuRtt1X+O7cQdGqwHPpv4Yfb1JzXOzzTeCRiqTpno/QQYHbnsDHA4vtPLAXeGk8mgGzmInBrMbWyMjlgYBuLUfTruF8Jdexdyk2QX9e19JlFLXAq8DRuADrfWLdRySEKKeUFrrS1Px72iYOnbsqLds2XJJ4jiFvQycFWiLL868PAxWKwYvL9xlZTgzM3GXl2OwWlEeHmiHA12Sh7bb0U437tJ8So4dpKwoG/8Af7wjo9AVxbgzUnBXVKD8GoJPGJWFxRTlZOEyOrB7G7GZnYS6NWBG2awUeXpjt1lxmwzgdmNyOnEVlVGQ4yIrsxRbRSGB9gyUvYR0tw+5Jm8ICscY3hTl6YFyl+JZnEP40YPYMo6xLyyMvcEN8S4voUlmGt7ZWRS6jZQbPQh0FxFoceKIakpBy3hyfRqSVeSgsKwcs6MMS0UhrQqSaZO/DaUNHPGK4ahHQ0Iz0glLPUqhvx+pkY1x26w0dJYQaC8ho8hBqrbhkVeCZ1YpZrsDo7MSlwkqLRbyAoL4vvPVrIrtAgoGH93EsIw9+KXmYMrMpyTAh9wwf+zajTm7AIddkRnelNSwaHLMHuRarHgVFxOzdwfNf0nGpu0YrQZKAr3ICA+nUpuxZldQarKyt1kb9kc1w4TG215BUEk+TTKPEp6dgcVRiXY5OWYN4lCjaMqsNowuJ26jkVzfAHL8gyi12rCbzQQU5RO7/2daHtqP1WDHbNO43RqX20Kp1ZMCbx/yfQLIswWR6xtIobcPxZ7eeFaUE5lxlLDsY3hUVuDhqMS3rIyA0jJy/QP5JSKCEi9vQvLy8C0vIy0olD1NmqPcbiIz0miUmUpYfjYBJQVol6bcYiU1rBEpjZtwLCiMPN9AXEYjPmUlRGYcxae0CIPTTZm3jYzABhTbvAkpyqNJVhq+BYUYXS5SQ8L4uUUMlRZrzcfeYq/EVllOhcVKpcev5VFZRwktzMHlVmQGhJIaEg7Viapyu7E47ZhcLvxLCmlzaA/N0g6QFxhEeoMmHPML4Zh/CBVmD/xKi/AtK+K1IDfdrrnhvL6KSqkkrXXHWv1+1wKllBHYDwwAUoHNwGit9e4zbXNZ2zAhRJ07W/t1SRKr39swnXejpDVPvPs2pQ4nOBw4gQqrjQqLlQqM2A1m7KaqR7nFSonVk3IPG26DAQ2YXU487BWYnU5cBiNaGfCsLMerspwKswcF3r5Umi142CuxOO1Umi1UWqy4lQHQuA0G3AZjTTgmpwOj2w2A0e3Cw16JyeWk0uJBucWKw2ypWdfssONTXkKJzQu72eMCj+ylY3HYMbpclFttF1yHwe3Gbag6u6y0plnGEYxuF8lhTXAZjefYump7m70CD6eDYqsnDpP5PLZxEVaUh9KaCrMHxR427Ccc9+OMLhceTjtuZcCgNT4VpfhWlGFxOjBqNznefhzzC0Krs5wd1xrvynJ8y0vxsldgdVRSbvYg19uPQpt3zb6fyORyYnE6KPOoOq5mp4PIvEyMbje5Xr7ke/meup3W+JWXElBWjH9ZMZ72crJ8AkkLCMFpMGJ2ObHaK/GxV+DhsJPv6UOut19NPRang+icdKJyMnAYTRRbbVSYPag0mTFoNw0LcggoLSI1MJSUoHDKLFbcBgNWRyUtc4/RurKEDG0gw8OLSpMZt8FArpcvmX5BJx3PwNIigkoL8XA6KLVYKbHaeL2xP706nF+udAUnVl2ByVrra6pfTwLQWr9wpm3Otw0b9sFMUv1DaytUIUQtiijIYuE9t5/Xumdrvy7VqcBOwAGt9S/VAcwBhgFn/MV3XpRiXtMONf9JAaA1FqcDk9uF0e3G5HZhcrmwuBwElxVjK8zFoKuSH6fBiN1kxmkwYtCVKK0pN1vI8Q/C7HQQVpyHxenEbjLjMJoIrCjDw2nH4NagwKA1Fqcds8uF02TCbjTjUgqtVE3dDqMJ/5ICbE4HnvZKbPZKXAZFvs2HEg8bjYry8C8rxsPprDq9pRQoUCisRo2nUWE3WShWFlxaY3M6sFRWgKMSnC5MSmMxGnBabBTavCgzGAktLSK8MA+70Uiu1Qu7yYQXGg+DosxkpsxgwuqoxKe0BC97BTajwmg04DKYsaNI8fDmoG8gLoOBJhnFhGkXOQYTOUYzNrudwIoSTFpXJYUeNoJ8vAgLDqYkP4+c7BxK3W4qLVacFg98LCb8DA7i3G48y3NxlpZQmp/Ofh9/Ci02ilH4K01DZyVmg4F8mxdlTjdBRXn452cR6O1NeHg4HkYnu7SZA9pYkxj7lBbhnZOJRYEKDMHqYaRZaR7G4kLMNhsWqycVTjOp+ZWkOVyUm8zYDSYiLQaaGd3YzCbAQGV5MYVFZZQUFRLSqBFNWrbG4XRxOCeXA+V2LBYrJpMJZbHgVuBlMBDmYSbUYkaVOykvcGOy+mO22XAB5eUVOB1FuC0WygwGylwucvLyCfIwEx/ZmOAGDSiyOzmUX4AtM59MZx52pwsjJRh1AWWNm5Jl9cZYXIA77QgRZhPNo5oQGtIMV1kpJVkZhEY3xSswmLS0NJL376fYUURoWBhBQUFYrVYwGikuL6e8tBQPg5mAFvF4eXlhMpkwGAy4XC7sdjtKKby9vCjNzSavqIis3Hy8fGxEREQQHByMoTo5c7vdHDt2DKfTidVqRSnF4eJStheXE2kx0sRswIgvDkcDACwWCxaLhbCwsIv6il8hGgFHT3idCnT+7UpKqXuBewEiIyMvT2RCiCvepUqszqthuhALvEsgwBujlw8mZcCKG+Vw4OHtjYfFA4uHBwajEaVUzX8SLpcLp9PJ8d45g8GAh4cHxur1TqS1xu1243a7MRqNNXUcX3au8TyXktvtPimeP7o+l/n94i5h3YFAVFgDaN3yHGsmnFrU9Kqap1FRUURFRV10PJ5eXoQAZ4rGYDDQqFGjk8pCQyHxot+5Xjjdl/yUrn2t9TRgGlT1WJ1Pxef7a1gIUX9dqsTqnA3Thf7aa9djwO8Oxmw+9ymlE+LCaDRiPM2pq7pMqoA/VVIlRB1KBRqf8DoCSK+jWIQQ9cyl+p/6nA2T1nqa1rqj1rpjSEjIJQpDCCF+t81Ac6VUtFLKAtwCLKrjmIQQ9cSlSqykYRJC1EtaayfwEPANsAeYq7XeVbdRCSHqi0tyKlBr7VRKHW+YjMB0aZiEEPWF1nopsLSu4xBC1D+X7Aah0jAJIYQQ4s9GRkMLIYQQQtQSSayEEEIIIWqJJFZCCCGEELVEEishhBBCiFoiiZUQQgghRC2RxEoIIYQQopao4/Pn1WkQSmUDh3/HJsFAziUK51Kqr3GDxF4X6mvccH6xN9Fa/yGmXfidbdgf/d/1SlRf4waJvS5cVPt1RSRWv5dSaovWumNdx/F71de4QWKvC/U1bqjfsV9q9fnY1NfY62vcILHXhYuNW04FCiGEEELUEkmshBBCCCFqSX1NrKbVdQAXqL7GDRJ7XaivcUP9jv1Sq8/Hpr7GXl/jBom9LlxU3PVyjJUQQgghxJWovvZYCSGEEEJccepVYqWUulYptU8pdUApNbGu4zkbpVRjpdQqpdQepdQupdQj1eWBSqlvlVLJ1X8D6jrW01FKGZVS25RSX1W/ri9x+yul5iml9lYf+671IXal1N+rPyc7lVKzlVLWKzVupdR0pVSWUmrnCWVnjFUpNan6O7tPKXVN3UR9ZagvbZi0X3WjvrZfIG3YiepNYqWUMgJvA4OANsBopVSbuo3qrJzAo1rr1kAX4MHqeCcC32mtmwPfVb++Ej0C7DnhdX2J+3Xga611KyCOqn24omNXSjUCHgY6aq1jACNwC1du3DOAa39TdtpYqz/ztwBtq7d5p/q7/KdTz9owab/qRr1rv0DasFNorevFA+gKfHPC60nApLqO63fEvxAYAOwDwqvLwoF9dR3baWKNqP5g9QW+qi6rD3H7AoeoHjt4QvkVHTvQCDgKBAIm4Ctg4JUcNxAF7DzXMf7t9xT4Buha1/HX0TGrt22YtF+XJe562X5VxyVt2AmPetNjxa//cMelVpdd8ZRSUUB7YCPQQGt9DKD6b2gdhnYmU4EnAPcJZfUh7qZANvBh9WmAD5RSXlzhsWut04BXgSPAMaBQa72cKzzu3zhTrPX2e3sJ1MtjIe3XZVMv2y+QNuy36lNipU5TdsVf0qiU8ga+AP6mtS6q63jORSl1PZCltU6q61gugAlIAN7VWrcHSrlyup7PqPpc/jAgGmgIeCmlxtZtVLWmXn5vL5F6dyyk/bqs6mX7BdKG/VZ9SqxSgcYnvI4A0usolvOilDJT1Sh9orX+sro4UykVXr08HMiqq/jOoDswVCmVAswB+iqlPubKjxuqPiOpWuuN1a/nUdVQXemx9wcOaa2ztdYO4EugG1d+3Cc6U6z17nt7CdWrYyHt12VXX9svkDbsJPUpsdoMNFdKRSulLFQNJltUxzGdkVJKAf8D9mitXzth0SLgjurnd1A1duGKobWepLWO0FpHUXWMV2qtx3KFxw2gtc4AjiqlWlYX9QN2c+XHfgToopTyrP7c9KNq0OqVHveJzhTrIuAWpZSHUioaaA5sqoP4rgT1pg2T9uvyq8ftF0gbdrK6HkD2OwebDQb2AweBp+o6nnPE2oOq7sIdwPbqx2AgiKqBlcnVfwPrOtaz7ENvfh38WS/iBuKBLdXHfQEQUB9iB54D9gI7gVmAx5UaNzCbqnEUDqp+zd19tliBp6q/s/uAQXUdfx0fu3rRhkn7VWcx18v2qzp2acOqH3LndSGEEEKIWlKfTgUKIYQQQlzRJLESQgghhKglklgJIYQQQtQSSayEEEIIIWqJJFZCCCGEELVEEishhBBCiFoiiZUQQgghRC2RxEoIIYQQopb8fxKqvix2UYRSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training \n",
    "batch_size = 32\n",
    "seq_length = 6\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = prediction_model(input_size=92, batch_size=batch_size).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "weights = torch.FloatTensor([1,10])\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X_features, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch, criterion)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    # ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    # ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    # ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    # ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "    ax[0].plot(train_losses_history, label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, label=f\"test accuracy fold{fold}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# put the legend out of the figure, and adjust the position, prevent the figure from being covered\n",
    "# ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# save the figure\n",
    "# fig.savefig(\"/DATA/User/wuxinchao/project/pMHC-TCR/result/pMHC_without_em_with_encoder_loss_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((32, 92)).to(device)\n",
    "# # a.shape\n",
    "d = model(c).to(device)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "# loss = nn.CrossEntropyLoss()(model(a).to(torch.float32).view(32, 1, 1), torch.ones((32, 1, 1)).to(torch.float32).to(device)) / 32\n",
    "# loss.to(device)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "a = torch.randint(0,2,(32,1)) # mask\n",
    "b = torch.zeros((32,2))\n",
    "# get the value of b where a is 1\n",
    "a = a.to(torch.bool)\n",
    "b[(a==1).squeeze(),1] = 1\n",
    "b[(a==0).squeeze(),0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58281,\n",
       " 58416,\n",
       " 58542,\n",
       " 58611,\n",
       " 58658,\n",
       " 58702,\n",
       " 58690,\n",
       " 58742,\n",
       " 58743,\n",
       " 58814,\n",
       " 58780,\n",
       " 58782,\n",
       " 58809,\n",
       " 58851,\n",
       " 58811,\n",
       " 58862,\n",
       " 58845,\n",
       " 58924,\n",
       " 58926,\n",
       " 58956,\n",
       " 59000,\n",
       " 58959,\n",
       " 58986,\n",
       " 59007,\n",
       " 59003,\n",
       " 59028,\n",
       " 59011,\n",
       " 59036,\n",
       " 59093,\n",
       " 59022,\n",
       " 59026,\n",
       " 59039,\n",
       " 59058,\n",
       " 59074,\n",
       " 59073,\n",
       " 59092,\n",
       " 59129,\n",
       " 59109,\n",
       " 59126,\n",
       " 59136,\n",
       " 59095,\n",
       " 59103,\n",
       " 59117,\n",
       " 59178,\n",
       " 59150,\n",
       " 59128,\n",
       " 59169,\n",
       " 59169,\n",
       " 59168,\n",
       " 59220,\n",
       " 59168,\n",
       " 59187,\n",
       " 59174,\n",
       " 59140,\n",
       " 59204,\n",
       " 59260,\n",
       " 59192,\n",
       " 59215,\n",
       " 59253,\n",
       " 59172,\n",
       " 59172,\n",
       " 59230,\n",
       " 59222,\n",
       " 59185,\n",
       " 59212,\n",
       " 59218,\n",
       " 59261,\n",
       " 59255,\n",
       " 59297,\n",
       " 59282,\n",
       " 59255,\n",
       " 59293,\n",
       " 59311,\n",
       " 59274,\n",
       " 59273,\n",
       " 59282,\n",
       " 59276,\n",
       " 59299,\n",
       " 59301,\n",
       " 59253,\n",
       " 59271,\n",
       " 59323,\n",
       " 59251,\n",
       " 59325,\n",
       " 59350,\n",
       " 59265,\n",
       " 59328,\n",
       " 59363,\n",
       " 59334,\n",
       " 59329,\n",
       " 59348,\n",
       " 59340,\n",
       " 59357,\n",
       " 59336,\n",
       " 59315,\n",
       " 59335,\n",
       " 59358,\n",
       " 59328,\n",
       " 59318,\n",
       " 59357]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss = loss_fn(d, torch.ones((32,2)).to(device))\n",
    "# loss.backward()\n",
    "# train_losses_history\n",
    "# test_losses_history\n",
    "# test_accuracy_history\n",
    "train_accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model input shape is : torch.Size([32, 92])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2870, -0.0434],\n",
       "        [-0.3169, -0.0655],\n",
       "        [-0.2865, -0.0411],\n",
       "        [-0.3154, -0.0643],\n",
       "        [-0.3112, -0.0605],\n",
       "        [-0.2862, -0.0417],\n",
       "        [-0.2947, -0.0502],\n",
       "        [-0.3223, -0.0679],\n",
       "        [-0.3114, -0.0614],\n",
       "        [-0.3071, -0.0575],\n",
       "        [-0.3100, -0.0598],\n",
       "        [-0.3044, -0.0542],\n",
       "        [-0.2960, -0.0493],\n",
       "        [-0.3085, -0.0567],\n",
       "        [-0.3342, -0.0772],\n",
       "        [-0.3270, -0.0721],\n",
       "        [-0.3018, -0.0537],\n",
       "        [-0.3169, -0.0647],\n",
       "        [-0.2889, -0.0440],\n",
       "        [-0.3067, -0.0579],\n",
       "        [-0.2876, -0.0417],\n",
       "        [-0.2872, -0.0405],\n",
       "        [-0.3142, -0.0639],\n",
       "        [-0.3049, -0.0558],\n",
       "        [-0.3075, -0.0570],\n",
       "        [-0.2959, -0.0517],\n",
       "        [-0.2968, -0.0511],\n",
       "        [-0.3162, -0.0621],\n",
       "        [-0.2941, -0.0467],\n",
       "        [-0.2973, -0.0524],\n",
       "        [-0.3051, -0.0581],\n",
       "        [-0.3202, -0.0669]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss.backward()\n",
    "# torch.zeros((32,2))\n",
    "model(torch.randn((32,92)).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, the features are concatanated and used to predict the binding affinity of pMHC-TCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_pred(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        super(pMHC_TCR_pred, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        # use the encoded features to predict the binding affinity through MLP\n",
    "        self.Linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_encode(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        self.input_size = input_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.seq_length, self.input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :] # return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 82, 'BseqCDR3': 24}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "print(len_map)\n",
    "# drop the rows with length == max length, which is much longer than the others\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < len_map[\"AseqCDR3\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.200e+02, 2.288e+03, 2.800e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.100e+01]),\n",
       " array([ 5. , 12.6, 20.2, 27.8, 35.4, 43. , 50.6, 58.2, 65.8, 73.4, 81. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df[\"AseqCDR3\"].value_counts()\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0) # find the longest seq\n",
    "# df.loc[df[\"AseqCDR3\"].str.len() == 83, \"AseqCDR3\"]\n",
    "\n",
    "plt.hist(df[\"AseqCDR3\"].str.len().sort_values(axis=0))\n",
    "# plt.show()\n",
    "# df = df.loc[df[\"AseqCDR3\"].str.len() < 83, :]\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_map\n",
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/20230228.csv\")\n",
    "# df.loc[df[\"AseqCDR3\"].str.contains(\"_\"),]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
