{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "# check the kernel running in the notebook\n",
    "# !uname -a\n",
    "# find the variables in the notebook\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seqCDR[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_encode_data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # df = df[df[\"AseqCDR3\"].str.contains(\"_\")==False and df[\"BseqCDR3\"].str.contains(\"_\")==False].drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        # drop the rows with duplicate CDR3 sequences\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        \n",
    "        # drop the rows with length == max length, which is much longer than the others\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # If there is any NaN value, drop the row\n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        # concatenate the encoded features\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "        \n",
    "        # discard the duplicate rows, keep the first one\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9865399841646872"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df[\"AseqCDR3\"].str.contains(\"_\")==False and df[\"BseqCDR3\"].str.contains(\"_\")==False].drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "# sum(df[\"BseqCDR3\"].str.contains(\"_\")==False) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 49)\n",
    "            nn.Conv1d(5, 10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 25) based on the formula for conv1d: (W + 2P - K)/S + 1 = (49 + 2*1 - 3)/2 + 1 = 25\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 10, 23), 25 - 2 = 23 \n",
    "\n",
    "            nn.Conv1d(10, 15, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12) based on the formula for conv1d: (W + 2P - K)/S + 1 = (23 + 2*1 - 3)/2 + 1 = 12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 10), 12 - 2 = 10\n",
    "\n",
    "            nn.Conv1d(15, 20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 5) based on the formula for conv1d: (W + 2P - K)/S + 1 = (10 + 2*1 - 3)/2 + 1 = 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 3)\n",
    "\n",
    "            nn.Conv1d(20, 20 , kernel_size=5, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 1) based on the formula for conv1d: (W + 2P - K)/S + 1 = (3 + 2*1 - 5)/2 + 1 = 1\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 1)\n",
    "            nn.ConvTranspose1d(20, 20, kernel_size=5, stride=2, padding=1),\n",
    "            # (batch_size, 20, 3), based on the formula for convtranspose1d: (W−1)S−2P+F = (1-1)*2-2*1+5= 3\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(20, 15, kernel_size=3, stride=3, padding=1),\n",
    "            # (batch_size, 15, 5), based on the formula for convtranspose1d: (W−1)S−2P+F = (3-1)*3-2*1+3= 7\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(15, 10, kernel_size=7, stride=3, padding=1),\n",
    "            # (batch_size, 10, 23) based on the formula for convtranspose1d: (W−1)S−2P+F = (7-1)*3-2*1+7= 23\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(10, 5, kernel_size=7, stride=2, padding=1),\n",
    "            # (batch_size, 5, 49) based on the formula for convtranspose1d: (W−1)S−2P+F = (23-1)*2-2*1+7= 49\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = torch.tensor(x, dtype=np.float32)\n",
    "        # x = torch.tensor(x, dtype=torch.float)\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        # print(f\"encoding shape: {encoded.shape}\")\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2492 (0%)]\tLoss: 1.468642\n",
      "Train Epoch: 1 [1600/2492 (65%)]\tLoss: 1.236508\n",
      "Train Epoch: 2 [0/2492 (0%)]\tLoss: 1.275537\n",
      "Train Epoch: 2 [1600/2492 (65%)]\tLoss: 1.287476\n",
      "Train Epoch: 3 [0/2492 (0%)]\tLoss: 1.213971\n",
      "Train Epoch: 3 [1600/2492 (65%)]\tLoss: 1.186140\n",
      "Train Epoch: 4 [0/2492 (0%)]\tLoss: 1.132698\n",
      "Train Epoch: 4 [1600/2492 (65%)]\tLoss: 1.129370\n",
      "Train Epoch: 5 [0/2492 (0%)]\tLoss: 1.173454\n",
      "Train Epoch: 5 [1600/2492 (65%)]\tLoss: 1.134380\n",
      "Train Epoch: 6 [0/2492 (0%)]\tLoss: 1.217406\n",
      "Train Epoch: 6 [1600/2492 (65%)]\tLoss: 1.098882\n",
      "Train Epoch: 7 [0/2492 (0%)]\tLoss: 1.187191\n",
      "Train Epoch: 7 [1600/2492 (65%)]\tLoss: 1.134753\n",
      "Train Epoch: 8 [0/2492 (0%)]\tLoss: 1.094056\n",
      "Train Epoch: 8 [1600/2492 (65%)]\tLoss: 1.104883\n",
      "Train Epoch: 9 [0/2492 (0%)]\tLoss: 1.131941\n",
      "Train Epoch: 9 [1600/2492 (65%)]\tLoss: 1.148100\n",
      "Train Epoch: 10 [0/2492 (0%)]\tLoss: 1.120489\n",
      "Train Epoch: 10 [1600/2492 (65%)]\tLoss: 1.175966\n",
      "Train Epoch: 11 [0/2492 (0%)]\tLoss: 1.113285\n",
      "Train Epoch: 11 [1600/2492 (65%)]\tLoss: 1.026940\n",
      "Train Epoch: 12 [0/2492 (0%)]\tLoss: 1.147370\n",
      "Train Epoch: 12 [1600/2492 (65%)]\tLoss: 1.224221\n",
      "Train Epoch: 13 [0/2492 (0%)]\tLoss: 1.118744\n",
      "Train Epoch: 13 [1600/2492 (65%)]\tLoss: 1.162566\n",
      "Train Epoch: 14 [0/2492 (0%)]\tLoss: 1.020469\n",
      "Train Epoch: 14 [1600/2492 (65%)]\tLoss: 0.973583\n",
      "Train Epoch: 15 [0/2492 (0%)]\tLoss: 1.060959\n",
      "Train Epoch: 15 [1600/2492 (65%)]\tLoss: 1.105564\n",
      "Train Epoch: 16 [0/2492 (0%)]\tLoss: 1.188799\n",
      "Train Epoch: 16 [1600/2492 (65%)]\tLoss: 1.088326\n",
      "Train Epoch: 17 [0/2492 (0%)]\tLoss: 1.135020\n",
      "Train Epoch: 17 [1600/2492 (65%)]\tLoss: 1.165980\n",
      "Train Epoch: 18 [0/2492 (0%)]\tLoss: 1.050911\n",
      "Train Epoch: 18 [1600/2492 (65%)]\tLoss: 0.988616\n",
      "Train Epoch: 19 [0/2492 (0%)]\tLoss: 1.120778\n",
      "Train Epoch: 19 [1600/2492 (65%)]\tLoss: 1.014441\n",
      "Train Epoch: 20 [0/2492 (0%)]\tLoss: 1.000105\n",
      "Train Epoch: 20 [1600/2492 (65%)]\tLoss: 1.060050\n",
      "Train Epoch: 21 [0/2492 (0%)]\tLoss: 1.110094\n",
      "Train Epoch: 21 [1600/2492 (65%)]\tLoss: 1.096785\n",
      "Train Epoch: 22 [0/2492 (0%)]\tLoss: 1.036026\n",
      "Train Epoch: 22 [1600/2492 (65%)]\tLoss: 1.116344\n",
      "Train Epoch: 23 [0/2492 (0%)]\tLoss: 1.075500\n",
      "Train Epoch: 23 [1600/2492 (65%)]\tLoss: 1.074945\n",
      "Train Epoch: 24 [0/2492 (0%)]\tLoss: 1.182025\n",
      "Train Epoch: 24 [1600/2492 (65%)]\tLoss: 0.999277\n",
      "Train Epoch: 25 [0/2492 (0%)]\tLoss: 1.063120\n",
      "Train Epoch: 25 [1600/2492 (65%)]\tLoss: 1.031433\n",
      "Train Epoch: 26 [0/2492 (0%)]\tLoss: 1.024140\n",
      "Train Epoch: 26 [1600/2492 (65%)]\tLoss: 1.091652\n",
      "Train Epoch: 27 [0/2492 (0%)]\tLoss: 1.102050\n",
      "Train Epoch: 27 [1600/2492 (65%)]\tLoss: 1.064000\n",
      "Train Epoch: 28 [0/2492 (0%)]\tLoss: 1.016311\n",
      "Train Epoch: 28 [1600/2492 (65%)]\tLoss: 1.066129\n",
      "Train Epoch: 29 [0/2492 (0%)]\tLoss: 1.087573\n",
      "Train Epoch: 29 [1600/2492 (65%)]\tLoss: 1.125719\n",
      "Train Epoch: 30 [0/2492 (0%)]\tLoss: 0.962278\n",
      "Train Epoch: 30 [1600/2492 (65%)]\tLoss: 1.090974\n",
      "Train Epoch: 31 [0/2492 (0%)]\tLoss: 1.010249\n",
      "Train Epoch: 31 [1600/2492 (65%)]\tLoss: 0.958299\n",
      "Train Epoch: 32 [0/2492 (0%)]\tLoss: 1.048597\n",
      "Train Epoch: 32 [1600/2492 (65%)]\tLoss: 1.020795\n",
      "Train Epoch: 33 [0/2492 (0%)]\tLoss: 1.171275\n",
      "Train Epoch: 33 [1600/2492 (65%)]\tLoss: 1.096765\n",
      "Train Epoch: 34 [0/2492 (0%)]\tLoss: 1.004625\n",
      "Train Epoch: 34 [1600/2492 (65%)]\tLoss: 1.107678\n",
      "Train Epoch: 35 [0/2492 (0%)]\tLoss: 0.984790\n",
      "Train Epoch: 35 [1600/2492 (65%)]\tLoss: 1.009890\n",
      "Train Epoch: 36 [0/2492 (0%)]\tLoss: 1.055265\n",
      "Train Epoch: 36 [1600/2492 (65%)]\tLoss: 0.978427\n",
      "Train Epoch: 37 [0/2492 (0%)]\tLoss: 1.030521\n",
      "Train Epoch: 37 [1600/2492 (65%)]\tLoss: 1.002843\n",
      "Train Epoch: 38 [0/2492 (0%)]\tLoss: 1.039482\n",
      "Train Epoch: 38 [1600/2492 (65%)]\tLoss: 1.155331\n",
      "Train Epoch: 39 [0/2492 (0%)]\tLoss: 1.096979\n",
      "Train Epoch: 39 [1600/2492 (65%)]\tLoss: 0.985810\n",
      "Train Epoch: 40 [0/2492 (0%)]\tLoss: 1.019064\n",
      "Train Epoch: 40 [1600/2492 (65%)]\tLoss: 1.068701\n",
      "Train Epoch: 41 [0/2492 (0%)]\tLoss: 0.981648\n",
      "Train Epoch: 41 [1600/2492 (65%)]\tLoss: 1.061226\n",
      "Train Epoch: 42 [0/2492 (0%)]\tLoss: 1.100741\n",
      "Train Epoch: 42 [1600/2492 (65%)]\tLoss: 0.907066\n",
      "Train Epoch: 43 [0/2492 (0%)]\tLoss: 1.052612\n",
      "Train Epoch: 43 [1600/2492 (65%)]\tLoss: 1.033943\n",
      "Train Epoch: 44 [0/2492 (0%)]\tLoss: 1.108917\n",
      "Train Epoch: 44 [1600/2492 (65%)]\tLoss: 1.039254\n",
      "Train Epoch: 45 [0/2492 (0%)]\tLoss: 1.128276\n",
      "Train Epoch: 45 [1600/2492 (65%)]\tLoss: 1.063681\n",
      "Train Epoch: 46 [0/2492 (0%)]\tLoss: 1.017427\n",
      "Train Epoch: 46 [1600/2492 (65%)]\tLoss: 1.056468\n",
      "Train Epoch: 47 [0/2492 (0%)]\tLoss: 0.927886\n",
      "Train Epoch: 47 [1600/2492 (65%)]\tLoss: 0.975901\n",
      "Train Epoch: 48 [0/2492 (0%)]\tLoss: 1.067977\n",
      "Train Epoch: 48 [1600/2492 (65%)]\tLoss: 1.063406\n",
      "Train Epoch: 49 [0/2492 (0%)]\tLoss: 1.023902\n",
      "Train Epoch: 49 [1600/2492 (65%)]\tLoss: 1.070126\n",
      "Train Epoch: 50 [0/2492 (0%)]\tLoss: 0.973525\n",
      "Train Epoch: 50 [1600/2492 (65%)]\tLoss: 1.028866\n",
      "Train Epoch: 51 [0/2492 (0%)]\tLoss: 1.034768\n",
      "Train Epoch: 51 [1600/2492 (65%)]\tLoss: 1.023065\n",
      "Train Epoch: 52 [0/2492 (0%)]\tLoss: 1.079240\n",
      "Train Epoch: 52 [1600/2492 (65%)]\tLoss: 1.020331\n",
      "Train Epoch: 53 [0/2492 (0%)]\tLoss: 1.014481\n",
      "Train Epoch: 53 [1600/2492 (65%)]\tLoss: 0.973482\n",
      "Train Epoch: 54 [0/2492 (0%)]\tLoss: 0.973539\n",
      "Train Epoch: 54 [1600/2492 (65%)]\tLoss: 0.959690\n",
      "Train Epoch: 55 [0/2492 (0%)]\tLoss: 0.949436\n",
      "Train Epoch: 55 [1600/2492 (65%)]\tLoss: 0.968658\n",
      "Train Epoch: 56 [0/2492 (0%)]\tLoss: 0.991902\n",
      "Train Epoch: 56 [1600/2492 (65%)]\tLoss: 0.878998\n",
      "Train Epoch: 57 [0/2492 (0%)]\tLoss: 0.966757\n",
      "Train Epoch: 57 [1600/2492 (65%)]\tLoss: 0.947773\n",
      "Train Epoch: 58 [0/2492 (0%)]\tLoss: 1.024265\n",
      "Train Epoch: 58 [1600/2492 (65%)]\tLoss: 1.009764\n",
      "Train Epoch: 59 [0/2492 (0%)]\tLoss: 1.009129\n",
      "Train Epoch: 59 [1600/2492 (65%)]\tLoss: 1.010448\n",
      "Train Epoch: 60 [0/2492 (0%)]\tLoss: 1.024998\n",
      "Train Epoch: 60 [1600/2492 (65%)]\tLoss: 1.022033\n",
      "Train Epoch: 61 [0/2492 (0%)]\tLoss: 1.033858\n",
      "Train Epoch: 61 [1600/2492 (65%)]\tLoss: 0.879596\n",
      "Train Epoch: 62 [0/2492 (0%)]\tLoss: 1.163102\n",
      "Train Epoch: 62 [1600/2492 (65%)]\tLoss: 0.985999\n",
      "Train Epoch: 63 [0/2492 (0%)]\tLoss: 0.923245\n",
      "Train Epoch: 63 [1600/2492 (65%)]\tLoss: 1.009920\n",
      "Train Epoch: 64 [0/2492 (0%)]\tLoss: 0.985646\n",
      "Train Epoch: 64 [1600/2492 (65%)]\tLoss: 1.142280\n",
      "Train Epoch: 65 [0/2492 (0%)]\tLoss: 0.997131\n",
      "Train Epoch: 65 [1600/2492 (65%)]\tLoss: 0.963389\n",
      "Train Epoch: 66 [0/2492 (0%)]\tLoss: 1.033719\n",
      "Train Epoch: 66 [1600/2492 (65%)]\tLoss: 1.052374\n",
      "Train Epoch: 67 [0/2492 (0%)]\tLoss: 1.050275\n",
      "Train Epoch: 67 [1600/2492 (65%)]\tLoss: 0.954240\n",
      "Train Epoch: 68 [0/2492 (0%)]\tLoss: 0.922377\n",
      "Train Epoch: 68 [1600/2492 (65%)]\tLoss: 0.975679\n",
      "Train Epoch: 69 [0/2492 (0%)]\tLoss: 0.979517\n",
      "Train Epoch: 69 [1600/2492 (65%)]\tLoss: 1.012562\n",
      "Train Epoch: 70 [0/2492 (0%)]\tLoss: 1.073729\n",
      "Train Epoch: 70 [1600/2492 (65%)]\tLoss: 0.968540\n",
      "Train Epoch: 71 [0/2492 (0%)]\tLoss: 0.999273\n",
      "Train Epoch: 71 [1600/2492 (65%)]\tLoss: 1.068881\n",
      "Train Epoch: 72 [0/2492 (0%)]\tLoss: 1.068403\n",
      "Train Epoch: 72 [1600/2492 (65%)]\tLoss: 1.037086\n",
      "Train Epoch: 73 [0/2492 (0%)]\tLoss: 1.017095\n",
      "Train Epoch: 73 [1600/2492 (65%)]\tLoss: 0.991752\n",
      "Train Epoch: 74 [0/2492 (0%)]\tLoss: 1.070366\n",
      "Train Epoch: 74 [1600/2492 (65%)]\tLoss: 0.923418\n",
      "Train Epoch: 75 [0/2492 (0%)]\tLoss: 1.004347\n",
      "Train Epoch: 75 [1600/2492 (65%)]\tLoss: 1.072095\n",
      "Train Epoch: 76 [0/2492 (0%)]\tLoss: 1.186456\n",
      "Train Epoch: 76 [1600/2492 (65%)]\tLoss: 0.906786\n",
      "Train Epoch: 77 [0/2492 (0%)]\tLoss: 1.044504\n",
      "Train Epoch: 77 [1600/2492 (65%)]\tLoss: 0.918351\n",
      "Train Epoch: 78 [0/2492 (0%)]\tLoss: 0.953731\n",
      "Train Epoch: 78 [1600/2492 (65%)]\tLoss: 0.997595\n",
      "Train Epoch: 79 [0/2492 (0%)]\tLoss: 1.000235\n",
      "Train Epoch: 79 [1600/2492 (65%)]\tLoss: 0.989727\n",
      "Train Epoch: 80 [0/2492 (0%)]\tLoss: 1.077654\n",
      "Train Epoch: 80 [1600/2492 (65%)]\tLoss: 0.940934\n",
      "Train Epoch: 81 [0/2492 (0%)]\tLoss: 0.943922\n",
      "Train Epoch: 81 [1600/2492 (65%)]\tLoss: 1.044702\n",
      "Train Epoch: 82 [0/2492 (0%)]\tLoss: 1.009663\n",
      "Train Epoch: 82 [1600/2492 (65%)]\tLoss: 0.933196\n",
      "Train Epoch: 83 [0/2492 (0%)]\tLoss: 0.977088\n",
      "Train Epoch: 83 [1600/2492 (65%)]\tLoss: 0.935945\n",
      "Train Epoch: 84 [0/2492 (0%)]\tLoss: 0.964113\n",
      "Train Epoch: 84 [1600/2492 (65%)]\tLoss: 0.941565\n",
      "Train Epoch: 85 [0/2492 (0%)]\tLoss: 0.951498\n",
      "Train Epoch: 85 [1600/2492 (65%)]\tLoss: 1.000130\n",
      "Train Epoch: 86 [0/2492 (0%)]\tLoss: 1.001057\n",
      "Train Epoch: 86 [1600/2492 (65%)]\tLoss: 0.960935\n",
      "Train Epoch: 87 [0/2492 (0%)]\tLoss: 1.033340\n",
      "Train Epoch: 87 [1600/2492 (65%)]\tLoss: 0.980982\n",
      "Train Epoch: 88 [0/2492 (0%)]\tLoss: 0.925969\n",
      "Train Epoch: 88 [1600/2492 (65%)]\tLoss: 0.978657\n",
      "Train Epoch: 89 [0/2492 (0%)]\tLoss: 0.986266\n",
      "Train Epoch: 89 [1600/2492 (65%)]\tLoss: 0.828628\n",
      "Train Epoch: 90 [0/2492 (0%)]\tLoss: 0.946313\n",
      "Train Epoch: 90 [1600/2492 (65%)]\tLoss: 0.897014\n",
      "Train Epoch: 91 [0/2492 (0%)]\tLoss: 0.889903\n",
      "Train Epoch: 91 [1600/2492 (65%)]\tLoss: 0.934134\n",
      "Train Epoch: 92 [0/2492 (0%)]\tLoss: 0.963248\n",
      "Train Epoch: 92 [1600/2492 (65%)]\tLoss: 0.975043\n",
      "Train Epoch: 93 [0/2492 (0%)]\tLoss: 0.994393\n",
      "Train Epoch: 93 [1600/2492 (65%)]\tLoss: 0.906132\n",
      "Train Epoch: 94 [0/2492 (0%)]\tLoss: 0.971365\n",
      "Train Epoch: 94 [1600/2492 (65%)]\tLoss: 0.945282\n",
      "Train Epoch: 95 [0/2492 (0%)]\tLoss: 1.013861\n",
      "Train Epoch: 95 [1600/2492 (65%)]\tLoss: 1.029369\n",
      "Train Epoch: 96 [0/2492 (0%)]\tLoss: 1.030643\n",
      "Train Epoch: 96 [1600/2492 (65%)]\tLoss: 0.954426\n",
      "Train Epoch: 97 [0/2492 (0%)]\tLoss: 1.013569\n",
      "Train Epoch: 97 [1600/2492 (65%)]\tLoss: 0.977947\n",
      "Train Epoch: 98 [0/2492 (0%)]\tLoss: 0.938759\n",
      "Train Epoch: 98 [1600/2492 (65%)]\tLoss: 0.958370\n",
      "Train Epoch: 99 [0/2492 (0%)]\tLoss: 0.946591\n",
      "Train Epoch: 99 [1600/2492 (65%)]\tLoss: 0.904869\n",
      "Train Epoch: 100 [0/2492 (0%)]\tLoss: 0.916178\n",
      "Train Epoch: 100 [1600/2492 (65%)]\tLoss: 0.996770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8e7cb052b0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training the autoencoder to encode the TCR sequence\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    # model_accuracy = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        # TCR_encode_losses.append(loss.item() / model.batch_size)\n",
    "        # TCR_encode_losses.append(loss.item())\n",
    "        # sum up batch loss\n",
    "        batch_loss += loss.item()\n",
    "        # update the accuracy of the model\n",
    "        # pred = output.data.max(1, keepdim=True)[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            # print(f\"The accuracy of the model is \")\n",
    "            \n",
    "    # return batch_loss / len(train_loader.dataset)\n",
    "    return batch_loss / len(data)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "TCR_accuracy = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.set_title(\"TCR encode loss\")\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# load the model\n",
    "# model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "# model.load_state_dict(torch.load(\"/DATA/User/wuxinchao/project/pMHC-TCR/ckpt/TCR_autoencoder.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 1))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    TCR_seq = TCR_seq.view(1, 5, 49).float()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2492"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for test, not used\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "kernel_size, stride, padding, seq_length\n",
    "# pool of size=3, stride=2\n",
    "# m = nn.MaxPool1d(3, stride=1)\n",
    "# m = nn.Conv1d(16, 33, 3, stride=2, padding=1)\n",
    "m = nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=1)\n",
    "# m = nn.MaxUnpool1d(kernel_size=3, stride=1)\n",
    "input = torch.randn(20, 16, 3)\n",
    "output = m(input)\n",
    "output.shape\n",
    "# TCRData[0][0].shape\n",
    "len(TCRData)\n",
    "# model(TCRData[0:3][0].float().view(3,5,seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TCR_encode(nn.Module):\n",
    "    '''\n",
    "    LSTM for TCR sequence encoding.\n",
    "    The input size of LSTM is (batch_size, seq_length, input_size), the output size is (batch_size, seq_length, hidden_size)\n",
    "    '''\n",
    "    def __init__(self, seq_length, hidden_size, num_layers, device):\n",
    "        super(LSTM_TCR_encode, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(seq_length, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model for TCR sequence encoding, this may not be used in the future\n",
    "# How to use the LSTM model to encode the TCR sequence\n",
    "# The optimization \n",
    "def train_LSTM_TCR_encode(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, seq_length, 5)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training: {batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%) \\\n",
    "                  Loss: {loss.item():.6f}\")\n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the LSTM model\n",
    "model = LSTM_TCR_encode(seq_length, hidden_size, num_layers, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    Here the input is the TCR sequence, neoantigen sequence, and HLA type.\n",
    "    The output should be the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 file_path, \n",
    "                 only_CDR3: bool = False, \n",
    "                 only_experimental: bool = False, \n",
    "                 TCR_encode: str = [\"LSTM\", \"CNN\"],\n",
    "                 encoding_model: nn.Module = None,\n",
    "                 encoding_size = 20) -> None:\n",
    "        df, HLA_encode, y, feature_size  = self.basic_io(file_path, only_experimental=only_experimental)\n",
    "\n",
    "        # convert from object to tensor\n",
    "        X_TCR_seq = torch.zeros((len(df), 0))\n",
    "        for region in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            TCR_seq = df.loc[:, region].values\n",
    "            TCR_seq_encode = torch.zeros((0, TCR_seq[0].shape[1]))\n",
    "            for i in range(len(TCR_seq)):\n",
    "                encoding = torch.from_numpy(TCR_seq[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                TCR_seq_encode = torch.cat((TCR_seq_encode, encoding), dim=0)\n",
    "\n",
    "            X_TCR_seq = torch.cat((TCR_seq_encode, X_TCR_seq), dim=1)\n",
    "        print(f\"X_TCR_seq shape {X_TCR_seq.shape}\")\n",
    "        if TCR_encode == \"CNN\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, feature_size)\n",
    "        elif TCR_encode == \"LSTM\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, feature_size)\n",
    "        else:\n",
    "            raise ValueError(\"The TCR encoding method is not supported yet.\")\n",
    "        \n",
    "        # encoding model \n",
    "        X_features, _ = encoding_model(X_TCR_seq)\n",
    "        X_features = X_features.view(-1, encoding_size).data\n",
    "\n",
    "        # add the neoantigen sequence encoding features\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            neo = df.loc[:, seq].values\n",
    "            neo_encode = torch.zeros((0, neo[0].shape[1]))\n",
    "            for i in range(len(neo)):\n",
    "                encoding = torch.from_numpy(neo[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                neo_encode = torch.cat((neo_encode, encoding), dim=0)\n",
    "            X_features = torch.cat((X_features, neo_encode), dim=1)\n",
    "\n",
    "        X_features = torch.cat((X_features, torch.from_numpy(HLA_encode)), dim=1)\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "            \n",
    "    \n",
    "    def basic_io(self, file_path, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "        # drop the random generate samples and duplicated \n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        # df = df[df[\"AseqCDR3\"].str.contains(\"_\")==False].drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        input_feature_size = sum(len_map.values())\n",
    "\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # drop the rows with nan\n",
    "        df = df.dropna()\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "        y.value_counts().plot.pie(autopct='%.2f')\n",
    "        return df, X_HLA_encoded, y.values, input_feature_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_TCR_seq shape torch.Size([2492, 245])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, TCR_encode=\"CNN\", only_experimental=True, encoding_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCRData[0][0].shape\n",
    "# 97.41 / 2.59\n",
    "# df = pd.read_csv(file_path, index_col=0)\n",
    "# df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the seq containing _ \n",
    "df = df[df[\"AseqCDR3\"].str.contains(\"_\")==False].drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "df[\"AseqCDR3\"].apply(lambda x:len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"BseqCDR3\"].apply(lambda x:len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "    length = len_map[chain]\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# drop the rows with nan\n",
    "df = df.dropna()\n",
    "\n",
    "X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "HLAencoder = OneHotEncoder()\n",
    "X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 batch_size=32,) -> None:\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "        # self.linear_layer = nn.Sequential(\n",
    "        #     nn.Linear(input_size, 32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(32, 16)\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(16, 8),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(8, 2),\n",
    "        # )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(self.batch_size, self.input_size)\n",
    "        # print(f\"The model input shape is : {input.shape}\")\n",
    "        output = self.linear_layer(input)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # one-hot encoding the target\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        target = target.to(torch.bool)\n",
    "        one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "        one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "        one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "\n",
    "        data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "        # data.data extract the item of the torch without the hook on the computation graph\n",
    "        data = data.view(-1, 20+5*6+2).to(torch.float32)\n",
    "        output = model(data) # output shape: (batch_size, 2)\n",
    "        # based on the one-hot target to decide the weights of loss for each class\n",
    "        # weight = (one_hot_target == torch.tensor([0, 1]).to(device)).sum() / (one_hot_target == torch.tensor([1, 0]).to(device)).sum()\n",
    "        # weights = torch.tensor([1, 1/weight]).to(device)\n",
    "        # loss = nn.CrossEntropyLoss(weight=weights)(output, one_hot_target.data)\n",
    "        loss = criterion(output, one_hot_target.data)\n",
    "        train_loss += loss.item() / len(train_loader.dataset)\n",
    "        # The loss has been weighted, and the loss has already been divided by the batch size\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # get the prediction\n",
    "        pred = output.argmax(dim=1, keepdim=True) # shape: (batch_size, 1)\n",
    "        correct += pred.eq(one_hot_target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training stage for Flod {fold} Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \\\n",
    "                ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "            # print(f\"The current output is {output}, and the target is {one_hot_target}\")\n",
    "            # print(f\"The weight of the batch is {weights}, \\n The output is {output}, \\n The one_hot_target is {one_hot_target.data}\")\n",
    "    return train_loss, correct / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_history = []\n",
    "    target_history = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # one-hot encoding the target\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            target = target.to(torch.bool)\n",
    "            one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "            one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "            one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "\n",
    "            data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "            data = data.view(-1, 20+5*6+2).to(torch.float32).data\n",
    "            output = model(data) # output shape: (batch_size, 2)\n",
    "            # get the test loss and prediction\n",
    "            test_loss += nn.CrossEntropyLoss()(output, one_hot_target.data).item() / len(test_loader.dataset)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            target = one_hot_target.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            pred_history.extend(output.cpu().detach().numpy()) # shape: (batch_size, 2)\n",
    "            target_history.extend(one_hot_target.cpu().detach().numpy()) # shape: (batch_size, 2)\n",
    "            # print(f\"The shape of one_hot_target is {target.shape}, and the shape of pred is {pred.shape}\")\n",
    "            # auc += roc_auc_score(target.data.cpu().numpy(), pred.cpu().numpy()) / len(test_loader)\n",
    "            # fpr, tpr, _ = roc_curve(target.data.cpu().numpy(), pred.cpu().numpy()) \n",
    "            # print(f\"The fpr is {fpr}, and the tpr is {tpr}\")\n",
    "            # print(f\"The auc is {auc}\")\n",
    "\n",
    "\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    auc = roc_auc_score(target_history, pred_history)\n",
    "    print(f\"The auc of this fold is {auc}\")\n",
    "    return test_loss, correct / len(test_loader.dataset), pred_history, target_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 32\n",
    "input_size = 52\n",
    "folds = 5\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = prediction_model(input_size=input_size, batch_size=batch_size).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Fold 0-------------------\n",
      "Training stage for Flod 0 Epoch: 1 [0/1993                 (0%)]\tLoss: 0.698787\n",
      "Test set for fold0: Average Loss:           0.0185, Accuracy: 429/499           (86%)\n",
      "The auc of this fold is 0.9277065807060303\n",
      "Training stage for Flod 0 Epoch: 2 [0/1993                 (0%)]\tLoss: 0.642850\n",
      "Test set for fold0: Average Loss:           0.0145, Accuracy: 429/499           (86%)\n",
      "The auc of this fold is 0.9358636685274\n",
      "Training stage for Flod 0 Epoch: 3 [0/1993                 (0%)]\tLoss: 0.453299\n",
      "Test set for fold0: Average Loss:           0.0128, Accuracy: 430/499           (86%)\n",
      "The auc of this fold is 0.934232250963126\n",
      "Training stage for Flod 0 Epoch: 4 [0/1993                 (0%)]\tLoss: 0.500965\n",
      "Test set for fold0: Average Loss:           0.0126, Accuracy: 430/499           (86%)\n",
      "The auc of this fold is 0.9258933485336898\n",
      "Training stage for Flod 0 Epoch: 5 [0/1993                 (0%)]\tLoss: 0.482460\n",
      "Test set for fold0: Average Loss:           0.0125, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9255444610425348\n",
      "Training stage for Flod 0 Epoch: 6 [0/1993                 (0%)]\tLoss: 0.439113\n",
      "Test set for fold0: Average Loss:           0.0125, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9329398930733548\n",
      "Training stage for Flod 0 Epoch: 7 [0/1993                 (0%)]\tLoss: 0.314617\n",
      "Test set for fold0: Average Loss:           0.0125, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9310971774510575\n",
      "Training stage for Flod 0 Epoch: 8 [0/1993                 (0%)]\tLoss: 0.438674\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9310234688261656\n",
      "Training stage for Flod 0 Epoch: 9 [0/1993                 (0%)]\tLoss: 0.440339\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9357211651859423\n",
      "Training stage for Flod 0 Epoch: 10 [0/1993                 (0%)]\tLoss: 0.375966\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9345123437377152\n",
      "Training stage for Flod 0 Epoch: 11 [0/1993                 (0%)]\tLoss: 0.438405\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9319423696831513\n",
      "Training stage for Flod 0 Epoch: 12 [0/1993                 (0%)]\tLoss: 0.313596\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9313330450507116\n",
      "Training stage for Flod 0 Epoch: 13 [0/1993                 (0%)]\tLoss: 0.439731\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9342666483214089\n",
      "Training stage for Flod 0 Epoch: 14 [0/1993                 (0%)]\tLoss: 0.344751\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9306450978850539\n",
      "Training stage for Flod 0 Epoch: 15 [0/1993                 (0%)]\tLoss: 0.376029\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9312888198757763\n",
      "Training stage for Flod 0 Epoch: 16 [0/1993                 (0%)]\tLoss: 0.438616\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9316475351835837\n",
      "Training stage for Flod 0 Epoch: 17 [0/1993                 (0%)]\tLoss: 0.344667\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9323747936158502\n",
      "Training stage for Flod 0 Epoch: 18 [0/1993                 (0%)]\tLoss: 0.407263\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9316327934586052\n",
      "Training stage for Flod 0 Epoch: 19 [0/1993                 (0%)]\tLoss: 0.469582\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9310529522761224\n",
      "Training stage for Flod 0 Epoch: 20 [0/1993                 (0%)]\tLoss: 0.438935\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9324091909741332\n",
      "Training stage for Flod 0 Epoch: 21 [0/1993                 (0%)]\tLoss: 0.344926\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9322372041827188\n",
      "Training stage for Flod 0 Epoch: 22 [0/1993                 (0%)]\tLoss: 0.344583\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9320848730246089\n",
      "Training stage for Flod 0 Epoch: 23 [0/1993                 (0%)]\tLoss: 0.563251\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9340160389967765\n",
      "Training stage for Flod 0 Epoch: 24 [0/1993                 (0%)]\tLoss: 0.407228\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9328907539900935\n",
      "Training stage for Flod 0 Epoch: 25 [0/1993                 (0%)]\tLoss: 0.375850\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.930419058102052\n",
      "Training stage for Flod 0 Epoch: 26 [0/1993                 (0%)]\tLoss: 0.407077\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.932247031999371\n",
      "Training stage for Flod 0 Epoch: 27 [0/1993                 (0%)]\tLoss: 0.407072\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9319964226747386\n",
      "Training stage for Flod 0 Epoch: 28 [0/1993                 (0%)]\tLoss: 0.438295\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9319030584165422\n",
      "Training stage for Flod 0 Epoch: 29 [0/1993                 (0%)]\tLoss: 0.469524\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9303109521188773\n",
      "Training stage for Flod 0 Epoch: 30 [0/1993                 (0%)]\tLoss: 0.469583\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9320259061246954\n",
      "Training stage for Flod 0 Epoch: 31 [0/1993                 (0%)]\tLoss: 0.438313\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9303846607437691\n",
      "Training stage for Flod 0 Epoch: 32 [0/1993                 (0%)]\tLoss: 0.498792\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9313379589590376\n",
      "Training stage for Flod 0 Epoch: 33 [0/1993                 (0%)]\tLoss: 0.407148\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9304043163770737\n",
      "Training stage for Flod 0 Epoch: 34 [0/1993                 (0%)]\tLoss: 0.375901\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9311070052677097\n",
      "Training stage for Flod 0 Epoch: 35 [0/1993                 (0%)]\tLoss: 0.438299\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9323502240742196\n",
      "Training stage for Flod 0 Epoch: 36 [0/1993                 (0%)]\tLoss: 0.407199\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9332347275729225\n",
      "Training stage for Flod 0 Epoch: 37 [0/1993                 (0%)]\tLoss: 0.438267\n",
      "Test set for fold0: Average Loss:           0.0124, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.9342175092381477\n",
      "Training stage for Flod 0 Epoch: 38 [0/1993                 (0%)]\tLoss: 0.500880\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 432/499           (87%)\n",
      "The auc of this fold is 0.935244516078308\n",
      "Training stage for Flod 0 Epoch: 39 [0/1993                 (0%)]\tLoss: 0.376249\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9354852975862882\n",
      "Training stage for Flod 0 Epoch: 40 [0/1993                 (0%)]\tLoss: 0.376103\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9354263306863747\n",
      "Training stage for Flod 0 Epoch: 41 [0/1993                 (0%)]\tLoss: 0.375883\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9357752181775296\n",
      "Training stage for Flod 0 Epoch: 42 [0/1993                 (0%)]\tLoss: 0.407147\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9356179731110936\n",
      "Training stage for Flod 0 Epoch: 43 [0/1993                 (0%)]\tLoss: 0.407102\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9358096155358127\n",
      "Training stage for Flod 0 Epoch: 44 [0/1993                 (0%)]\tLoss: 0.407100\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9361437613019892\n",
      "Training stage for Flod 0 Epoch: 45 [0/1993                 (0%)]\tLoss: 0.377539\n",
      "Test set for fold0: Average Loss:           0.0123, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.936094622218728\n",
      "Training stage for Flod 0 Epoch: 46 [0/1993                 (0%)]\tLoss: 0.375813\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9357801320858558\n",
      "Training stage for Flod 0 Epoch: 47 [0/1993                 (0%)]\tLoss: 0.313318\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9359668606022485\n",
      "Training stage for Flod 0 Epoch: 48 [0/1993                 (0%)]\tLoss: 0.407054\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9357506486358991\n",
      "Training stage for Flod 0 Epoch: 49 [0/1993                 (0%)]\tLoss: 0.407055\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9380208742825694\n",
      "Training stage for Flod 0 Epoch: 50 [0/1993                 (0%)]\tLoss: 0.438284\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9360798804937496\n",
      "Training stage for Flod 0 Epoch: 51 [0/1993                 (0%)]\tLoss: 0.407050\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9358882380690307\n",
      "Training stage for Flod 0 Epoch: 52 [0/1993                 (0%)]\tLoss: 0.313313\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9379569934743297\n",
      "Training stage for Flod 0 Epoch: 53 [0/1993                 (0%)]\tLoss: 0.375792\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9376621589747622\n",
      "Training stage for Flod 0 Epoch: 54 [0/1993                 (0%)]\tLoss: 0.438286\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.938163377624027\n",
      "Training stage for Flod 0 Epoch: 55 [0/1993                 (0%)]\tLoss: 0.438286\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9379078543910685\n",
      "Training stage for Flod 0 Epoch: 56 [0/1993                 (0%)]\tLoss: 0.469536\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9369152449091911\n",
      "Training stage for Flod 0 Epoch: 57 [0/1993                 (0%)]\tLoss: 0.313297\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9381289802657442\n",
      "Training stage for Flod 0 Epoch: 58 [0/1993                 (0%)]\tLoss: 0.438284\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9379815630159605\n",
      "Training stage for Flod 0 Epoch: 59 [0/1993                 (0%)]\tLoss: 0.344681\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9374115496501298\n",
      "Training stage for Flod 0 Epoch: 60 [0/1993                 (0%)]\tLoss: 0.375953\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9380700133658307\n",
      "Training stage for Flod 0 Epoch: 61 [0/1993                 (0%)]\tLoss: 0.344541\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9380307020992217\n",
      "Training stage for Flod 0 Epoch: 62 [0/1993                 (0%)]\tLoss: 0.469525\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9368808475509081\n",
      "Training stage for Flod 0 Epoch: 63 [0/1993                 (0%)]\tLoss: 0.469856\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9375786225332181\n",
      "Training stage for Flod 0 Epoch: 64 [0/1993                 (0%)]\tLoss: 0.438279\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9381732054406793\n",
      "Training stage for Flod 0 Epoch: 65 [0/1993                 (0%)]\tLoss: 0.313285\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9381289802657442\n",
      "Training stage for Flod 0 Epoch: 66 [0/1993                 (0%)]\tLoss: 0.375791\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9386105432817045\n",
      "Training stage for Flod 0 Epoch: 67 [0/1993                 (0%)]\tLoss: 0.313280\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9385614041984432\n",
      "Training stage for Flod 0 Epoch: 68 [0/1993                 (0%)]\tLoss: 0.407025\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9387677883481405\n",
      "Training stage for Flod 0 Epoch: 69 [0/1993                 (0%)]\tLoss: 0.438273\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9366154965012974\n",
      "Training stage for Flod 0 Epoch: 70 [0/1993                 (0%)]\tLoss: 0.407027\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9358145294441387\n",
      "Training stage for Flod 0 Epoch: 71 [0/1993                 (0%)]\tLoss: 0.344528\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9375589668999135\n",
      "Training stage for Flod 0 Epoch: 72 [0/1993                 (0%)]\tLoss: 0.469523\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9374705165500433\n",
      "Training stage for Flod 0 Epoch: 73 [0/1993                 (0%)]\tLoss: 0.375777\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9374361191917604\n",
      "Training stage for Flod 0 Epoch: 74 [0/1993                 (0%)]\tLoss: 0.344527\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.93765233115811\n",
      "Training stage for Flod 0 Epoch: 75 [0/1993                 (0%)]\tLoss: 0.407023\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9377555232329585\n",
      "Training stage for Flod 0 Epoch: 76 [0/1993                 (0%)]\tLoss: 0.375769\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.937848887491155\n",
      "Training stage for Flod 0 Epoch: 77 [0/1993                 (0%)]\tLoss: 0.407026\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9382469140655713\n",
      "Training stage for Flod 0 Epoch: 78 [0/1993                 (0%)]\tLoss: 0.500767\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9375687947165658\n",
      "Training stage for Flod 0 Epoch: 79 [0/1993                 (0%)]\tLoss: 0.500770\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9373918940168253\n",
      "Training stage for Flod 0 Epoch: 80 [0/1993                 (0%)]\tLoss: 0.407025\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9375098278166523\n",
      "Training stage for Flod 0 Epoch: 81 [0/1993                 (0%)]\tLoss: 0.375779\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9378881987577641\n",
      "Training stage for Flod 0 Epoch: 82 [0/1993                 (0%)]\tLoss: 0.344647\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9380552716408522\n",
      "Training stage for Flod 0 Epoch: 83 [0/1993                 (0%)]\tLoss: 0.438270\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.938006132557591\n",
      "Training stage for Flod 0 Epoch: 84 [0/1993                 (0%)]\tLoss: 0.344524\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9379127682993946\n",
      "Training stage for Flod 0 Epoch: 85 [0/1993                 (0%)]\tLoss: 0.469519\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9373280132085855\n",
      "Training stage for Flod 0 Epoch: 86 [0/1993                 (0%)]\tLoss: 0.375773\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9378881987577641\n",
      "Training stage for Flod 0 Epoch: 87 [0/1993                 (0%)]\tLoss: 0.407023\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9376769006997405\n",
      "Training stage for Flod 0 Epoch: 88 [0/1993                 (0%)]\tLoss: 0.407023\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9373918940168253\n",
      "Training stage for Flod 0 Epoch: 89 [0/1993                 (0%)]\tLoss: 0.375772\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9371412846921928\n",
      "Training stage for Flod 0 Epoch: 90 [0/1993                 (0%)]\tLoss: 0.407030\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9372444767670416\n",
      "Training stage for Flod 0 Epoch: 91 [0/1993                 (0%)]\tLoss: 0.438328\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9372297350420631\n",
      "Training stage for Flod 0 Epoch: 92 [0/1993                 (0%)]\tLoss: 0.438272\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9380994968157874\n",
      "Training stage for Flod 0 Epoch: 93 [0/1993                 (0%)]\tLoss: 0.594516\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9375687947165656\n",
      "Training stage for Flod 0 Epoch: 94 [0/1993                 (0%)]\tLoss: 0.375776\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9370233508923658\n",
      "Training stage for Flod 0 Epoch: 95 [0/1993                 (0%)]\tLoss: 0.438266\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9371560264171711\n",
      "Training stage for Flod 0 Epoch: 96 [0/1993                 (0%)]\tLoss: 0.407021\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9373968079251513\n",
      "Training stage for Flod 0 Epoch: 97 [0/1993                 (0%)]\tLoss: 0.407029\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9371265429672145\n",
      "Training stage for Flod 0 Epoch: 98 [0/1993                 (0%)]\tLoss: 0.500863\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.937185509867128\n",
      "Training stage for Flod 0 Epoch: 99 [0/1993                 (0%)]\tLoss: 0.469643\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9368513641009513\n",
      "Training stage for Flod 0 Epoch: 100 [0/1993                 (0%)]\tLoss: 0.469517\n",
      "Test set for fold0: Average Loss:           0.0122, Accuracy: 435/499           (87%)\n",
      "The auc of this fold is 0.9375147417249784\n",
      "-------------------Fold 1-------------------\n",
      "Training stage for Flod 1 Epoch: 1 [0/1993                 (0%)]\tLoss: 0.691767\n",
      "Test set for fold1: Average Loss:           0.0203, Accuracy: 322/499           (65%)\n",
      "The auc of this fold is 0.9227730167465995\n",
      "Training stage for Flod 1 Epoch: 2 [0/1993                 (0%)]\tLoss: 0.685461\n",
      "Test set for fold1: Average Loss:           0.0156, Accuracy: 396/499           (79%)\n",
      "The auc of this fold is 0.9226059438635112\n",
      "Training stage for Flod 1 Epoch: 3 [0/1993                 (0%)]\tLoss: 0.552205\n",
      "Test set for fold1: Average Loss:           0.0135, Accuracy: 426/499           (85%)\n",
      "The auc of this fold is 0.9309792436512303\n",
      "Training stage for Flod 1 Epoch: 4 [0/1993                 (0%)]\tLoss: 0.428660\n",
      "Test set for fold1: Average Loss:           0.0129, Accuracy: 426/499           (85%)\n",
      "The auc of this fold is 0.9320013365830648\n",
      "Training stage for Flod 1 Epoch: 5 [0/1993                 (0%)]\tLoss: 0.573606\n",
      "Test set for fold1: Average Loss:           0.0126, Accuracy: 433/499           (87%)\n",
      "The auc of this fold is 0.9333477474644232\n",
      "Training stage for Flod 1 Epoch: 6 [0/1993                 (0%)]\tLoss: 0.491081\n",
      "Test set for fold1: Average Loss:           0.0124, Accuracy: 433/499           (87%)\n",
      "The auc of this fold is 0.9381240663574182\n",
      "Training stage for Flod 1 Epoch: 7 [0/1993                 (0%)]\tLoss: 0.396895\n",
      "Test set for fold1: Average Loss:           0.0122, Accuracy: 437/499           (88%)\n",
      "The auc of this fold is 0.9356474565610504\n",
      "Training stage for Flod 1 Epoch: 8 [0/1993                 (0%)]\tLoss: 0.414227\n",
      "Test set for fold1: Average Loss:           0.0122, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9353526220614827\n",
      "Training stage for Flod 1 Epoch: 9 [0/1993                 (0%)]\tLoss: 0.369719\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376425033414576\n",
      "Training stage for Flod 1 Epoch: 10 [0/1993                 (0%)]\tLoss: 0.380991\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9362518672851641\n",
      "Training stage for Flod 1 Epoch: 11 [0/1993                 (0%)]\tLoss: 0.411214\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9359177215189873\n",
      "Training stage for Flod 1 Epoch: 12 [0/1993                 (0%)]\tLoss: 0.442711\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9360405692271405\n",
      "Training stage for Flod 1 Epoch: 13 [0/1993                 (0%)]\tLoss: 0.377661\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9363108341850774\n",
      "Training stage for Flod 1 Epoch: 14 [0/1993                 (0%)]\tLoss: 0.377538\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374754304583695\n",
      "Training stage for Flod 1 Epoch: 15 [0/1993                 (0%)]\tLoss: 0.407192\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376670728830883\n",
      "Training stage for Flod 1 Epoch: 16 [0/1993                 (0%)]\tLoss: 0.345414\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374803443666956\n",
      "Training stage for Flod 1 Epoch: 17 [0/1993                 (0%)]\tLoss: 0.375965\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376572450664362\n",
      "Training stage for Flod 1 Epoch: 18 [0/1993                 (0%)]\tLoss: 0.376435\n",
      "Test set for fold1: Average Loss:           0.0121, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9383992452236811\n",
      "Training stage for Flod 1 Epoch: 19 [0/1993                 (0%)]\tLoss: 0.375809\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937848887491155\n",
      "Training stage for Flod 1 Epoch: 20 [0/1993                 (0%)]\tLoss: 0.501810\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9380012186492648\n",
      "Training stage for Flod 1 Epoch: 21 [0/1993                 (0%)]\tLoss: 0.344967\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9379913908326127\n",
      "Training stage for Flod 1 Epoch: 22 [0/1993                 (0%)]\tLoss: 0.469014\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9379471656576776\n",
      "Training stage for Flod 1 Epoch: 23 [0/1993                 (0%)]\tLoss: 0.345044\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378734570327856\n",
      "Training stage for Flod 1 Epoch: 24 [0/1993                 (0%)]\tLoss: 0.376234\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375638808082396\n",
      "Training stage for Flod 1 Epoch: 25 [0/1993                 (0%)]\tLoss: 0.344811\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378046623162198\n",
      "Training stage for Flod 1 Epoch: 26 [0/1993                 (0%)]\tLoss: 0.313999\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9379864769242865\n",
      "Training stage for Flod 1 Epoch: 27 [0/1993                 (0%)]\tLoss: 0.344796\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9373329271169116\n",
      "Training stage for Flod 1 Epoch: 28 [0/1993                 (0%)]\tLoss: 0.469788\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9372297350420631\n",
      "Training stage for Flod 1 Epoch: 29 [0/1993                 (0%)]\tLoss: 0.344704\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374213774667821\n",
      "Training stage for Flod 1 Epoch: 30 [0/1993                 (0%)]\tLoss: 0.376063\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374754304583695\n",
      "Training stage for Flod 1 Epoch: 31 [0/1993                 (0%)]\tLoss: 0.376229\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374754304583695\n",
      "Training stage for Flod 1 Epoch: 32 [0/1993                 (0%)]\tLoss: 0.376220\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375294834499568\n",
      "Training stage for Flod 1 Epoch: 33 [0/1993                 (0%)]\tLoss: 0.407220\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378390596745028\n",
      "Training stage for Flod 1 Epoch: 34 [0/1993                 (0%)]\tLoss: 0.406766\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375687947165658\n",
      "Training stage for Flod 1 Epoch: 35 [0/1993                 (0%)]\tLoss: 0.407056\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378046623162197\n",
      "Training stage for Flod 1 Epoch: 36 [0/1993                 (0%)]\tLoss: 0.407044\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937578622533218\n",
      "Training stage for Flod 1 Epoch: 37 [0/1993                 (0%)]\tLoss: 0.407382\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9373624105668685\n",
      "Training stage for Flod 1 Epoch: 38 [0/1993                 (0%)]\tLoss: 0.375639\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375196556333045\n",
      "Training stage for Flod 1 Epoch: 39 [0/1993                 (0%)]\tLoss: 0.407353\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377358675996541\n",
      "Training stage for Flod 1 Epoch: 40 [0/1993                 (0%)]\tLoss: 0.344527\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378144901328721\n",
      "Training stage for Flod 1 Epoch: 41 [0/1993                 (0%)]\tLoss: 0.407021\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.938011046465917\n",
      "Training stage for Flod 1 Epoch: 42 [0/1993                 (0%)]\tLoss: 0.375992\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374852582750217\n",
      "Training stage for Flod 1 Epoch: 43 [0/1993                 (0%)]\tLoss: 0.344531\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937809576224546\n",
      "Training stage for Flod 1 Epoch: 44 [0/1993                 (0%)]\tLoss: 0.344927\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376769006997405\n",
      "Training stage for Flod 1 Epoch: 45 [0/1993                 (0%)]\tLoss: 0.438092\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374656026417172\n",
      "Training stage for Flod 1 Epoch: 46 [0/1993                 (0%)]\tLoss: 0.469631\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374508609167388\n",
      "Training stage for Flod 1 Epoch: 47 [0/1993                 (0%)]\tLoss: 0.375978\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9371953376837802\n",
      "Training stage for Flod 1 Epoch: 48 [0/1993                 (0%)]\tLoss: 0.500677\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9372444767670415\n",
      "Training stage for Flod 1 Epoch: 49 [0/1993                 (0%)]\tLoss: 0.469387\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375442251749351\n",
      "Training stage for Flod 1 Epoch: 50 [0/1993                 (0%)]\tLoss: 0.438496\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376081059831747\n",
      "Training stage for Flod 1 Epoch: 51 [0/1993                 (0%)]\tLoss: 0.469364\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937657245066436\n",
      "Training stage for Flod 1 Epoch: 52 [0/1993                 (0%)]\tLoss: 0.500538\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937814490132872\n",
      "Training stage for Flod 1 Epoch: 53 [0/1993                 (0%)]\tLoss: 0.375869\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376228477081532\n",
      "Training stage for Flod 1 Epoch: 54 [0/1993                 (0%)]\tLoss: 0.500632\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9373230993002595\n",
      "Training stage for Flod 1 Epoch: 55 [0/1993                 (0%)]\tLoss: 0.375834\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9373378410252378\n",
      "Training stage for Flod 1 Epoch: 56 [0/1993                 (0%)]\tLoss: 0.375863\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937495086091674\n",
      "Training stage for Flod 1 Epoch: 57 [0/1993                 (0%)]\tLoss: 0.375795\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376375894331317\n",
      "Training stage for Flod 1 Epoch: 58 [0/1993                 (0%)]\tLoss: 0.407084\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9373968079251513\n",
      "Training stage for Flod 1 Epoch: 59 [0/1993                 (0%)]\tLoss: 0.407019\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377948344995675\n",
      "Training stage for Flod 1 Epoch: 60 [0/1993                 (0%)]\tLoss: 0.375779\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375982781665225\n",
      "Training stage for Flod 1 Epoch: 61 [0/1993                 (0%)]\tLoss: 0.375893\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9376474172497838\n",
      "Training stage for Flod 1 Epoch: 62 [0/1993                 (0%)]\tLoss: 0.438287\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937696556333045\n",
      "Training stage for Flod 1 Epoch: 63 [0/1993                 (0%)]\tLoss: 0.407252\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375491390832613\n",
      "Training stage for Flod 1 Epoch: 64 [0/1993                 (0%)]\tLoss: 0.344515\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9374656026417171\n",
      "Training stage for Flod 1 Epoch: 65 [0/1993                 (0%)]\tLoss: 0.407022\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.938124066357418\n",
      "Training stage for Flod 1 Epoch: 66 [0/1993                 (0%)]\tLoss: 0.344622\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378390596745028\n",
      "Training stage for Flod 1 Epoch: 67 [0/1993                 (0%)]\tLoss: 0.407057\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377948344995675\n",
      "Training stage for Flod 1 Epoch: 68 [0/1993                 (0%)]\tLoss: 0.469558\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378046623162198\n",
      "Training stage for Flod 1 Epoch: 69 [0/1993                 (0%)]\tLoss: 0.438253\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.938163377624027\n",
      "Training stage for Flod 1 Epoch: 70 [0/1993                 (0%)]\tLoss: 0.375830\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377653510496108\n",
      "Training stage for Flod 1 Epoch: 71 [0/1993                 (0%)]\tLoss: 0.438378\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375737086248919\n",
      "Training stage for Flod 1 Epoch: 72 [0/1993                 (0%)]\tLoss: 0.438351\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9379815630159604\n",
      "Training stage for Flod 1 Epoch: 73 [0/1993                 (0%)]\tLoss: 0.375914\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9382469140655711\n",
      "Training stage for Flod 1 Epoch: 74 [0/1993                 (0%)]\tLoss: 0.438148\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937853801399481\n",
      "Training stage for Flod 1 Epoch: 75 [0/1993                 (0%)]\tLoss: 0.438302\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377948344995676\n",
      "Training stage for Flod 1 Epoch: 76 [0/1993                 (0%)]\tLoss: 0.438289\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9383009670571586\n",
      "Training stage for Flod 1 Epoch: 77 [0/1993                 (0%)]\tLoss: 0.407011\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9382174306156144\n",
      "Training stage for Flod 1 Epoch: 78 [0/1993                 (0%)]\tLoss: 0.375825\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378832848494378\n",
      "Training stage for Flod 1 Epoch: 79 [0/1993                 (0%)]\tLoss: 0.375874\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375638808082396\n",
      "Training stage for Flod 1 Epoch: 80 [0/1993                 (0%)]\tLoss: 0.375767\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9383402783237675\n",
      "Training stage for Flod 1 Epoch: 81 [0/1993                 (0%)]\tLoss: 0.438275\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9378292318578505\n",
      "Training stage for Flod 1 Epoch: 82 [0/1993                 (0%)]\tLoss: 0.375863\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9383501061404198\n",
      "Training stage for Flod 1 Epoch: 83 [0/1993                 (0%)]\tLoss: 0.344535\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9384827816652253\n",
      "Training stage for Flod 1 Epoch: 84 [0/1993                 (0%)]\tLoss: 0.375798\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9379422517493514\n",
      "Training stage for Flod 1 Epoch: 85 [0/1993                 (0%)]\tLoss: 0.438295\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9381388080823965\n",
      "Training stage for Flod 1 Epoch: 86 [0/1993                 (0%)]\tLoss: 0.344586\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375049139083261\n",
      "Training stage for Flod 1 Epoch: 87 [0/1993                 (0%)]\tLoss: 0.469572\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9379864769242865\n",
      "Training stage for Flod 1 Epoch: 88 [0/1993                 (0%)]\tLoss: 0.469485\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9373378410252379\n",
      "Training stage for Flod 1 Epoch: 89 [0/1993                 (0%)]\tLoss: 0.407042\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377014702413711\n",
      "Training stage for Flod 1 Epoch: 90 [0/1993                 (0%)]\tLoss: 0.469480\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375442251749352\n",
      "Training stage for Flod 1 Epoch: 91 [0/1993                 (0%)]\tLoss: 0.375897\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9381142385407659\n",
      "Training stage for Flod 1 Epoch: 92 [0/1993                 (0%)]\tLoss: 0.438288\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9371609403254972\n",
      "Training stage for Flod 1 Epoch: 93 [0/1993                 (0%)]\tLoss: 0.313386\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377997484078937\n",
      "Training stage for Flod 1 Epoch: 94 [0/1993                 (0%)]\tLoss: 0.375916\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9377997484078937\n",
      "Training stage for Flod 1 Epoch: 95 [0/1993                 (0%)]\tLoss: 0.438318\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9384926094818775\n",
      "Training stage for Flod 1 Epoch: 96 [0/1993                 (0%)]\tLoss: 0.406986\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9381093246324397\n",
      "Training stage for Flod 1 Epoch: 97 [0/1993                 (0%)]\tLoss: 0.500623\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9381682915323533\n",
      "Training stage for Flod 1 Epoch: 98 [0/1993                 (0%)]\tLoss: 0.438216\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.9375098278166523\n",
      "Training stage for Flod 1 Epoch: 99 [0/1993                 (0%)]\tLoss: 0.407008\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937691642424719\n",
      "Training stage for Flod 1 Epoch: 100 [0/1993                 (0%)]\tLoss: 0.375793\n",
      "Test set for fold1: Average Loss:           0.0120, Accuracy: 438/499           (88%)\n",
      "The auc of this fold is 0.937342754933564\n",
      "-------------------Fold 2-------------------\n",
      "Training stage for Flod 2 Epoch: 1 [0/1994                 (0%)]\tLoss: 0.690058\n",
      "Test set for fold2: Average Loss:           0.0189, Accuracy: 393/498           (79%)\n",
      "The auc of this fold is 0.9255881366961802\n",
      "Training stage for Flod 2 Epoch: 2 [0/1994                 (0%)]\tLoss: 0.633699\n",
      "Test set for fold2: Average Loss:           0.0132, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.917335489341563\n",
      "Training stage for Flod 2 Epoch: 3 [0/1994                 (0%)]\tLoss: 0.479331\n",
      "Test set for fold2: Average Loss:           0.0125, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.9199581944745716\n",
      "Training stage for Flod 2 Epoch: 4 [0/1994                 (0%)]\tLoss: 0.472524\n",
      "Test set for fold2: Average Loss:           0.0124, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.9189574254106604\n",
      "Training stage for Flod 2 Epoch: 5 [0/1994                 (0%)]\tLoss: 0.348615\n",
      "Test set for fold2: Average Loss:           0.0124, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.9172812604760309\n",
      "Training stage for Flod 2 Epoch: 6 [0/1994                 (0%)]\tLoss: 0.433183\n",
      "Test set for fold2: Average Loss:           0.0124, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.9172270316104987\n",
      "Training stage for Flod 2 Epoch: 7 [0/1994                 (0%)]\tLoss: 0.432330\n",
      "Test set for fold2: Average Loss:           0.0124, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.917503105835026\n",
      "Training stage for Flod 2 Epoch: 8 [0/1994                 (0%)]\tLoss: 0.377629\n",
      "Test set for fold2: Average Loss:           0.0124, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.9186813511861333\n",
      "Training stage for Flod 2 Epoch: 9 [0/1994                 (0%)]\tLoss: 0.376759\n",
      "Test set for fold2: Average Loss:           0.0124, Accuracy: 432/498           (87%)\n",
      "The auc of this fold is 0.9180946934590128\n",
      "Training stage for Flod 2 Epoch: 10 [0/1994                 (0%)]\tLoss: 0.438827\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9183855573741397\n",
      "Training stage for Flod 2 Epoch: 11 [0/1994                 (0%)]\tLoss: 0.314410\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9225463903295144\n",
      "Training stage for Flod 2 Epoch: 12 [0/1994                 (0%)]\tLoss: 0.376760\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9236408274338901\n",
      "Training stage for Flod 2 Epoch: 13 [0/1994                 (0%)]\tLoss: 0.469197\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9228766934195736\n",
      "Training stage for Flod 2 Epoch: 14 [0/1994                 (0%)]\tLoss: 0.375731\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9218315552838634\n",
      "Training stage for Flod 2 Epoch: 15 [0/1994                 (0%)]\tLoss: 0.381335\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9236408274338901\n",
      "Training stage for Flod 2 Epoch: 16 [0/1994                 (0%)]\tLoss: 0.377144\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9231083985723019\n",
      "Training stage for Flod 2 Epoch: 17 [0/1994                 (0%)]\tLoss: 0.500520\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9233943325905621\n",
      "Training stage for Flod 2 Epoch: 18 [0/1994                 (0%)]\tLoss: 0.531439\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.922906272800773\n",
      "Training stage for Flod 2 Epoch: 19 [0/1994                 (0%)]\tLoss: 0.407733\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9225611800201139\n",
      "Training stage for Flod 2 Epoch: 20 [0/1994                 (0%)]\tLoss: 0.345125\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9222062274457219\n",
      "Training stage for Flod 2 Epoch: 21 [0/1994                 (0%)]\tLoss: 0.439112\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.922304825383053\n",
      "Training stage for Flod 2 Epoch: 22 [0/1994                 (0%)]\tLoss: 0.375917\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9220780501271914\n",
      "Training stage for Flod 2 Epoch: 23 [0/1994                 (0%)]\tLoss: 0.500382\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9210131924040149\n",
      "Training stage for Flod 2 Epoch: 24 [0/1994                 (0%)]\tLoss: 0.375644\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9204511841612273\n",
      "Training stage for Flod 2 Epoch: 25 [0/1994                 (0%)]\tLoss: 0.438018\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9209047346729505\n",
      "Training stage for Flod 2 Epoch: 26 [0/1994                 (0%)]\tLoss: 0.469823\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9202490583896985\n",
      "Training stage for Flod 2 Epoch: 27 [0/1994                 (0%)]\tLoss: 0.438740\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9212202480724103\n",
      "Training stage for Flod 2 Epoch: 28 [0/1994                 (0%)]\tLoss: 0.438243\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.921580130543669\n",
      "Training stage for Flod 2 Epoch: 29 [0/1994                 (0%)]\tLoss: 0.375866\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9222358068269212\n",
      "Training stage for Flod 2 Epoch: 30 [0/1994                 (0%)]\tLoss: 0.407062\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.922171718167656\n",
      "Training stage for Flod 2 Epoch: 31 [0/1994                 (0%)]\tLoss: 0.407038\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.922250596517521\n",
      "Training stage for Flod 2 Epoch: 32 [0/1994                 (0%)]\tLoss: 0.376097\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9220435408491254\n",
      "Training stage for Flod 2 Epoch: 33 [0/1994                 (0%)]\tLoss: 0.375819\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9207765573544201\n",
      "Training stage for Flod 2 Epoch: 34 [0/1994                 (0%)]\tLoss: 0.344593\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9213434954940742\n",
      "Training stage for Flod 2 Epoch: 35 [0/1994                 (0%)]\tLoss: 0.407087\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9213089862160084\n",
      "Training stage for Flod 2 Epoch: 36 [0/1994                 (0%)]\tLoss: 0.344740\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9205251326142257\n",
      "Training stage for Flod 2 Epoch: 37 [0/1994                 (0%)]\tLoss: 0.344760\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9209392439510166\n",
      "Training stage for Flod 2 Epoch: 38 [0/1994                 (0%)]\tLoss: 0.344535\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9214667429157382\n",
      "Training stage for Flod 2 Epoch: 39 [0/1994                 (0%)]\tLoss: 0.344846\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9207173985920215\n",
      "Training stage for Flod 2 Epoch: 40 [0/1994                 (0%)]\tLoss: 0.407441\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9220435408491254\n",
      "Training stage for Flod 2 Epoch: 41 [0/1994                 (0%)]\tLoss: 0.438353\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9211167202382127\n",
      "Training stage for Flod 2 Epoch: 42 [0/1994                 (0%)]\tLoss: 0.407092\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9208850150854844\n",
      "Training stage for Flod 2 Epoch: 43 [0/1994                 (0%)]\tLoss: 0.531943\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9219893119835934\n",
      "Training stage for Flod 2 Epoch: 44 [0/1994                 (0%)]\tLoss: 0.375681\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9221273490958569\n",
      "Training stage for Flod 2 Epoch: 45 [0/1994                 (0%)]\tLoss: 0.375841\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9224428624953165\n",
      "Training stage for Flod 2 Epoch: 46 [0/1994                 (0%)]\tLoss: 0.376027\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9220583305397251\n",
      "Training stage for Flod 2 Epoch: 47 [0/1994                 (0%)]\tLoss: 0.407081\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9220090315710594\n",
      "Training stage for Flod 2 Epoch: 48 [0/1994                 (0%)]\tLoss: 0.469500\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9219055037368618\n",
      "Training stage for Flod 2 Epoch: 49 [0/1994                 (0%)]\tLoss: 0.344678\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9226696377511783\n",
      "Training stage for Flod 2 Epoch: 50 [0/1994                 (0%)]\tLoss: 0.500851\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.922428072804717\n",
      "Training stage for Flod 2 Epoch: 51 [0/1994                 (0%)]\tLoss: 0.407126\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9225759697107137\n",
      "Training stage for Flod 2 Epoch: 52 [0/1994                 (0%)]\tLoss: 0.313347\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.922684427441778\n",
      "Training stage for Flod 2 Epoch: 53 [0/1994                 (0%)]\tLoss: 0.376152\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9224527222890497\n",
      "Training stage for Flod 2 Epoch: 54 [0/1994                 (0%)]\tLoss: 0.407072\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9223935635266511\n",
      "Training stage for Flod 2 Epoch: 55 [0/1994                 (0%)]\tLoss: 0.313367\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9228175346571749\n",
      "Training stage for Flod 2 Epoch: 56 [0/1994                 (0%)]\tLoss: 0.469418\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9225661099169806\n",
      "Training stage for Flod 2 Epoch: 57 [0/1994                 (0%)]\tLoss: 0.407096\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9222456666206544\n",
      "Training stage for Flod 2 Epoch: 58 [0/1994                 (0%)]\tLoss: 0.438221\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.923073889294236\n",
      "Training stage for Flod 2 Epoch: 59 [0/1994                 (0%)]\tLoss: 0.375784\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.922640058369979\n",
      "Training stage for Flod 2 Epoch: 60 [0/1994                 (0%)]\tLoss: 0.438334\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9233696831062292\n",
      "Training stage for Flod 2 Epoch: 61 [0/1994                 (0%)]\tLoss: 0.469549\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9233598233124964\n",
      "Training stage for Flod 2 Epoch: 62 [0/1994                 (0%)]\tLoss: 0.407630\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9231576975409674\n",
      "Training stage for Flod 2 Epoch: 63 [0/1994                 (0%)]\tLoss: 0.438168\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.923966200627083\n",
      "Training stage for Flod 2 Epoch: 64 [0/1994                 (0%)]\tLoss: 0.344557\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9247648439194651\n",
      "Training stage for Flod 2 Epoch: 65 [0/1994                 (0%)]\tLoss: 0.344713\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9250606377314587\n",
      "Training stage for Flod 2 Epoch: 66 [0/1994                 (0%)]\tLoss: 0.407038\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9249324604129281\n",
      "Training stage for Flod 2 Epoch: 67 [0/1994                 (0%)]\tLoss: 0.500677\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9242965037171422\n",
      "Training stage for Flod 2 Epoch: 68 [0/1994                 (0%)]\tLoss: 0.375879\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9242472047484767\n",
      "Training stage for Flod 2 Epoch: 69 [0/1994                 (0%)]\tLoss: 0.375972\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9240105696988818\n",
      "Training stage for Flod 2 Epoch: 70 [0/1994                 (0%)]\tLoss: 0.407013\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9242274851610104\n",
      "Training stage for Flod 2 Epoch: 71 [0/1994                 (0%)]\tLoss: 0.344562\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9238774624834848\n",
      "Training stage for Flod 2 Epoch: 72 [0/1994                 (0%)]\tLoss: 0.407070\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.924188045986078\n",
      "Training stage for Flod 2 Epoch: 73 [0/1994                 (0%)]\tLoss: 0.438178\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9246317367040682\n",
      "Training stage for Flod 2 Epoch: 74 [0/1994                 (0%)]\tLoss: 0.407181\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9238183037210861\n",
      "Training stage for Flod 2 Epoch: 75 [0/1994                 (0%)]\tLoss: 0.407066\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9241239573268127\n",
      "Training stage for Flod 2 Epoch: 76 [0/1994                 (0%)]\tLoss: 0.407068\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9239464810396167\n",
      "Training stage for Flod 2 Epoch: 77 [0/1994                 (0%)]\tLoss: 0.375978\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9249521800003944\n",
      "Training stage for Flod 2 Epoch: 78 [0/1994                 (0%)]\tLoss: 0.406988\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9244246810356728\n",
      "Training stage for Flod 2 Epoch: 79 [0/1994                 (0%)]\tLoss: 0.438522\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9238380233085524\n",
      "Training stage for Flod 2 Epoch: 80 [0/1994                 (0%)]\tLoss: 0.438337\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9243901717576068\n",
      "Training stage for Flod 2 Epoch: 81 [0/1994                 (0%)]\tLoss: 0.407025\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.924143676914279\n",
      "Training stage for Flod 2 Epoch: 82 [0/1994                 (0%)]\tLoss: 0.375833\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9243704521701406\n",
      "Training stage for Flod 2 Epoch: 83 [0/1994                 (0%)]\tLoss: 0.375775\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9240302892863481\n",
      "Training stage for Flod 2 Epoch: 84 [0/1994                 (0%)]\tLoss: 0.407095\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9241929758829446\n",
      "Training stage for Flod 2 Epoch: 85 [0/1994                 (0%)]\tLoss: 0.407066\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9251444459781901\n",
      "Training stage for Flod 2 Epoch: 86 [0/1994                 (0%)]\tLoss: 0.375822\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9249521800003944\n",
      "Training stage for Flod 2 Epoch: 87 [0/1994                 (0%)]\tLoss: 0.407126\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9253564315434522\n",
      "Training stage for Flod 2 Epoch: 88 [0/1994                 (0%)]\tLoss: 0.375758\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.924878231547396\n",
      "Training stage for Flod 2 Epoch: 89 [0/1994                 (0%)]\tLoss: 0.375901\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9248979511348623\n",
      "Training stage for Flod 2 Epoch: 90 [0/1994                 (0%)]\tLoss: 0.344872\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9257015243241111\n",
      "Training stage for Flod 2 Epoch: 91 [0/1994                 (0%)]\tLoss: 0.407003\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9255092583463155\n",
      "Training stage for Flod 2 Epoch: 92 [0/1994                 (0%)]\tLoss: 0.344589\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9256374356648458\n",
      "Training stage for Flod 2 Epoch: 93 [0/1994                 (0%)]\tLoss: 0.375804\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9261402851452348\n",
      "Training stage for Flod 2 Epoch: 94 [0/1994                 (0%)]\tLoss: 0.375810\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9262191634950996\n",
      "Training stage for Flod 2 Epoch: 95 [0/1994                 (0%)]\tLoss: 0.407026\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9256226459742463\n",
      "Training stage for Flod 2 Epoch: 96 [0/1994                 (0%)]\tLoss: 0.344599\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9252183944311885\n",
      "Training stage for Flod 2 Epoch: 97 [0/1994                 (0%)]\tLoss: 0.438209\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.924735264538266\n",
      "Training stage for Flod 2 Epoch: 98 [0/1994                 (0%)]\tLoss: 0.407062\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9254648892745163\n",
      "Training stage for Flod 2 Epoch: 99 [0/1994                 (0%)]\tLoss: 0.469444\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9244345408294059\n",
      "Training stage for Flod 2 Epoch: 100 [0/1994                 (0%)]\tLoss: 0.406985\n",
      "Test set for fold2: Average Loss:           0.0123, Accuracy: 435/498           (87%)\n",
      "The auc of this fold is 0.9253909408215181\n",
      "-------------------Fold 3-------------------\n",
      "Training stage for Flod 3 Epoch: 1 [0/1994                 (0%)]\tLoss: 0.698153\n",
      "Test set for fold3: Average Loss:           0.0208, Accuracy: 157/498           (32%)\n",
      "The auc of this fold is 0.8965362544615567\n",
      "Training stage for Flod 3 Epoch: 2 [0/1994                 (0%)]\tLoss: 0.685166\n",
      "Test set for fold3: Average Loss:           0.0198, Accuracy: 419/498           (84%)\n",
      "The auc of this fold is 0.9149592790518823\n",
      "Training stage for Flod 3 Epoch: 3 [0/1994                 (0%)]\tLoss: 0.634857\n",
      "Test set for fold3: Average Loss:           0.0163, Accuracy: 421/498           (85%)\n",
      "The auc of this fold is 0.9129725306146594\n",
      "Training stage for Flod 3 Epoch: 4 [0/1994                 (0%)]\tLoss: 0.440966\n",
      "Test set for fold3: Average Loss:           0.0146, Accuracy: 421/498           (85%)\n",
      "The auc of this fold is 0.9188933367513952\n",
      "Training stage for Flod 3 Epoch: 5 [0/1994                 (0%)]\tLoss: 0.450105\n",
      "Test set for fold3: Average Loss:           0.0136, Accuracy: 421/498           (85%)\n",
      "The auc of this fold is 0.9208159965293525\n",
      "Training stage for Flod 3 Epoch: 6 [0/1994                 (0%)]\tLoss: 0.495899\n",
      "Test set for fold3: Average Loss:           0.0132, Accuracy: 421/498           (85%)\n",
      "The auc of this fold is 0.9205842913766245\n",
      "Training stage for Flod 3 Epoch: 7 [0/1994                 (0%)]\tLoss: 0.384191\n",
      "Test set for fold3: Average Loss:           0.0131, Accuracy: 421/498           (85%)\n",
      "The auc of this fold is 0.9138056831851078\n",
      "Training stage for Flod 3 Epoch: 8 [0/1994                 (0%)]\tLoss: 0.378012\n",
      "Test set for fold3: Average Loss:           0.0131, Accuracy: 422/498           (85%)\n",
      "The auc of this fold is 0.9212301078661436\n",
      "Training stage for Flod 3 Epoch: 9 [0/1994                 (0%)]\tLoss: 0.439820\n",
      "Test set for fold3: Average Loss:           0.0131, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9143233223560963\n",
      "Training stage for Flod 3 Epoch: 10 [0/1994                 (0%)]\tLoss: 0.315352\n",
      "Test set for fold3: Average Loss:           0.0131, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.912834493502396\n",
      "Training stage for Flod 3 Epoch: 11 [0/1994                 (0%)]\tLoss: 0.469756\n",
      "Test set for fold3: Average Loss:           0.0131, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9236457573307567\n",
      "Training stage for Flod 3 Epoch: 12 [0/1994                 (0%)]\tLoss: 0.438774\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9205695016860247\n",
      "Training stage for Flod 3 Epoch: 13 [0/1994                 (0%)]\tLoss: 0.379471\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9232513655814321\n",
      "Training stage for Flod 3 Epoch: 14 [0/1994                 (0%)]\tLoss: 0.469735\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9216294295123346\n",
      "Training stage for Flod 3 Epoch: 15 [0/1994                 (0%)]\tLoss: 0.469846\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9214667429157383\n",
      "Training stage for Flod 3 Epoch: 16 [0/1994                 (0%)]\tLoss: 0.375999\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9186567017018004\n",
      "Training stage for Flod 3 Epoch: 17 [0/1994                 (0%)]\tLoss: 0.376008\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9207864171481532\n",
      "Training stage for Flod 3 Epoch: 18 [0/1994                 (0%)]\tLoss: 0.313519\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9189771449981265\n",
      "Training stage for Flod 3 Epoch: 19 [0/1994                 (0%)]\tLoss: 0.376585\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9215209717812702\n",
      "Training stage for Flod 3 Epoch: 20 [0/1994                 (0%)]\tLoss: 0.439621\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9204511841612273\n",
      "Training stage for Flod 3 Epoch: 21 [0/1994                 (0%)]\tLoss: 0.375862\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9209836130228155\n",
      "Training stage for Flod 3 Epoch: 22 [0/1994                 (0%)]\tLoss: 0.375886\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9196476109719784\n",
      "Training stage for Flod 3 Epoch: 23 [0/1994                 (0%)]\tLoss: 0.438837\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9204807635424266\n",
      "Training stage for Flod 3 Epoch: 24 [0/1994                 (0%)]\tLoss: 0.438322\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9206533099327562\n",
      "Training stage for Flod 3 Epoch: 25 [0/1994                 (0%)]\tLoss: 0.407107\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9201406006586343\n",
      "Training stage for Flod 3 Epoch: 26 [0/1994                 (0%)]\tLoss: 0.375861\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9202194790084992\n",
      "Training stage for Flod 3 Epoch: 27 [0/1994                 (0%)]\tLoss: 0.344637\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9205793614797578\n",
      "Training stage for Flod 3 Epoch: 28 [0/1994                 (0%)]\tLoss: 0.438296\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9208307862199523\n",
      "Training stage for Flod 3 Epoch: 29 [0/1994                 (0%)]\tLoss: 0.375826\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9200518625150362\n",
      "Training stage for Flod 3 Epoch: 30 [0/1994                 (0%)]\tLoss: 0.375875\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9178876377906173\n",
      "Training stage for Flod 3 Epoch: 31 [0/1994                 (0%)]\tLoss: 0.407624\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9203821656050957\n",
      "Training stage for Flod 3 Epoch: 32 [0/1994                 (0%)]\tLoss: 0.313564\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9204906233361598\n",
      "Training stage for Flod 3 Epoch: 33 [0/1994                 (0%)]\tLoss: 0.438309\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9211660192068782\n",
      "Training stage for Flod 3 Epoch: 34 [0/1994                 (0%)]\tLoss: 0.407034\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9205645717891582\n",
      "Training stage for Flod 3 Epoch: 35 [0/1994                 (0%)]\tLoss: 0.407040\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9183263986117409\n",
      "Training stage for Flod 3 Epoch: 36 [0/1994                 (0%)]\tLoss: 0.407246\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9198497367435075\n",
      "Training stage for Flod 3 Epoch: 37 [0/1994                 (0%)]\tLoss: 0.438303\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9200666522056358\n",
      "Training stage for Flod 3 Epoch: 38 [0/1994                 (0%)]\tLoss: 0.344664\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9199680542683047\n",
      "Training stage for Flod 3 Epoch: 39 [0/1994                 (0%)]\tLoss: 0.438300\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9204511841612274\n",
      "Training stage for Flod 3 Epoch: 40 [0/1994                 (0%)]\tLoss: 0.438283\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9202391985959653\n",
      "Training stage for Flod 3 Epoch: 41 [0/1994                 (0%)]\tLoss: 0.469530\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9204511841612275\n",
      "Training stage for Flod 3 Epoch: 42 [0/1994                 (0%)]\tLoss: 0.375801\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9208061367356195\n",
      "Training stage for Flod 3 Epoch: 43 [0/1994                 (0%)]\tLoss: 0.438275\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9211019305476129\n",
      "Training stage for Flod 3 Epoch: 44 [0/1994                 (0%)]\tLoss: 0.407139\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9205004831298929\n",
      "Training stage for Flod 3 Epoch: 45 [0/1994                 (0%)]\tLoss: 0.344552\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9205054130267595\n",
      "Training stage for Flod 3 Epoch: 46 [0/1994                 (0%)]\tLoss: 0.375784\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9205645717891582\n",
      "Training stage for Flod 3 Epoch: 47 [0/1994                 (0%)]\tLoss: 0.313388\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9210279820946146\n",
      "Training stage for Flod 3 Epoch: 48 [0/1994                 (0%)]\tLoss: 0.407039\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9213829346690068\n",
      "Training stage for Flod 3 Epoch: 49 [0/1994                 (0%)]\tLoss: 0.407034\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9211709491037448\n",
      "Training stage for Flod 3 Epoch: 50 [0/1994                 (0%)]\tLoss: 0.438563\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9206582398296228\n",
      "Training stage for Flod 3 Epoch: 51 [0/1994                 (0%)]\tLoss: 0.407295\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9205547119954249\n",
      "Training stage for Flod 3 Epoch: 52 [0/1994                 (0%)]\tLoss: 0.407074\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9208800851886179\n",
      "Training stage for Flod 3 Epoch: 53 [0/1994                 (0%)]\tLoss: 0.313295\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9203673759144959\n",
      "Training stage for Flod 3 Epoch: 54 [0/1994                 (0%)]\tLoss: 0.407054\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9194356254067164\n",
      "Training stage for Flod 3 Epoch: 55 [0/1994                 (0%)]\tLoss: 0.407107\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9212744769379425\n",
      "Training stage for Flod 3 Epoch: 56 [0/1994                 (0%)]\tLoss: 0.344531\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9212941965254087\n",
      "Training stage for Flod 3 Epoch: 57 [0/1994                 (0%)]\tLoss: 0.438290\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9210477016820808\n",
      "Training stage for Flod 3 Epoch: 58 [0/1994                 (0%)]\tLoss: 0.375782\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9214322336376722\n",
      "Training stage for Flod 3 Epoch: 59 [0/1994                 (0%)]\tLoss: 0.407022\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9207371181794877\n",
      "Training stage for Flod 3 Epoch: 60 [0/1994                 (0%)]\tLoss: 0.375783\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9200025635463707\n",
      "Training stage for Flod 3 Epoch: 61 [0/1994                 (0%)]\tLoss: 0.375789\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9191250419041234\n",
      "Training stage for Flod 3 Epoch: 62 [0/1994                 (0%)]\tLoss: 0.375790\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9211315099288123\n",
      "Training stage for Flod 3 Epoch: 63 [0/1994                 (0%)]\tLoss: 0.438277\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9209638934353493\n",
      "Training stage for Flod 3 Epoch: 64 [0/1994                 (0%)]\tLoss: 0.344621\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.920968823332216\n",
      "Training stage for Flod 3 Epoch: 65 [0/1994                 (0%)]\tLoss: 0.407020\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9203082171520971\n",
      "Training stage for Flod 3 Epoch: 66 [0/1994                 (0%)]\tLoss: 0.344736\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9212202480724103\n",
      "Training stage for Flod 3 Epoch: 67 [0/1994                 (0%)]\tLoss: 0.376004\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.920801206838753\n",
      "Training stage for Flod 3 Epoch: 68 [0/1994                 (0%)]\tLoss: 0.375830\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9207913470450197\n",
      "Training stage for Flod 3 Epoch: 69 [0/1994                 (0%)]\tLoss: 0.375777\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9190806728323242\n",
      "Training stage for Flod 3 Epoch: 70 [0/1994                 (0%)]\tLoss: 0.469514\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9210723511664136\n",
      "Training stage for Flod 3 Epoch: 71 [0/1994                 (0%)]\tLoss: 0.407067\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9210378418883478\n",
      "Training stage for Flod 3 Epoch: 72 [0/1994                 (0%)]\tLoss: 0.313283\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9214125140502061\n",
      "Training stage for Flod 3 Epoch: 73 [0/1994                 (0%)]\tLoss: 0.313274\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9215209717812705\n",
      "Training stage for Flod 3 Epoch: 74 [0/1994                 (0%)]\tLoss: 0.344627\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9195440831377808\n",
      "Training stage for Flod 3 Epoch: 75 [0/1994                 (0%)]\tLoss: 0.407024\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9212103882786773\n",
      "Training stage for Flod 3 Epoch: 76 [0/1994                 (0%)]\tLoss: 0.407018\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9220090315710596\n",
      "Training stage for Flod 3 Epoch: 77 [0/1994                 (0%)]\tLoss: 0.407074\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9213927944627398\n",
      "Training stage for Flod 3 Epoch: 78 [0/1994                 (0%)]\tLoss: 0.469514\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9217625367277317\n",
      "Training stage for Flod 3 Epoch: 79 [0/1994                 (0%)]\tLoss: 0.375767\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9221125594052572\n",
      "Training stage for Flod 3 Epoch: 80 [0/1994                 (0%)]\tLoss: 0.407021\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9222111573425883\n",
      "Training stage for Flod 3 Epoch: 81 [0/1994                 (0%)]\tLoss: 0.500793\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.92183648518073\n",
      "Training stage for Flod 3 Epoch: 82 [0/1994                 (0%)]\tLoss: 0.375918\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9219991717773264\n",
      "Training stage for Flod 3 Epoch: 83 [0/1994                 (0%)]\tLoss: 0.344522\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9201258109680346\n",
      "Training stage for Flod 3 Epoch: 84 [0/1994                 (0%)]\tLoss: 0.375781\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9212646171442094\n",
      "Training stage for Flod 3 Epoch: 85 [0/1994                 (0%)]\tLoss: 0.375770\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9224379325984501\n",
      "Training stage for Flod 3 Epoch: 86 [0/1994                 (0%)]\tLoss: 0.407018\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9219646624992605\n",
      "Training stage for Flod 3 Epoch: 87 [0/1994                 (0%)]\tLoss: 0.313275\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9225611800201139\n",
      "Training stage for Flod 3 Epoch: 88 [0/1994                 (0%)]\tLoss: 0.375767\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9224576521859162\n",
      "Training stage for Flod 3 Epoch: 89 [0/1994                 (0%)]\tLoss: 0.407021\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9224576521859162\n",
      "Training stage for Flod 3 Epoch: 90 [0/1994                 (0%)]\tLoss: 0.469519\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9222259470331882\n",
      "Training stage for Flod 3 Epoch: 91 [0/1994                 (0%)]\tLoss: 0.407054\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9227682356885094\n",
      "Training stage for Flod 3 Epoch: 92 [0/1994                 (0%)]\tLoss: 0.375780\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9221717181676559\n",
      "Training stage for Flod 3 Epoch: 93 [0/1994                 (0%)]\tLoss: 0.375771\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9218562047681962\n",
      "Training stage for Flod 3 Epoch: 94 [0/1994                 (0%)]\tLoss: 0.469638\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.921067421269547\n",
      "Training stage for Flod 3 Epoch: 95 [0/1994                 (0%)]\tLoss: 0.375767\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9225020212577153\n",
      "Training stage for Flod 3 Epoch: 96 [0/1994                 (0%)]\tLoss: 0.375768\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9224477923921832\n",
      "Training stage for Flod 3 Epoch: 97 [0/1994                 (0%)]\tLoss: 0.344535\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9220731202303247\n",
      "Training stage for Flod 3 Epoch: 98 [0/1994                 (0%)]\tLoss: 0.438262\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.922082980024058\n",
      "Training stage for Flod 3 Epoch: 99 [0/1994                 (0%)]\tLoss: 0.375770\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.9216836583778667\n",
      "Training stage for Flod 3 Epoch: 100 [0/1994                 (0%)]\tLoss: 0.344521\n",
      "Test set for fold3: Average Loss:           0.0130, Accuracy: 423/498           (85%)\n",
      "The auc of this fold is 0.922304825383053\n",
      "-------------------Fold 4-------------------\n",
      "Training stage for Flod 4 Epoch: 1 [0/1994                 (0%)]\tLoss: 0.688556\n",
      "Test set for fold4: Average Loss:           0.0205, Accuracy: 404/498           (81%)\n",
      "The auc of this fold is 0.9046952337757094\n",
      "Training stage for Flod 4 Epoch: 2 [0/1994                 (0%)]\tLoss: 0.686752\n",
      "Test set for fold4: Average Loss:           0.0183, Accuracy: 427/498           (86%)\n",
      "The auc of this fold is 0.9161621738873222\n",
      "Training stage for Flod 4 Epoch: 3 [0/1994                 (0%)]\tLoss: 0.602848\n",
      "Test set for fold4: Average Loss:           0.0135, Accuracy: 427/498           (86%)\n",
      "The auc of this fold is 0.9306511407781349\n",
      "Training stage for Flod 4 Epoch: 4 [0/1994                 (0%)]\tLoss: 0.469945\n",
      "Test set for fold4: Average Loss:           0.0127, Accuracy: 427/498           (86%)\n",
      "The auc of this fold is 0.926238883082566\n",
      "Training stage for Flod 4 Epoch: 5 [0/1994                 (0%)]\tLoss: 0.379695\n",
      "Test set for fold4: Average Loss:           0.0127, Accuracy: 429/498           (86%)\n",
      "The auc of this fold is 0.9151909842046104\n",
      "Training stage for Flod 4 Epoch: 6 [0/1994                 (0%)]\tLoss: 0.498823\n",
      "Test set for fold4: Average Loss:           0.0126, Accuracy: 429/498           (86%)\n",
      "The auc of this fold is 0.9140718976159019\n",
      "Training stage for Flod 4 Epoch: 7 [0/1994                 (0%)]\tLoss: 0.439379\n",
      "Test set for fold4: Average Loss:           0.0126, Accuracy: 430/498           (86%)\n",
      "The auc of this fold is 0.9219548027055273\n",
      "Training stage for Flod 4 Epoch: 8 [0/1994                 (0%)]\tLoss: 0.376999\n",
      "Test set for fold4: Average Loss:           0.0126, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9278805387391295\n",
      "Training stage for Flod 4 Epoch: 9 [0/1994                 (0%)]\tLoss: 0.347963\n",
      "Test set for fold4: Average Loss:           0.0126, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9289700459466387\n",
      "Training stage for Flod 4 Epoch: 10 [0/1994                 (0%)]\tLoss: 0.469959\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9338999428131962\n",
      "Training stage for Flod 4 Epoch: 11 [0/1994                 (0%)]\tLoss: 0.378348\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9241732562954783\n",
      "Training stage for Flod 4 Epoch: 12 [0/1994                 (0%)]\tLoss: 0.376163\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.940989134507306\n",
      "Training stage for Flod 4 Epoch: 13 [0/1994                 (0%)]\tLoss: 0.438352\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9393524087476091\n",
      "Training stage for Flod 4 Epoch: 14 [0/1994                 (0%)]\tLoss: 0.469682\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9281467531699236\n",
      "Training stage for Flod 4 Epoch: 15 [0/1994                 (0%)]\tLoss: 0.375951\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9245577882510696\n",
      "Training stage for Flod 4 Epoch: 16 [0/1994                 (0%)]\tLoss: 0.407073\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.935709214963223\n",
      "Training stage for Flod 4 Epoch: 17 [0/1994                 (0%)]\tLoss: 0.471724\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.92544516968705\n",
      "Training stage for Flod 4 Epoch: 18 [0/1994                 (0%)]\tLoss: 0.376586\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9272002129715446\n",
      "Training stage for Flod 4 Epoch: 19 [0/1994                 (0%)]\tLoss: 0.375874\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.93027153871941\n",
      "Training stage for Flod 4 Epoch: 20 [0/1994                 (0%)]\tLoss: 0.375872\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9274220583305397\n",
      "Training stage for Flod 4 Epoch: 21 [0/1994                 (0%)]\tLoss: 0.469587\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.926416359369762\n",
      "Training stage for Flod 4 Epoch: 22 [0/1994                 (0%)]\tLoss: 0.469523\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9302813985131431\n",
      "Training stage for Flod 4 Epoch: 23 [0/1994                 (0%)]\tLoss: 0.438252\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9324653428250281\n",
      "Training stage for Flod 4 Epoch: 24 [0/1994                 (0%)]\tLoss: 0.407188\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.927062175859281\n",
      "Training stage for Flod 4 Epoch: 25 [0/1994                 (0%)]\tLoss: 0.407047\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9263867799885626\n",
      "Training stage for Flod 4 Epoch: 26 [0/1994                 (0%)]\tLoss: 0.469500\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9279051882234624\n",
      "Training stage for Flod 4 Epoch: 27 [0/1994                 (0%)]\tLoss: 0.407058\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9268255408096862\n",
      "Training stage for Flod 4 Epoch: 28 [0/1994                 (0%)]\tLoss: 0.438263\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9286989016189782\n",
      "Training stage for Flod 4 Epoch: 29 [0/1994                 (0%)]\tLoss: 0.407104\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9284819861568494\n",
      "Training stage for Flod 4 Epoch: 30 [0/1994                 (0%)]\tLoss: 0.375830\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9253761511309183\n",
      "Training stage for Flod 4 Epoch: 31 [0/1994                 (0%)]\tLoss: 0.375854\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9268354006034194\n",
      "Training stage for Flod 4 Epoch: 32 [0/1994                 (0%)]\tLoss: 0.375793\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9276241841020686\n",
      "Training stage for Flod 4 Epoch: 33 [0/1994                 (0%)]\tLoss: 0.375804\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.928979905740372\n",
      "Training stage for Flod 4 Epoch: 34 [0/1994                 (0%)]\tLoss: 0.438863\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9274664274023388\n",
      "Training stage for Flod 4 Epoch: 35 [0/1994                 (0%)]\tLoss: 0.438271\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9271164047248132\n",
      "Training stage for Flod 4 Epoch: 36 [0/1994                 (0%)]\tLoss: 0.532007\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.927496006783538\n",
      "Training stage for Flod 4 Epoch: 37 [0/1994                 (0%)]\tLoss: 0.407053\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.927239652146477\n",
      "Training stage for Flod 4 Epoch: 38 [0/1994                 (0%)]\tLoss: 0.375935\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9278953284297292\n",
      "Training stage for Flod 4 Epoch: 39 [0/1994                 (0%)]\tLoss: 0.375781\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9275206562678708\n",
      "Training stage for Flod 4 Epoch: 40 [0/1994                 (0%)]\tLoss: 0.438272\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9275896748240027\n",
      "Training stage for Flod 4 Epoch: 41 [0/1994                 (0%)]\tLoss: 0.407207\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9267713119441541\n",
      "Training stage for Flod 4 Epoch: 42 [0/1994                 (0%)]\tLoss: 0.375775\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9267663820472876\n",
      "Training stage for Flod 4 Epoch: 43 [0/1994                 (0%)]\tLoss: 0.407051\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9266480645224902\n",
      "Training stage for Flod 4 Epoch: 44 [0/1994                 (0%)]\tLoss: 0.438297\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9262832521543649\n",
      "Training stage for Flod 4 Epoch: 45 [0/1994                 (0%)]\tLoss: 0.407026\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9262142335982331\n",
      "Training stage for Flod 4 Epoch: 46 [0/1994                 (0%)]\tLoss: 0.375802\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9260367573110371\n",
      "Training stage for Flod 4 Epoch: 47 [0/1994                 (0%)]\tLoss: 0.344537\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9259677387549052\n",
      "Training stage for Flod 4 Epoch: 48 [0/1994                 (0%)]\tLoss: 0.407021\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9261304253515016\n",
      "Training stage for Flod 4 Epoch: 49 [0/1994                 (0%)]\tLoss: 0.407022\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9262536727731656\n",
      "Training stage for Flod 4 Epoch: 50 [0/1994                 (0%)]\tLoss: 0.469517\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.926150144938968\n",
      "Training stage for Flod 4 Epoch: 51 [0/1994                 (0%)]\tLoss: 0.375853\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9259874583423715\n",
      "Training stage for Flod 4 Epoch: 52 [0/1994                 (0%)]\tLoss: 0.344542\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9264163593697621\n",
      "Training stage for Flod 4 Epoch: 53 [0/1994                 (0%)]\tLoss: 0.375776\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.926248742876299\n",
      "Training stage for Flod 4 Epoch: 54 [0/1994                 (0%)]\tLoss: 0.313286\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9258790006113072\n",
      "Training stage for Flod 4 Epoch: 55 [0/1994                 (0%)]\tLoss: 0.375768\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9254944686557157\n",
      "Training stage for Flod 4 Epoch: 56 [0/1994                 (0%)]\tLoss: 0.438271\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9250754274220583\n",
      "Training stage for Flod 4 Epoch: 57 [0/1994                 (0%)]\tLoss: 0.344634\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.924710615053933\n",
      "Training stage for Flod 4 Epoch: 58 [0/1994                 (0%)]\tLoss: 0.469594\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9250507779377255\n",
      "Training stage for Flod 4 Epoch: 59 [0/1994                 (0%)]\tLoss: 0.375768\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9258642109207076\n",
      "Training stage for Flod 4 Epoch: 60 [0/1994                 (0%)]\tLoss: 0.438267\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9260663366922364\n",
      "Training stage for Flod 4 Epoch: 61 [0/1994                 (0%)]\tLoss: 0.407019\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9257163140147109\n",
      "Training stage for Flod 4 Epoch: 62 [0/1994                 (0%)]\tLoss: 0.469524\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9260761964859696\n",
      "Training stage for Flod 4 Epoch: 63 [0/1994                 (0%)]\tLoss: 0.344541\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.925593066593047\n",
      "Training stage for Flod 4 Epoch: 64 [0/1994                 (0%)]\tLoss: 0.469817\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9261008459703022\n",
      "Training stage for Flod 4 Epoch: 65 [0/1994                 (0%)]\tLoss: 0.375772\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9262783222574983\n",
      "Training stage for Flod 4 Epoch: 66 [0/1994                 (0%)]\tLoss: 0.344602\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9264064995760288\n",
      "Training stage for Flod 4 Epoch: 67 [0/1994                 (0%)]\tLoss: 0.344530\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9261747944233005\n",
      "Training stage for Flod 4 Epoch: 68 [0/1994                 (0%)]\tLoss: 0.500758\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9260515470016366\n",
      "Training stage for Flod 4 Epoch: 69 [0/1994                 (0%)]\tLoss: 0.407022\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9261353552483681\n",
      "Training stage for Flod 4 Epoch: 70 [0/1994                 (0%)]\tLoss: 0.407017\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9251592356687897\n",
      "Training stage for Flod 4 Epoch: 71 [0/1994                 (0%)]\tLoss: 0.500799\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9263867799885627\n",
      "Training stage for Flod 4 Epoch: 72 [0/1994                 (0%)]\tLoss: 0.344523\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9255289779337816\n",
      "Training stage for Flod 4 Epoch: 73 [0/1994                 (0%)]\tLoss: 0.344562\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9251986748437222\n",
      "Training stage for Flod 4 Epoch: 74 [0/1994                 (0%)]\tLoss: 0.313276\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9265346768945593\n",
      "Training stage for Flod 4 Epoch: 75 [0/1994                 (0%)]\tLoss: 0.438269\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9252036047405888\n",
      "Training stage for Flod 4 Epoch: 76 [0/1994                 (0%)]\tLoss: 0.407017\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9247401944351323\n",
      "Training stage for Flod 4 Epoch: 77 [0/1994                 (0%)]\tLoss: 0.407188\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9261156356609019\n",
      "Training stage for Flod 4 Epoch: 78 [0/1994                 (0%)]\tLoss: 0.438262\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9257853325708425\n",
      "Training stage for Flod 4 Epoch: 79 [0/1994                 (0%)]\tLoss: 0.438312\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9261402851452348\n",
      "Training stage for Flod 4 Epoch: 80 [0/1994                 (0%)]\tLoss: 0.344526\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9257360336021769\n",
      "Training stage for Flod 4 Epoch: 81 [0/1994                 (0%)]\tLoss: 0.344527\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9254155903058507\n",
      "Training stage for Flod 4 Epoch: 82 [0/1994                 (0%)]\tLoss: 0.407013\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.925267693399854\n",
      "Training stage for Flod 4 Epoch: 83 [0/1994                 (0%)]\tLoss: 0.407079\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9254155903058507\n",
      "Training stage for Flod 4 Epoch: 84 [0/1994                 (0%)]\tLoss: 0.375770\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.925524048036915\n",
      "Training stage for Flod 4 Epoch: 85 [0/1994                 (0%)]\tLoss: 0.344521\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9252775531935872\n",
      "Training stage for Flod 4 Epoch: 86 [0/1994                 (0%)]\tLoss: 0.407075\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9259036500956401\n",
      "Training stage for Flod 4 Epoch: 87 [0/1994                 (0%)]\tLoss: 0.438269\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9260663366922365\n",
      "Training stage for Flod 4 Epoch: 88 [0/1994                 (0%)]\tLoss: 0.375830\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9265297469976927\n",
      "Training stage for Flod 4 Epoch: 89 [0/1994                 (0%)]\tLoss: 0.438259\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9258050521583088\n",
      "Training stage for Flod 4 Epoch: 90 [0/1994                 (0%)]\tLoss: 0.407029\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9252578336061208\n",
      "Training stage for Flod 4 Epoch: 91 [0/1994                 (0%)]\tLoss: 0.407019\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9258740707144406\n",
      "Training stage for Flod 4 Epoch: 92 [0/1994                 (0%)]\tLoss: 0.375766\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9253564315434522\n",
      "Training stage for Flod 4 Epoch: 93 [0/1994                 (0%)]\tLoss: 0.438266\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9252726232967206\n",
      "Training stage for Flod 4 Epoch: 94 [0/1994                 (0%)]\tLoss: 0.407017\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9252381140186547\n",
      "Training stage for Flod 4 Epoch: 95 [0/1994                 (0%)]\tLoss: 0.469643\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9251888150499891\n",
      "Training stage for Flod 4 Epoch: 96 [0/1994                 (0%)]\tLoss: 0.438260\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9251690954625229\n",
      "Training stage for Flod 4 Epoch: 97 [0/1994                 (0%)]\tLoss: 0.469517\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9247155449507996\n",
      "Training stage for Flod 4 Epoch: 98 [0/1994                 (0%)]\tLoss: 0.344522\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9253022026779198\n",
      "Training stage for Flod 4 Epoch: 99 [0/1994                 (0%)]\tLoss: 0.469505\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9256423655617124\n",
      "Training stage for Flod 4 Epoch: 100 [0/1994                 (0%)]\tLoss: 0.407017\n",
      "Test set for fold4: Average Loss:           0.0125, Accuracy: 431/498           (87%)\n",
      "The auc of this fold is 0.9251099367001241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8d93caaa30>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "# ax[2].set_title(\"Test ROC\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X_features, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    # Randomly select the train and test samples to generate train and test datasets\n",
    "    # train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    # test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "    test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "    \n",
    "    # weighted random sampler for train datasets\n",
    "    y_train = [TCRData.y[i] for i in train_idx]\n",
    "    class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "    weights = 1. / class_sample_count\n",
    "    samples_weight = torch.from_numpy(np.array([weights[t] for t in y_train]))\n",
    "    train_sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "    \n",
    "    # use all of the testing samples for testing\n",
    "    # y_test = [TCRData.y[i] for i in test_idx]\n",
    "    # class_sample_count = np.array([len(np.where(y_test == t)[0]) for t in np.unique(y_test)])\n",
    "    # weights = 1. / class_sample_count\n",
    "    # samples_weight = torch.from_numpy(np.array([weights[t] for t in y_test]))\n",
    "    # test_sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "    pred_history = []\n",
    "    target_history = []\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_dataloader, optimizer, epoch, criterion)\n",
    "        test_losses, test_correct, pred_history, target_history = test(fold, model, device, test_dataloader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "\n",
    "    ax[0].plot(train_losses_history, label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, label=f\"test accuracy fold{fold}\")\n",
    "    # break\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = [i.cpu().detach().numpy() for i in pred_history]\n",
    "# target = [i.cpu().detach().numpy() for i in target_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((32, 92)).to(device)\n",
    "# # a.shape\n",
    "d = model(c).to(device)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "# loss = nn.CrossEntropyLoss()(model(a).to(torch.float32).view(32, 1, 1), torch.ones((32, 1, 1)).to(torch.float32).to(device)) / 32\n",
    "# loss.to(device)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "a = torch.randint(0,2,(32,1)) # mask\n",
    "b = torch.zeros((32,2))\n",
    "# get the value of b where a is 1\n",
    "a = a.to(torch.bool)\n",
    "b[(a==1).squeeze(),1] = 1\n",
    "b[(a==0).squeeze(),0] = 1\n",
    "# loss = loss_fn(d, torch.ones((32,2)).to(device))\n",
    "# loss.backward()\n",
    "# train_losses_history\n",
    "# test_losses_history\n",
    "# test_accuracy_history\n",
    "# train_accuracy_history\n",
    "# nn.Softmax(dim=1)(torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float32))\n",
    "# output = nn.Softmax(dim=1)(torch.tensor([[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]], dtype=torch.float32))\n",
    "# nn.CrossEntropyLoss()(output, torch.tensor([[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]))\n",
    "# (b == torch.tensor([1,0])).sum() / (b == torch.tensor([0,1])).sum()\n",
    "# b\n",
    "# 17 / 15\n",
    "\n",
    "# a = torch.tensor([[1,2,3,4,5],[3,3,4,5,6]], dtype=torch.float32) # print(a.shape) shape: 2, 5\n",
    "# a.argmax(dim=1, keepdim=True).shape # shape: 2, 1\n",
    "# a.argmax(dim=1) # shape: 1, 2\n",
    "\n",
    "TCRData.y.sum() / len(TCRData.y)\n",
    "# loss.backward()\n",
    "# torch.zeros((32,2))\n",
    "# model(torch.randn((32,92)).to(device))\n",
    "num_dict = {1:2, 2:3, 3:0}\n",
    "# delete the non-zero key\n",
    "num_dict.pop(1)\n",
    "num_dict.pop(2)\n",
    "# print(num_dict.keys())\n",
    "# get the key\n",
    "print(list(num_dict.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925879447590026\n"
     ]
    }
   ],
   "source": [
    "# test_data = test_loader[0]\n",
    "# test_dataloader.dataset[0]\n",
    "pred_history = []\n",
    "target_history = []\n",
    "for data, target in test_dataloader:\n",
    "    model.eval()\n",
    "    data = data.view(-1, 52).to(torch.float32).data.to(device)\n",
    "    # print(data.shape)\n",
    "    one_hot_target = torch.zeros((len(target), 2)).to(device)\n",
    "    one_hot_target[(target==1).squeeze(),1] = 1\n",
    "    one_hot_target[(target==0).squeeze(),0] = 1\n",
    "    pred = model(data)\n",
    "    pred_history.extend(pred.cpu().detach().numpy()) # batch_size, 2\n",
    "    target_history.extend(one_hot_target.cpu().detach().numpy()) # batch_size, 2\n",
    "\n",
    "auc = roc_auc_score(target_history, pred_history)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0052, 0.0052, 0.0052,  ..., 0.0001, 0.0001, 0.0001],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fpr, tpr, _ = roc_curve(one_hot_target.argmax(dim=1, keepdim=True).cpu().numpy(), pred.cpu().detach().numpy())\n",
    "# auc = roc_auc_score(one_hot_target.argmax(dim=1, keepdim=True).cpu().numpy(), pred.cpu().detach().numpy())\n",
    "\n",
    "# # test_loss /= len(test_loader.dataset)\n",
    "# plt.plot(fpr, tpr, lable=f\"Fold {fold} AUC: {str(auc)}\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()\n",
    "# len(test_dataloader.dataset) / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = test_loader[0]\n",
    "# test_dataloader.dataset[0]\n",
    "pred_history = []\n",
    "target_history = []\n",
    "for data, target in test_dataloader:\n",
    "    data = data.view(-1, 52).to(torch.float32).data.to(device)\n",
    "    # print(data.shape)\n",
    "    one_hot_target = torch.zeros((len(target), 2)).to(device)\n",
    "    one_hot_target[(target==1).squeeze(),1] = 1\n",
    "    one_hot_target[(target==0).squeeze(),0] = 1\n",
    "    pred = model(data)\n",
    "    pred_history.extend(pred.cpu().detach().numpy()) # batch_size, 2\n",
    "    target_history.extend(one_hot_target.cpu().detach().numpy()) # batch_size, 2\n",
    "\n",
    "auc = roc_auc_score(target_history, pred_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3119765947034173"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc\n",
    "# target_history[-10:], pred_history[-10:]\n",
    "# np.argmax(pred_history, axis=1).sum() / len(pred_history)\n",
    "# test_dataloader.dataset[:][1].sum() / len(test_dataloader.dataset[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 52])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m20\u001b[39m\u001b[39m+\u001b[39m\u001b[39m5\u001b[39m\u001b[39m*\u001b[39m\u001b[39m6\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m pred \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mbool)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb Cell 23\u001b[0m in \u001b[0;36mprediction_model.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(f\"The model input shape is : {input.shape}\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_layer(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "# for data, target in test_dataloader:\n",
    "#     print(target.shape)\n",
    "\n",
    "# calculate the auc score of the test model\n",
    "# for data, target in test_dataloader:\n",
    "#     data = data.view(-1, 20+5*6+2).to(device)\n",
    "#     pred = model(data.to(device))\n",
    "    \n",
    "#     target = target.to(torch.float32).view(-1, 1)\n",
    "#     target = target.to(torch.bool)\n",
    "#     one_hot_target = torch.zeros((len(target), 2))\n",
    "#     one_hot_target[(target==1).squeeze(),1] = 1\n",
    "#     one_hot_target[(target==0).squeeze(),0] = 1\n",
    "#     one_hot_target = one_hot_target.to(device)\n",
    "#     fpr, tpr, _ = roc_curve(one_hot_target.argmax(dim=1, keepdim=True).cpu().numpy(), pred.cpu().detach().numpy())\n",
    "#     auc = roc_auc_score(one_hot_target.argmax(dim=1, keepdim=True).cpu().numpy(), pred.cpu().detach().numpy())\n",
    "#     plt.plot(fpr, tpr, label=f\"Fold {fold} AUC: {str(auc)}\")\n",
    "#     plt.ylabel(\"True Positive Rate\")\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.legend(loc=4)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction_model(\n",
       "  (linear_layer): Sequential(\n",
       "    (0): Linear(in_features=52, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=8, out_features=2, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_hot_target.argmax(dim=1, keepdim=True).cpu().numpy().shape\n",
    "# output.cpu().numpy()[:, 1]\n",
    "# pred.shape\n",
    "# auc\n",
    "# plt.plot(fpr, tpr)\n",
    "# fpr.shape\n",
    "# tpr.shape\n",
    "# len(test_dataset)\n",
    "# one_hot_target = OneHotEncoder().fit_transform(np.array(TCRData.y).reshape(-1,1)).toarray()\n",
    "# pred = nn.Softmax(dim=1)(model(torch.tensor(test_dataset.X_features, dtype=torch.float32).to(device)))\n",
    "\n",
    "# auc = roc_auc_score(one_hot_target.argmax(dim=1, keepdim=True).cpu().numpy(), pred.cpu().detach().numpy())\n",
    "# fpr, tpr, _ = roc_curve(one_hot_target.argmax(dim=1, keepdim=True).cpu().numpy(), pred.cpu().detach().numpy())\n",
    "# plt.plot(fpr, tpr, label=f\"Fold {fold} AUC: {str(auc)}\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, the features are concatanated and used to predict the binding affinity of pMHC-TCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_pred(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        super(pMHC_TCR_pred, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        # use the encoded features to predict the binding affinity through MLP\n",
    "        self.Linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_encode(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        self.input_size = input_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.seq_length, self.input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :] # return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 82, 'BseqCDR3': 24}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "print(len_map)\n",
    "# drop the rows with length == max length, which is much longer than the others\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < len_map[\"AseqCDR3\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.200e+02, 2.288e+03, 2.800e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.100e+01]),\n",
       " array([ 5. , 12.6, 20.2, 27.8, 35.4, 43. , 50.6, 58.2, 65.8, 73.4, 81. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df[\"AseqCDR3\"].value_counts()\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0) # find the longest seq\n",
    "# df.loc[df[\"AseqCDR3\"].str.len() == 83, \"AseqCDR3\"]\n",
    "\n",
    "plt.hist(df[\"AseqCDR3\"].str.len().sort_values(axis=0))\n",
    "# plt.show()\n",
    "# df = df.loc[df[\"AseqCDR3\"].str.len() < 83, :]\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_map\n",
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/20230228.csv\")\n",
    "# df.loc[df[\"AseqCDR3\"].str.contains(\"_\"),]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
