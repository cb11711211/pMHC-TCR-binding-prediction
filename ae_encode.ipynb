{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seqCDR[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_encode_data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # drop the rows with duplicate CDR3 sequences\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        \n",
    "        # drop the rows with length == max length, which is much longer than the others\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # If there is any NaN value, drop the row\n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        # concatenate the encoded features\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "        \n",
    "        # discard the duplicate rows, keep the first one\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 49)\n",
    "            nn.Conv1d(5, 10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 25) based on the formula for conv1d: (W + 2P - K)/S + 1 = (49 + 2*1 - 3)/2 + 1 = 25\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 10, 23), 25 - 2 = 23 \n",
    "\n",
    "            nn.Conv1d(10, 15, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12) based on the formula for conv1d: (W + 2P - K)/S + 1 = (23 + 2*1 - 3)/2 + 1 = 12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 10), 12 - 2 = 10\n",
    "\n",
    "            nn.Conv1d(15, 20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 5) based on the formula for conv1d: (W + 2P - K)/S + 1 = (10 + 2*1 - 3)/2 + 1 = 5\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 3)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 3)\n",
    "            nn.ConvTranspose1d(20, 15, kernel_size=3, stride=3, padding=1),\n",
    "            # (batch_size, 15, 5), based on the formula for convtranspose1d: (W−1)S−2P+F = (3-1)*3-2*1+3= 7\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(15, 10, kernel_size=7, stride=3, padding=1),\n",
    "            # (batch_size, 10, 23) based on the formula for convtranspose1d: (W−1)S−2P+F = (7-1)*3-2*1+7= 23\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(10, 5, kernel_size=7, stride=2, padding=1),\n",
    "            # (batch_size, 5, 49) based on the formula for convtranspose1d: (W−1)S−2P+F = (23-1)*2-2*1+7= 49\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = torch.tensor(x, dtype=np.float32)\n",
    "        # x = torch.tensor(x, dtype=torch.float)\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        # print(f\"encoding shape: {encoded.shape}\")\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2492 (0%)]\tLoss: 1.477677\n",
      "Train Epoch: 1 [1600/2492 (65%)]\tLoss: 1.350907\n",
      "Train Epoch: 2 [0/2492 (0%)]\tLoss: 1.342836\n",
      "Train Epoch: 2 [1600/2492 (65%)]\tLoss: 1.277666\n",
      "Train Epoch: 3 [0/2492 (0%)]\tLoss: 1.371520\n",
      "Train Epoch: 3 [1600/2492 (65%)]\tLoss: 1.228520\n",
      "Train Epoch: 4 [0/2492 (0%)]\tLoss: 1.058242\n",
      "Train Epoch: 4 [1600/2492 (65%)]\tLoss: 1.212461\n",
      "Train Epoch: 5 [0/2492 (0%)]\tLoss: 1.194871\n",
      "Train Epoch: 5 [1600/2492 (65%)]\tLoss: 1.141067\n",
      "Train Epoch: 6 [0/2492 (0%)]\tLoss: 1.076621\n",
      "Train Epoch: 6 [1600/2492 (65%)]\tLoss: 1.084999\n",
      "Train Epoch: 7 [0/2492 (0%)]\tLoss: 1.016304\n",
      "Train Epoch: 7 [1600/2492 (65%)]\tLoss: 1.020128\n",
      "Train Epoch: 8 [0/2492 (0%)]\tLoss: 1.165288\n",
      "Train Epoch: 8 [1600/2492 (65%)]\tLoss: 1.047874\n",
      "Train Epoch: 9 [0/2492 (0%)]\tLoss: 1.072751\n",
      "Train Epoch: 9 [1600/2492 (65%)]\tLoss: 1.124893\n",
      "Train Epoch: 10 [0/2492 (0%)]\tLoss: 1.083767\n",
      "Train Epoch: 10 [1600/2492 (65%)]\tLoss: 1.096058\n",
      "Train Epoch: 11 [0/2492 (0%)]\tLoss: 1.075512\n",
      "Train Epoch: 11 [1600/2492 (65%)]\tLoss: 1.050098\n",
      "Train Epoch: 12 [0/2492 (0%)]\tLoss: 0.946232\n",
      "Train Epoch: 12 [1600/2492 (65%)]\tLoss: 1.095696\n",
      "Train Epoch: 13 [0/2492 (0%)]\tLoss: 1.080045\n",
      "Train Epoch: 13 [1600/2492 (65%)]\tLoss: 1.068270\n",
      "Train Epoch: 14 [0/2492 (0%)]\tLoss: 1.028569\n",
      "Train Epoch: 14 [1600/2492 (65%)]\tLoss: 1.039325\n",
      "Train Epoch: 15 [0/2492 (0%)]\tLoss: 1.018275\n",
      "Train Epoch: 15 [1600/2492 (65%)]\tLoss: 1.037414\n",
      "Train Epoch: 16 [0/2492 (0%)]\tLoss: 0.888578\n",
      "Train Epoch: 16 [1600/2492 (65%)]\tLoss: 1.104778\n",
      "Train Epoch: 17 [0/2492 (0%)]\tLoss: 0.937551\n",
      "Train Epoch: 17 [1600/2492 (65%)]\tLoss: 0.987194\n",
      "Train Epoch: 18 [0/2492 (0%)]\tLoss: 1.017655\n",
      "Train Epoch: 18 [1600/2492 (65%)]\tLoss: 1.048730\n",
      "Train Epoch: 19 [0/2492 (0%)]\tLoss: 1.065848\n",
      "Train Epoch: 19 [1600/2492 (65%)]\tLoss: 0.961034\n",
      "Train Epoch: 20 [0/2492 (0%)]\tLoss: 0.953835\n",
      "Train Epoch: 20 [1600/2492 (65%)]\tLoss: 1.052648\n",
      "Train Epoch: 21 [0/2492 (0%)]\tLoss: 1.009027\n",
      "Train Epoch: 21 [1600/2492 (65%)]\tLoss: 1.071401\n",
      "Train Epoch: 22 [0/2492 (0%)]\tLoss: 1.004069\n",
      "Train Epoch: 22 [1600/2492 (65%)]\tLoss: 0.910799\n",
      "Train Epoch: 23 [0/2492 (0%)]\tLoss: 1.005258\n",
      "Train Epoch: 23 [1600/2492 (65%)]\tLoss: 0.964073\n",
      "Train Epoch: 24 [0/2492 (0%)]\tLoss: 1.115289\n",
      "Train Epoch: 24 [1600/2492 (65%)]\tLoss: 1.014602\n",
      "Train Epoch: 25 [0/2492 (0%)]\tLoss: 0.893420\n",
      "Train Epoch: 25 [1600/2492 (65%)]\tLoss: 0.930163\n",
      "Train Epoch: 26 [0/2492 (0%)]\tLoss: 0.987651\n",
      "Train Epoch: 26 [1600/2492 (65%)]\tLoss: 0.936745\n",
      "Train Epoch: 27 [0/2492 (0%)]\tLoss: 0.930551\n",
      "Train Epoch: 27 [1600/2492 (65%)]\tLoss: 0.936848\n",
      "Train Epoch: 28 [0/2492 (0%)]\tLoss: 0.989225\n",
      "Train Epoch: 28 [1600/2492 (65%)]\tLoss: 0.890543\n",
      "Train Epoch: 29 [0/2492 (0%)]\tLoss: 0.961237\n",
      "Train Epoch: 29 [1600/2492 (65%)]\tLoss: 0.915233\n",
      "Train Epoch: 30 [0/2492 (0%)]\tLoss: 0.985428\n",
      "Train Epoch: 30 [1600/2492 (65%)]\tLoss: 0.992015\n",
      "Train Epoch: 31 [0/2492 (0%)]\tLoss: 0.944742\n",
      "Train Epoch: 31 [1600/2492 (65%)]\tLoss: 0.913454\n",
      "Train Epoch: 32 [0/2492 (0%)]\tLoss: 1.043494\n",
      "Train Epoch: 32 [1600/2492 (65%)]\tLoss: 0.976972\n",
      "Train Epoch: 33 [0/2492 (0%)]\tLoss: 0.907710\n",
      "Train Epoch: 33 [1600/2492 (65%)]\tLoss: 0.939199\n",
      "Train Epoch: 34 [0/2492 (0%)]\tLoss: 0.947804\n",
      "Train Epoch: 34 [1600/2492 (65%)]\tLoss: 1.002420\n",
      "Train Epoch: 35 [0/2492 (0%)]\tLoss: 0.885300\n",
      "Train Epoch: 35 [1600/2492 (65%)]\tLoss: 1.026557\n",
      "Train Epoch: 36 [0/2492 (0%)]\tLoss: 1.097920\n",
      "Train Epoch: 36 [1600/2492 (65%)]\tLoss: 1.052989\n",
      "Train Epoch: 37 [0/2492 (0%)]\tLoss: 0.794875\n",
      "Train Epoch: 37 [1600/2492 (65%)]\tLoss: 0.992775\n",
      "Train Epoch: 38 [0/2492 (0%)]\tLoss: 0.861277\n",
      "Train Epoch: 38 [1600/2492 (65%)]\tLoss: 0.978820\n",
      "Train Epoch: 39 [0/2492 (0%)]\tLoss: 0.935404\n",
      "Train Epoch: 39 [1600/2492 (65%)]\tLoss: 0.915312\n",
      "Train Epoch: 40 [0/2492 (0%)]\tLoss: 0.883184\n",
      "Train Epoch: 40 [1600/2492 (65%)]\tLoss: 0.919640\n",
      "Train Epoch: 41 [0/2492 (0%)]\tLoss: 0.833755\n",
      "Train Epoch: 41 [1600/2492 (65%)]\tLoss: 0.899517\n",
      "Train Epoch: 42 [0/2492 (0%)]\tLoss: 0.949330\n",
      "Train Epoch: 42 [1600/2492 (65%)]\tLoss: 0.821147\n",
      "Train Epoch: 43 [0/2492 (0%)]\tLoss: 0.924431\n",
      "Train Epoch: 43 [1600/2492 (65%)]\tLoss: 1.072689\n",
      "Train Epoch: 44 [0/2492 (0%)]\tLoss: 0.890868\n",
      "Train Epoch: 44 [1600/2492 (65%)]\tLoss: 0.875608\n",
      "Train Epoch: 45 [0/2492 (0%)]\tLoss: 1.007851\n",
      "Train Epoch: 45 [1600/2492 (65%)]\tLoss: 0.921131\n",
      "Train Epoch: 46 [0/2492 (0%)]\tLoss: 0.932808\n",
      "Train Epoch: 46 [1600/2492 (65%)]\tLoss: 0.839623\n",
      "Train Epoch: 47 [0/2492 (0%)]\tLoss: 0.830653\n",
      "Train Epoch: 47 [1600/2492 (65%)]\tLoss: 0.923603\n",
      "Train Epoch: 48 [0/2492 (0%)]\tLoss: 0.819394\n",
      "Train Epoch: 48 [1600/2492 (65%)]\tLoss: 0.884722\n",
      "Train Epoch: 49 [0/2492 (0%)]\tLoss: 0.821128\n",
      "Train Epoch: 49 [1600/2492 (65%)]\tLoss: 0.876857\n",
      "Train Epoch: 50 [0/2492 (0%)]\tLoss: 0.963131\n",
      "Train Epoch: 50 [1600/2492 (65%)]\tLoss: 0.895836\n",
      "Train Epoch: 51 [0/2492 (0%)]\tLoss: 0.936330\n",
      "Train Epoch: 51 [1600/2492 (65%)]\tLoss: 0.961957\n",
      "Train Epoch: 52 [0/2492 (0%)]\tLoss: 0.839482\n",
      "Train Epoch: 52 [1600/2492 (65%)]\tLoss: 0.956170\n",
      "Train Epoch: 53 [0/2492 (0%)]\tLoss: 1.030073\n",
      "Train Epoch: 53 [1600/2492 (65%)]\tLoss: 0.933243\n",
      "Train Epoch: 54 [0/2492 (0%)]\tLoss: 0.898738\n",
      "Train Epoch: 54 [1600/2492 (65%)]\tLoss: 0.903788\n",
      "Train Epoch: 55 [0/2492 (0%)]\tLoss: 0.956329\n",
      "Train Epoch: 55 [1600/2492 (65%)]\tLoss: 0.923535\n",
      "Train Epoch: 56 [0/2492 (0%)]\tLoss: 0.784029\n",
      "Train Epoch: 56 [1600/2492 (65%)]\tLoss: 0.842347\n",
      "Train Epoch: 57 [0/2492 (0%)]\tLoss: 0.870510\n",
      "Train Epoch: 57 [1600/2492 (65%)]\tLoss: 0.907664\n",
      "Train Epoch: 58 [0/2492 (0%)]\tLoss: 0.820815\n",
      "Train Epoch: 58 [1600/2492 (65%)]\tLoss: 0.889791\n",
      "Train Epoch: 59 [0/2492 (0%)]\tLoss: 0.921926\n",
      "Train Epoch: 59 [1600/2492 (65%)]\tLoss: 0.901907\n",
      "Train Epoch: 60 [0/2492 (0%)]\tLoss: 0.906366\n",
      "Train Epoch: 60 [1600/2492 (65%)]\tLoss: 0.923260\n",
      "Train Epoch: 61 [0/2492 (0%)]\tLoss: 0.777985\n",
      "Train Epoch: 61 [1600/2492 (65%)]\tLoss: 0.825499\n",
      "Train Epoch: 62 [0/2492 (0%)]\tLoss: 0.913373\n",
      "Train Epoch: 62 [1600/2492 (65%)]\tLoss: 0.915789\n",
      "Train Epoch: 63 [0/2492 (0%)]\tLoss: 0.917901\n",
      "Train Epoch: 63 [1600/2492 (65%)]\tLoss: 0.912743\n",
      "Train Epoch: 64 [0/2492 (0%)]\tLoss: 0.864692\n",
      "Train Epoch: 64 [1600/2492 (65%)]\tLoss: 0.875059\n",
      "Train Epoch: 65 [0/2492 (0%)]\tLoss: 0.923347\n",
      "Train Epoch: 65 [1600/2492 (65%)]\tLoss: 0.895280\n",
      "Train Epoch: 66 [0/2492 (0%)]\tLoss: 0.865701\n",
      "Train Epoch: 66 [1600/2492 (65%)]\tLoss: 0.899536\n",
      "Train Epoch: 67 [0/2492 (0%)]\tLoss: 0.886468\n",
      "Train Epoch: 67 [1600/2492 (65%)]\tLoss: 0.933376\n",
      "Train Epoch: 68 [0/2492 (0%)]\tLoss: 0.871276\n",
      "Train Epoch: 68 [1600/2492 (65%)]\tLoss: 0.871159\n",
      "Train Epoch: 69 [0/2492 (0%)]\tLoss: 0.989501\n",
      "Train Epoch: 69 [1600/2492 (65%)]\tLoss: 0.919033\n",
      "Train Epoch: 70 [0/2492 (0%)]\tLoss: 0.830371\n",
      "Train Epoch: 70 [1600/2492 (65%)]\tLoss: 0.976397\n",
      "Train Epoch: 71 [0/2492 (0%)]\tLoss: 0.940511\n",
      "Train Epoch: 71 [1600/2492 (65%)]\tLoss: 0.788673\n",
      "Train Epoch: 72 [0/2492 (0%)]\tLoss: 0.829579\n",
      "Train Epoch: 72 [1600/2492 (65%)]\tLoss: 1.025248\n",
      "Train Epoch: 73 [0/2492 (0%)]\tLoss: 0.938975\n",
      "Train Epoch: 73 [1600/2492 (65%)]\tLoss: 0.891388\n",
      "Train Epoch: 74 [0/2492 (0%)]\tLoss: 0.912043\n",
      "Train Epoch: 74 [1600/2492 (65%)]\tLoss: 0.892188\n",
      "Train Epoch: 75 [0/2492 (0%)]\tLoss: 0.953280\n",
      "Train Epoch: 75 [1600/2492 (65%)]\tLoss: 0.869268\n",
      "Train Epoch: 76 [0/2492 (0%)]\tLoss: 0.896835\n",
      "Train Epoch: 76 [1600/2492 (65%)]\tLoss: 0.861433\n",
      "Train Epoch: 77 [0/2492 (0%)]\tLoss: 0.848759\n",
      "Train Epoch: 77 [1600/2492 (65%)]\tLoss: 0.895191\n",
      "Train Epoch: 78 [0/2492 (0%)]\tLoss: 0.794963\n",
      "Train Epoch: 78 [1600/2492 (65%)]\tLoss: 0.872615\n",
      "Train Epoch: 79 [0/2492 (0%)]\tLoss: 0.811540\n",
      "Train Epoch: 79 [1600/2492 (65%)]\tLoss: 0.955132\n",
      "Train Epoch: 80 [0/2492 (0%)]\tLoss: 0.852599\n",
      "Train Epoch: 80 [1600/2492 (65%)]\tLoss: 0.854397\n",
      "Train Epoch: 81 [0/2492 (0%)]\tLoss: 0.838292\n",
      "Train Epoch: 81 [1600/2492 (65%)]\tLoss: 0.839297\n",
      "Train Epoch: 82 [0/2492 (0%)]\tLoss: 0.866311\n",
      "Train Epoch: 82 [1600/2492 (65%)]\tLoss: 0.804607\n",
      "Train Epoch: 83 [0/2492 (0%)]\tLoss: 0.762789\n",
      "Train Epoch: 83 [1600/2492 (65%)]\tLoss: 0.926547\n",
      "Train Epoch: 84 [0/2492 (0%)]\tLoss: 0.875444\n",
      "Train Epoch: 84 [1600/2492 (65%)]\tLoss: 0.880989\n",
      "Train Epoch: 85 [0/2492 (0%)]\tLoss: 0.893529\n",
      "Train Epoch: 85 [1600/2492 (65%)]\tLoss: 0.856061\n",
      "Train Epoch: 86 [0/2492 (0%)]\tLoss: 0.882870\n",
      "Train Epoch: 86 [1600/2492 (65%)]\tLoss: 0.756512\n",
      "Train Epoch: 87 [0/2492 (0%)]\tLoss: 0.904015\n",
      "Train Epoch: 87 [1600/2492 (65%)]\tLoss: 0.913533\n",
      "Train Epoch: 88 [0/2492 (0%)]\tLoss: 0.883713\n",
      "Train Epoch: 88 [1600/2492 (65%)]\tLoss: 0.871366\n",
      "Train Epoch: 89 [0/2492 (0%)]\tLoss: 0.951826\n",
      "Train Epoch: 89 [1600/2492 (65%)]\tLoss: 0.715110\n",
      "Train Epoch: 90 [0/2492 (0%)]\tLoss: 0.905199\n",
      "Train Epoch: 90 [1600/2492 (65%)]\tLoss: 0.863014\n",
      "Train Epoch: 91 [0/2492 (0%)]\tLoss: 0.844062\n",
      "Train Epoch: 91 [1600/2492 (65%)]\tLoss: 0.981476\n",
      "Train Epoch: 92 [0/2492 (0%)]\tLoss: 0.852935\n",
      "Train Epoch: 92 [1600/2492 (65%)]\tLoss: 0.918994\n",
      "Train Epoch: 93 [0/2492 (0%)]\tLoss: 0.937616\n",
      "Train Epoch: 93 [1600/2492 (65%)]\tLoss: 0.813826\n",
      "Train Epoch: 94 [0/2492 (0%)]\tLoss: 0.929529\n",
      "Train Epoch: 94 [1600/2492 (65%)]\tLoss: 0.817700\n",
      "Train Epoch: 95 [0/2492 (0%)]\tLoss: 0.916849\n",
      "Train Epoch: 95 [1600/2492 (65%)]\tLoss: 0.825887\n",
      "Train Epoch: 96 [0/2492 (0%)]\tLoss: 0.860106\n",
      "Train Epoch: 96 [1600/2492 (65%)]\tLoss: 0.825246\n",
      "Train Epoch: 97 [0/2492 (0%)]\tLoss: 0.872491\n",
      "Train Epoch: 97 [1600/2492 (65%)]\tLoss: 0.850987\n",
      "Train Epoch: 98 [0/2492 (0%)]\tLoss: 0.803089\n",
      "Train Epoch: 98 [1600/2492 (65%)]\tLoss: 0.890350\n",
      "Train Epoch: 99 [0/2492 (0%)]\tLoss: 0.910899\n",
      "Train Epoch: 99 [1600/2492 (65%)]\tLoss: 0.790722\n",
      "Train Epoch: 100 [0/2492 (0%)]\tLoss: 0.913803\n",
      "Train Epoch: 100 [1600/2492 (65%)]\tLoss: 0.925122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1e21245880>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training the autoencoder to encode the TCR sequence\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        # TCR_encode_losses.append(loss.item() / model.batch_size)\n",
    "        # TCR_encode_losses.append(loss.item())\n",
    "        # sum up batch loss\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.set_title(\"TCR encode loss\")\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# load the model\n",
    "# model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "# model.load_state_dict(torch.load(\"/DATA/User/wuxinchao/project/pMHC-TCR/ckpt/TCR_autoencoder.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 3))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    TCR_seq = TCR_seq.view(1, 5, 49).float()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test, not used\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "kernel_size, stride, padding, seq_length\n",
    "# pool of size=3, stride=2\n",
    "# m = nn.MaxPool1d(3, stride=1)\n",
    "# m = nn.Conv1d(16, 33, 3, stride=2, padding=1)\n",
    "m = nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=1)\n",
    "# m = nn.MaxUnpool1d(kernel_size=3, stride=1)\n",
    "input = torch.randn(20, 16, 3)\n",
    "output = m(input)\n",
    "output.shape\n",
    "# TCRData[0][0].shape\n",
    "len(TCRData)\n",
    "# model(TCRData[0:3][0].float().view(3,5,seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TCR_encode(nn.Module):\n",
    "    '''\n",
    "    LSTM for TCR sequence encoding.\n",
    "    The input size of LSTM is (batch_size, seq_length, input_size), the output size is (batch_size, seq_length, hidden_size)\n",
    "    '''\n",
    "    def __init__(self, seq_length, hidden_size, num_layers, device):\n",
    "        super(LSTM_TCR_encode, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(seq_length, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(self.device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model for TCR sequence encoding, this may not be used in the future\n",
    "# How to use the LSTM model to encode the TCR sequence\n",
    "# The optimization \n",
    "def train_LSTM_TCR_encode(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, seq_length, 5)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training: {batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%) \\\n",
    "                  Loss: {loss.item():.6f}\")\n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the LSTM model\n",
    "model = LSTM_TCR_encode(seq_length, hidden_size, num_layers, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset for the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    Here the input is the TCR sequence, neoantigen sequence, and HLA type.\n",
    "    The output should be the encoded features of TCR sequence, and the Atchley factor of neoantigen sequence, and the HLA one-hot encoding.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 file_path, \n",
    "                 only_CDR3: bool = False, \n",
    "                 only_experimental: bool = False, \n",
    "                 TCR_encode: str = [\"LSTM\", \"CNN\"],\n",
    "                 encoding_model: nn.Module = None) -> None:\n",
    "        df, HLA_encode, y  = self.basic_io(file_path, only_experimental=only_experimental)\n",
    "\n",
    "        # convert from object to tensor\n",
    "        X_TCR_seq = torch.zeros((len(df), 0))\n",
    "        for region in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            TCR_seq = df.loc[:, region].values\n",
    "            TCR_seq_encode = torch.zeros((0, TCR_seq[0].shape[1]))\n",
    "            for i in range(len(TCR_seq)):\n",
    "                encoding = torch.from_numpy(TCR_seq[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                TCR_seq_encode = torch.cat((TCR_seq_encode, encoding), dim=0)\n",
    "\n",
    "            X_TCR_seq = torch.cat((TCR_seq_encode, X_TCR_seq), dim=1)\n",
    "        \n",
    "        if TCR_encode == \"CNN\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        elif TCR_encode == \"LSTM\":\n",
    "            X_TCR_seq = X_TCR_seq.view(-1, 5, 49)\n",
    "        else:\n",
    "            raise ValueError(\"The TCR encoding method is not supported yet.\")\n",
    "        \n",
    "        # encoding model \n",
    "        X_features, _ = encoding_model(X_TCR_seq)\n",
    "        X_features = X_features.view(-1, 20 * 3)\n",
    "\n",
    "        # add the neoantigen sequence encoding features\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            neo = df.loc[:, seq].values\n",
    "            neo_encode = torch.zeros((0, neo[0].shape[1]))\n",
    "            for i in range(len(neo)):\n",
    "                encoding = torch.from_numpy(neo[i][0])\n",
    "                encoding = encoding.reshape(1, -1)\n",
    "                neo_encode = torch.cat((neo_encode, encoding), dim=0)\n",
    "            X_features = torch.cat((X_features, neo_encode), dim=1)\n",
    "\n",
    "        X_features = torch.cat((X_features, torch.from_numpy(HLA_encode)), dim=1)\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "            \n",
    "    \n",
    "    def basic_io(self, file_path, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # drop the rows with nan\n",
    "        df = df.dropna()\n",
    "\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        return df, X_HLA_encoded, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, TCR_encode=\"CNN\", only_experimental=True, encoding_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([92])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCRData[0][0].shape                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "    length = len_map[chain]\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# drop the rows with nan\n",
    "df = df.dropna()\n",
    "\n",
    "X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "HLAencoder = OneHotEncoder()\n",
    "X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "\n",
    "y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 batch_size=32,) -> None:\n",
    "        super(prediction_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(self.batch_size, self.input_size)\n",
    "        # print(f\"The model input shape is : {input.shape}\")\n",
    "        output = self.linear_layer(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # one-hot encoding the target\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        target = target.to(torch.bool)\n",
    "        one_hot_target = torch.zeros((target.shape[0], 2))\n",
    "        one_hot_target[(target==1).squeeze(), 1] = 1\n",
    "        one_hot_target[(target==0).squeeze(), 0] = 1\n",
    "\n",
    "        data, one_hot_target = data.to(device), one_hot_target.to(device)\n",
    "        # print(output, one_hot_target)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, one_hot_target.data)\n",
    "        print(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.sigmoid().round()\n",
    "        correct += pred.eq(target.view_as(pred)).sum()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Training stage for Flod {fold} Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \\\n",
    "                ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "    return train_loss, correct\n",
    "\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).to(torch.float32)\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output.reshape(1,-1), target.reshape(1,-1)).item()  # sum up loss\n",
    "            # print(test_loss)\n",
    "            pred = output.sigmoid().round()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n",
      "-------------------Fold 0-------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb 单元格 17\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m test_accuracy_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     train_losses, train_correct \u001b[39m=\u001b[39m train(fold, model, device, train_loader, optimizer, epoch, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     test_losses, test_correct \u001b[39m=\u001b[39m test(fold, model, device, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     train_losses_history\u001b[39m.\u001b[39mappend(train_losses)\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb 单元格 17\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fold, model, device, train_loader, optimizer, epoch, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m data, one_hot_target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), one_hot_target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(output, one_hot_target)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, one_hot_target\u001b[39m.\u001b[39mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb 单元格 17\u001b[0m in \u001b[0;36mprediction_model.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# print(f\"The model input shape is : {input.shape}\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_layer(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training \n",
    "batch_size = 32\n",
    "seq_length = 6\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = prediction_model(input_size=92, batch_size=batch_size).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "weights = torch.FloatTensor([1,10])\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X_features, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch, criterion)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    # ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    # ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    # ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    # ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "    ax[0].plot(train_losses_history, label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, label=f\"test accuracy fold{fold}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# put the legend out of the figure, and adjust the position, prevent the figure from being covered\n",
    "# ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# save the figure\n",
    "# fig.savefig(\"/DATA/User/wuxinchao/project/pMHC-TCR/result/pMHC_without_em_with_encoder_loss_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((32, 92)).to(device)\n",
    "# # a.shape\n",
    "d = model(c).to(device)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "# loss = nn.CrossEntropyLoss()(model(a).to(torch.float32).view(32, 1, 1), torch.ones((32, 1, 1)).to(torch.float32).to(device)) / 32\n",
    "# loss.to(device)\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "a = torch.randint(0,2,(32,1)) # mask\n",
    "b = torch.zeros((32,2))\n",
    "# get the value of b where a is 1\n",
    "a = a.to(torch.bool)\n",
    "b[(a==1).squeeze(),1] = 1\n",
    "b[(a==0).squeeze(),0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(d, torch.ones((32,2)).to(device))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, the features are concatanated and used to predict the binding affinity of pMHC-TCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_pred(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        super(pMHC_TCR_pred, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        # use the encoded features to predict the binding affinity through MLP\n",
    "        self.Linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_encode(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        self.input_size = input_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.seq_length, self.input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :] # return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 82, 'BseqCDR3': 24}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "print(len_map)\n",
    "# drop the rows with length == max length, which is much longer than the others\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < len_map[\"AseqCDR3\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.200e+02, 2.288e+03, 2.800e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.100e+01]),\n",
       " array([ 5. , 12.6, 20.2, 27.8, 35.4, 43. , 50.6, 58.2, 65.8, 73.4, 81. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df[\"AseqCDR3\"].value_counts()\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0) # find the longest seq\n",
    "# df.loc[df[\"AseqCDR3\"].str.len() == 83, \"AseqCDR3\"]\n",
    "\n",
    "plt.hist(df[\"AseqCDR3\"].str.len().sort_values(axis=0))\n",
    "# plt.show()\n",
    "# df = df.loc[df[\"AseqCDR3\"].str.len() < 83, :]\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_map\n",
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/20230228.csv\")\n",
    "# df.loc[df[\"AseqCDR3\"].str.contains(\"_\"),]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
