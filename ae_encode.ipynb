{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seqCDR[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_encode_data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        \n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # concatenate the encoded features\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "        \n",
    "        # discard the duplicate rows, keep the first one\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(df[\"Class\"].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 47)\n",
    "            nn.Conv1d(in_channels=5, out_channels=10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 24), based on the formula for conv1d: (W−F+2P)/S+1 = (47-3+2*1)/2+1 = 24\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            # (batch_size, 10, 12)\n",
    "            nn.Conv1d(in_channels=10, out_channels=20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 6), based on the formula for conv1d: (W−F+2P)/S+1 = (12-3+2*1)/2+1 = 6\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=1),\n",
    "            # (batch_size, 20, 6)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 6)\n",
    "            nn.ConvTranspose1d(in_channels=20, out_channels=10, kernel_size=3, stride=3, padding=1),\n",
    "            # (batch_size, 10, 16), based on the formula for convtranspose1d: (W−1)S−2P+F = (6-1)*3-2*1+3 = 16 \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=10, out_channels=5, kernel_size=4, stride=3, padding=1),\n",
    "            # (batch_size, 5, 47), based on the formula for convtranspose1d: (W−1)S−2P+F = (16-1)*3-2*1+4 = 47\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        # print(x.type())\n",
    "        encoded = self.encoder(x)\n",
    "        output = self.decoder(encoded)\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1874379/3053764840.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 2\n",
    "\n",
    "# load the model\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "# model.load_state_dict(torch.load(\"/DATA/User/wuxinchao/project/pMHC-TCR/ckpt/TCR_autoencoder.pt\"))\n",
    "# model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 6))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    TCR_seq = TCR_seq.view(1, 5, 47).double()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset class for pMHC-TCR dataset. \n",
    "    The csv file will be read and the TCR sequence will be encoded by the autoencoder or LSTM, as well as the sequence of neoantigen.\n",
    "    While the HLA type will be one-hot encoded. After that, the encoded sequnce and HLA type will be concatenated as the input of the\n",
    "    neural network.\n",
    "    '''\n",
    "    def __init__(self, file_path, only_CDR3=False, only_experimental=True, TCR_encode=None):\n",
    "        df, HLA_encode, y  = self.basic_io(file_path, only_CDR3, only_experimental=only_experimental)\n",
    "        \n",
    "        if only_CDR3 and TCR_encode is not None:\n",
    "            X_features = torch.cat((torch.from_numpy(HLA_encode), TCR_encode.view(1,-1)), dim=1)\n",
    "        elif only_CDR3:\n",
    "            X_features = torch.from_numpy(HLA_encode)\n",
    "            for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR3\", \"BseqCDR3\"]:\n",
    "                X_features = torch.cat((X_features, torch.from_numpy(df[seq].values)), dim=1)\n",
    "        else:\n",
    "            X_features = torch.from_numpy(HLA_encode)\n",
    "            for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_1\", \"AseqCDR_2\", \"AseqCDR_3\", \"BseqCDR_1\", \"BseqCDR_2\", \"BseqCDR_3\"]:\n",
    "                X_features = torch.cat((X_features, torch.from_numpy(df[seq].values)), dim=1)\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def basic_io(self, file_path, only_CDR3=True, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        return df, X_HLA_encoded, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n"
     ]
    }
   ],
   "source": [
    "# TCR_encode = TCR_encode.view(-1, 20*6)\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "# for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "#     if only_CDR3:\n",
    "#         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "#         df.drop(columns=[chain], inplace=True)\n",
    "#     else:\n",
    "#         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "#         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "#         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "#         df.drop(columns=[chain], inplace=True)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "    length = len_map[chain]\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, only_TCR_seq=True, only_experimental=True, TCR_encode=TCR_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size=16, \n",
    "                 batch_size=32, \n",
    "                 num_layers=2, \n",
    "                 device=\"cpu\", \n",
    "                 use_whole_data=False) -> None:\n",
    "        super(pMHC_TCR_model, self).__init__()\n",
    "        if use_whole_data:\n",
    "            self.batch_size = 0\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        # self.label = nn.Linear(hidden_size, 1)\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, int(hidden_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden_size/2), int(hidden_size/4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden_size/4), int(hidden_size/8)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden_size/8), 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.batch_size == 0:\n",
    "            self.batch_size = input.shape[0]\n",
    "            x = input.float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "            # pred = self.label(output[-1])\n",
    "            pred = self.linear_layer(output[-1])\n",
    "        else:\n",
    "            x = input.view(-1, self.batch_size, self.input_size).float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "            # pred = self.label(output[-1])\n",
    "            pred = self.linear_layer(output[-1])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, target.shape)\n",
    "        output = output.to(torch.float32)\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        # print(output.shape, target.shape)\n",
    "        loss = nn.CrossEntropyLoss()(output.view(1,-1), target.view(1,-1))\n",
    "        train_loss += loss.item() / len(train_loader.dataset)  # sum up batch loss\n",
    "        pred = output.sigmoid().round()\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Fold/Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                fold, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(train_loader.dataset)))\n",
    "    # return the average loss\n",
    "    # print(f\"The batch size is {model.batch_size}\")\n",
    "    return train_loss, correct / len(train_loader.dataset)\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).to(torch.float32)\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output.reshape(1,-1), target.reshape(1,-1)).item()  # sum up loss\n",
    "            # print(test_loss)\n",
    "            pred = output.sigmoid().round()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n",
      "-------------------Fold 0-------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb 单元格 10\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m test_accuracy_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     train_losses, train_correct \u001b[39m=\u001b[39m train(fold, model, device, train_loader, optimizer, epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     test_losses, test_correct \u001b[39m=\u001b[39m test(fold, model, device, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     train_losses_history\u001b[39m.\u001b[39mappend(train_losses)\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb 单元格 10\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fold, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msigmoid()\u001b[39m.\u001b[39mround()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39meq(target\u001b[39m.\u001b[39mview_as(pred))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training \n",
    "batch_size = 32\n",
    "seq_length = 6\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = pMHC_TCR_model(input_size=122, hidden_size=16, batch_size=batch_size, num_layers=2, device=device, use_whole_data=False).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "weights = torch.FloatTensor([5,6])\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "    \n",
    "    # print(f\"The length of train_loader is {len(train_loader)}\") # 34\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\") # 8\n",
    "    # print(f\"The length of train_dataset is {len(train_loader.dataset)}\")\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# put the legend out of the figure, and adjust the position, prevent the figure from being covered\n",
    "# ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# save the figure\n",
    "fig.savefig(\"/DATA/User/wuxinchao/project/pMHC-TCR/result/pMHC_without_em_with_encoder_loss_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, the features are concatanated and used to predict the binding affinity of pMHC-TCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_pred(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        super(pMHC_TCR_pred, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        # use the encoded features to predict the binding affinity through MLP\n",
    "        self.Linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_encode(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        self.input_size = input_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.seq_length, self.input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :] # return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/home/wuxinchao/data/project/data/seqData/20230228.csv\"\n",
    "# TCRData = pMHC_TCRDataset(file_path, only_CDR3=True, only_experimental=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCRData[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"AseqCDR3\"].value_counts()\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0) # find the longest seq\n",
    "# df.loc[df[\"AseqCDR3\"].str.len() == 83, \"AseqCDR3\"]\n",
    "\n",
    "# plt.hist(df[\"AseqCDR3\"].str.len().sort_values(axis=0))\n",
    "# plt.show()\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < 83, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_map\n",
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/20230228.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>HLA</th>\n",
       "      <th>AseqCDR3</th>\n",
       "      <th>BseqCDR3</th>\n",
       "      <th>Neo_first3</th>\n",
       "      <th>Neo_last3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>positive</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>CAASIKAL_NTGKLIF******************************...</td>\n",
       "      <td>CAWTKAGGGETQYF</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -1.01...</td>\n",
       "      <td>[[-0.228, 1.399, -4.76, 0.67, -2.647, 0.26, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>positive</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>CAASWD*CW_GTSYGKLTF***************************...</td>\n",
       "      <td>CASSITPGANYGYTF</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -1.01...</td>\n",
       "      <td>[[-0.228, 1.399, -4.76, 0.67, -2.647, 0.26, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>positive</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CALI*ML_NNRKLIW*******************************...</td>\n",
       "      <td>CASSLQDRARGYTF</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>positive</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CAENLTLL*_TGFQKLVF****************************...</td>\n",
       "      <td>CASSLSEGSRGYTF</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>positive</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CLVGDES_AGNKLTF*******************************...</td>\n",
       "      <td>CASSLYRGPPQHF</td>\n",
       "      <td>[[-0.591, -1.302, -0.733, 1.57, -0.146, -0.032...</td>\n",
       "      <td>[[-0.228, 1.399, -4.76, 0.67, -2.647, -0.384, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>negative</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CAGGCFG_GNQFYF********************************...</td>\n",
       "      <td>CASSLGGSWAQYF</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>negative</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CAENRLRKI_ALGMCCIA****************************...</td>\n",
       "      <td>CSVAGQGLTTEAFF</td>\n",
       "      <td>[[-0.591, -1.302, -0.733, 1.57, -0.146, -0.032...</td>\n",
       "      <td>[[-0.228, 1.399, -4.76, 0.67, -2.647, -0.384, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>negative</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CAENRLRKI_ALGMCCIA****************************...</td>\n",
       "      <td>CSVAGQGLTTEAFF</td>\n",
       "      <td>[[-0.032, 0.326, 2.213, 0.908, 1.313, -0.032, ...</td>\n",
       "      <td>[[-0.228, 1.399, -4.76, 0.67, -2.647, -0.384, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>negative</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CAENRLRKI_ALGMCCIA****************************...</td>\n",
       "      <td>CSVAGQGLTTEAFF</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>negative</td>\n",
       "      <td>HLA-A*11:01</td>\n",
       "      <td>CAENRLRKI_ALGMCCIA****************************...</td>\n",
       "      <td>CSVAGQGLTTEAFF</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...</td>\n",
       "      <td>[[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Class          HLA  \\\n",
       "10    positive  HLA-A*02:01   \n",
       "34    positive  HLA-A*02:01   \n",
       "53    positive  HLA-A*11:01   \n",
       "54    positive  HLA-A*11:01   \n",
       "59    positive  HLA-A*11:01   \n",
       "...        ...          ...   \n",
       "1499  negative  HLA-A*11:01   \n",
       "1624  negative  HLA-A*11:01   \n",
       "1625  negative  HLA-A*11:01   \n",
       "1626  negative  HLA-A*11:01   \n",
       "1627  negative  HLA-A*11:01   \n",
       "\n",
       "                                               AseqCDR3         BseqCDR3  \\\n",
       "10    CAASIKAL_NTGKLIF******************************...   CAWTKAGGGETQYF   \n",
       "34    CAASWD*CW_GTSYGKLTF***************************...  CASSITPGANYGYTF   \n",
       "53    CALI*ML_NNRKLIW*******************************...   CASSLQDRARGYTF   \n",
       "54    CAENLTLL*_TGFQKLVF****************************...   CASSLSEGSRGYTF   \n",
       "59    CLVGDES_AGNKLTF*******************************...    CASSLYRGPPQHF   \n",
       "...                                                 ...              ...   \n",
       "1499  CAGGCFG_GNQFYF********************************...    CASSLGGSWAQYF   \n",
       "1624  CAENRLRKI_ALGMCCIA****************************...   CSVAGQGLTTEAFF   \n",
       "1625  CAENRLRKI_ALGMCCIA****************************...   CSVAGQGLTTEAFF   \n",
       "1626  CAENRLRKI_ALGMCCIA****************************...   CSVAGQGLTTEAFF   \n",
       "1627  CAENRLRKI_ALGMCCIA****************************...   CSVAGQGLTTEAFF   \n",
       "\n",
       "                                             Neo_first3  \\\n",
       "10    [[-1.337, -0.279, -0.544, 1.242, -1.262, -1.01...   \n",
       "34    [[-1.337, -0.279, -0.544, 1.242, -1.262, -1.01...   \n",
       "53    [[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...   \n",
       "54    [[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...   \n",
       "59    [[-0.591, -1.302, -0.733, 1.57, -0.146, -0.032...   \n",
       "...                                                 ...   \n",
       "1499  [[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...   \n",
       "1624  [[-0.591, -1.302, -0.733, 1.57, -0.146, -0.032...   \n",
       "1625  [[-0.032, 0.326, 2.213, 0.908, 1.313, -0.032, ...   \n",
       "1626  [[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...   \n",
       "1627  [[-1.337, -0.279, -0.544, 1.242, -1.262, -1.33...   \n",
       "\n",
       "                                              Neo_last3  \n",
       "10    [[-0.228, 1.399, -4.76, 0.67, -2.647, 0.26, 0....  \n",
       "34    [[-0.228, 1.399, -4.76, 0.67, -2.647, 0.26, 0....  \n",
       "53    [[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...  \n",
       "54    [[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...  \n",
       "59    [[-0.228, 1.399, -4.76, 0.67, -2.647, -0.384, ...  \n",
       "...                                                 ...  \n",
       "1499  [[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...  \n",
       "1624  [[-0.228, 1.399, -4.76, 0.67, -2.647, -0.384, ...  \n",
       "1625  [[-0.228, 1.399, -4.76, 0.67, -2.647, -0.384, ...  \n",
       "1626  [[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...  \n",
       "1627  [[-1.337, -0.279, -0.544, 1.242, -1.262, -0.38...  \n",
       "\n",
       "[127 rows x 6 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"AseqCDR3\"].str.contains(\"_\"),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38012, 6)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
