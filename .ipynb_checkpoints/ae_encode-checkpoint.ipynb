{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seqCDR(seqCDR):\n",
    "    encoding_list = []\n",
    "    for i in range(len(seqCDR)):\n",
    "        if seqCDR[i] == \"*\":\n",
    "            encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "        elif seqCDR[i] == \"_\":\n",
    "            # print(\"Error: seqCDR contains '_'\")\n",
    "            # encoding_list.append(np.zeros(5).reshape(1,5))\n",
    "            return np.nan\n",
    "        else:\n",
    "            encoding_list.append(af.loc[seqCDR[i]].values.reshape(1,5))\n",
    "    return np.array(encoding_list).reshape(1,-1)\n",
    "\n",
    "af = pd.read_csv(\"~/data/project/pMHC-TCR/library/Atchley_factors.csv\")\n",
    "af.index = af[\"Amino acid\"]\n",
    "af.drop(columns=[\"Amino acid\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_encode_data(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        # drop the rows with duplicate CDR3 sequences\n",
    "        df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "        \n",
    "        # drop the rows with length == max length, which is much longer than the others\n",
    "        df = df.loc[df[\"AseqCDR3\"].str.len() < 50, :]\n",
    "\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        print(len_map)\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(\n",
    "                lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "        \n",
    "        # If there is any NaN value, drop the row\n",
    "        df = df.dropna()\n",
    "        print(df.shape)\n",
    "\n",
    "        # concatenate the encoded features\n",
    "        X_features = torch.zeros((len(df),0))\n",
    "        for seq in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            X_features = torch.cat((X_features, \n",
    "            torch.from_numpy(np.vstack(df[seq].values))), dim=1)\n",
    "\n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "        \n",
    "        # discard the duplicate rows, keep the first one\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_features[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCR_autoencoder(nn.Module):\n",
    "    '''\n",
    "    The autoencoder for TCR sequence.\n",
    "    For 230221 dataset, the sequnce length is 41 (20+21), and the input size is 41*5,\n",
    "    the hidden size is 10. And the output size is 41*5. We apply convolutional neural\n",
    "    network to encode the sequence, and apply deconvolutional neural network to decode\n",
    "    the sequence. The activation function for convolutional neural network is ReLU,\n",
    "    because it is a non-linear function, and it is easy to calculate the gradient.\n",
    "    For the decoder, we use the same activation function as the encoder.\n",
    "\n",
    "    Param:\n",
    "        input_size: the input size of the autoencoder\n",
    "        hidden_size: the hidden size of the autoencoder\n",
    "        output_size: the output size of the autoencoder, which is the same as the input size\n",
    "    '''\n",
    "    def __init__(self, kernel_size=3, stride=2, padding=1, batch_size=16):\n",
    "        super(TCR_autoencoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            # (batch_size, 5, 49)\n",
    "            nn.Conv1d(in_channels=5, out_channels=10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 25), based on the formula for conv1d: (W−F+2P)/S+1 = (49-3+2*1)/2+1 = 25\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 10, 23), based on the formula for maxpool1d: (W−F)/S+1 = (25-3)/1+1 = 23\n",
    "            nn.Conv1d(in_channels=10, out_channels=20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12), based on the formula for conv1d: (W−F+2P)/S+1 = (23-3+2*1)/2+1 = 12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=1),\n",
    "            # (batch_size, 20, 11), based on the formula for maxpool1d: (W−F)/S+1 = (12-2)/1+1 = 11\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(in_channels=20, out_channels=20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 6), (11-3+2)/2+1=6\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # (batch_size, 20, 6)\n",
    "            nn.ConvTranspose1d(20, 20, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 20, 12) based on the formula for convtranspose1d: (W−1)S−2P+F = (6-1)*2-2*1+3= 12\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose1d(20, 10, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 10, 23) based on the formula for convtranspose1d: (W−1)S−2P+F = (12-1)*2-2*1+3= 23\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxUnpool1d(kernel_size=3, stride=1),\n",
    "            # (batch_size, 20, 25) based on the formula for maxunpool1d: (W−1)S+F = (23-1)/1+3 = 25\n",
    "\n",
    "            nn.ConvTranspose1d(10, 5, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            # (batch_size, 5, 49) based on the formula for convtranspose1d: (W−1)S−2P+F = (25-1)*2-2*1+3= 49\n",
    "            # nn.Sigmoid()\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = torch.tensor(x, dtype=np.float32)\n",
    "        # x = torch.tensor(x, dtype=torch.float)\n",
    "        x = input.float()\n",
    "        encoded = self.encoder(x)\n",
    "        print(encoded.shape)\n",
    "        encoded = encoded.float()\n",
    "        output = self.decoder(encoded)\n",
    "        return encoded, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 25, 'BseqCDR3': 24}\n",
      "(2492, 6)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TCRData[0][0].shape\n",
    "len(TCRData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor(): argument 'dtype' must be torch.dtype, not type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m TCR_encode_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     TCR_encode_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     TCR_encode_losses\u001b[38;5;241m.\u001b[39mappend(TCR_encode_loss)\n\u001b[1;32m     49\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTCR encode loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(model, train_loader, optimizer, criterion, epoch, seq_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m5\u001b[39m, seq_length)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m _, output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(output.shape, data.shape)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, data)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mTCR_autoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 56\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     58\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoded)\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor(): argument 'dtype' must be torch.dtype, not type"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training the autoencoder to encode the TCR sequence\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        data = data.view(batch_size, 5, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(data)\n",
    "        # print(output.shape, data.shape)\n",
    "        loss = criterion(output, data)\n",
    "        # TCR_encode_losses.append(loss.item() / model.batch_size)\n",
    "        # TCR_encode_losses.append(loss.item())\n",
    "        # sum up batch loss\n",
    "        batch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    return batch_loss / len(train_loader.dataset)\n",
    "\n",
    "# parameters setting\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 2\n",
    "seq_length = int(TCRData[0][0].shape[0] / 5)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train the autoencoder\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# plot the loss\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "TCR_encode_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    TCR_encode_loss = train_autoencoder(model, train_loader, optimizer, criterion, epoch, seq_length)\n",
    "    TCR_encode_losses.append(TCR_encode_loss)\n",
    "ax.set_title(\"TCR encode loss\")\n",
    "ax.plot(TCR_encode_losses, label=\"TCR encode loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20, 7])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTCRData\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mTCR_autoencoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     61\u001b[0m encoded \u001b[38;5;241m=\u001b[39m encoded\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 62\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded, output\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'indices'"
     ]
    }
   ],
   "source": [
    "model(TCRData[0:3][0].float().view(3,5,seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1874379/3053764840.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 2\n",
    "\n",
    "# load the model\n",
    "model = TCR_autoencoder(kernel_size=kernel_size, stride=stride, padding=padding, batch_size=batch_size)\n",
    "# model.load_state_dict(torch.load(\"/DATA/User/wuxinchao/project/pMHC-TCR/ckpt/TCR_autoencoder.pt\"))\n",
    "# model.eval()\n",
    "\n",
    "# encode the TCR sequence\n",
    "file_path = \"~/data/project/data/seqData/20230228.csv\"\n",
    "TCRData = TCR_encode_data(file_path)\n",
    "# TCR_loader = DataLoader(TCRData, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "TCR_encode = torch.zeros((0, 20, 6))\n",
    "for i in range(len(TCRData)):\n",
    "    TCR_seq = TCRData[i][0]\n",
    "    TCR_seq = TCR_seq.view(1, 5, 47).double()\n",
    "    encoded, _ = model(TCR_seq)\n",
    "    TCR_encode = torch.cat((TCR_encode, encoded), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCRDataset(Dataset):\n",
    "    '''\n",
    "    The dataset class for pMHC-TCR dataset. \n",
    "    The csv file will be read and the TCR sequence will be encoded by the autoencoder or LSTM, as well as the sequence of neoantigen.\n",
    "    While the HLA type will be one-hot encoded. After that, the encoded sequnce and HLA type will be concatenated as the input of the\n",
    "    neural network.\n",
    "    '''\n",
    "    def __init__(self, file_path, only_CDR3=False, only_experimental=True, TCR_encode=None):\n",
    "        df, HLA_encode, y  = self.basic_io(file_path, only_CDR3, only_experimental=only_experimental)\n",
    "        \n",
    "        if only_CDR3 and TCR_encode is not None:\n",
    "            X_features = torch.cat((torch.from_numpy(HLA_encode), TCR_encode.view(1,-1)), dim=1)\n",
    "        elif only_CDR3:\n",
    "            X_features = torch.from_numpy(HLA_encode)\n",
    "            for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR3\", \"BseqCDR3\"]:\n",
    "                X_features = torch.cat((X_features, torch.from_numpy(df[seq].values)), dim=1)\n",
    "        else:\n",
    "            X_features = torch.from_numpy(HLA_encode)\n",
    "            for seq in [\"Neo_first3\", \"Neo_last3\", \"AseqCDR_1\", \"AseqCDR_2\", \"AseqCDR_3\", \"BseqCDR_1\", \"BseqCDR_2\", \"BseqCDR_3\"]:\n",
    "                X_features = torch.cat((X_features, torch.from_numpy(df[seq].values)), dim=1)\n",
    "\n",
    "        self.X_features = X_features\n",
    "        self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def basic_io(self, file_path, only_CDR3=True, only_experimental=True):\n",
    "        # return the dataframe, contain the \n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        # for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "        #     if only_CDR3:\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        #     else:\n",
    "        #         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "        #         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "        #         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "        #         df.drop(columns=[chain], inplace=True)\n",
    "        df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "        df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "        df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "        # encode the Neo_first3, Neo_last3\n",
    "        for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "            df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "        # encode the CDR3 region\n",
    "        len_map = {\n",
    "            \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "            \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "        }\n",
    "        for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "            length = len_map[chain]\n",
    "            df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "            df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "        if not only_experimental:\n",
    "            df_ps = df[df[\"Class\"] == \"positive\"]\n",
    "            df_ng_ex = df[df[\"Class\"] == \"negative\"]\n",
    "            df_ng_em = df.copy()\n",
    "            df_ng_em = df_ng_em[df_ng_em[\"Class\"] == \"positive\"]\n",
    "            df_ng_em[\"AseqCDR_3\"] = df_ng_em[\"AseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"AseqCDR_3\"]) - set(x))))\n",
    "            df_ng_em[\"BseqCDR_3\"] = df_ng_em[\"BseqCDR_3\"].apply(\n",
    "                lambda x: random.choice(list(set(df_ng_em[\"BseqCDR_3\"]) - set(x))))\n",
    "            df_ng = pd.concat([df_ng_em, df_ng_ex], axis=0)\n",
    "            df_ng.index = range(len(df_ng))\n",
    "            df = pd.concat([df_ps, df_ng], axis=0)\n",
    "\n",
    "        X_HLA = df[\"HLA\"].values.reshape(-1, 1)\n",
    "        HLAencoder = OneHotEncoder()\n",
    "        X_HLA_encoded = HLAencoder.fit_transform(X_HLA).toarray()\n",
    "        \n",
    "        y = df[\"Class\"].apply(lambda x: 1 if x == \"positive\" else 0).values\n",
    "\n",
    "        return df, X_HLA_encoded, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n",
      "Error: seqCDR contains '_'\n"
     ]
    }
   ],
   "source": [
    "# TCR_encode = TCR_encode.view(-1, 20*6)\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "# for chain in [\"AseqCDR\", \"BseqCDR\"]:\n",
    "#     if only_CDR3:\n",
    "#         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "#         df.drop(columns=[chain], inplace=True)\n",
    "#     else:\n",
    "#         df[chain+\"_1\"] = df[chain].str.split(\"_\").str[0]\n",
    "#         df[chain+\"_2\"] = df[chain].str.split(\"_\").str[1]\n",
    "#         df[chain+\"_3\"] = df[chain].str.split(\"_\").str[2]\n",
    "#         df.drop(columns=[chain], inplace=True)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "# encode the CDR3 region\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "for chain in [\"AseqCDR3\", \"BseqCDR3\"]:\n",
    "    length = len_map[chain]\n",
    "    df[chain] = df[chain].apply(lambda x: x + \"*\" * (length - len(x)))\n",
    "    df[chain] = df[chain].apply(lambda x: encode_seqCDR(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCRData = pMHC_TCRDataset(file_path, only_TCR_seq=True, only_experimental=True, TCR_encode=TCR_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size=16, \n",
    "                 batch_size=32, \n",
    "                 num_layers=2, \n",
    "                 device=\"cpu\", \n",
    "                 use_whole_data=False) -> None:\n",
    "        super(pMHC_TCR_model, self).__init__()\n",
    "        if use_whole_data:\n",
    "            self.batch_size = 0\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        # self.label = nn.Linear(hidden_size, 1)\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, int(hidden_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden_size/2), int(hidden_size/4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden_size/4), int(hidden_size/8)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(hidden_size/8), 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.batch_size == 0:\n",
    "            self.batch_size = input.shape[0]\n",
    "            x = input.float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "            # pred = self.label(output[-1])\n",
    "            pred = self.linear_layer(output[-1])\n",
    "        else:\n",
    "            x = input.view(-1, self.batch_size, self.input_size).float()\n",
    "            h_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers * 1, self.batch_size, self.hidden_size).to(self.device))\n",
    "            output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "            # pred = self.label(output[-1])\n",
    "            pred = self.linear_layer(output[-1])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output.shape, target.shape)\n",
    "        output = output.to(torch.float32)\n",
    "        target = target.to(torch.float32).view(-1, 1)\n",
    "        # print(output.shape, target.shape)\n",
    "        loss = nn.CrossEntropyLoss()(output.view(1,-1), target.view(1,-1))\n",
    "        train_loss += loss.item() / len(train_loader.dataset)  # sum up batch loss\n",
    "        pred = output.sigmoid().round()\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Fold/Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                fold, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(train_loader.dataset)))\n",
    "    # return the average loss\n",
    "    # print(f\"The batch size is {model.batch_size}\")\n",
    "    return train_loss, correct / len(train_loader.dataset)\n",
    "\n",
    "def test(fold, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).to(torch.float32)\n",
    "            target = target.to(torch.float32).view(-1, 1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output.reshape(1,-1), target.reshape(1,-1)).item()  # sum up loss\n",
    "            # print(test_loss)\n",
    "            pred = output.sigmoid().round()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set for fold{fold}: Average Loss: \\\n",
    "          {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \\\n",
    "          ({100. * correct / len(test_loader.dataset):.0f}%)\")\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\")\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for training\n",
      "-------------------Fold 0-------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb 单元格 10\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m test_accuracy_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     train_losses, train_correct \u001b[39m=\u001b[39m train(fold, model, device, train_loader, optimizer, epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     test_losses, test_correct \u001b[39m=\u001b[39m test(fold, model, device, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     train_losses_history\u001b[39m.\u001b[39mappend(train_losses)\n",
      "\u001b[1;32m/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb 单元格 10\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fold, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msigmoid()\u001b[39m.\u001b[39mround()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39meq(target\u001b[39m.\u001b[39mview_as(pred))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.17.13.13/home/wuxinchao/data/project/pMHC-TCR/ae_encode.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training \n",
    "batch_size = 32\n",
    "seq_length = 6\n",
    "folds = 5\n",
    "repeats = 12\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} for training\")\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model = pMHC_TCR_model(input_size=122, hidden_size=16, batch_size=batch_size, num_layers=2, device=device, use_whole_data=False).to(device)\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "weights = torch.FloatTensor([5,6])\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].set_title(\"loss\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(TCRData.X, TCRData.y)):\n",
    "    print(f\"-------------------Fold {fold}-------------------\")\n",
    "    if batch_size == 1:\n",
    "    # using the subsampler to get the data\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_dataset = torch.utils.data.Subset(TCRData, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(TCRData, test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(train_dataset), sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, batch_size=len(test_dataset), sampler=test_subsampler)\n",
    "    else:\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=train_subsampler, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(TCRData, \n",
    "            batch_size=batch_size, sampler=test_subsampler, drop_last=True)\n",
    "    \n",
    "    # print(f\"The length of train_loader is {len(train_loader)}\") # 34\n",
    "    # print(f\"The length of test_loader is {len(test_loader)}\") # 8\n",
    "    # print(f\"The length of train_dataset is {len(train_loader.dataset)}\")\n",
    "        \n",
    "    model.apply(reset_weights)\n",
    "    train_losses_history = []\n",
    "    test_losses_history = []\n",
    "    train_accuracy_history = []\n",
    "    test_accuracy_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_losses, train_correct = train(fold, model, device, train_loader, optimizer, epoch)\n",
    "        test_losses, test_correct = test(fold, model, device, test_loader)\n",
    "        train_losses_history.append(train_losses)\n",
    "        test_losses_history.append(test_losses)\n",
    "        train_accuracy_history.append(train_correct)\n",
    "        test_accuracy_history.append(test_correct)\n",
    "    ax[0].plot(train_losses_history, \"r*--\" ,label=f\"train loss fold{fold}\")\n",
    "    ax[0].plot(test_losses_history, \"bs--\", label=f\"test loss fold{fold}\")\n",
    "    ax[1].plot(train_accuracy_history, \"g^--\", label=f\"train accuracy fold{fold}\")\n",
    "    ax[1].plot(test_accuracy_history, \"yo--\", label=f\"test accuracy fold{fold}\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# put the legend out of the figure, and adjust the position, prevent the figure from being covered\n",
    "# ax[0].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "# ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# save the figure\n",
    "fig.savefig(\"/DATA/User/wuxinchao/project/pMHC-TCR/result/pMHC_without_em_with_encoder_loss_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding, the features are concatanated and used to predict the binding affinity of pMHC-TCR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pMHC_TCR_pred(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        super(pMHC_TCR_pred, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        # use the encoded features to predict the binding affinity through MLP\n",
    "        self.Linear_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_encode(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size, batch_size, num_layers, device, use_whole_data=False):\n",
    "        self.input_size = input_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.use_whole_data = use_whole_data\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.seq_length, self.input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        return out[:, -1, :] # return the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AseqCDR3': 82, 'BseqCDR3': 24}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df[\"Neo_first3\"] = df[\"NeoAA\"].str[:3]\n",
    "df[\"Neo_last3\"] = df[\"NeoAA\"].str[-3:]\n",
    "df = df.drop(columns=[\"NeoAA\"])\n",
    "\n",
    "# encode the Neo_first3, Neo_last3\n",
    "for seq in [\"Neo_first3\", \"Neo_last3\"]:\n",
    "    df[seq] = df[seq].apply(lambda x: encode_seqCDR(x))\n",
    "\n",
    "# encode the CDR3 region\n",
    "df = df.drop_duplicates(subset=[\"AseqCDR3\", \"BseqCDR3\"], keep=\"first\")\n",
    "\n",
    "len_map = {\n",
    "    \"AseqCDR3\": df[\"AseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "    \"BseqCDR3\": df[\"BseqCDR3\"].apply(lambda x: len(x)).max(),\n",
    "}\n",
    "print(len_map)\n",
    "# drop the rows with length == max length, which is much longer than the others\n",
    "df = df.loc[df[\"AseqCDR3\"].str.len() < len_map[\"AseqCDR3\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.200e+02, 2.288e+03, 2.800e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.100e+01]),\n",
       " array([ 5. , 12.6, 20.2, 27.8, 35.4, 43. , 50.6, 58.2, 65.8, 73.4, 81. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df[\"AseqCDR3\"].value_counts()\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0) # find the longest seq\n",
    "# df.loc[df[\"AseqCDR3\"].str.len() == 83, \"AseqCDR3\"]\n",
    "\n",
    "plt.hist(df[\"AseqCDR3\"].str.len().sort_values(axis=0))\n",
    "# plt.show()\n",
    "# df = df.loc[df[\"AseqCDR3\"].str.len() < 83, :]\n",
    "# df[\"AseqCDR3\"].str.len().sort_values(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_map\n",
    "df.to_csv(\"/home/wuxinchao/data/project/data/seqData/20230228.csv\")\n",
    "# df.loc[df[\"AseqCDR3\"].str.contains(\"_\"),]\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
